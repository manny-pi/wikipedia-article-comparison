{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib \n",
    "import networkx as nx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_id(k): \n",
    "    \"\"\"Returns a 10-byte hash ID for a given string.\"\"\"\n",
    "\n",
    "    bytes_ = k.encode(\"UTF-8\")                  \n",
    "    hash_id = hashlib.sha1(bytes_).hexdigest() \n",
    "    hash_id = hash_id[:10]  \n",
    "    return hash_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(query):\n",
    "    \"\"\"Returns a DataFrame with articles on the specified query, q.\"\"\"\n",
    "\n",
    "    # track titles that failed to load content\n",
    "    count = 0\n",
    "\n",
    "    # create DataFrame to store title, and content\n",
    "    df = pd.DataFrame(columns=[\"title\", \"content\"])\n",
    "\n",
    "    # get the titles from Wikipedia\n",
    "    titles = wiki.search(query, suggestion=True, results=100)[0]    \n",
    "\n",
    "    # iterate through titles, store the content for each one in the DataFrame \n",
    "    for title in titles: \n",
    "        hash_id = get_hash_id(title)\n",
    "\n",
    "        try: \n",
    "            content = wiki.page(title, auto_suggest=False).content\n",
    "        except Exception: \n",
    "            content = ''\n",
    "        df.at[hash_id, \"title\"] = title\n",
    "        df.at[hash_id, \"content\"] = content\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "# Get articles for each of the following topics: \n",
    "# * Computer Science, \n",
    "# * Neuroscience, and \n",
    "# * Mathematics.\n",
    "\n",
    "comp_sci = get_articles(\"Computer Science\")\n",
    "neuro_sci = get_articles(\"Neuroscience\")\n",
    "maths = get_articles(\"Mathematics\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save/read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_sci.to_csv(\"wiki_comp_sci.csv\", index_label=\"hash_id\")\n",
    "# neuro_sci.to_csv(\"wiki_neurosci.csv\", index_label=\"hash_id\")\n",
    "# maths.to_csv(\"wiki_maths.csv\", index_label=\"hash_id\")\n",
    "\n",
    "comp_sci = pd.read_csv(\"wiki_comp_sci.csv\", index_col=\"hash_id\").fillna('')\n",
    "neuro_sci = pd.read_csv(\"wiki_neurosci.csv\", index_col=\"hash_id\").fillna('')\n",
    "maths = pd.read_csv(\"wiki_maths.csv\", index_col=\"hash_id\").fillna('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(corpus, column=None): \n",
    "    \"\"\"\n",
    "    corpus: a pandas DataFrame that contains the documents.\n",
    "    column: the column to be used for pairwise comparison. \n",
    "    \n",
    "    Returns, a DataFrame with pairwise comparisons (cosine similarity) for each document. \n",
    "    \"\"\"\n",
    "    \n",
    "    docs = corpus[column].to_numpy()                            # store the relevant documents\n",
    "    tfidf = TfidfVectorizer().fit_transform(docs)               # vectorize the documents\n",
    "    pairwise_similarity = tfidf * tfidf.T                       # compute the pairwise cosine similarity \n",
    "    pairwise_similarity = pairwise_similarity.toarray()         # convert to numpy 2D array\n",
    "    df = pd.DataFrame(\n",
    "        pairwise_similarity,\n",
    "        index=corpus.index, \n",
    "        columns=corpus.index\n",
    "    )\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sci_sim_matrix = similarity_matrix(comp_sci, \"content\")\n",
    "neuro_sci_sim_matrix = similarity_matrix(neuro_sci, \"content\")\n",
    "maths_sim_matrix = similarity_matrix(maths, \"content\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/c53crljd2y523fsqplb9bml00000gp/T/ipykernel_90900/908350272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpages_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpages_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpages_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpages_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hash_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pages_df' is not defined"
     ]
    }
   ],
   "source": [
    "pages_df.index = pages_df.hash_id\n",
    "pages_df = pages_df.drop(labels=['hash_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a nx.Graph object \n",
    "pages_graph = nx.Graph()\n",
    "page_ids = cosine_similarity.index.to_list() # get list of article IDs\n",
    "\n",
    "# iterate over the page IDs\n",
    "for left_node in page_ids: \n",
    "    # add the article and its title to the graph\n",
    "    pages_graph.add_node(\n",
    "        left_node, \n",
    "        title=pages_df.loc[left_node][\"title\"], \n",
    "        average_similarity=cosine_similarity.loc[left_node][\"average_similarity\"]\n",
    "    )\n",
    "\n",
    "    # iterate over the other page IDs\n",
    "    for right_node in page_ids:\n",
    "        # avoid self-loops (when a node has an edge to itself)\n",
    "        if left_node != right_node: \n",
    "            # add the node and its edges to the graph (cosine similarity score)\n",
    "            pages_graph.add_edge(left_node, right_node, cosine_similarity=cosine_similarity.loc[left_node, right_node])\n",
    "\n",
    "# Sample node:edge \n",
    "print(f\"f511669021 -> 972d8cef69 - Cosine Similarity: {pages_graph.get_edge_data('f511669021', '972d8cef69')}\")\n",
    "for n in pages_graph.nodes.data(): print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(pages_graph, \"wikipedia-graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity.average_similarity.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
