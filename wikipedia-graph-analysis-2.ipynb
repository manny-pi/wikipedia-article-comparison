{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib \n",
    "import networkx as nx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_id(k): \n",
    "    \"\"\"Returns a 10-byte hash ID for a given string.\"\"\"\n",
    "\n",
    "    bytes_ = k.encode(\"UTF-8\")                  \n",
    "    hash_id = hashlib.sha1(bytes_).hexdigest() \n",
    "    hash_id = hash_id[:10]  \n",
    "    return hash_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(query):\n",
    "    \"\"\"Returns a DataFrame with articles on the specified query, q.\"\"\"\n",
    "\n",
    "    # track titles that failed to load content\n",
    "    count = 0\n",
    "\n",
    "    # create DataFrame to store title, and content\n",
    "    df = pd.DataFrame(columns=[\"title\", \"content\"])\n",
    "\n",
    "    # get the titles from Wikipedia\n",
    "    titles = wiki.search(query, suggestion=True, results=100)[0]    \n",
    "\n",
    "    # iterate through titles, store the content for each one in the DataFrame \n",
    "    for title in titles: \n",
    "        hash_id = get_hash_id(title)\n",
    "\n",
    "        try: \n",
    "            content = wiki.page(title, auto_suggest=False).content\n",
    "        except Exception: \n",
    "            content = ''\n",
    "        df.at[hash_id, \"title\"] = title\n",
    "        df.at[hash_id, \"content\"] = content\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/Omani/Library/Python/3.8/lib/python/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "# Get articles for each of the following topics: \n",
    "# * Computer Science, \n",
    "# * Neuroscience, and \n",
    "# * Mathematics.\n",
    "\n",
    "comp_sci = get_articles(\"Computer Science\")\n",
    "neuro_sci = get_articles(\"Neuroscience\")\n",
    "maths = get_articles(\"Mathematics\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save/read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_sci.to_csv(\"wiki_comp_sci.csv\", index_label=\"hash_id\")\n",
    "# neuro_sci.to_csv(\"wiki_neurosci.csv\", index_label=\"hash_id\")\n",
    "# maths.to_csv(\"wiki_maths.csv\", index_label=\"hash_id\")\n",
    "\n",
    "comp_sci = pd.read_csv(\"wiki_comp_sci.csv\", index_col=\"hash_id\").fillna('')\n",
    "neuro_sci = pd.read_csv(\"wiki_neurosci.csv\", index_col=\"hash_id\").fillna('')\n",
    "maths = pd.read_csv(\"wiki_maths.csv\", index_col=\"hash_id\").fillna('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(corpus, column=None): \n",
    "    \"\"\"\n",
    "    corpus: a pandas DataFrame that contains the documents.\n",
    "    column: the column to be used for pairwise comparison. \n",
    "    \n",
    "    Returns, a DataFrame with pairwise comparisons (cosine similarity) for each document. \n",
    "    \"\"\"\n",
    "    \n",
    "    docs = corpus[column].to_numpy()                            # store the relevant documents\n",
    "    tfidf = TfidfVectorizer().fit_transform(docs)               # vectorize the documents\n",
    "    pairwise_similarity = tfidf * tfidf.T                       # compute the pairwise cosine similarity \n",
    "    pairwise_similarity = pairwise_similarity.toarray()         # convert to numpy 2D array\n",
    "    df = pd.DataFrame(\n",
    "        pairwise_similarity,\n",
    "        index=corpus.index, \n",
    "        columns=corpus.index\n",
    "    )\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sci_sim_matrix = similarity_matrix(comp_sci, \"content\")\n",
    "neuro_sci_sim_matrix = similarity_matrix(neuro_sci, \"content\")\n",
    "maths_sim_matrix = similarity_matrix(maths, \"content\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, sim_matrix): \n",
    "    \"\"\"\n",
    "    df: a pandas DataFrame containing the metadata and data of the documents.\n",
    "    sim_matrix: a pandas DataFrame containing pairwise comparisons for each document in 'df'.\n",
    "    \"\"\"\n",
    "    ids = df.index\n",
    "    g = nx.Graph()\n",
    "    for left_node in ids: \n",
    "        # get this documents average similarity to others \n",
    "        avg_sim = sum(sim_matrix.loc[left_node])/len(sim_matrix.loc[left_node])     \n",
    "\n",
    "        # add the node to the graph\n",
    "        g.add_node(left_node, title=df.loc[left_node][\"title\"], avg_sim=avg_sim)\n",
    "\n",
    "        # add edges to the graph\n",
    "        for right_node in ids: \n",
    "            if left_node != right_node: \n",
    "                sim = sim_matrix.loc[left_node, right_node]\n",
    "                g.add_edge(left_node, right_node, similarity=sim)    \n",
    "\n",
    "    return g    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(create_graph(comp_sci, comp_sci_sim_matrix), \"comp_sci.graphml\")\n",
    "nx.write_graphml(create_graph(neuro_sci, neuro_sci_sim_matrix), \"neuro_sci.graphml\")\n",
    "nx.write_graphml(create_graph(maths, maths_sim_matrix), \"maths.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      Dynamics\n",
       "content            \n",
       "Name: 7d5536610a, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_sci.loc['7d5536610a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.1104910179196"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(comp_sci_sim_matrix.loc['0957e6b9c8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.436731484059344"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(comp_sci_sim_matrix.loc['d16723e94c'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
