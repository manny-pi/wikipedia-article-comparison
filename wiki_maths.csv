,title,content
77e774bacb,Mathematics,"Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.
Most mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or—in modern mathematics—entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.
Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.


== Etymology ==
The word mathematics comes from Ancient Greek máthēma (μάθημα), meaning ""that which is learnt"", ""what one gets to know"", hence also ""study"" and ""science"". The word came to have the narrower and more technical meaning of ""mathematical study"" even in Classical times. Its adjective is mathēmatikós (μαθηματικός), meaning ""related to learning"" or ""studious"", which likewise further came to mean ""mathematical"". In particular, mathēmatikḗ tékhnē (μαθηματικὴ τέχνη; Latin: ars mathematica) meant ""the mathematical art"".Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant ""learners"" rather than ""mathematicians"" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established.In Latin, and in English until around 1700, the term mathematics more commonly meant ""astrology"" (or sometimes ""astronomy"") rather than ""mathematics""; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning ""astrologers"", is sometimes mistranslated as a condemnation of mathematicians.The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly ""all things mathematical"", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek. In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math.


== Areas of mathematics ==

Before the Renaissance, mathematics was divided into two main areas: arithmetic—regarding the manipulation of numbers, and geometry, regarding the study of shapes. Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas–arithmetic, geometry, algebra, calculus–endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics. The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the seventeenth century.At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics. The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas. Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have ""geometry"" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.


=== Number theory ===

Number theory began with the manipulation of numbers, that is, natural numbers 
  
    
      
        (
        
          N
        
        )
        ,
      
    
    {\displaystyle (\mathbb {N} ),}
   and later expanded to integers 
  
    
      
        (
        
          Z
        
        )
      
    
    {\displaystyle (\mathbb {Z} )}
   and rational numbers 
  
    
      
        (
        
          Q
        
        )
        .
      
    
    {\displaystyle (\mathbb {Q} ).}
   Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations. Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria. The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra. Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), diophantine equations, and transcendence theory (problem oriented).


=== Geometry ===

Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements.The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space.Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically.Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions.In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem. In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space.Today's subareas of geometry include:
Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines.
Affine geometry, the study of properties relative to parallelism and independent from the concept of length.
Differential geometry, the study of curves, surfaces, and their generalizations, which are defined using differentiable functions.
Manifold theory, the study of shapes that are not necessarily embedded in a larger space.
Riemannian geometry, the study of distance properties in curved spaces.
Algebraic geometry, the study of curves, surfaces, and their generalizations, which are defined using polynomials.
Topology, the study of properties that are kept under continuous deformations.
Algebraic topology, the use in topology of algebraic methods, mainly homological algebra.
Discrete geometry, the study of finite configurations in geometry.
Convex geometry, the study of convex sets, which takes its importance from its applications in optimization.
Complex geometry, the geometry obtained by replacing real numbers with complex numbers.


=== Algebra ===

Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra. Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution. Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side. The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise.
Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers. Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas.
Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid. The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether. (The latter term appears mainly in an educational context, in opposition to elementary algebra, which is concerned with the older way of manipulating formulas.)
Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include:
group theory;
field theory;
vector spaces, whose study is essentially the same as linear algebra;
ring theory;
commutative algebra, which is the study of commutative rings, includes the study of polynomials, and is a foundational part of algebraic geometry;
homological algebra;
Lie algebra and Lie group theory;
Boolean algebra, which is widely used for the study of the logical structure of computers.The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory. The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.


=== Calculus and analysis ===

Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz. It is fundamentally the study of the relationship of variables that depend on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results. Presently, ""calculus"" refers mainly to the elementary part of this theory, and ""analysis"" is commonly used for advanced parts.
Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include:
Multivariable calculus
Functional analysis, where variables represent varying functions;
Integration, measure theory and potential theory, all strongly related with probability theory on a continuum;
Ordinary differential equations;
Partial differential equations;
Numerical analysis, mainly devoted to the computation on computers of solutions of ordinary and partial differential equations that arise in many applications.


=== Discrete mathematics ===

Discrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers. Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply. Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics.The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century. The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems.Discrete mathematics includes:
Combinatorics, the art of enumerating mathematical objects that satisfy some given constraints. Originally, these objects were elements or subsets of a given set; this has been extended to various objects, which establishes a strong link between combinatorics and other parts of discrete mathematics. For example, discrete geometry includes counting configurations of geometric shapes
Graph theory and hypergraphs
Coding theory, including error correcting codes and a part of cryptography
Matroid theory
Discrete geometry
Discrete probability distributions
Game theory (although continuous games are also studied, most common games, such as chess and poker are discrete)
Discrete optimization, including combinatorial optimization, integer programming, constraint programming


=== Mathematical logic and set theory ===

The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century. Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory.In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour. Examples of such intuitive definitions are ""a set is a collection of objects"", ""natural number is what is used for counting"", ""a point is a shape with a zero length in every direction"", ""a curve is a trace left by a moving point"", etc.
This became the foundational crisis of mathematics. It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have. For example, in Peano arithmetic, the natural numbers are defined by ""zero is a number"", ""each number has a unique successor"", ""each number but zero has a unique predecessor"", and some rules of reasoning. This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910.The ""nature"" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called ""intuition""—to guide their study and proofs. The approach allows considering ""logics"" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system. This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic, which explicitly lacks the law of excluded middle.These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory. Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, program certification, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.


=== Statistics and other decision sciences ===

The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods especially probability theory. Statisticians generate data with random sampling or randomized experiments. The design of a statistical sample or experiment determines the analytical methods that will be used. Analysis of data from observational studies is done using statistical models and the theory of inference, using model selection and estimation. The models and consequential predictions should then be tested against new data.Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics.


=== Computational mathematics ===

Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis broadly includes the study of approximation and discretization with special focus on rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.


== History ==


=== Ancient ===
The history of mathematics is an ever-growing series of abstractions. Evolutionarily speaking, the first abstraction to ever be discovered, one shared by many animals, was probably that of numbers: the realization that, for example, a collection of two apples and a collection of two oranges (say) have something in common, namely that there are two of them. As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years.

Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time.In the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right. Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof. His book, Elements, is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea, 2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD).

The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.


=== Medieval and later ===

During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe.During the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1642–1726/27) and Gottfried Leibniz (1646–1716) in the 17th century. Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved.Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, ""The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.""


== Symbolic notation and terminology ==

Mathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas. More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs, such as + (plus), × (multiplication), 
  
    
      
        ∫
      
    
    {\textstyle \int }
   (integral), = (equal), and < (less than). All these symbols are generally grouped according to specific rules to form expressions and formulas. Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses.
Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary.Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism. Other technical terms are words of the common language that are used in an accurate meaning that may differs slightly from their common meaning. For example, in mathematics, ""or"" means ""one, the other or both"", while, in common language, it is either ambiguous or means ""one or the other but not both"" (in mathematics, the latter is called ""exclusive or""). Finally, many mathematical terms are common words that are used with a completely different meaning. This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, ""every free module is flat"" and ""a field is always a ring"".


== Relationship with sciences ==
Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws. The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model. Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used. For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.There is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation. In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied ""durch planmässiges Tattonieren"" (through systematic experimentation). However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence.


=== Pure and applied mathematics ===

Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics. For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians. However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece.In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics. This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred.The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere. Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the ""pure theory"".An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis. An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high. For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry.In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas. The Mathematics Subject Classification has a section for ""general applied mathematics"" but does not mention ""pure mathematics"". However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge.


=== Unreasonable effectiveness ===
The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner. It is the fact that many mathematical theories, even the ""purest"" have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced. Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.
A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem. A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It is almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.In the 19th century, the internal development of geometry (pure mathematics) lead to define and study non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of the special relativity is a non-Euclidean space of dimension four, and spacetime of the general relativity is a (curved) manifold of dimension four.A striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon 
  
    
      
        
          Ω
          
            −
          
        
        .
      
    
    {\displaystyle \Omega ^{-}.}
   In both cases, the equations of the theories had unexplained solutions, which led to conjecture the existence of an unknown particle, and to search these particles. In both cases, these particles were discovered a few years later by specific experiments.


=== Specific sciences ===


==== Physics ====

Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly, and is also the motivation of major mathematical developments. See above for examples of this strong interaction.


==== Computing ====

The rise of technology in the 20th century opened the way to a new science: computing. This field is closely related to mathematics in several ways. Theoretical computer science is essentially mathematical in nature. Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, graph theory, and so on.In return, computing has also become essential for obtaining new results. This is a group of techniques known as experimental mathematics, which is the use of experimentation to discover mathematical insights. The most well-known example is the four-color theorem, which was proven in 1976 with the help of a computer. This revolutionized traditional mathematics, where the rule was that the mathematician should verify each part of the proof. In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer. An international team had since worked on writing a formal proof; it was finished (and verified) in 2015.Once written formally, a proof can be verified using a program called a proof assistant. These programs are useful in situations where one is uncertain about a proof's correctness.A major open problem in theoretical computer science is P versus NP. It is one of the seven Millennium Prize Problems.


==== Biology and chemistry ====

Biology uses probability extensively - for example, in ecology or neurobiology. Most of the discussion of probability in biology, however, centers on the concept of evolutionary fitness.Ecology heavily uses modeling to simulate population dynamics, study ecosystems such as the predator-prey model, measure pollution diffusion, or to assess climate change. The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations. However, there is the problem of model validation. This is particularly acute when the results of modeling influence political decisions; the existence of contradictory models could allow nations to choose the most favorable model.Genotype evolution can be modeled with the Hardy-Weinberg principle.Phylogeography uses probabilistic models.Medicine uses statistical hypothesis testing, run on data from clinical trials, to determine whether a new treatment works.Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions. It turns out that the form of macromolecules in biology is variable and determines the action. Such modeling uses Euclidean geometry; neighboring atoms form a polyhedron whose distances and angles are fixed by the laws of interaction.


==== Earth sciences ====

Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes. Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models.


==== Social sciences ====

Areas of mathematics used in the social sciences include probability/statistics and differential equations (stochastic or deterministic). These areas used in fields such as sociology, psychology, economics, finance, and linguistics.

The fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man'). In this model, each individual aims solely to accumulate as much profit as possible, and always makes optimal choices using perfect information. This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms which would be very difficult to discover by a ""literary"" analysis. For example, explanations of economic cycles are not trivial. Without mathematical modeling, it is hard to go beyond simple statistical observations or unproven speculation.However, many people have rejected or criticized the concept of Homo economicus. Economists note that real people usually have limited information and often make poor choices. Also, as shown in laboratory experiments, people care about fairness and sometimes altruism, not just personal gain. According to critics, mathematization is a veneer that allows for the material's scientific valorization.At the start of the 20th century, there was a movement to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis. Towards the end of the 19th century, Nicolas-Remi Brück and Charles Henri Lagrange had extended their analysis into geopolitics. They wanted to establish the historical existence of vast movements that took peoples to their apogee, then to their decline. More recently, Peter Turchin has been working on developing cliodynamics since the 1990s. (In particular, he discovered the Turchin cycle, which predicts that violence spikes in a short cycle of ~50-year intervals, superimposed over a longer cycle of ~200–300 years.)
Even so, mathematization of the social sciences is not without danger. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences. The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses elementary mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models can be subject to controversy.


== Relationship with astrology and esotericism ==
Mathematics has had a close relationship with astrology for a long time. Biased by astral themes, it had motivated the study of astronomy. Renowned mathematicians have also been considered to be renowned astrologists; for example, Ptolemy, Arab astronomers, Regiomantus, Cardano, Kepler, or John Dee. In the Middle Ages, astrology was considered a science that included mathematics. In his encyclopedia, Theodor Zwinger wrote that astrology was a mathematical science that studied the ""active movement of bodies as they act on other bodies"". He reserved to mathematics the need to ""calculate with probability the influences [of stars]"" to foresee their ""conjunctions and oppositions"".These disciplines are no longer considered sciences.


== Philosophy ==


=== Reality ===
The connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions  that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects.Armand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views.
 Something becomes objective (as opposed to ""subjective"") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together. Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ...
Nevertheless, Platonism and the concurrent views on abstraction do not explain the unreasonable effectiveness of mathematics.


=== Proposed definitions ===

There is no general consensus about a definition of mathematics or its epistemological status—that is, its place among other human activities. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, ""mathematics is what mathematicians do"". This makes sense, as there is a strong consensus among them about what is mathematics and what is not. Most proposed definitions try to define mathematics by its object of study.Aristotle defined mathematics as ""the science of quantity"" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property ""separable in thought"" from real instances set mathematics apart. In the 19th century, when mathematicians began to address topics—such as infinite sets—which have no clear-cut relation to physical reality, a variety of new definitions were given. With the large number of new areas of mathematics that appeared since the beginning of the 20th century and continue to appear, defining mathematics by this object of study becomes an impossible task.
Another approach for defining mathematics is to use its methods. So, an area of study can be qualified as mathematics as soon as one can prove theorems—assertions whose validity relies on a proof, that is, a purely-logical deduction. Others take the perspective that mathematics is an investigation of axiomatic set theory, as this study is now a foundational discipline for much of modern mathematics.


=== Rigor ===

Mathematical reasoning requires rigor. This means that the definitions must be absolutely unambiguous and the proofs must be reducible to a succession of applications of inference rules, without any use of empirical evidence and intuition. Rigorous reasoning is not specific to mathematics, but, in mathematics, the standard of rigor is much higher than elsewhere. Despite mathematics' concision, rigorous proofs can require hundreds of pages to express. The emergence of computer-assisted proofs has allowed proof lengths to further expand, such as the 255-page Feit–Thompson theorem. The result of this trend is a philosophy of the quasi-empiricist proof that can not be considered infallible, but has a probability attached to it.The concept of rigor in mathematics dates back to ancient Greece, where their society encouraged logical, deductive reasoning. However, this rigorous approach would tend to discourage exploration of new approaches, such as irrational numbers and concepts of infinity. The method of demonstrating rigorous proof was enhanced in the sixteenth century through the use of symbolic notation. In the 18th century, social transition led to mathematicians earning their keep through teaching, which led to more careful thinking about the underlying concepts of mathematics. This produced more rigorous approaches, while transitioning from geometric methods to algebraic and then arithmetic proofs.At the end of the 19th century, it appeared that the definitions of the basic concepts of mathematics were not accurate enough for avoiding paradoxes (non-Euclidean geometries and Weierstrass function) and contradictions (Russell's paradox). This was solved by the inclusion of axioms with the apodictic inference rules of mathematical theories; the re-introduction of axiomatic method pioneered by the ancient Greeks. It results that ""rigor"" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a ""rigorous proof"" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof, wherein it may be demonstrably refuted by other mathematicians. After a proof has been accepted for many years or even decades, it can then be considered as reliable.Nevertheless, the concept of ""rigor"" may remain useful for teaching to beginners what is a mathematical proof.


== Training and practice ==


=== Education ===

Mathematics has a remarkable ability to cross cultural boundaries and time periods. As a human activity, the practice of mathematics has a social side, which includes education, careers, recognition, popularization, and so on. In education, mathematics is a core part of the curriculum and forms an important element of the STEM academic disciplines. Prominent careers for professional mathematicians include math teacher or professor, statistician, actuary, financial analyst, economist, accountant, commodity trader, or computer consultant.Archaeological evidence shows that instruction in mathematics occurred as early as the second millennium BCE in ancient Babylonia. Comparable evidence has been unearthed for scribal mathematics training in the ancient Near East and then for the Greco-Roman world starting around 300 BCE. The oldest known mathematics textbook is the Rhind papyrus, dated from circa 1650 BCE in Eygpt. Due to a scarcity of books, mathematical teachings in ancient India were communicated using memorized oral tradition since the Vedic period (c. 1500 – c. 500 BCE). In Imperial China during the Tang dynasty (618–907 CE), a mathematics curriculum was adopted for the civil service exam to join the state bureaucracy.Following the Dark Ages, mathematics education in Europe was provided by religious schools as part of the Quadrivium. Formal instruction in pedagogy began with Jesuit schools in the 16th and 17th century. Most mathematical curriculum remained at a basic and practical level until the nineteenth century, when it began to flourish in France and Germany. The oldest journal addressing instruction in mathematics was L'Enseignement Mathématique, which began publication in 1899. The Western advancements in science and technology led to the establishment of centralized education systems in many nation-states, with mathematics as a core component—initially for its military applications.  While the content of courses varies, in the present day nearly all countries teach mathematics to students for significant amounts of time.During school, mathematical capabilities and positive expectations have a strong association with career interest in the field. Extrinsic factors such as feedback motivation by teachers, parents, and peer groups can influence the level of interest in mathematics. Some students studying math may develop an apprehension or fear about their performance in the subject. This is known as math anxiety or math phobia, and is considered the most prominent of the disorders impacting academic performance. Math anxiety can develop due to various  factors such as parental and teacher attitudes, social stereotypes, and personal traits. Help to counteract the anxiety can come from changes in instructional approaches, by interactions with parents and teachers, and by tailored treatments for the individual.


=== Psychology (aesthetic, creativity and intuition) ===
The validity of a mathematical theorem relies only on the rigor of its proof, which could theoretically be done automatically by a computer program. This does not mean that there is no place for creativity in a mathematical work. On the contrary, many important mathematical results (theorems) are solutions of problems that other mathematicians failed to solve, and the invention of a way for solving them may be a fundamental way of the solving process. An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians.Creativity and rigor are not the only psychological aspects of the activity of mathematicians. Some mathematicians can see their activity as a game, more specifically as solving puzzles. This aspect of mathematical activity is emphasized in recreational mathematics.
Mathematicians can find an aesthetic value to mathematics. Like beauty, it is hard to define, it is commonly related to elegance, which involves qualities like simplicity, symmetry, completeness, and generality. G. H. Hardy in A Mathematician's Apology expressed the belief that the aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He also identified other criteria such as significance, unexpectedness, and inevitability, which contribute to mathematical aesthetic. Paul Erdős expressed this sentiment more ironically by speaking of ""The Book"", a supposed divine collection of the most beautiful proofs. The 1998 book Proofs from THE BOOK, inspired by Erdős, is a collection of particularly succinct and revelatory mathematical arguments. Some examples of particularly elegant results included are Euclid's proof that there are infinitely many prime numbers and the fast Fourier transform for harmonic analysis.Some feel that to consider mathematics a science is to downplay its artistry and history in the seven traditional liberal arts. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematical results are created (as in art) or discovered (as in science). The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.
In the 20th century, the mathematician L. E. J. Brouwer even initiated a philosophical perspective known as intuitionism, which primarily identifies mathematics with certain creative processes in the mind. Intuitionism is in turn one flavor of a stance known as constructivism, which only considers a mathematical object valid if it can be directly constructed, not merely guaranteed by logic indirectly. This leads committed constructivists to reject certain results, particularly arguments like existential proofs based on the law of excluded middle.In the end, neither constructivism nor intuitionism displaced classical mathematics or achieved mainstream acceptance. However, these programs have motivated specific developments, such as intuitionistic logic and other foundational insights, which are appreciated in their own right.


== Cultural impact ==


=== Artistic expression ===

Notes that sound well together to a Western ear are sounds whose fundamental frequencies of vibration are in simple ratios. For example, an octave doubles the frequency and a perfect fifth multiplies it by 
  
    
      
        
          
            3
            2
          
        
      
    
    {\displaystyle {\frac {3}{2}}}
  .This link between frequencies and harmony was discussed in Traité de l'harmonie réduite à ses principes naturels by Jean-Philippe Rameau, a French baroque composer and music theoretician. It rests on the analysis of harmonics (noted 2 to 15 in the following figure) of a fundamental Do (noted 1); the first harmonics and their octaves sound well together.

The curve in red has a logarithmic shape, which reflects the following two phenomena:

The pitch of the sound, which in our auditory system is proportional to the logarithm of the sound's frequency.
The harmonic frequencies, which are integer multiples of the fundamental frequency.
Humans, as well as some other animals, find symmetric patterns to be more beautiful. Mathematically, the symmetries of an object form a group known as the symmetry group.For example, the group underlying mirror symmetry is the cyclic group of two elements, 
  
    
      
        
          Z
        
        
          /
        
        2
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /2\mathbb {Z} }
  . A Rorschach test is a figure invariant by this symmetry, as well as a butterfly, and animal bodies more generally (at least on the surface). Waves on the sea surface possess translation symmetry: moving one's viewpoint by the distance between wave crests does not change one's view of the sea. Furthermore, fractals possess (usually approximate) self-similarity.


=== Popularization ===
Popular mathematics is the act of presenting mathematics without technical terms. Presenting mathematics may be hard since the general public suffers from mathematical anxiety and mathematical objects are highly abstract. However, popular mathematics writing can overcome this by using applications or cultural links. Despite this, mathematics is rarely the topic of popularization in printed or televised media.


== Awards and prize problems ==

The most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to up to four individuals. It is considered the mathematical equivalent of the Nobel Prize.Other prestigious mathematics awards include:
The Abel Prize, instituted in 2002 and first awarded in 2003
The Chern Medal for lifetime achievement, introduced in 2009 and first awarded in 2010
The AMS Leroy P. Steele Prize, awarded since 1970
The Wolf Prize in Mathematics, also for lifetime achievement, instituted in 1978A famous list of 23 open problems, called ""Hilbert's problems"", was compiled in 1900 by German mathematician David Hilbert. This list has achieved great celebrity among mathematicians, and, as of 2022, at least thirteen of the problems (depending how some are interpreted) have been solved.A new list of seven important problems, titled the ""Millennium Prize Problems"", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward. To date, only one of these problems, the Poincaré conjecture, has been solved.


== See also ==


== Notes ==


== References ==


== Bibliography ==


=== Further reading ==="
eaab21a260,Function (mathematics),"In mathematics, a function from a set X to a set Y assigns to each element of X exactly one element of Y. The set X is called the domain of the function and the set Y is called the codomain of the function.Functions were originally the idealization of how a varying quantity depends on another quantity. For example, the position of a planet is a function of time. Historically, the concept was elaborated with the infinitesimal calculus at the end of the 17th century, and, until the 19th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). The concept of a function was formalized at the end of the 19th century in terms of set theory, and this greatly enlarged the domains of application of the concept.
A function is most often denoted by letters such as f, g and h, and the value of a function f at an element x of its domain is denoted by f(x); the numerical value resulting from the function evaluation at a particular input value is denoted by replacing x with this value; for example, the value of f at x = 4 is denoted by f(4). When the function is not named and is represented by an expression E, the value of the function at, say, x = 4 may be denoted by E|x=4. For example, the value at 4 of the function that maps x to 
  
    
      
        (
        x
        +
        1
        
          )
          
            2
          
        
      
    
    {\displaystyle (x+1)^{2}}
   may be denoted by 
  
    
      
        
          
            
            
              (
              x
              +
              1
              
                )
                
                  2
                
              
            
            |
          
          
            x
            =
            4
          
        
      
    
    {\displaystyle \left.(x+1)^{2}\right\vert _{x=4}}
   (which results in 25).A function is uniquely represented by the set of all pairs (x, f (x)), called the graph of the function, a popular means of illustrating the function. When the domain and the codomain are sets of real numbers, each such pair may be thought of as the Cartesian coordinates of a point in the plane.
Functions are widely used in science, engineering, and in most fields of mathematics. It has been said that functions are ""the central objects of investigation"" in most fields of mathematics.


== Definition ==

A function from a set X to a set Y is an assignment of an element of Y to each element of X. The set X is called the domain of the function and the set Y is called the codomain of the function.
A function, its domain, and its codomain, are declared by the notation f: X→Y, and the value of a function f at an element x of X, denoted by f(x), is called the image of x under f, or the value of f applied to the argument x. 
Functions are also called maps or mappings, though some authors make some distinction between ""maps"" and ""functions"" (see § Other terms).
Two functions f and g are equal if their domain and codomain sets are the same and their output values agree on the whole domain. More formally, given  f: X → Y and g: X → Y, we have f = g if and only if f(x) = g(x) for all x ∈ X.The domain and codomain are not always explicitly given when a function is defined, and, without some (possibly difficult) computation, one might only know that the domain is contained in a larger set. Typically, this occurs in mathematical analysis, where ""a function from X to Y "" often refers to a function that may have a proper subset of X as domain. For example, a ""function from the reals to the reals"" may refer to a real-valued function of a real variable. However, a ""function from the reals to the reals"" does not mean that the domain of the function is the whole set of the real numbers, but only that the domain is a set of real numbers that contains a non-empty open interval. Such a function is then called a partial function. For example, if f is a function that has the real numbers as domain and codomain, then a function mapping the value x to the value g(x) = 1/f(x) is a function g from the reals to the reals, whose domain is the set of the reals x, such that f(x) ≠ 0.
The range or image of a function is the set of the images of all elements in the domain.


=== Total, univalent relation ===
Any subset of the Cartesian product of two sets X and Y defines a binary relation R ⊆ X × Y between these two sets. It is immediate that an arbitrary relation may contain pairs that violate the necessary conditions for a function given above.
A binary relation is univalent (also called right-unique) if

  
    
      
        ∀
        x
        ∈
        X
        ,
        ∀
        y
        ∈
        Y
        ,
        ∀
        z
        ∈
        Y
        ,
        
        (
        (
        x
        ,
        y
        )
        ∈
        R
        ∧
        (
        x
        ,
        z
        )
        ∈
        R
        )
        
        ⟹
        
        y
        =
        z
        .
      
    
    {\displaystyle \forall x\in X,\forall y\in Y,\forall z\in Y,\quad ((x,y)\in R\land (x,z)\in R)\implies y=z.}
  A binary relation is total if

  
    
      
        ∀
        x
        ∈
        X
        ,
        ∃
        y
        ∈
        Y
        ,
        
        (
        x
        ,
        y
        )
        ∈
        R
        .
      
    
    {\displaystyle \forall x\in X,\exists y\in Y,\quad (x,y)\in R.}
  A partial function is a binary relation that is univalent, and a function is a binary relation that is univalent and total.
Various properties of functions and function composition may be reformulated in the language of relations. For example, a function is injective if the converse relation RT ⊆ Y × X is univalent, where the converse relation is defined as RT = {(y, x) | (x, y) ∈ R}.


=== Set exponentiation ===

The set of all functions from a set 
  
    
      
        X
      
    
    {\displaystyle X}
   to a set 
  
    
      
        Y
      
    
    {\displaystyle Y}
   is commonly denoted as

  
    
      
        
          Y
          
            X
          
        
        ,
      
    
    {\displaystyle Y^{X},}
  which is read as 
  
    
      
        Y
      
    
    {\displaystyle Y}
   to the power 
  
    
      
        X
      
    
    {\displaystyle X}
  .
This notation is the same as the notation for the Cartesian product of a family of copies of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   indexed by 
  
    
      
        X
      
    
    {\displaystyle X}
  :

  
    
      
        
          Y
          
            X
          
        
        =
        
          ∏
          
            x
            ∈
            X
          
        
        Y
        .
      
    
    {\displaystyle Y^{X}=\prod _{x\in X}Y.}
  The identity of these two notations is motivated by the fact that a function 
  
    
      
        f
      
    
    {\displaystyle f}
   can be identified with the element of the Cartesian product such that the component of index 
  
    
      
        x
      
    
    {\displaystyle x}
   is 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  .
When 
  
    
      
        Y
      
    
    {\displaystyle Y}
   has two elements, 
  
    
      
        
          Y
          
            X
          
        
      
    
    {\displaystyle Y^{X}}
   is commonly denoted 
  
    
      
        
          2
          
            X
          
        
      
    
    {\displaystyle 2^{X}}
   and called the powerset of X. It can be identified with the set of all subsets of 
  
    
      
        X
      
    
    {\displaystyle X}
  , through the one-to-one correspondence that associates to each subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   the function 
  
    
      
        f
      
    
    {\displaystyle f}
   such that 
  
    
      
        f
        (
        x
        )
        =
        1
      
    
    {\displaystyle f(x)=1}
   if 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
   and 
  
    
      
        f
        (
        x
        )
        =
        0
      
    
    {\displaystyle f(x)=0}
   otherwise.


== Notation ==
There are various standard ways for denoting functions. The most commonly used notation is functional notation, which is the first notation described below.


=== Functional notation ===
In functional notation, the function is immediately given a name, such as f, and its definition is given by what f does to the explicit argument x, using a formula in terms of x. For example, the function which takes a real number as input and outputs that number plus 1 is denoted by

  
    
      
        f
        (
        x
        )
        =
        x
        +
        1
      
    
    {\displaystyle f(x)=x+1}
  .If a function is defined in this notation, its domain and codomain are implicitly taken to both be 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , the set of real numbers. If the formula cannot be evaluated at all real numbers, then the domain is implicitly taken to be the maximal subset of 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   on which the formula can be evaluated; see Domain of a function.
A more complicated example is the function

  
    
      
        f
        (
        x
        )
        =
        sin
        ⁡
        (
        
          x
          
            2
          
        
        +
        1
        )
      
    
    {\displaystyle f(x)=\sin(x^{2}+1)}
  .In this example, the function f takes a real number as input, squares it, then adds 1 to the result, then takes the sine of the result, and returns the final result as the output.
When the symbol denoting the function consists of several characters and no ambiguity may arise, the parentheses of functional notation might be omitted. For example, it is common to write sin x instead of sin(x).
Functional notation was first used by Leonhard Euler in 1734. Some widely used functions are represented by a symbol consisting of several letters (usually two or three, generally an abbreviation of their name). In this case, a roman type is customarily used instead, such as ""sin"" for the sine function, in contrast to italic font for single-letter symbols.
When using this notation, one often encounters the abuse of notation whereby the notation f(x) can refer to the value of f at x, or to the function itself. If the variable x was previously declared, then the notation f(x) unambiguously means the value of f at x. Otherwise, it is useful to understand the notation as being both simultaneously; this allows one to denote composition of two functions f and g in a succinct manner by the notation f(g(x)).
However, distinguishing f and f(x) can become important in cases where functions themselves serve as inputs for other functions. (A function taking another function as an input is termed a functional.) Other approaches of notating functions, detailed below, avoid this problem but are less commonly used.


=== Arrow notation ===
Arrow notation defines the rule of a function inline, without requiring a name to be given to the function. For example, 
  
    
      
        x
        ↦
        x
        +
        1
      
    
    {\displaystyle x\mapsto x+1}
   is the function which takes a real number as input and outputs that number plus 1. Again a domain and codomain of 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is implied.
The domain and codomain can also be explicitly stated, for example:

  
    
      
        
          
            
              
                sqr
                :
                
                  Z
                
              
              
                
                →
                
                  Z
                
              
            
            
              
                x
              
              
                
                ↦
                
                  x
                  
                    2
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {sqr} \colon \mathbb {Z} &\to \mathbb {Z} \\x&\mapsto x^{2}.\end{aligned}}}
  This defines a function sqr from the integers to the integers that returns the square of its input.
As a common application of the arrow notation, suppose 
  
    
      
        f
        :
        X
        ×
        X
        →
        Y
        ;
        
        (
        x
        ,
        t
        )
        ↦
        f
        (
        x
        ,
        t
        )
      
    
    {\displaystyle f\colon X\times X\to Y;\;(x,t)\mapsto f(x,t)}
   is a function in two variables, and we want to refer to a partially applied function 
  
    
      
        X
        →
        Y
      
    
    {\displaystyle X\to Y}
   produced by fixing the second argument to the value t0 without introducing a new function name.  The map in question could be denoted 
  
    
      
        x
        ↦
        f
        (
        x
        ,
        
          t
          
            0
          
        
        )
      
    
    {\displaystyle x\mapsto f(x,t_{0})}
   using the arrow notation. The expression 
  
    
      
        x
        ↦
        f
        (
        x
        ,
        
          t
          
            0
          
        
        )
      
    
    {\displaystyle x\mapsto f(x,t_{0})}
   (read: ""the map taking x to f(x, t0)"") represents this new function with just one argument, whereas the expression f(x0, t0) refers to the value of the function f at the point (x0, t0).


=== Index notation ===
Index notation is often used instead of functional notation. That is, instead of writing f (x), one writes 
  
    
      
        
          f
          
            x
          
        
        .
      
    
    {\displaystyle f_{x}.}
  
This is typically the case for functions whose domain is the set of the natural numbers. Such a function is called a sequence, and, in this case the element 
  
    
      
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{n}}
   is called the nth element of the sequence.
The index notation is also often used for distinguishing some variables called parameters from the ""true variables"". In fact, parameters are specific variables that are considered as being fixed during the study of a problem.  For example, the map 
  
    
      
        x
        ↦
        f
        (
        x
        ,
        t
        )
      
    
    {\displaystyle x\mapsto f(x,t)}
   (see above) would be denoted 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
   using index notation, if we define the collection of maps 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
   by the formula 
  
    
      
        
          f
          
            t
          
        
        (
        x
        )
        =
        f
        (
        x
        ,
        t
        )
      
    
    {\displaystyle f_{t}(x)=f(x,t)}
   for all 
  
    
      
        x
        ,
        t
        ∈
        X
      
    
    {\displaystyle x,t\in X}
  .


=== Dot notation ===
In the notation

  
    
      
        x
        ↦
        f
        (
        x
        )
        ,
      
    
    {\displaystyle x\mapsto f(x),}
  
the symbol x does not represent any value, it is simply a placeholder meaning that, if x is replaced by any value on the left of the arrow, it should be replaced by the same value on the right of the arrow. Therefore, x may be replaced by any symbol, often an interpunct "" ⋅ "". This may be useful for distinguishing the function f (⋅) from its value f (x) at x.
For example, 
  
    
      
        a
        (
        ⋅
        
          )
          
            2
          
        
      
    
    {\displaystyle a(\cdot )^{2}}
   may stand for the function 
  
    
      
        x
        ↦
        a
        
          x
          
            2
          
        
      
    
    {\displaystyle x\mapsto ax^{2}}
  , and 
  
    
      
        
          ∫
          
            a
          
          
            
            (
            ⋅
            )
          
        
        f
        (
        u
        )
        
        d
        u
      
    
    {\textstyle \int _{a}^{\,(\cdot )}f(u)\,du}
   may stand for a function defined by an integral with variable upper bound: 
  
    
      
        x
        ↦
        
          ∫
          
            a
          
          
            x
          
        
        f
        (
        u
        )
        
        d
        u
      
    
    {\textstyle x\mapsto \int _{a}^{x}f(u)\,du}
  .


=== Specialized notations ===
There are other, specialized notations for functions in sub-disciplines of mathematics.  For example, in linear algebra and functional analysis, linear forms and the vectors they act upon are denoted using a dual pair to show the underlying duality.  This is similar to the use of bra–ket notation in quantum mechanics.  In logic and the theory of computation, the function notation of lambda calculus is used to explicitly express the basic notions of function abstraction and application.  In category theory and homological algebra, networks of functions are described in terms of how they and their compositions commute with each other using commutative diagrams that extend and generalize the arrow notation for functions described above.


== Other terms ==

A function is often also called a map or a mapping, but some authors make a distinction between the term ""map"" and ""function"". For example, the term ""map"" is often reserved for a ""function"" with some sort of special structure (e.g. maps of manifolds). In particular map is often used in place of homomorphism for the sake of succinctness (e.g., linear map or map from G to H instead of group homomorphism from G to H). Some authors reserve the word mapping for the case where the structure of the codomain belongs explicitly to the definition of the function.
Some authors, such as Serge Lang, use ""function"" only to refer to maps for which the codomain is a subset of the real or complex numbers, and use the term mapping for more general functions.
In the theory of dynamical systems, a map denotes an evolution function used to create discrete dynamical systems. See also Poincaré map.
Whichever definition of map is used, related terms like domain, codomain, injective, continuous have the same meaning as for a function.


== Specifying a function ==
Given a function 
  
    
      
        f
      
    
    {\displaystyle f}
  , by definition, to each element 
  
    
      
        x
      
    
    {\displaystyle x}
   of the domain of the function 
  
    
      
        f
      
    
    {\displaystyle f}
  , there is a unique element associated to it, the value 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   of 
  
    
      
        f
      
    
    {\displaystyle f}
   at 
  
    
      
        x
      
    
    {\displaystyle x}
  . There are several ways to specify or describe how 
  
    
      
        x
      
    
    {\displaystyle x}
   is related to 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  , both explicitly and implicitly. Sometimes, a theorem or an axiom asserts the existence of a function having some properties, without describing it more precisely. Often, the specification or description is referred to as the definition of the function 
  
    
      
        f
      
    
    {\displaystyle f}
  .


=== By listing function values ===
On a finite set, a function may be defined by listing the elements of the codomain that are associated to the elements of the domain. For example, if 
  
    
      
        A
        =
        {
        1
        ,
        2
        ,
        3
        }
      
    
    {\displaystyle A=\{1,2,3\}}
  , then one can define a function 
  
    
      
        f
        :
        A
        →
        
          R
        
      
    
    {\displaystyle f\colon A\to \mathbb {R} }
   by 
  
    
      
        f
        (
        1
        )
        =
        2
        ,
        f
        (
        2
        )
        =
        3
        ,
        f
        (
        3
        )
        =
        4.
      
    
    {\displaystyle f(1)=2,f(2)=3,f(3)=4.}
  


=== By a formula ===
Functions are often defined by a formula that describes a combination of arithmetic operations and previously defined functions; such a formula allows computing the value of the function from the value of any element of the domain.
For example, in the above example, 
  
    
      
        f
      
    
    {\displaystyle f}
   can be defined by the formula 
  
    
      
        f
        (
        n
        )
        =
        n
        +
        1
      
    
    {\displaystyle f(n)=n+1}
  , for 
  
    
      
        n
        ∈
        {
        1
        ,
        2
        ,
        3
        }
      
    
    {\displaystyle n\in \{1,2,3\}}
  .
When a function is defined this way, the determination of its domain is sometimes difficult. If the formula that defines the function contains divisions, the values of the variable for which a denominator is zero must be excluded from the domain; thus, for a complicated function, the determination of the domain passes through the computation of the zeros of auxiliary functions. Similarly, if square roots occur in the definition of a function from 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   to 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   the domain is included in the set of the values of the variable for which the arguments of the square roots are nonnegative.
For example, 
  
    
      
        f
        (
        x
        )
        =
        
          
            1
            +
            
              x
              
                2
              
            
          
        
      
    
    {\displaystyle f(x)={\sqrt {1+x^{2}}}}
   defines a function 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f\colon \mathbb {R} \to \mathbb {R} }
   whose domain is 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   because 
  
    
      
        1
        +
        
          x
          
            2
          
        
      
    
    {\displaystyle 1+x^{2}}
   is always positive if x is a real number. On the other hand, 
  
    
      
        f
        (
        x
        )
        =
        
          
            1
            −
            
              x
              
                2
              
            
          
        
      
    
    {\displaystyle f(x)={\sqrt {1-x^{2}}}}
   defines a function from the reals to the reals whose domain is reduced to the interval [−1, 1]. (In old texts, such a domain was called the domain of definition of the function.)
Functions are often classified by the nature of formulas that define them:

A quadratic function is a function that may be written 
  
    
      
        f
        (
        x
        )
        =
        a
        
          x
          
            2
          
        
        +
        b
        x
        +
        c
        ,
      
    
    {\displaystyle f(x)=ax^{2}+bx+c,}
   where a, b, c are constants.
More generally, a polynomial function is a function that can be defined by a formula involving only additions, subtractions, multiplications, and exponentiation to nonnegative integers. For example, 
  
    
      
        f
        (
        x
        )
        =
        
          x
          
            3
          
        
        −
        3
        x
        −
        1
        ,
      
    
    {\displaystyle f(x)=x^{3}-3x-1,}
   and 
  
    
      
        f
        (
        x
        )
        =
        (
        x
        −
        1
        )
        (
        
          x
          
            3
          
        
        +
        1
        )
        +
        2
        
          x
          
            2
          
        
        −
        1.
      
    
    {\displaystyle f(x)=(x-1)(x^{3}+1)+2x^{2}-1.}
  
A rational function is the same, with divisions also allowed, such as 
  
    
      
        f
        (
        x
        )
        =
        
          
            
              x
              −
              1
            
            
              x
              +
              1
            
          
        
        ,
      
    
    {\displaystyle f(x)={\frac {x-1}{x+1}},}
   and 
  
    
      
        f
        (
        x
        )
        =
        
          
            1
            
              x
              +
              1
            
          
        
        +
        
          
            3
            x
          
        
        −
        
          
            2
            
              x
              −
              1
            
          
        
        .
      
    
    {\displaystyle f(x)={\frac {1}{x+1}}+{\frac {3}{x}}-{\frac {2}{x-1}}.}
  
An algebraic function is the same, with nth roots and roots of polynomials also allowed.
An elementary function is the same, with logarithms and exponential functions allowed.


=== Inverse and implicit functions ===
A function 
  
    
      
        f
        :
        X
        →
        Y
        ,
      
    
    {\displaystyle f\colon X\to Y,}
   with domain X and codomain Y, is bijective, if for every y in Y, there is one and only one element x in X such that y = f(x). In this case, the inverse function of f is the function 
  
    
      
        
          f
          
            −
            1
          
        
        :
        Y
        →
        X
      
    
    {\displaystyle f^{-1}\colon Y\to X}
   that maps 
  
    
      
        y
        ∈
        Y
      
    
    {\displaystyle y\in Y}
   to the element 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   such that y = f(x). For example, the natural logarithm is a bijective function from the positive real numbers to the real numbers. It thus has an inverse, called the exponential function, that maps the real numbers onto the positive numbers.
If a function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   is not bijective, it may occur that one can select subsets 
  
    
      
        E
        ⊆
        X
      
    
    {\displaystyle E\subseteq X}
   and 
  
    
      
        F
        ⊆
        Y
      
    
    {\displaystyle F\subseteq Y}
   such that the restriction of f to E is a bijection from E to F, and has thus an inverse. The inverse trigonometric functions are defined this way. For example, the cosine function induces, by restriction, a bijection from the interval [0, π] onto the interval [−1, 1], and its inverse function, called arccosine, maps [−1, 1] onto [0, π]. The other inverse trigonometric functions are defined similarly.
More generally, given a binary relation R between two sets X and Y, let E be a subset of X such that, for every 
  
    
      
        x
        ∈
        E
        ,
      
    
    {\displaystyle x\in E,}
   there is some 
  
    
      
        y
        ∈
        Y
      
    
    {\displaystyle y\in Y}
   such that x R y. If one has a criterion allowing selecting such an y for every 
  
    
      
        x
        ∈
        E
        ,
      
    
    {\displaystyle x\in E,}
   this defines a function 
  
    
      
        f
        :
        E
        →
        Y
        ,
      
    
    {\displaystyle f\colon E\to Y,}
   called an implicit function, because it is implicitly defined by the relation R.
For example, the equation of the unit circle 
  
    
      
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        =
        1
      
    
    {\displaystyle x^{2}+y^{2}=1}
   defines a relation on real numbers. If −1 < x < 1 there are two possible values of y, one positive and one negative. For x = ± 1, these two values become both equal to 0. Otherwise, there is no possible value of y. This means that the equation defines two implicit functions with domain [−1, 1] and respective codomains [0, +∞) and (−∞, 0].
In this example, the equation can be solved in y, giving 
  
    
      
        y
        =
        ±
        
          
            1
            −
            
              x
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle y=\pm {\sqrt {1-x^{2}}},}
   but, in more complicated examples, this is impossible. For example, the relation 
  
    
      
        
          y
          
            5
          
        
        +
        y
        +
        x
        =
        0
      
    
    {\displaystyle y^{5}+y+x=0}
   defines y as an implicit function of x, called the Bring radical, which has 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   as domain and range. The Bring radical cannot be expressed in terms of the four arithmetic operations and nth roots.
The implicit function theorem provides mild differentiability conditions for existence and uniqueness of an implicit function in the neighborhood of a point.


=== Using differential calculus ===
Many functions can be defined as the antiderivative of another function. This is the case of the natural logarithm, which is the antiderivative of 1/x that is 0 for x = 1. Another common example is the error function.
More generally, many functions, including most special functions, can be defined as solutions of differential equations. The simplest example is probably the exponential function, which can be defined as the unique function that is equal to its derivative and takes the value 1 for x = 0.
Power series can be used to define functions on the domain in which they converge. For example, the exponential function is given by 
  
    
      
        
          e
          
            x
          
        
        =
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          
            
              x
              
                n
              
            
            
              n
              !
            
          
        
      
    
    {\displaystyle e^{x}=\sum _{n=0}^{\infty }{x^{n} \over n!}}
  . However, as the coefficients of a series are quite arbitrary, a function that is the sum of a convergent series is generally defined otherwise, and the sequence of the coefficients is the result of some computation based on another definition. Then, the power series can be used to enlarge the domain of the function. Typically, if a function for a real variable is the sum of its Taylor series in some interval, this power series allows immediately enlarging the domain to a subset of the complex numbers, the disc of convergence of the series. Then analytic continuation allows enlarging further the domain for including almost the whole complex plane. This process is the method that is generally used for defining the logarithm, the exponential and the trigonometric functions of a complex number.


=== By recurrence ===

Functions whose domain are the nonnegative integers, known as sequences, are often defined by recurrence relations.
The factorial function on the nonnegative integers (
  
    
      
        n
        ↦
        n
        !
      
    
    {\displaystyle n\mapsto n!}
  ) is a basic example, as it can be defined by the recurrence relation

  
    
      
        n
        !
        =
        n
        (
        n
        −
        1
        )
        !
        
        
          for
        
        
        n
        >
        0
        ,
      
    
    {\displaystyle n!=n(n-1)!\quad {\text{for}}\quad n>0,}
  and the initial condition

  
    
      
        0
        !
        =
        1.
      
    
    {\displaystyle 0!=1.}
  


== Representing a function ==
A graph is commonly used to give an intuitive picture of a function.  As an example of how a graph helps to understand a function, it is easy to see from its graph whether a function is increasing or decreasing. Some functions may also be represented by bar charts.


=== Graphs and plots ===

Given a function 
  
    
      
        f
        :
        X
        →
        Y
        ,
      
    
    {\displaystyle f\colon X\to Y,}
   its graph is, formally, the set

  
    
      
        G
        =
        {
        (
        x
        ,
        f
        (
        x
        )
        )
        ∣
        x
        ∈
        X
        }
        .
      
    
    {\displaystyle G=\{(x,f(x))\mid x\in X\}.}
  In the frequent case where X and Y are subsets of the real numbers (or may be identified with such subsets, e.g. intervals), an element 
  
    
      
        (
        x
        ,
        y
        )
        ∈
        G
      
    
    {\displaystyle (x,y)\in G}
   may be identified with a point having coordinates x, y in a 2-dimensional coordinate system, e.g. the Cartesian plane. Parts of this may create a plot that represents (parts of) the function. The use of plots is so ubiquitous that they too are called the graph of the function. Graphic representations of functions are also possible in other coordinate systems. For example, the graph of the square function

  
    
      
        x
        ↦
        
          x
          
            2
          
        
        ,
      
    
    {\displaystyle x\mapsto x^{2},}
  consisting of all points with coordinates 
  
    
      
        (
        x
        ,
        
          x
          
            2
          
        
        )
      
    
    {\displaystyle (x,x^{2})}
   for 
  
    
      
        x
        ∈
        
          R
        
        ,
      
    
    {\displaystyle x\in \mathbb {R} ,}
   yields, when depicted in Cartesian coordinates, the well known parabola. If the same quadratic function 
  
    
      
        x
        ↦
        
          x
          
            2
          
        
        ,
      
    
    {\displaystyle x\mapsto x^{2},}
   with the same formal graph, consisting of pairs of numbers, is plotted instead in polar coordinates 
  
    
      
        (
        r
        ,
        θ
        )
        =
        (
        x
        ,
        
          x
          
            2
          
        
        )
        ,
      
    
    {\displaystyle (r,\theta )=(x,x^{2}),}
   the plot obtained is Fermat's spiral.


=== Tables ===

A function can be represented as a table of values.  If the domain of a function is finite, then the function can be completely specified in this way.  For example, the multiplication function 
  
    
      
        f
        :
        {
        1
        ,
        …
        ,
        5
        
          }
          
            2
          
        
        →
        
          R
        
      
    
    {\displaystyle f\colon \{1,\ldots ,5\}^{2}\to \mathbb {R} }
   defined as 
  
    
      
        f
        (
        x
        ,
        y
        )
        =
        x
        y
      
    
    {\displaystyle f(x,y)=xy}
   can be represented by the familiar multiplication table

On the other hand, if a function's domain is continuous, a table can give the values of the function at specific values of the domain.  If an intermediate value is needed, interpolation can be used to estimate the value of the function.  For example, a portion of a table for the sine function might be given as follows, with values rounded to 6 decimal places:

Before the advent of handheld calculators and personal computers, such tables were often compiled and published for functions such as logarithms and trigonometric functions.


=== Bar chart ===

Bar charts are often used for representing functions whose domain is a finite set, the natural numbers, or the integers. In this case, an element x of the domain is represented by an interval of the x-axis, and the corresponding value of the function, f(x), is represented by a rectangle whose base is the interval corresponding to x and whose height is f(x) (possibly negative, in which case the bar extends below the x-axis).


== General properties ==
This section describes general properties of functions, that are independent of specific properties of the domain and the codomain.


=== Standard functions ===
There are a number of standard functions that occur frequently:

For every set X, there is a unique function, called the empty function, or empty map, from the empty set to X. The graph of an empty function is the empty set. The existence of empty functions is needed both for the coherency of the theory and for avoiding exceptions concerning the empty set in many statements. Under the usual set-theoretic definition of a function as an ordered triplet (or equivalent ones), there is exactly one empty function for each set, thus the empty function 
  
    
      
        ∅
        ↦
        X
      
    
    {\displaystyle \varnothing \mapsto X}
   is not equal to 
  
    
      
        ∅
        ↦
        Y
      
    
    {\displaystyle \varnothing \mapsto Y}
   if and only if 
  
    
      
        X
        ≠
        Y
      
    
    {\displaystyle X\neq Y}
  , although their graph are both the empty set.
For every set X and every singleton set {s}, there is a unique function from X to {s}, which maps every element of X to s. This is a surjection (see below) unless X is the empty set.
Given a function 
  
    
      
        f
        :
        X
        →
        Y
        ,
      
    
    {\displaystyle f\colon X\to Y,}
   the canonical surjection of f onto its image 
  
    
      
        f
        (
        X
        )
        =
        {
        f
        (
        x
        )
        ∣
        x
        ∈
        X
        }
      
    
    {\displaystyle f(X)=\{f(x)\mid x\in X\}}
   is the function from X to f(X) that maps x to f(x).
For every subset A of a set X, the inclusion map of A into X is the injective (see below) function that maps every element of A to itself.
The identity function on a set X, often denoted by idX, is the inclusion of X into itself.


=== Function composition ===

Given two functions 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   and 
  
    
      
        g
        :
        Y
        →
        Z
      
    
    {\displaystyle g\colon Y\to Z}
   such that the domain of g is the codomain of f, their composition is the function 
  
    
      
        g
        ∘
        f
        :
        X
        →
        Z
      
    
    {\displaystyle g\circ f\colon X\rightarrow Z}
   defined by

  
    
      
        (
        g
        ∘
        f
        )
        (
        x
        )
        =
        g
        (
        f
        (
        x
        )
        )
        .
      
    
    {\displaystyle (g\circ f)(x)=g(f(x)).}
  That is, the value of 
  
    
      
        g
        ∘
        f
      
    
    {\displaystyle g\circ f}
   is obtained by first applying f to x to obtain y = f(x) and then applying g to the result y to obtain g(y) = g(f(x)). In the notation the function that is applied first is always written on the right.
The composition 
  
    
      
        g
        ∘
        f
      
    
    {\displaystyle g\circ f}
   is an operation on functions that is defined only if the codomain of the first function is the domain of the second one. Even when both 
  
    
      
        g
        ∘
        f
      
    
    {\displaystyle g\circ f}
   and 
  
    
      
        f
        ∘
        g
      
    
    {\displaystyle f\circ g}
   satisfy these conditions, the composition is not necessarily commutative, that is, the functions 
  
    
      
        g
        ∘
        f
      
    
    {\displaystyle g\circ f}
   and 
  
    
      
        f
        ∘
        g
      
    
    {\displaystyle f\circ g}
   need not be equal, but may deliver different values for the same argument. For example, let f(x) = x2 and g(x) = x + 1, then 
  
    
      
        g
        (
        f
        (
        x
        )
        )
        =
        
          x
          
            2
          
        
        +
        1
      
    
    {\displaystyle g(f(x))=x^{2}+1}
   and 
  
    
      
        f
        (
        g
        (
        x
        )
        )
        =
        (
        x
        +
        1
        
          )
          
            2
          
        
      
    
    {\displaystyle f(g(x))=(x+1)^{2}}
   agree just for 
  
    
      
        x
        =
        0.
      
    
    {\displaystyle x=0.}
  
The function composition is associative in the sense that, if one of 
  
    
      
        (
        h
        ∘
        g
        )
        ∘
        f
      
    
    {\displaystyle (h\circ g)\circ f}
   and 
  
    
      
        h
        ∘
        (
        g
        ∘
        f
        )
      
    
    {\displaystyle h\circ (g\circ f)}
   is defined, then the other is also defined, and they are equal. Thus, one writes

  
    
      
        h
        ∘
        g
        ∘
        f
        =
        (
        h
        ∘
        g
        )
        ∘
        f
        =
        h
        ∘
        (
        g
        ∘
        f
        )
        .
      
    
    {\displaystyle h\circ g\circ f=(h\circ g)\circ f=h\circ (g\circ f).}
  The identity functions 
  
    
      
        
          id
          
            X
          
        
      
    
    {\displaystyle \operatorname {id} _{X}}
   and 
  
    
      
        
          id
          
            Y
          
        
      
    
    {\displaystyle \operatorname {id} _{Y}}
   are respectively a right identity and a left identity for functions from X to Y. That is, if f is a function with domain X, and codomain Y, one has

  
    
      
        f
        ∘
        
          id
          
            X
          
        
        =
        
          id
          
            Y
          
        
        ∘
        f
        =
        f
        .
      
    
    {\displaystyle f\circ \operatorname {id} _{X}=\operatorname {id} _{Y}\circ f=f.}
  

		
		


=== Image and preimage ===

Let 
  
    
      
        f
        :
        X
        →
        Y
        .
      
    
    {\displaystyle f\colon X\to Y.}
   The image under f of an element x of the domain X is f(x). If A is any subset of X, then the image of A under f, denoted f(A), is the subset of the codomain Y consisting of all images of elements of A, that is,

  
    
      
        f
        (
        A
        )
        =
        {
        f
        (
        x
        )
        ∣
        x
        ∈
        A
        }
        .
      
    
    {\displaystyle f(A)=\{f(x)\mid x\in A\}.}
  The image of f is the image of the whole domain, that is, f(X). It is also called the range of f, although the term range may also refer to the codomain.On the other hand, the inverse image or preimage under f of an element y of the codomain Y is the set of all elements of the domain X whose images under f equal y. In symbols, the preimage of y is denoted by 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
      
    
    {\displaystyle f^{-1}(y)}
   and is given by the equation

  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
        =
        {
        x
        ∈
        X
        ∣
        f
        (
        x
        )
        =
        y
        }
        .
      
    
    {\displaystyle f^{-1}(y)=\{x\in X\mid f(x)=y\}.}
  Likewise, the preimage of a subset B of the codomain Y is the set of the preimages of the elements of B, that is, it is the subset of the domain X consisting of all elements of X whose images belong to B. It is denoted by 
  
    
      
        
          f
          
            −
            1
          
        
        (
        B
        )
      
    
    {\displaystyle f^{-1}(B)}
   and is given by the equation

  
    
      
        
          f
          
            −
            1
          
        
        (
        B
        )
        =
        {
        x
        ∈
        X
        ∣
        f
        (
        x
        )
        ∈
        B
        }
        .
      
    
    {\displaystyle f^{-1}(B)=\{x\in X\mid f(x)\in B\}.}
  For example, the preimage of 
  
    
      
        {
        4
        ,
        9
        }
      
    
    {\displaystyle \{4,9\}}
   under the square function is the set 
  
    
      
        {
        −
        3
        ,
        −
        2
        ,
        2
        ,
        3
        }
      
    
    {\displaystyle \{-3,-2,2,3\}}
  .
By definition of a function, the image of an element x of the domain is always a single element of the codomain. However, the preimage 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
      
    
    {\displaystyle f^{-1}(y)}
   of an element y of the codomain may be empty or contain any number of elements. For example, if f is the function from the integers to themselves that maps every integer to 0, then 
  
    
      
        
          f
          
            −
            1
          
        
        (
        0
        )
        =
        
          Z
        
      
    
    {\displaystyle f^{-1}(0)=\mathbb {Z} }
  .
If 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   is a function, A and B are subsets of X, and C and D are subsets of Y, then one has the following properties:

  
    
      
        A
        ⊆
        B
        ⟹
        f
        (
        A
        )
        ⊆
        f
        (
        B
        )
      
    
    {\displaystyle A\subseteq B\Longrightarrow f(A)\subseteq f(B)}
  

  
    
      
        C
        ⊆
        D
        ⟹
        
          f
          
            −
            1
          
        
        (
        C
        )
        ⊆
        
          f
          
            −
            1
          
        
        (
        D
        )
      
    
    {\displaystyle C\subseteq D\Longrightarrow f^{-1}(C)\subseteq f^{-1}(D)}
  

  
    
      
        A
        ⊆
        
          f
          
            −
            1
          
        
        (
        f
        (
        A
        )
        )
      
    
    {\displaystyle A\subseteq f^{-1}(f(A))}
  

  
    
      
        C
        ⊇
        f
        (
        
          f
          
            −
            1
          
        
        (
        C
        )
        )
      
    
    {\displaystyle C\supseteq f(f^{-1}(C))}
  

  
    
      
        f
        (
        
          f
          
            −
            1
          
        
        (
        f
        (
        A
        )
        )
        )
        =
        f
        (
        A
        )
      
    
    {\displaystyle f(f^{-1}(f(A)))=f(A)}
  

  
    
      
        
          f
          
            −
            1
          
        
        (
        f
        (
        
          f
          
            −
            1
          
        
        (
        C
        )
        )
        )
        =
        
          f
          
            −
            1
          
        
        (
        C
        )
      
    
    {\displaystyle f^{-1}(f(f^{-1}(C)))=f^{-1}(C)}
  The preimage by f of an element y of the codomain is sometimes called, in some contexts, the fiber of y under f.
If a function f has an inverse (see below), this inverse is denoted 
  
    
      
        
          f
          
            −
            1
          
        
        .
      
    
    {\displaystyle f^{-1}.}
   In this case 
  
    
      
        
          f
          
            −
            1
          
        
        (
        C
        )
      
    
    {\displaystyle f^{-1}(C)}
   may denote either the image by 
  
    
      
        
          f
          
            −
            1
          
        
      
    
    {\displaystyle f^{-1}}
   or the preimage by f of C. This is not a problem, as these sets are equal. The notation 
  
    
      
        f
        (
        A
        )
      
    
    {\displaystyle f(A)}
   and 
  
    
      
        
          f
          
            −
            1
          
        
        (
        C
        )
      
    
    {\displaystyle f^{-1}(C)}
   may be ambiguous in the case of sets that contain some subsets as elements, such as 
  
    
      
        {
        x
        ,
        {
        x
        }
        }
        .
      
    
    {\displaystyle \{x,\{x\}\}.}
   In this case, some care may be needed, for example, by using square brackets 
  
    
      
        f
        [
        A
        ]
        ,
        
          f
          
            −
            1
          
        
        [
        C
        ]
      
    
    {\displaystyle f[A],f^{-1}[C]}
   for images and preimages of subsets and ordinary parentheses for images and preimages of elements.


=== Injective, surjective and bijective functions ===
Let 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   be a function.
The function f is injective (or one-to-one, or is an injection) if f(a) ≠ f(b) for any two different elements a and b of X. Equivalently, f is injective if and only if, for any 
  
    
      
        y
        ∈
        Y
        ,
      
    
    {\displaystyle y\in Y,}
   the preimage 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
      
    
    {\displaystyle f^{-1}(y)}
   contains at most one element. An empty function is always injective. If X is not the empty set, then f is injective if and only if there exists a function 
  
    
      
        g
        :
        Y
        →
        X
      
    
    {\displaystyle g\colon Y\to X}
   such that 
  
    
      
        g
        ∘
        f
        =
        
          id
          
            X
          
        
        ,
      
    
    {\displaystyle g\circ f=\operatorname {id} _{X},}
   that is, if f has a left inverse. Proof: If f is injective, for defining g, one chooses an element 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
   in X (which exists as X is supposed to be nonempty), and one defines g by 
  
    
      
        g
        (
        y
        )
        =
        x
      
    
    {\displaystyle g(y)=x}
   if 
  
    
      
        y
        =
        f
        (
        x
        )
      
    
    {\displaystyle y=f(x)}
   and 
  
    
      
        g
        (
        y
        )
        =
        
          x
          
            0
          
        
      
    
    {\displaystyle g(y)=x_{0}}
   if 
  
    
      
        y
        ∉
        f
        (
        X
        )
        .
      
    
    {\displaystyle y\not \in f(X).}
   Conversely, if 
  
    
      
        g
        ∘
        f
        =
        
          id
          
            X
          
        
        ,
      
    
    {\displaystyle g\circ f=\operatorname {id} _{X},}
   and 
  
    
      
        y
        =
        f
        (
        x
        )
        ,
      
    
    {\displaystyle y=f(x),}
    then 
  
    
      
        x
        =
        g
        (
        y
        )
        ,
      
    
    {\displaystyle x=g(y),}
   and thus 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
        =
        {
        x
        }
        .
      
    
    {\displaystyle f^{-1}(y)=\{x\}.}
  
The function f is surjective (or onto, or is a surjection) if its range 
  
    
      
        f
        (
        X
        )
      
    
    {\displaystyle f(X)}
   equals its codomain 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , that is, if, for each element 
  
    
      
        y
      
    
    {\displaystyle y}
   of the codomain, there exists some element 
  
    
      
        x
      
    
    {\displaystyle x}
   of the domain such that 
  
    
      
        f
        (
        x
        )
        =
        y
      
    
    {\displaystyle f(x)=y}
   (in other words, the preimage 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
      
    
    {\displaystyle f^{-1}(y)}
   of every 
  
    
      
        y
        ∈
        Y
      
    
    {\displaystyle y\in Y}
   is nonempty). If, as usual in modern mathematics, the axiom of choice is assumed, then f is surjective if and only if there exists a function 
  
    
      
        g
        :
        Y
        →
        X
      
    
    {\displaystyle g\colon Y\to X}
   such that 
  
    
      
        f
        ∘
        g
        =
        
          id
          
            Y
          
        
        ,
      
    
    {\displaystyle f\circ g=\operatorname {id} _{Y},}
   that is, if f has a right inverse. The axiom of choice is needed, because, if f is surjective, one defines g by 
  
    
      
        g
        (
        y
        )
        =
        x
        ,
      
    
    {\displaystyle g(y)=x,}
   where 
  
    
      
        x
      
    
    {\displaystyle x}
   is an arbitrarily chosen element of 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
        .
      
    
    {\displaystyle f^{-1}(y).}
  
The function f is bijective (or is a bijection or a one-to-one correspondence) if it is both injective and surjective. That is, f is bijective if, for any 
  
    
      
        y
        ∈
        Y
        ,
      
    
    {\displaystyle y\in Y,}
   the preimage 
  
    
      
        
          f
          
            −
            1
          
        
        (
        y
        )
      
    
    {\displaystyle f^{-1}(y)}
   contains exactly one element. The function f is bijective if and only if it admits an inverse function, that is, a function 
  
    
      
        g
        :
        Y
        →
        X
      
    
    {\displaystyle g\colon Y\to X}
   such that 
  
    
      
        g
        ∘
        f
        =
        
          id
          
            X
          
        
      
    
    {\displaystyle g\circ f=\operatorname {id} _{X}}
   and 
  
    
      
        f
        ∘
        g
        =
        
          id
          
            Y
          
        
        .
      
    
    {\displaystyle f\circ g=\operatorname {id} _{Y}.}
   (Contrarily to the case of surjections, this does not require the axiom of choice; the proof is straightforward).
Every function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   may be factorized as the composition 
  
    
      
        i
        ∘
        s
      
    
    {\displaystyle i\circ s}
   of a surjection followed by an injection, where s is the canonical surjection of X onto f(X) and i is the canonical injection of f(X) into Y. This is the canonical factorization of f.
""One-to-one"" and ""onto"" are terms that were more common in the older English language literature; ""injective"", ""surjective"", and ""bijective"" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English.  As a word of caution, ""a one-to-one function"" is one that is injective, while a ""one-to-one correspondence"" refers to a bijective function.  Also, the statement ""f maps X onto Y"" differs from ""f  maps X into B"", in that the former implies that f is surjective, while the latter makes no assertion about the nature of f. In a complicated reasoning, the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms, which have also the advantage of being more symmetrical.


=== Restriction and extension ===

If 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   is a function and S is a subset of X, then the restriction of 
  
    
      
        f
      
    
    {\displaystyle f}
   to S, denoted 
  
    
      
        f
        
          
            |
          
          
            S
          
        
      
    
    {\displaystyle f|_{S}}
  , is the function from S to Y defined by

  
    
      
        f
        
          
            |
          
          
            S
          
        
        (
        x
        )
        =
        f
        (
        x
        )
      
    
    {\displaystyle f|_{S}(x)=f(x)}
  for all x in S. Restrictions can be used to define partial inverse functions: if there is a subset S of the domain of a function 
  
    
      
        f
      
    
    {\displaystyle f}
   such that 
  
    
      
        f
        
          
            |
          
          
            S
          
        
      
    
    {\displaystyle f|_{S}}
   is injective, then the canonical surjection of 
  
    
      
        f
        
          
            |
          
          
            S
          
        
      
    
    {\displaystyle f|_{S}}
   onto its image 
  
    
      
        f
        
          
            |
          
          
            S
          
        
        (
        S
        )
        =
        f
        (
        S
        )
      
    
    {\displaystyle f|_{S}(S)=f(S)}
   is a bijection, and thus has an inverse function from 
  
    
      
        f
        (
        S
        )
      
    
    {\displaystyle f(S)}
   to S. One application is the definition of inverse trigonometric functions. For example, the cosine function is injective when restricted to the interval [0, π]. The image of this restriction is the interval [−1, 1], and thus the restriction has an inverse function from [−1, 1] to [0, π], which is called arccosine and is denoted arccos.
Function restriction may also be used for ""gluing"" functions together. Let 
  
    
      
        X
        =
        
          ⋃
          
            i
            ∈
            I
          
        
        
          U
          
            i
          
        
      
    
    {\textstyle X=\bigcup _{i\in I}U_{i}}
   be the decomposition of X as a union of subsets, and suppose that a function 
  
    
      
        
          f
          
            i
          
        
        :
        
          U
          
            i
          
        
        →
        Y
      
    
    {\displaystyle f_{i}\colon U_{i}\to Y}
   is defined on each 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   such that for each pair 
  
    
      
        i
        ,
        j
      
    
    {\displaystyle i,j}
   of indices, the restrictions of 
  
    
      
        
          f
          
            i
          
        
      
    
    {\displaystyle f_{i}}
   and 
  
    
      
        
          f
          
            j
          
        
      
    
    {\displaystyle f_{j}}
   to 
  
    
      
        
          U
          
            i
          
        
        ∩
        
          U
          
            j
          
        
      
    
    {\displaystyle U_{i}\cap U_{j}}
   are equal. Then this defines a unique function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f\colon X\to Y}
   such that 
  
    
      
        f
        
          
            |
          
          
            
              U
              
                i
              
            
          
        
        =
        
          f
          
            i
          
        
      
    
    {\displaystyle f|_{U_{i}}=f_{i}}
   for all i. This is the way that functions on manifolds are defined.
An extension of a function f is a function g such that f is a restriction of g. A typical use of this concept is the process of analytic continuation, that allows extending functions whose domain is a small part of the complex plane to functions whose domain is almost the whole complex plane.
Here is another classical example of a function extension that is encountered when studying homographies of the real line. A homography is a function 
  
    
      
        h
        (
        x
        )
        =
        
          
            
              a
              x
              +
              b
            
            
              c
              x
              +
              d
            
          
        
      
    
    {\displaystyle h(x)={\frac {ax+b}{cx+d}}}
   such that ad − bc ≠ 0. Its domain is the set of all real numbers different from 
  
    
      
        −
        d
        
          /
        
        c
        ,
      
    
    {\displaystyle -d/c,}
   and its image is the set of all real numbers different from 
  
    
      
        a
        
          /
        
        c
        .
      
    
    {\displaystyle a/c.}
   If one extends the real line to the projectively extended real line by including ∞, one may extend h to a bijection from the extended real line to itself by setting 
  
    
      
        h
        (
        ∞
        )
        =
        a
        
          /
        
        c
      
    
    {\displaystyle h(\infty )=a/c}
   and 
  
    
      
        h
        (
        −
        d
        
          /
        
        c
        )
        =
        ∞
      
    
    {\displaystyle h(-d/c)=\infty }
  .


== Multivariate function ==

A multivariate function, or function of several variables is a function that depends on several arguments. Such functions are commonly encountered. For example, the position of a car on a road is a function of the time travelled and its average speed.
More formally, a function of n variables is a function whose domain is a set of n-tuples.
For example, multiplication of integers is a function of two variables, or bivariate function, whose domain is the set of all pairs (2-tuples) of integers, and whose codomain is the set of integers. The same is true for every binary operation. More generally, every mathematical operation is defined as a multivariate function.
The Cartesian product 
  
    
      
        
          X
          
            1
          
        
        ×
        ⋯
        ×
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1}\times \cdots \times X_{n}}
   of n sets 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1},\ldots ,X_{n}}
   is the set of all n-tuples 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle (x_{1},\ldots ,x_{n})}
   such that 
  
    
      
        
          x
          
            i
          
        
        ∈
        
          X
          
            i
          
        
      
    
    {\displaystyle x_{i}\in X_{i}}
   for every i with 
  
    
      
        1
        ≤
        i
        ≤
        n
      
    
    {\displaystyle 1\leq i\leq n}
  . Therefore, a function of n variables is a function

  
    
      
        f
        :
        U
        →
        Y
        ,
      
    
    {\displaystyle f\colon U\to Y,}
  where the domain U has the form

  
    
      
        U
        ⊆
        
          X
          
            1
          
        
        ×
        ⋯
        ×
        
          X
          
            n
          
        
        .
      
    
    {\displaystyle U\subseteq X_{1}\times \cdots \times X_{n}.}
  When using function notation, one usually omits the parentheses surrounding tuples, writing 
  
    
      
        f
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        )
      
    
    {\displaystyle f(x_{1},x_{2})}
   instead of 
  
    
      
        f
        (
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        )
        )
        .
      
    
    {\displaystyle f((x_{1},x_{2})).}
  
In the case where all the 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   are equal to the set 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   of real numbers, one has a function of several real variables. If the 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   are equal to the set 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
   of complex numbers, one has a function of several complex variables.
It is common to also consider functions whose codomain is a product of sets. For example, Euclidean division maps every pair (a, b) of integers with b ≠ 0 to a pair of integers called the quotient and the remainder:

  
    
      
        
          
            
              
                
                  Euclidean division
                
                :
                
                
                  Z
                
                ×
                (
                
                  Z
                
                ∖
                {
                0
                }
                )
              
              
                
                →
                
                  Z
                
                ×
                
                  Z
                
              
            
            
              
                (
                a
                ,
                b
                )
              
              
                
                ↦
                (
                quotient
                ⁡
                (
                a
                ,
                b
                )
                ,
                remainder
                ⁡
                (
                a
                ,
                b
                )
                )
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{Euclidean division}}\colon \quad \mathbb {Z} \times (\mathbb {Z} \setminus \{0\})&\to \mathbb {Z} \times \mathbb {Z} \\(a,b)&\mapsto (\operatorname {quotient} (a,b),\operatorname {remainder} (a,b)).\end{aligned}}}
  The codomain may also be a vector space. In this case, one talks of a vector-valued function. If the domain is contained in a Euclidean space, or more generally a manifold, a vector-valued function is often called a vector field.


== In calculus ==

The idea of function, starting in the 17th century, was fundamental to the new infinitesimal calculus. At that time, only real-valued functions of a real variable were considered, and all functions were assumed to be smooth. But the definition was soon extended to functions of several variables and to functions of a complex variable. In the second half of the 19th century, the mathematically rigorous definition of a function was introduced, and functions with arbitrary domains and codomains were defined.
Functions are now used throughout all areas of mathematics. In introductory calculus, when the word function is used without qualification, it means a real-valued function of a single real variable. The more general definition of a function is usually introduced to second or third year college students with STEM majors, and in their senior year they are introduced to calculus in a larger, more rigorous setting in courses such as real analysis and complex analysis.


=== Real function ===

A real function is a real-valued function of a real variable, that is, a function whose codomain is the field of real numbers and whose domain is a set of real numbers that contains an interval. In this section, these functions are simply called functions.
The functions that are most commonly considered in mathematics and its applications have some regularity, that is they are continuous, differentiable, and even analytic. This regularity insures that these functions can be visualized by their graphs. In this section, all functions are differentiable in some interval.
Functions enjoy pointwise operations, that is, if f and g are functions, their sum, difference and product are functions defined by

  
    
      
        
          
            
              
                (
                f
                +
                g
                )
                (
                x
                )
              
              
                
                =
                f
                (
                x
                )
                +
                g
                (
                x
                )
              
            
            
              
                (
                f
                −
                g
                )
                (
                x
                )
              
              
                
                =
                f
                (
                x
                )
                −
                g
                (
                x
                )
              
            
            
              
                (
                f
                ⋅
                g
                )
                (
                x
                )
              
              
                
                =
                f
                (
                x
                )
                ⋅
                g
                (
                x
                )
              
            
          
        
        .
      
    
    {\displaystyle {\begin{aligned}(f+g)(x)&=f(x)+g(x)\\(f-g)(x)&=f(x)-g(x)\\(f\cdot g)(x)&=f(x)\cdot g(x)\\\end{aligned}}.}
  The domains of the resulting functions are the intersection of the domains of f and g. The quotient of two functions is defined similarly by

  
    
      
        
          
            f
            g
          
        
        (
        x
        )
        =
        
          
            
              f
              (
              x
              )
            
            
              g
              (
              x
              )
            
          
        
        ,
      
    
    {\displaystyle {\frac {f}{g}}(x)={\frac {f(x)}{g(x)}},}
  but the domain of the resulting function is obtained by removing the zeros of g from the intersection of the domains of f and g.
The polynomial functions are defined by polynomials, and their domain is the whole set of real numbers. They include constant functions, linear functions and quadratic functions. Rational functions are quotients of two polynomial functions, and their domain is the real numbers with a finite number of them removed to avoid division by zero. The simplest rational function is the function 
  
    
      
        x
        ↦
        
          
            1
            x
          
        
        ,
      
    
    {\displaystyle x\mapsto {\frac {1}{x}},}
   whose graph is a hyperbola, and whose domain is the whole real line except for 0.
The derivative of a real differentiable function is a real function. An antiderivative of a continuous real function is a real function that has the original function as a derivative. For example, the function 
  
    
      
        x
        ↦
        
          
            1
            x
          
        
      
    
    {\displaystyle x\mapsto {\frac {1}{x}}}
   is continuous, and even differentiable, on the positive real numbers. Thus one antiderivative, which takes the value zero for x = 1, is a differentiable function called the natural logarithm.
A real function f is monotonic in an interval if the sign of 
  
    
      
        
          
            
              f
              (
              x
              )
              −
              f
              (
              y
              )
            
            
              x
              −
              y
            
          
        
      
    
    {\displaystyle {\frac {f(x)-f(y)}{x-y}}}
   does not depend of the choice of x and y in the interval. If the function is differentiable in the interval, it is monotonic if the sign of the derivative is constant in the interval. If a real function f is monotonic in an interval I, it has an inverse function, which is a real function with domain f(I) and image I. This is how inverse trigonometric functions are defined in terms of trigonometric functions, where the trigonometric functions are monotonic. Another example: the natural logarithm is monotonic on the positive real numbers, and its image is the whole real line; therefore it has an inverse function that is a bijection between the real numbers and the positive real numbers. This inverse is the exponential function.
Many other real functions are defined either by the implicit function theorem (the inverse function is a particular instance) or as solutions of differential equations. For example, the sine and the cosine functions are the solutions of the linear differential equation

  
    
      
        
          y
          ″
        
        +
        y
        =
        0
      
    
    {\displaystyle y''+y=0}
  such that

  
    
      
        sin
        ⁡
        0
        =
        0
        ,
        
        cos
        ⁡
        0
        =
        1
        ,
        
        
          
            
              ∂
              sin
              ⁡
              x
            
            
              ∂
              x
            
          
        
        (
        0
        )
        =
        1
        ,
        
        
          
            
              ∂
              cos
              ⁡
              x
            
            
              ∂
              x
            
          
        
        (
        0
        )
        =
        0.
      
    
    {\displaystyle \sin 0=0,\quad \cos 0=1,\quad {\frac {\partial \sin x}{\partial x}}(0)=1,\quad {\frac {\partial \cos x}{\partial x}}(0)=0.}
  


=== Vector-valued function ===

When the elements of the codomain of a function are vectors, the function is said to be a vector-valued function. These functions are particularly useful in applications, for example modeling physical properties. For example, the function that associates to each point of a fluid its velocity vector is a vector-valued function.
Some vector-valued functions are defined on a subset of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   or other spaces that share geometric or topological properties of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  , such as manifolds. These vector-valued functions are given the name vector fields.


== Function space ==

In mathematical analysis, and more specifically in functional analysis, a function space is a set of scalar-valued or vector-valued functions, which share a specific property and form a topological vector space. For example, the real smooth functions with a compact support (that is, they are zero outside some compact set) form a function space that is at the basis of the theory of distributions.
Function spaces play a fundamental role in advanced mathematical analysis, by allowing the use of their algebraic and topological properties for studying properties of functions. For example, all theorems of existence and uniqueness of solutions of ordinary or partial differential equations result of the study of function spaces.


== Multi-valued functions ==

Several methods for specifying functions of real or complex variables start from a local definition of the function at a point or on a neighbourhood of a point, and then extend by continuity the function to a much larger domain. Frequently, for a starting point 
  
    
      
        
          x
          
            0
          
        
        ,
      
    
    {\displaystyle x_{0},}
   there are several possible starting values for the function.
For example, in defining the square root as the inverse function of the square function, for any positive real number 
  
    
      
        
          x
          
            0
          
        
        ,
      
    
    {\displaystyle x_{0},}
   there are two choices for the value of the square root, one of which is positive and denoted 
  
    
      
        
          
            
              x
              
                0
              
            
          
        
        ,
      
    
    {\displaystyle {\sqrt {x_{0}}},}
   and another which is negative and denoted 
  
    
      
        −
        
          
            
              x
              
                0
              
            
          
        
        .
      
    
    {\displaystyle -{\sqrt {x_{0}}}.}
   These choices define two continuous functions, both having the nonnegative real numbers as a domain, and having either the nonnegative or the nonpositive real numbers as images. When looking at the graphs of these functions, one can see that, together, they form a single smooth curve. It is therefore often useful to consider these two square root functions as a single function that has two values for positive x, one value for 0 and no value for negative x.
In the preceding example, one choice, the positive square root, is more natural than the other. This is not the case in general. For example, let consider the implicit function that maps y to a root x of 
  
    
      
        
          x
          
            3
          
        
        −
        3
        x
        −
        y
        =
        0
      
    
    {\displaystyle x^{3}-3x-y=0}
   (see the figure on the right). For y = 0 one may choose either 
  
    
      
        0
        ,
        
          
            3
          
        
        ,
        
           or 
        
        −
        
          
            3
          
        
      
    
    {\displaystyle 0,{\sqrt {3}},{\text{ or }}-{\sqrt {3}}}
   for x. By the implicit function theorem, each choice defines a function; for the first one, the (maximal) domain is the interval [−2, 2] and the image is [−1, 1]; for the second one, the domain is [−2, ∞) and the image is [1, ∞); for the last one, the domain is (−∞, 2] and the image is (−∞, −1]. As the three graphs together form a smooth curve, and there is no reason for preferring one choice, these three functions are often considered as a single multi-valued function of y that has three values for −2 < y < 2, and only one value for y ≤ −2 and y ≥ −2.
Usefulness of the concept of multi-valued functions is clearer when considering complex functions, typically analytic functions. The domain to which a complex function may be extended by analytic continuation generally consists of almost the whole complex plane. However, when extending the domain through two different paths, one often gets different values. For example, when extending the domain of the square root function, along a path of complex numbers with positive imaginary parts, one gets i for the square root of −1; while, when extending through complex numbers with negative imaginary parts, one gets −i. There are generally two ways of solving the problem. One may define a function that is not continuous along some curve, called a branch cut. Such a function is called the principal value of the function. The other way is to consider that one has a multi-valued function, which is analytic everywhere except for isolated singularities, but whose value may ""jump"" if one follows a closed loop around a singularity. This jump is called the monodromy.


== In the foundations of mathematics and set theory ==
The definition of a function that is given in this article requires the concept of set, since the domain and the codomain of a function must be a set. This is not a problem in usual mathematics, as it is generally not difficult to consider only functions whose domain and codomain are sets, which are well defined, even if the domain is not explicitly defined. However, it is sometimes useful to consider more general functions.
For example, the singleton set may be considered as a function 
  
    
      
        x
        ↦
        {
        x
        }
        .
      
    
    {\displaystyle x\mapsto \{x\}.}
   Its domain would include all sets, and therefore would not be a set. In usual mathematics, one avoids this kind of problem by specifying a domain, which means that one has many singleton functions. However, when establishing foundations of mathematics, one may have to use functions whose domain, codomain or both are not specified, and some authors, often logicians, give precise definition for these weakly specified functions.These generalized functions may be critical in the development of a formalization of the foundations of mathematics. For example, Von Neumann–Bernays–Gödel set theory, is an extension of the set theory in which the collection of all sets is a class. This theory includes the replacement axiom, which may be stated as: If X is a set and F is a function, then F[X] is a set.


== In computer science ==

In computer programming, a function is, in general, a piece of a computer program, which implements the abstract concept of function. That is, it is a program unit that produces an output for each input. However, in many programming languages every subroutine is called a function, even when there is no output, and when the functionality consists simply of modifying some data in the computer memory.
Functional programming is the programming paradigm consisting of building programs by using only subroutines that behave like mathematical functions. For example, if_then_else is a function that takes three functions as arguments, and, depending on the result of the first function (true or false), returns the result of either the second or the third function. An important advantage of functional programming is that it makes easier program proofs, as being based on a well founded theory, the lambda calculus (see below).
Except for computer-language terminology, ""function"" has the usual mathematical meaning in computer science. In this area, a property of major interest is the computability of a function. For giving a precise meaning to this concept, and to the related concept of algorithm, several models of computation have been introduced, the old ones being general recursive functions, lambda calculus and Turing machine. The fundamental theorem of computability theory is that these three models of computation define the same set of computable functions, and that all the other models of computation that have ever been proposed define the same set of computable functions or a smaller one. The Church–Turing thesis is the claim that every philosophically acceptable definition of a computable function defines also the same functions.
General recursive functions are partial functions from integers to integers that can be defined from 

constant functions,
successor, and
projection functionsvia the operators

composition,
primitive recursion, and
minimization.Although defined only for functions from integers to integers, they can model any computable function as a consequence of the following properties:

a computation is the manipulation of finite sequences of symbols (digits of numbers, formulas, ...),
every sequence of symbols may be coded as a sequence of bits,
a bit sequence can be interpreted as the binary representation of an integer.Lambda calculus is a theory that defines computable functions without using set theory, and is the theoretical background of functional programming. It consists of terms that are either variables, function definitions (𝜆-terms), or applications of functions to terms. Terms are manipulated through some rules, (the α-equivalence, the β-reduction, and the η-conversion), which are the axioms of the theory and may be interpreted as rules of computation.
In its original form, lambda calculus does not include the concepts of domain and codomain of a function. Roughly speaking, they have been introduced in the theory under the name of type in typed lambda calculus. Most kinds of typed lambda calculi can define fewer functions than untyped lambda calculus.


== See also ==


=== Subpages ===


=== Generalizations ===


=== Related topics ===


== Notes ==


== References ==


== Sources ==
Bartle, Robert (1976). The Elements of Real Analysis (2nd ed.). Wiley. ISBN 978-0-471-05465-8. OCLC 465115030.
Bloch, Ethan D. (2011). Proofs and Fundamentals: A First Course in Abstract Mathematics. Springer. ISBN 978-1-4419-7126-5.
Cunningham, Daniel W. (2016). Set theory: A First Course. Cambridge University Press. ISBN 978-1-107-12032-7.
Gödel, Kurt (1940). The Consistency of the Continuum Hypothesis. Princeton University Press. ISBN 978-0-691-07927-1.
Halmos, Paul R. (1970). Naive Set Theory. Springer-Verlag. ISBN 978-0-387-90092-6.
Jech, Thomas (2003). Set theory (3rd ed.). Springer-Verlag. ISBN 978-3-540-44085-7.
Spivak, Michael (2008). Calculus (4th ed.). Publish or Perish. ISBN 978-0-914098-91-1.


== Further reading ==
Anton, Howard (1980). Calculus with Analytical Geometry. Wiley. ISBN 978-0-471-03248-9.
Bartle, Robert G. (1976). The Elements of Real Analysis (2nd ed.). Wiley. ISBN 978-0-471-05464-1.
Dubinsky, Ed; Harel, Guershon (1992). The Concept of Function: Aspects of Epistemology and Pedagogy. Mathematical Association of America. ISBN 978-0-88385-081-7.
Hammack, Richard (2009). ""12. Functions"" (PDF). Book of Proof. Virginia Commonwealth University. Retrieved 2012-08-01.
Husch, Lawrence S. (2001). Visual Calculus. University of Tennessee. Retrieved 2007-09-27.
Katz, Robert (1964). Axiomatic Analysis. D. C. Heath and Company.
Kleiner, Israel (1989). ""Evolution of the Function Concept: A Brief Survey"". The College Mathematics Journal. 20 (4): 282–300. CiteSeerX 10.1.1.113.6352. doi:10.2307/2686848. JSTOR 2686848.
Lützen, Jesper (2003). ""Between rigor and applications: Developments in the concept of function in mathematical analysis"".  In Porter, Roy (ed.). The Cambridge History of Science: The modern physical and mathematical sciences. Cambridge University Press. ISBN 978-0-521-57199-9. An approachable and diverting historical presentation.
Malik, M. A. (1980). ""Historical and pedagogical aspects of the definition of function"". International Journal of Mathematical Education in Science and Technology. 11 (4): 489–492. doi:10.1080/0020739800110404.
Reichenbach, Hans (1947). Elements of Symbolic Logic. Dover. ISBN 0-486-24004-5.
Ruthing, D. (1984). ""Old Intelligencer: Some definitions of the concept of function from Bernoulli, Joh. to Bourbaki, N."". Mathematical Intelligencer. 6 (4): 71–78. doi:10.1007/BF03026743. S2CID 189883712.
Thomas, George B.; Finney, Ross L. (1995). Calculus and Analytic Geometry (9th ed.). Addison-Wesley. ISBN 978-0-201-53174-9.


== External links ==

""Function"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
The Wolfram Functions Site gives formulae and visualizations of many mathematical functions.
NIST Digital Library of Mathematical Functions"
2ae5e3e3f8,Discrete mathematics,"Discrete mathematics is the study of mathematical structures that can be considered ""discrete"" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than ""continuous"" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic. By contrast, discrete mathematics excludes topics in ""continuous mathematics"" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term ""discrete mathematics"".The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.
Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in ""discrete"" steps and store data in ""discrete"" bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems.
Although the main objects of study in discrete mathematics are discrete objects, analytic methods from ""continuous"" mathematics are often employed as well.
In university curricula, ""Discrete Mathematics"" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well. Some high-school-level discrete mathematics textbooks have appeared as well. At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect.The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.


== Grand challenges, past and present ==

The history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field. In graph theory, much research was motivated by attempts to prove the four color theorem, first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).In logic, the second problem on David Hilbert's list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent. Gödel's second incompleteness theorem, proved in 1931, showed that this was not possible – at least not within arithmetic itself. Hilbert's tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. In 1970, Yuri Matiyasevich proved that this could not be done.
The need to break German codes in World War II led to advances in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England's Bletchley Park with the guidance of Alan Turing and his seminal work, On Computable Numbers. The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. The telecommunication industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.
Computational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools.
Several fields of discrete mathematics, particularly theoretical computer science, graph theory, and combinatorics, are important in addressing the challenging bioinformatics problems associated with understanding the tree of life.Currently, one of the most famous open problems in theoretical computer science is the P = NP problem, which involves the relationship between the complexity classes P and NP. The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof, along with prizes for six other mathematical problems.


== Topics in discrete mathematics ==


=== Theoretical computer science ===

Theoretical computer science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and mathematical logic. Included within theoretical computer science is the study of algorithms and data structures. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time, space, and other resources taken by computations. Automata theory and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems and representations of geometrical objects, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.


=== Information theory ===

Information theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.


=== Logic ===

Logic is the study of the principles of valid reasoning and inference, as well as of consistency, soundness, and completeness. For example, in most systems of logic (but not in intuitionistic logic) Peirce's law (((P→Q)→P)→P) is a theorem. For classical logic, it can be easily verified with a truth table. The study of mathematical proof is particularly important in logic, and has accumulated to automated theorem proving and formal verification of software.
Logical formulas are discrete structures, as are proofs, which form finite trees or, more generally, directed acyclic graph structures (with each inference step combining one or more premise branches to give a single conclusion). The truth values of logical formulas usually form a finite set, generally restricted to two values: true and false, but logic can also be continuous-valued, e.g., fuzzy logic. Concepts such as infinite proof trees or infinite derivation trees have also been studied, e.g. infinitary logic.


=== Set theory ===

Set theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.
In discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor's work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.


=== Combinatorics ===

Combinatorics studies the way in which discrete structures can be combined or arranged.
Enumerative combinatorics concentrates on counting the number of certain combinatorial objects - e.g. the twelvefold way provides a unified framework for counting permutations, combinations and partitions.
Analytic combinatorics concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.
Topological combinatorics concerns the use of techniques from topology and algebraic topology/combinatorial topology in combinatorics. 
Design theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties.
Partition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, partition theory is now considered a part of combinatorics or an independent field.
Order theory is the study of partially ordered sets, both finite and infinite.


=== Graph theory ===

Graph theory, the study of graphs and networks, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right. Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of topology, e.g. knot theory. Algebraic graph theory has close links with group theory and topological graph theory has close links to topology. There are also continuous graphs; however, for the most part, research in graph theory falls within the domain of discrete mathematics. 


=== Number theory ===

Number theory is concerned with the properties of numbers in general, particularly integers. It has applications to cryptography and cryptanalysis, particularly with regard to modular arithmetic, diophantine equations, linear and quadratic congruences, prime numbers and primality testing. Other discrete aspects of number theory include geometry of numbers. In analytic number theory, techniques from continuous mathematics are also used. Topics that go beyond discrete objects include transcendental numbers, diophantine approximation, p-adic analysis and function fields.


=== Algebraic structures ===

Algebraic structures occur as both discrete examples and continuous examples. Discrete algebras include: boolean algebra used in logic gates and programming; relational algebra used in databases; discrete and finite versions of groups, rings and fields are important in algebraic coding theory; discrete semigroups and monoids appear in the theory of formal languages.


=== Discrete analogues of continuous mathematics ===
There are many concepts and theories in continuous mathematics which have discrete versions, such as discrete calculus, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, discrete optimization, discrete probability theory, discrete probability distribution, difference equations, discrete dynamical systems, and discrete vector measures.


==== Calculus of finite differences, discrete analysis, and discrete calculus ====
In discrete calculus and the calculus of finite differences, a function defined on an interval of the integers is usually called a sequence. A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a recurrence relation or difference equation. Difference equations are similar to differential equations, but replace differentiation by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance, where there are integral transforms in harmonic analysis for studying continuous functions or analogue signals, there are discrete transforms for discrete functions or digital signals. As well as discrete metric spaces, there are more general discrete topological spaces, finite metric spaces, finite topological spaces. 
The time scale calculus is a unification of the theory of difference equations with that of differential equations, which has applications to fields requiring simultaneous modelling of discrete and continuous data. Another way of modeling such a situation is the notion of hybrid dynamical systems.


==== Discrete geometry ====
Discrete geometry and combinatorial geometry are about combinatorial properties of discrete collections of geometrical objects. A long-standing topic in discrete geometry is tiling of the plane. 
In algebraic geometry, the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field, and letting subvarieties or spectra of other rings provide the curves that lie in that space. Although the space in which the curves appear has a finite number of points, the curves are not so much sets of points as analogues of curves in continuous settings. For example, every point of the form 
  
    
      
        V
        (
        x
        −
        c
        )
        ⊂
        Spec
        ⁡
        K
        [
        x
        ]
        =
        
          
            A
          
          
            1
          
        
      
    
    {\displaystyle V(x-c)\subset \operatorname {Spec} K[x]=\mathbb {A} ^{1}}
   for 
  
    
      
        K
      
    
    {\displaystyle K}
   a field can be studied either as 
  
    
      
        Spec
        ⁡
        K
        [
        x
        ]
        
          /
        
        (
        x
        −
        c
        )
        ≅
        Spec
        ⁡
        K
      
    
    {\displaystyle \operatorname {Spec} K[x]/(x-c)\cong \operatorname {Spec} K}
  , a point, or as the spectrum 
  
    
      
        Spec
        ⁡
        K
        [
        x
        
          ]
          
            (
            x
            −
            c
            )
          
        
      
    
    {\displaystyle \operatorname {Spec} K[x]_{(x-c)}}
   of the local ring at (x-c), a point together with a neighborhood around it. Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space, making many features of calculus applicable even in finite settings.


==== Discrete modelling ====
In applied mathematics, discrete modelling is the discrete analogue of continuous modelling. In discrete modelling, discrete formulae are fit to data. A common method in this form of modelling is to use recurrence relation. Discretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. Numerical analysis provides an important example.


== See also ==
Outline of discrete mathematics
Cyberchase, a show that teaches Discrete Mathematics to children


== References ==


== Further reading ==


== External links ==

Discrete mathematics Archived 2011-08-29 at the Wayback Machine at the utk.edu Mathematics Archives, providing links to syllabi, tutorials, programs, etc.
Iowa Central: Electrical Technologies Program Discrete mathematics for Electrical engineering."
64818e560f,Applied mathematics,"Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, finance, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term ""applied mathematics"" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models.
In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics. 


== History ==

Historically, applied mathematics consisted principally of applied analysis, most notably differential equations; approximation theory (broadly construed, to include representations, asymptotic methods, variational methods, and numerical analysis); and applied probability. These areas of mathematics related directly to the development of Newtonian physics, and in fact, the distinction between mathematicians and physicists was not sharply drawn before the mid-19th century. This history left a pedagogical legacy in the United States: until the early 20th century, subjects such as classical mechanics were often taught in applied mathematics departments at American universities rather than in physics departments, and fluid mechanics may still be taught in applied mathematics departments. Engineering and computer science departments have traditionally made use of applied mathematics.


== Divisions ==

Today, the term ""applied mathematics"" is used in a broader sense. It includes the classical areas noted above as well as other areas that have become increasingly important in applications. Even fields such as number theory that are part of pure mathematics are now important in applications (such as cryptography), though they are not generally considered to be part of the field of applied mathematics per se.
There is no consensus as to what the various branches of applied mathematics are. Such categorizations are made difficult by the way mathematics and science change over time, and also by the way universities organize departments, courses, and degrees.
Many mathematicians distinguish between ""applied mathematics"", which is concerned with mathematical methods, and the ""applications of mathematics"" within science and engineering. A biologist using a population model and applying known mathematics would not be doing applied mathematics, but rather using it; however, mathematical biologists have posed problems that have stimulated the growth of pure mathematics. Mathematicians such as Poincaré and Arnold deny the existence of ""applied mathematics"" and claim that there are only ""applications of mathematics."" Similarly, non-mathematicians blend applied mathematics and applications of mathematics. The use and development of mathematics to solve industrial problems is also called ""industrial mathematics"".The success of modern numerical mathematical methods and software has led to the emergence of computational mathematics, computational science, and computational engineering, which use high-performance computing for the simulation of phenomena and the solution of problems in the sciences and engineering. These are often considered interdisciplinary.


=== Applicable mathematics ===
Sometimes, the term applicable mathematics is used to distinguish between the traditional applied mathematics that developed alongside physics and the many areas of mathematics that are applicable to real-world problems today, although there is no consensus as to a precise definition.Mathematicians often distinguish between ""applied mathematics"" on the one hand, and the ""applications of mathematics"" or ""applicable mathematics"" both within and outside of science and engineering, on the other. Some mathematicians emphasize the term applicable mathematics to separate or delineate the traditional applied areas from new applications arising from fields that were previously seen as pure mathematics. For example, from this viewpoint, an ecologist or geographer using population models and applying known mathematics would not be doing applied, but rather applicable, mathematics. Even fields such as number theory that are part of pure mathematics are now important in applications (such as cryptography), though they are not generally considered to be part of the field of applied mathematics per se. Such descriptions can lead to applicable mathematics being seen as a collection of mathematical methods such as real analysis, linear algebra, mathematical modelling, optimisation, combinatorics, probability and statistics, which are useful in areas outside traditional mathematics and not specific to mathematical physics.
Other authors prefer describing applicable mathematics as a union of ""new"" mathematical applications with the traditional fields of applied mathematics. With this outlook, the terms applied mathematics and applicable mathematics are thus interchangeable.


== Utility ==

Historically, mathematics was most important in the natural sciences and engineering. However, since World War II, fields outside the physical sciences have spawned the creation of new areas of mathematics, such as game theory and social choice theory, which grew out of economic considerations. Further, the utilization and development of mathematical methods expanded into other areas leading to the creation of new fields such as mathematical finance and data science.
The advent of the computer has enabled new applications: studying and using the new computer technology itself (computer science) to study problems arising in other areas of science (computational science) as well as the mathematics of computation (for example, theoretical computer science, computer algebra, numerical analysis). Statistics is probably the most widespread mathematical science used in the social sciences.


== Status in academic departments ==
Academic institutions are not consistent in the way they group and label courses, programs, and degrees in applied mathematics. At some schools, there is a single mathematics department, whereas others have separate departments for Applied Mathematics and (Pure) Mathematics. It is very common for Statistics departments to be separated at schools with graduate programs, but many undergraduate-only institutions include statistics under the mathematics department.
Many applied mathematics programs (as opposed to departments) consist primarily of cross-listed courses and jointly appointed faculty in departments representing applications. Some Ph.D. programs in applied mathematics require little or no coursework outside mathematics, while others require substantial coursework in a specific area of application. In some respects this difference reflects the distinction between ""application of mathematics"" and ""applied mathematics"".
Some universities in the U.K. host departments of Applied Mathematics and Theoretical Physics, but it is now much less common to have separate departments of pure and applied mathematics. A notable exception to this is the Department of Applied Mathematics and Theoretical Physics at the University of Cambridge, housing the Lucasian Professor of Mathematics whose past holders include Isaac Newton, Charles Babbage, James Lighthill, Paul Dirac, and Stephen Hawking.

Schools with separate applied mathematics departments range from Brown University, which has a large Division of Applied Mathematics that offers degrees through the doctorate, to Santa Clara University, which offers only the M.S. in applied mathematics. Research universities dividing their mathematics department into pure and applied sections include MIT. Students in this program also learn another skill (computer science, engineering, physics, pure math, etc.) to supplement their applied math skills.


== Associated mathematical sciences ==

Applied mathematics is associated with the following mathematical sciences:


=== Engineering and technological engineering ===
With applications of applied geometry together with applied chemistry. 


=== Scientific computing ===
Scientific computing includes applied mathematics (especially numerical analysis), computing science (especially high-performance computing), and mathematical modelling in a scientific discipline.


=== Computer science ===
Computer science relies on logic, algebra, discrete mathematics such as graph theory, and combinatorics.


=== Operations research and management science ===
Operations research and management science are often taught in faculties of engineering, business, and public policy.


=== Statistics ===
Applied mathematics has substantial overlap with the discipline of statistics. Statistical theorists study and improve statistical procedures with mathematics, and statistical research often raises mathematical questions. Statistical theory relies on probability and decision theory, and makes extensive use of scientific computing, analysis, and optimization; for the design of experiments, statisticians use algebra and combinatorial design. Applied mathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities).


=== Actuarial science ===
Actuarial science applies probability, statistics, and economic theory to assess risk in insurance, finance and other industries and professions.


=== Mathematical economics ===
Mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics. The applied methods usually refer to nontrivial mathematical techniques or approaches. Mathematical economics is based on statistics, probability, mathematical programming (as well as other computational methods), operations research, game theory, and some methods from mathematical analysis. In this regard, it resembles (but is distinct from) financial mathematics, another part of applied mathematics.According to the Mathematics Subject Classification (MSC), mathematical economics falls into the Applied mathematics/other classification of category 91:

Game theory, economics, social and behavioral scienceswith MSC2010 classifications for 'Game theory' at codes 91Axx and for 'Mathematical economics' at codes 91Bxx.


=== Other disciplines ===
The line between applied mathematics and specific areas of application is often blurred. Many universities teach mathematical and statistical courses outside the respective departments, in departments and areas including business, engineering, physics, chemistry, psychology, biology, computer science, scientific computation, and mathematical physics.


== See also ==
Engineering mathematics
Society for Industrial and Applied Mathematics


== References ==


== Further reading ==


=== Applicable mathematics ===
The Morehead Journal of Applicable Mathematics hosted by Morehead State University
Series on Concrete and Applicable Mathematics by World Scientific
Handbook of Applicable Mathematics Series by Walter Ledermann


== External links ==

 Media related to Applied mathematics at Wikimedia Commons
The Society for Industrial and Applied Mathematics (SIAM) is a professional society dedicated to promoting the interaction between mathematics and other scientific and technical communities. Aside from organizing and sponsoring numerous conferences, SIAM is a major publisher of research journals and books in applied mathematics.
The Applicable Mathematics Research Group at Notre Dame University
Centre for Applicable Mathematics at Liverpool Hope University
Applicable Mathematics research group at Glasgow Caledonian University"
ee36fb968b,Matrix (mathematics),"In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.
For example, 

is a matrix with two rows and three columns. This is often referred to as a ""two by three matrix"", a ""
  
    
      
        2
        ×
        3
      
    
    {\displaystyle 2\times 3}
   matrix"", or a matrix of dimension 
  
    
      
        2
        ×
        3
      
    
    {\displaystyle 2\times 3}
  .
Without further specifications, matrices represent linear maps, and allow explicit computations in linear algebra. Therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. For example, matrix multiplication represents composition of linear maps.
Not all matrices are related to linear algebra. This is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices. This article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such.
Square matrices, matrices with the same number of rows and columns, play a major role in matrix theory. Square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. The determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant.
In geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. In numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this often involves computing with matrices of huge dimension. Matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis.
Matrix theory is the branch of mathematics that focuses on the study of matrices. It was initially a sub-branch of linear algebra, but soon grew to include subjects related to graph theory, algebra, combinatorics and statistics.


== Definition ==
A matrix is a rectangular array of numbers (or other mathematical objects), called the entries of the matrix. Matrices are subject to standard operations such as addition and multiplication. Most commonly, a matrix over a field F is a rectangular array of elements of F. A real matrix and a complex matrix are matrices whose entries are respectively real numbers or complex numbers. More general types of entries are discussed below. For instance, this is a real matrix:

  
    
      
        
          A
        
        =
        
          
            [
            
              
                
                  −
                  1.3
                
                
                  0.6
                
              
              
                
                  20.4
                
                
                  5.5
                
              
              
                
                  9.7
                
                
                  −
                  6.2
                
              
            
            ]
          
        
        .
      
    
    {\displaystyle \mathbf {A} ={\begin{bmatrix}-1.3&0.6\\20.4&5.5\\9.7&-6.2\end{bmatrix}}.}
  The numbers, symbols, or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.


=== Size ===
The size of a matrix is defined by the number of rows and columns it contains. There is no limit to the numbers of rows and columns a matrix (in the usual sense) can have as long as they are positive integers. A matrix with m rows and n columns is called an m × n matrix, or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.
Matrices with a single row are called row vectors, and those with a single column are called column vectors. A matrix with the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.


== Notation ==
The specifics of symbolic matrix notation vary widely, with some prevailing trends.  Matrices are commonly written in box brackets or parentheses, so that an 
  
    
      
        m
        ×
        n
      
    
    {\displaystyle m\times n}
   matrix A represented as

This may be abbreviated by writing only a single generic term, possibly along with indices, as in

or 
  
    
      
        
          A
        
        =
        (
        
          a
          
            i
            ,
            j
          
        
        
          )
          
            1
            ≤
            i
            ,
            j
            ≤
            n
          
        
      
    
    {\displaystyle \mathbf {A} =(a_{i,j})_{1\leq i,j\leq n}}
   in the case that 
  
    
      
        n
        =
        m
      
    
    {\displaystyle n=m}
  .
Matrices are usually symbolized using upper-case letters (such as A in the examples above), while the corresponding lower-case letters, with two subscript indices (e.g., a11, or a1,1), represent the entries. In addition to using upper-case letters to symbolize matrices, many authors use a special typographical style, commonly boldface upright (non-italic), to further distinguish matrices from other mathematical objects. An alternative notation involves the use of a double-underline with the variable name, with or without boldface style, as in 
  
    
      
        
          
            
              A
              _
            
            _
          
        
      
    
    {\displaystyle {\underline {\underline {A}}}}
  .
The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j or (i, j) entry of the matrix, and commonly denoted by ai,j or aij.  Alternative notations for that entry are A[i,j] and Ai,j. For example, the (1, 3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):

  
    
      
        
          A
        
        =
        
          
            [
            
              
                
                  4
                
                
                  −
                  7
                
                
                  
                    
                      5
                    
                  
                
                
                  0
                
              
              
                
                  −
                  2
                
                
                  0
                
                
                  11
                
                
                  8
                
              
              
                
                  19
                
                
                  1
                
                
                  −
                  3
                
                
                  12
                
              
            
            ]
          
        
      
    
    {\displaystyle \mathbf {A} ={\begin{bmatrix}4&-7&\color {red}{5}&0\\-2&0&11&8\\19&1&-3&12\end{bmatrix}}}
  Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by the formula aij = i − j.

  
    
      
        
          A
        
        =
        
          
            [
            
              
                
                  0
                
                
                  −
                  1
                
                
                  −
                  2
                
                
                  −
                  3
                
              
              
                
                  1
                
                
                  0
                
                
                  −
                  1
                
                
                  −
                  2
                
              
              
                
                  2
                
                
                  1
                
                
                  0
                
                
                  −
                  1
                
              
            
            ]
          
        
      
    
    {\displaystyle \mathbf {A} ={\begin{bmatrix}0&-1&-2&-3\\1&0&-1&-2\\2&1&0&-1\end{bmatrix}}}
  In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i−j], or A = ((i−j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or indicated using m × n as a subscript. For instance, the matrix A above is 3 × 4, and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.
Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-by-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1. This article follows the more common convention in mathematical writing where enumeration starts from 1.
An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A.
The set of all m-by-n real matrices is often denoted 
  
    
      
        
          
            M
          
        
        (
        m
        ,
        n
        )
        ,
      
    
    {\displaystyle {\mathcal {M}}(m,n),}
   or 
  
    
      
        
          
            
              M
            
          
          
            m
            ×
            n
          
        
        
          R
        
        .
      
    
    {\displaystyle {\mathcal {M}}_{m\times n}\mathbb {R} .}
   The set of all m-by-n matrices over another field or over a ring R, is similarly denoted 
  
    
      
        
          
            M
          
        
        (
        m
        ,
        n
        ,
        R
        )
        ,
      
    
    {\displaystyle {\mathcal {M}}(m,n,R),}
   or 
  
    
      
        
          
            
              M
            
          
          
            m
            ×
            n
          
        
        (
        R
        )
        .
      
    
    {\displaystyle {\mathcal {M}}_{m\times n}(R).}
   If m = n, that is, in the case of square matrices, one does not repeat the dimension: 
  
    
      
        
          
            M
          
        
        (
        n
        ,
        R
        )
        ,
      
    
    {\displaystyle {\mathcal {M}}(n,R),}
   or 
  
    
      
        
          
            
              M
            
          
          
            n
          
        
        (
        R
        )
        .
      
    
    {\displaystyle {\mathcal {M}}_{n}(R).}
   Often, 
  
    
      
        M
      
    
    {\displaystyle M}
   is used in place of 
  
    
      
        
          
            M
          
        
        .
      
    
    {\displaystyle {\mathcal {M}}.}
  


== Basic operations ==
There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.


=== Addition, scalar multiplication, and transposition ===

Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.
The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.


=== Matrix multiplication ===

Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:

  
    
      
        [
        
          A
          B
        
        
          ]
          
            i
            ,
            j
          
        
        =
        
          a
          
            i
            ,
            1
          
        
        
          b
          
            1
            ,
            j
          
        
        +
        
          a
          
            i
            ,
            2
          
        
        
          b
          
            2
            ,
            j
          
        
        +
        ⋯
        +
        
          a
          
            i
            ,
            n
          
        
        
          b
          
            n
            ,
            j
          
        
        =
        
          ∑
          
            r
            =
            1
          
          
            n
          
        
        
          a
          
            i
            ,
            r
          
        
        
          b
          
            r
            ,
            j
          
        
        ,
      
    
    {\displaystyle [\mathbf {AB} ]_{i,j}=a_{i,1}b_{1,j}+a_{i,2}b_{2,j}+\cdots +a_{i,n}b_{n,j}=\sum _{r=1}^{n}a_{i,r}b_{r,j},}
  where 1 ≤ i ≤ m and 1 ≤ j ≤ p. For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:

  
    
      
        
          
            
              
                
                  
                    [
                    
                      
                        
                          
                            
                              2
                              _
                            
                          
                        
                        
                          
                            
                              3
                              _
                            
                          
                        
                        
                          
                            
                              4
                              _
                            
                          
                        
                      
                      
                        
                          1
                        
                        
                          0
                        
                        
                          0
                        
                      
                    
                    ]
                  
                
                
                  
                    [
                    
                      
                        
                          0
                        
                        
                          
                            
                              1000
                              _
                            
                          
                        
                      
                      
                        
                          1
                        
                        
                          
                            
                              100
                              _
                            
                          
                        
                      
                      
                        
                          0
                        
                        
                          
                            
                              10
                              _
                            
                          
                        
                      
                    
                    ]
                  
                
              
              
                
                =
                
                  
                    [
                    
                      
                        
                          3
                        
                        
                          
                            
                              2340
                              _
                            
                          
                        
                      
                      
                        
                          0
                        
                        
                          1000
                        
                      
                    
                    ]
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\begin{bmatrix}{\underline {2}}&{\underline {3}}&{\underline {4}}\\1&0&0\\\end{bmatrix}}{\begin{bmatrix}0&{\underline {1000}}\\1&{\underline {100}}\\0&{\underline {10}}\\\end{bmatrix}}&={\begin{bmatrix}3&{\underline {2340}}\\0&1000\\\end{bmatrix}}.\end{aligned}}}
  Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA + CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined. The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they generally need not be equal, that is:

AB ≠ BA,In other words, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers, whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:

  
    
      
        
          
            [
            
              
                
                  1
                
                
                  2
                
              
              
                
                  3
                
                
                  4
                
              
            
            ]
          
        
        
          
            [
            
              
                
                  0
                
                
                  1
                
              
              
                
                  0
                
                
                  0
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  0
                
                
                  1
                
              
              
                
                  0
                
                
                  3
                
              
            
            ]
          
        
        ,
      
    
    {\displaystyle {\begin{bmatrix}1&2\\3&4\\\end{bmatrix}}{\begin{bmatrix}0&1\\0&0\\\end{bmatrix}}={\begin{bmatrix}0&1\\0&3\\\end{bmatrix}},}
  whereas

  
    
      
        
          
            [
            
              
                
                  0
                
                
                  1
                
              
              
                
                  0
                
                
                  0
                
              
            
            ]
          
        
        
          
            [
            
              
                
                  1
                
                
                  2
                
              
              
                
                  3
                
                
                  4
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  3
                
                
                  4
                
              
              
                
                  0
                
                
                  0
                
              
            
            ]
          
        
        .
      
    
    {\displaystyle {\begin{bmatrix}0&1\\0&0\\\end{bmatrix}}{\begin{bmatrix}1&2\\3&4\\\end{bmatrix}}={\begin{bmatrix}3&4\\0&0\\\end{bmatrix}}.}
  Besides the ordinary matrix multiplication just described, other less frequently used operations on matrices that can be considered forms of multiplication also exist, such as the Hadamard product and the Kronecker product. They arise in solving matrix equations such as the Sylvester equation.


=== Row operations ===

There are three types of row operations:

row addition, that is adding a row to another.
row multiplication, that is multiplying all entries of a row by a non-zero constant;
row switching, that is interchanging two rows of a matrix;These operations are used in several ways, including solving linear equations and finding matrix inverses.


=== Submatrix ===
A submatrix of a matrix is obtained by deleting any collection of rows and/or columns. For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:

  
    
      
        
          A
        
        =
        
          
            [
            
              
                
                  1
                
                
                  
                    
                      2
                    
                  
                
                
                  3
                
                
                  4
                
              
              
                
                  5
                
                
                  
                    
                      6
                    
                  
                
                
                  7
                
                
                  8
                
              
              
                
                  
                    
                      9
                    
                  
                
                
                  
                    
                      10
                    
                  
                
                
                  
                    
                      11
                    
                  
                
                
                  
                    
                      12
                    
                  
                
              
            
            ]
          
        
        →
        
          
            [
            
              
                
                  1
                
                
                  3
                
                
                  4
                
              
              
                
                  5
                
                
                  7
                
                
                  8
                
              
            
            ]
          
        
        .
      
    
    {\displaystyle \mathbf {A} ={\begin{bmatrix}1&\color {red}{2}&3&4\\5&\color {red}{6}&7&8\\\color {red}{9}&\color {red}{10}&\color {red}{11}&\color {red}{12}\end{bmatrix}}\rightarrow {\begin{bmatrix}1&3&4\\5&7&8\end{bmatrix}}.}
  The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.A principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain. Other authors define a principal submatrix as one in which the first k rows and columns, for some number k, are the ones that remain; this type of submatrix has also been called a leading principal submatrix.


== Linear equations ==

Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equation

  
    
      
        
          A
          x
        
        =
        
          b
        
      
    
    {\displaystyle \mathbf {Ax} =\mathbf {b} }
  is equivalent to the system of linear equations

  
    
      
        
          
            
              
                
                  a
                  
                    1
                    ,
                    1
                  
                
                
                  x
                  
                    1
                  
                
                +
                
                  a
                  
                    1
                    ,
                    2
                  
                
                
                  x
                  
                    2
                  
                
                +
              
              
                
                ⋯
                +
                
                  a
                  
                    1
                    ,
                    n
                  
                
                
                  x
                  
                    n
                  
                
                =
                
                  b
                  
                    1
                  
                
              
            
            
              
              
                 
                 
                ⋮
              
            
            
              
                
                  a
                  
                    m
                    ,
                    1
                  
                
                
                  x
                  
                    1
                  
                
                +
                
                  a
                  
                    m
                    ,
                    2
                  
                
                
                  x
                  
                    2
                  
                
                +
              
              
                
                ⋯
                +
                
                  a
                  
                    m
                    ,
                    n
                  
                
                
                  x
                  
                    n
                  
                
                =
                
                  b
                  
                    m
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}a_{1,1}x_{1}+a_{1,2}x_{2}+&\cdots +a_{1,n}x_{n}=b_{1}\\&\ \ \vdots \\a_{m,1}x_{1}+a_{m,2}x_{2}+&\cdots +a_{m,n}x_{n}=b_{m}\end{aligned}}}
  Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, then this can be done by writing

  
    
      
        
          x
        
        =
        
          
            A
          
          
            −
            1
          
        
        
          b
        
      
    
    {\displaystyle \mathbf {x} =\mathbf {A} ^{-1}\mathbf {b} }
  where A−1 is the inverse matrix of A. If A has no inverse, solutions—if any—can be found using its generalized inverse.


== Linear transformations ==

Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.
For example, the 2×2 matrix

  
    
      
        
          A
        
        =
        
          
            [
            
              
                
                  a
                
                
                  c
                
              
              
                
                  b
                
                
                  d
                
              
            
            ]
          
        
      
    
    {\displaystyle \mathbf {A} ={\begin{bmatrix}a&c\\b&d\end{bmatrix}}}
  can be viewed as the transform of the unit square into a parallelogram with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d). The parallelogram pictured at the right is obtained by multiplying A with each of the column vectors 
  
    
      
        
          
            [
            
              
                
                  0
                
              
              
                
                  0
                
              
            
            ]
          
        
        ,
        
          
            [
            
              
                
                  1
                
              
              
                
                  0
                
              
            
            ]
          
        
        ,
        
          
            [
            
              
                
                  1
                
              
              
                
                  1
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}0\\0\end{bmatrix}},{\begin{bmatrix}1\\0\end{bmatrix}},{\begin{bmatrix}1\\1\end{bmatrix}}}
  , and 
  
    
      
        
          
            [
            
              
                
                  0
                
              
              
                
                  1
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}0\\1\end{bmatrix}}}
   in turn. These vectors define the vertices of the unit square.
The following table shows several 2×2 real matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.

Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps: if a k-by-m matrix B represents another linear map g: Rm → Rk, then the composition g ∘ f is represented by BA since

(g ∘ f)(x) = g(f(x)) = g(Ax) = B(Ax) = (BA)x.The last equality follows from the above-mentioned associativity of matrix multiplication.
The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors. Equivalently it is the dimension of the image of the linear map represented by A. The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.


== Square matrix ==

A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied.
The entries aii form the main diagonal of a square matrix. They lie on the imaginary line that runs from the top left corner to the bottom right corner of the matrix.


=== Main types ===


==== Diagonal and triangular matrix ====
If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.


==== Identity matrix ====

The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,

  
    
      
        
          
            I
          
          
            1
          
        
        =
        
          
            [
            
              
                
                  1
                
              
            
            ]
          
        
        ,
         
        
          
            I
          
          
            2
          
        
        =
        
          
            [
            
              
                
                  1
                
                
                  0
                
              
              
                
                  0
                
                
                  1
                
              
            
            ]
          
        
        ,
         
        …
        ,
         
        
          
            I
          
          
            n
          
        
        =
        
          
            [
            
              
                
                  1
                
                
                  0
                
                
                  ⋯
                
                
                  0
                
              
              
                
                  0
                
                
                  1
                
                
                  ⋯
                
                
                  0
                
              
              
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋱
                
                
                  ⋮
                
              
              
                
                  0
                
                
                  0
                
                
                  ⋯
                
                
                  1
                
              
            
            ]
          
        
      
    
    {\displaystyle \mathbf {I} _{1}={\begin{bmatrix}1\end{bmatrix}},\ \mathbf {I} _{2}={\begin{bmatrix}1&0\\0&1\end{bmatrix}},\ \ldots ,\ \mathbf {I} _{n}={\begin{bmatrix}1&0&\cdots &0\\0&1&\cdots &0\\\vdots &\vdots &\ddots &\vdots \\0&0&\cdots &1\end{bmatrix}}}
  It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:

AIn = ImA = A for any m-by-n matrix A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.


==== Symmetric or skew-symmetric matrix ====
A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.
By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real. This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.


==== Invertible matrix and its inverse ====
A square matrix A is called invertible or non-singular if there exists a matrix B such that

AB = BA = In ,where In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.


==== Definite matrix ====
A symmetric real matrix A is called positive-definite if the associated quadratic form

f (x) = xTA xhas a positive value for every nonzero vector x in Rn. If f (x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite. If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.
A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible. The table at the right shows two possibilities for 2-by-2 matrices.
Allowing as input two different vectors instead yields the bilinear form associated to A:
BA (x, y) = xTAy.In the case of complex matrices, the same terminology and result apply, with symmetric matrix, quadratic form, bilinear form, and transpose xT replaced respectively by  Hermitian matrix, Hermitian form, sesquilinear form, and conjugate transpose xH.


==== Orthogonal matrix ====

An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:

  
    
      
        
          
            A
          
          
            
              T
            
          
        
        =
        
          
            A
          
          
            −
            1
          
        
        ,
        
      
    
    {\displaystyle \mathbf {A} ^{\mathrm {T} }=\mathbf {A} ^{-1},\,}
  which entails

  
    
      
        
          
            A
          
          
            
              T
            
          
        
        
          A
        
        =
        
          A
        
        
          
            A
          
          
            
              T
            
          
        
        =
        
          
            I
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbf {A} ^{\mathrm {T} }\mathbf {A} =\mathbf {A} \mathbf {A} ^{\mathrm {T} }=\mathbf {I} _{n},}
  where In is the identity matrix of size n.
An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation without reflection, i.e., the transformation preserves the orientation of the transformed structure, while every orthogonal matrix with determinant -1 reverses the orientation, i.e., is a composition of a pure reflection and a (possibly null) rotation. The identity matrices have determinant 1, and are pure rotations by an angle zero.
The complex analogue of an orthogonal matrix is a unitary matrix.


=== Main operations ===


==== Trace ====
The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:

tr(AB) = tr(BA).This is immediate from the definition of matrix multiplication:

  
    
      
        tr
        ⁡
        (
        
          A
          B
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          a
          
            i
            j
          
        
        
          b
          
            j
            i
          
        
        =
        tr
        ⁡
        (
        
          B
          A
        
        )
        .
      
    
    {\displaystyle \operatorname {tr} (\mathbf {AB} )=\sum _{i=1}^{m}\sum _{j=1}^{n}a_{ij}b_{ji}=\operatorname {tr} (\mathbf {BA} ).}
  It follows that the trace of the product of more than two matrices is independent of cyclic permutations of the matrices, however this does not in general apply for arbitrary permutations (for example, tr(ABC) ≠ tr(BAC), in general). Also, the trace of a matrix is equal to that of its transpose, that is,

tr(A) = tr(AT).


==== Determinant ====

The determinant of a square matrix A (denoted det(A) or |A|) is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.
The determinant of 2-by-2 matrices is given by

  
    
      
        det
        
          
            [
            
              
                
                  a
                
                
                  b
                
              
              
                
                  c
                
                
                  d
                
              
            
            ]
          
        
        =
        a
        d
        −
        b
        c
        .
      
    
    {\displaystyle \det {\begin{bmatrix}a&b\\c&d\end{bmatrix}}=ad-bc.}
  The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.The determinant of a product of square matrices equals the product of their determinants:

det(AB) = det(A) · det(B), or using alternate notation:
|AB| = |A| · |B|.Adding a multiple of any row to another row, or a multiple of any column to another column does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1. Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices, the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices. This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.


==== Eigenvalues and eigenvectors ====

A number λ and a non-zero vector v satisfying

  
    
      
        
          A
        
        
          v
        
        =
        λ
        
          v
        
      
    
    {\displaystyle \mathbf {A} \mathbf {v} =\lambda \mathbf {v} }
  are called an eigenvalue and an eigenvector of A, respectively. The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent to

  
    
      
        det
        (
        
          A
        
        −
        λ
        
          I
        
        )
        =
        0.
      
    
    {\displaystyle \det(\mathbf {A} -\lambda \mathbf {I} )=0.}
  The polynomial pA in an indeterminate X given by evaluation of the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix. They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.


== Computational aspects ==
Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.To choose the most appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra. As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.
Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. Calculating the matrix product of two n-by-n matrices using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this ""naive"" algorithm; it needs only n2.807 multiplications. A refined approach also incorporates specific features of the computing devices.
In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace expansion (adj(A) denotes the adjugate matrix of A)

A−1 = adj(A) / det(A)may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.Most computer programming languages support arrays but are not designed with built-in commands for matrices. Instead, available external libraries provide matrix operations on arrays, in nearly all currently used programming languages. Matrix manipulation was among the earliest numerical applications of computers. The original Dartmouth BASIC had built-in commands for matrix arithmetic on arrays from its second edition implementation in 1964. As early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.


== Decomposition ==

There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank, or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.
The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U). Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form. Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.

The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix. If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right. Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via

An = (VDV−1)n = VDV−1VDV−1...VDV−1 = VDnV−1and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices. To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.


== Abstract algebraic aspects and generalizations ==
Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension is tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realized as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers. Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.


=== Matrices with more general entries ===
This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication, and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance, they may be complex in the case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.
More generally, matrices with entries in a ring R are widely used in mathematics. Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) (also denoted Mn(R)) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn. If the ring R is commutative, that is, its multiplication is commutative, then the ring M(n, R) is also an associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible. Matrices over superrings are called supermatrices.Matrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be square matrices, and thus need not be members of any ring; but their sizes must fulfill certain compatibility conditions.


=== Relationship to linear maps ===
Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such that

  
    
      
        f
        (
        
          
            v
          
          
            j
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          a
          
            i
            ,
            j
          
        
        
          
            w
          
          
            i
          
        
        
        
          
            for
          
        
         
        j
        =
        1
        ,
        …
        ,
        n
        .
      
    
    {\displaystyle f(\mathbf {v} _{j})=\sum _{i=1}^{m}a_{i,j}\mathbf {w} _{i}\qquad {\mbox{for}}\ j=1,\ldots ,n.}
  In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices. Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.These properties can be restated more naturally: the category of all matrices with entries in a field 
  
    
      
        k
      
    
    {\displaystyle k}
   with multiplication as composition is equivalent to the category of finite-dimensional vector spaces and linear maps over this field.
More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.


=== Matrix groups ===

A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements. A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group. Since a group every element must be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.
Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group. Orthogonal matrices, determined by the condition

MTM = I,form the orthogonal group. Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.
Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group. General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.


=== Infinite matrices ===
It is also possible to consider matrices with infinitely many rows and/or columns even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication, and transposition can still be defined without problem; however, matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.
If R is any ring with unity, then the ring of endomorphisms of 
  
    
      
        M
        =
        
          ⨁
          
            i
            ∈
            I
          
        
        R
      
    
    {\displaystyle M=\bigoplus _{i\in I}R}
   as a right R module is isomorphic to the ring of column finite matrices 
  
    
      
        
          
            C
            F
            M
          
          
            I
          
        
        (
        R
        )
      
    
    {\displaystyle \mathrm {CFM} _{I}(R)}
   whose entries are indexed by 
  
    
      
        I
        ×
        I
      
    
    {\displaystyle I\times I}
  , and whose columns each contain only finitely many nonzero entries. The endomorphisms of M considered as a left R module result in an analogous object, the row finite matrices 
  
    
      
        
          
            R
            F
            M
          
          
            I
          
        
        (
        R
        )
      
    
    {\displaystyle \mathrm {RFM} _{I}(R)}
   whose rows each only have finitely many nonzero entries.
If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries because each of those columns does. Products of two matrices of the given type are well defined (provided that the column-index and row-index sets match), are of the same type, and correspond to the composition of linear maps.
If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously, the matrices whose row sums are absolutely convergent series also form a ring.
Infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that must be imposed. However, the explicit point of view of matrices tends to obfuscate the matter, and the abstract and more powerful tools of functional analysis can be used instead.


=== Empty matrix ===
An empty matrix is a matrix in which the number of rows or columns (or both) is zero. Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite-dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.


== Applications ==
There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose. Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.Complex numbers can be represented by particular real 2-by-2 matrices via

  
    
      
        a
        +
        i
        b
        ↔
        
          
            [
            
              
                
                  a
                
                
                  −
                  b
                
              
              
                
                  b
                
                
                  a
                
              
            
            ]
          
        
        ,
      
    
    {\displaystyle a+ib\leftrightarrow {\begin{bmatrix}a&-b\\b&a\end{bmatrix}},}
  under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions and Clifford algebras in general.
Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break. Computer graphics uses matrices to represent objects; to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation; and to apply image convolutions such as sharpening, blurring, edge detection, and more. Matrices over a polynomial ring are important in the study of control theory.
Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.


=== Graph theory ===

The adjacency matrix of a finite graph is a basic notion of graph theory. It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example ""yes"" and ""no"", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges. These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.


=== Analysis and geometry ===
The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,

  
    
      
        H
        (
        f
        )
        =
        
          [
          
            
              
                
                  ∂
                  
                    2
                  
                
                f
              
              
                ∂
                
                  x
                  
                    i
                  
                
                
                ∂
                
                  x
                  
                    j
                  
                
              
            
          
          ]
        
        .
      
    
    {\displaystyle H(f)=\left[{\frac {\partial ^{2}f}{\partial x_{i}\,\partial x_{j}}}\right].}
  It encodes information about the local growth behaviour of the function: given a critical point x = (x1, ..., xn), that is, a point where the first partial derivatives 
  
    
      
        ∂
        f
        
          /
        
        ∂
        
          x
          
            i
          
        
      
    
    {\displaystyle \partial f/\partial x_{i}}
   of ƒ vanish, the function has a local minimum if the Hessian matrix is positive definite. Quadratic programming can be used to find global minima or maxima of quadratic functions closely related to the ones attached to matrices (see above).Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as

  
    
      
        J
        (
        f
        )
        =
        
          
            [
            
              
                
                  ∂
                  
                    f
                    
                      i
                    
                  
                
                
                  ∂
                  
                    x
                    
                      j
                    
                  
                
              
            
            ]
          
          
            1
            ≤
            i
            ≤
            m
            ,
            1
            ≤
            j
            ≤
            n
          
        
        .
      
    
    {\displaystyle J(f)=\left[{\frac {\partial f_{i}}{\partial x_{j}}}\right]_{1\leq i\leq m,1\leq j\leq n}.}
  If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has a decisive influence on the set of possible solutions of the equation in question.The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen concerning a sufficiently fine grid, which in turn can be recast as a matrix equation.


=== Probability theory and statistics ===

Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states. A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain-like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.Statistics also makes use of matrices in many different forms. Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables. Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear function

yi ≈ axi + b, i = 1, ..., Nwhich can be formulated in terms of matrices, related to the singular value decomposition of matrices.Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.


=== Symmetries and transformations in physics ===

Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors. For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.


=== Linear combinations of quantum states ===
The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states. This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the ""mixed"" state of a quantum system as a linear combination of elementary, ""pure"" eigenstates.Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.


=== Normal modes ===
A general application of matrices in physics is the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms. They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.


=== Geometrical optics ===
Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix analysis: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies.
The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.


=== Electronics ===
Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.
The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21), and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.


== History ==
Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations, including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano introduced the method to Europe when he published Ars Magna. The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683. The Dutch mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659). Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays. Cramer presented his rule in 1750.
The term ""matrix"" (Latin for ""womb"", ""dam"" (non-human female animal kept for breeding), ""source"", ""origin"", ""list"", ""register"", derived from mater—mother) was coined by James Joseph Sylvester in 1850, who understood a matrix as an object giving rise to several determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:
I have in previous papers defined a ""Matrix"" as a rectangular array of terms, out of which different systems of determinants may be engendered as from the womb of a common parent.
Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead, he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition. Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices in which he proposed and demonstrated the Cayley–Hamilton theorem.The English mathematician Cuthbert Edmund Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.The modern study of determinants sprang from several sources. Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomial

  
    
      
        
          a
          
            1
          
        
        
          a
          
            2
          
        
        ⋯
        
          a
          
            n
          
        
        
          ∏
          
            i
            <
            j
          
        
        (
        
          a
          
            j
          
        
        −
        
          a
          
            i
          
        
        )
        
      
    
    {\displaystyle a_{1}a_{2}\cdots a_{n}\prod _{i<j}(a_{j}-a_{i})\;}
  ,where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real. Jacobi studied ""functional determinants""—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten and Weierstrass' Zur Determinantentheorie, both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.
Many theorems were first established for small matrices only, for example, the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century, the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Wilhelm Jordan. In the early 20th century, matrices attained a central role in linear algebra, partially due to their use in classification of the hypercomplex number systems of the previous century.
The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns. Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.


=== Other historical usages of the word ""matrix"" in mathematics ===
The word has been used in unusual ways by at least two authors of historical importance.
Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word ""matrix"" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the ""bottom"" (0 order) the function is identical to its extension:
Let us give the name of matrix to any function, of however many variables, that does not involve any apparent variables. Then, any possible function other than a matrix derives from a matrix by means of generalization, that is, by considering the proposition that the function in question is true with all possible values or with some value of one of the arguments, the other argument or arguments remaining undetermined.
For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by ""considering"" the function for all possible values of ""individuals"" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a ""matrix"" of values by ""considering"" the function for all possible values of ""individuals"" bi substituted in place of variable y:

∀bj∀ai: Φ(ai, bj).Alfred Tarski in his 1946 Introduction to Logic used the word ""matrix"" synonymously with the notion of truth table as used in mathematical logic.


== See also ==


== Notes ==


== References ==
Anton, Howard (1987), Elementary Linear Algebra (5th ed.), New York: Wiley, ISBN 0-471-84819-0
Arnold, Vladimir I.; Cooke, Roger (1992), Ordinary differential equations, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-3-540-54813-3
Artin, Michael (1991), Algebra, Prentice Hall, ISBN 978-0-89871-510-1
Association for Computing Machinery (1979), Computer Graphics, Tata McGraw–Hill, ISBN 978-0-07-059376-3
Baker, Andrew J. (2003), Matrix Groups: An Introduction to Lie Group Theory, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-1-85233-470-3
Bau III, David; Trefethen, Lloyd N. (1997), Numerical linear algebra, Philadelphia, PA: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9
Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields, Boston: Houghton Mifflin Co., ISBN 0-395-14017-X
Bretscher, Otto (2005), Linear Algebra with Applications (3rd ed.), Prentice Hall
Bronson, Richard (1970), Matrix Methods: An Introduction, New York: Academic Press, LCCN 70097490
Bronson, Richard (1989), Schaum's outline of theory and problems of matrix operations, New York: McGraw–Hill, ISBN 978-0-07-007978-6
Brown, William C. (1991), Matrices and vector spaces, New York, NY: Marcel Dekker, ISBN 978-0-8247-8419-5
Coburn, Nathaniel (1955), Vector and tensor analysis, New York, NY: Macmillan, OCLC 1029828
Conrey, J. Brian (2007), Ranks of elliptic curves and random matrix theory, Cambridge University Press, ISBN 978-0-521-69964-8
Fraleigh, John B. (1976), A First Course In Abstract Algebra (2nd ed.), Reading: Addison-Wesley, ISBN 0-201-01984-1
Fudenberg, Drew; Tirole, Jean (1983), Game Theory, MIT Press
Gilbarg, David; Trudinger, Neil S. (2001), Elliptic partial differential equations of second order (2nd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-3-540-41160-4
Godsil, Chris; Royle, Gordon (2004), Algebraic Graph Theory, Graduate Texts in Mathematics, vol. 207, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-95220-8
Golub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Johns Hopkins, ISBN 978-0-8018-5414-9
Greub, Werner Hildbert (1975), Linear algebra, Graduate Texts in Mathematics, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-90110-7
Halmos, Paul Richard (1982), A Hilbert space problem book, Graduate Texts in Mathematics, vol. 19 (2nd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-90685-0, MR 0675952
Horn, Roger A.; Johnson, Charles R. (1985), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6
Householder, Alston S. (1975), The theory of matrices in numerical analysis, New York, NY: Dover Publications, MR 0378371
Kreyszig, Erwin (1972), Advanced Engineering Mathematics (3rd ed.), New York: Wiley, ISBN 0-471-50728-8.
Krzanowski, Wojtek J. (1988), Principles of multivariate analysis, Oxford Statistical Science Series, vol. 3, The Clarendon Press Oxford University Press, ISBN 978-0-19-852211-9, MR 0969370
Itô, Kiyosi, ed. (1987), Encyclopedic dictionary of mathematics. Vol. I-IV (2nd ed.), MIT Press, ISBN 978-0-262-09026-1, MR 0901762
Lang, Serge (1969), Analysis II, Addison-Wesley
Lang, Serge (1987a), Calculus of several variables (3rd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-96405-8
Lang, Serge (1987b), Linear algebra, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-96412-6
Lang, Serge (2002), Algebra, Graduate Texts in Mathematics, vol. 211 (Revised third ed.), New York: Springer-Verlag, ISBN 978-0-387-95385-4, MR 1878556
Latouche, Guy; Ramaswami, Vaidyanathan (1999), Introduction to matrix analytic methods in stochastic modeling (1st ed.), Philadelphia, PA: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-425-8
Manning, Christopher D.; Schütze, Hinrich (1999), Foundations of statistical natural language processing, MIT Press, ISBN 978-0-262-13360-9
Mehata, K. M.; Srinivasan, S. K. (1978), Stochastic processes, New York, NY: McGraw–Hill, ISBN 978-0-07-096612-3
Mirsky, Leonid (1990), An Introduction to Linear Algebra, Courier Dover Publications, ISBN 978-0-486-66434-7
Nering, Evar D. (1970), Linear Algebra and Matrix Theory (2nd ed.), New York: Wiley, LCCN 76-91646
Nocedal, Jorge; Wright, Stephen J. (2006), Numerical Optimization (2nd ed.), Berlin, DE; New York, NY: Springer-Verlag, p. 449, ISBN 978-0-387-30303-1
Oualline, Steve (2003), Practical C++ programming, O'Reilly, ISBN 978-0-596-00419-4
Press, William H.; Flannery, Brian P.; Teukolsky, Saul A.; Vetterling, William T. (1992), ""LU Decomposition and Its Applications"" (PDF), Numerical Recipes in FORTRAN: The Art of Scientific Computing (2nd ed.), Cambridge University Press, pp. 34–42, archived from the original on 2009-09-06{{citation}}:  CS1 maint: unfit URL (link)
Protter, Murray H.; Morrey, Charles B. Jr. (1970), College Calculus with Analytic Geometry (2nd ed.), Reading: Addison-Wesley, LCCN 76087042
Punnen, Abraham P.; Gutin, Gregory (2002), The traveling salesman problem and its variations, Boston, MA: Kluwer Academic Publishers, ISBN 978-1-4020-0664-7
Reichl, Linda E. (2004), The transition to chaos: conservative classical systems and quantum manifestations, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-98788-0
Rowen, Louis Halle (2008), Graduate Algebra: noncommutative view, Providence, RI: American Mathematical Society, ISBN 978-0-8218-4153-2
Šolin, Pavel (2005), Partial Differential Equations and the Finite Element Method, Wiley-Interscience, ISBN 978-0-471-76409-0
Stinson, Douglas R. (2005), Cryptography, Discrete Mathematics and its Applications, Chapman & Hall/CRC, ISBN 978-1-58488-508-5
Stoer, Josef; Bulirsch, Roland (2002), Introduction to Numerical Analysis (3rd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-95452-3
Ward, J. P. (1997), Quaternions and Cayley numbers, Mathematics and its Applications, vol. 403, Dordrecht, NL: Kluwer Academic Publishers Group, doi:10.1007/978-94-011-5768-1, ISBN 978-0-7923-4513-8, MR 1458894
Wolfram, Stephen (2003), The Mathematica Book (5th ed.), Champaign, IL: Wolfram Media, ISBN 978-1-57955-022-6


=== Physics references ===
Bohm, Arno (2001), Quantum Mechanics: Foundations and Applications, Springer, ISBN 0-387-95330-2
Burgess, Cliff; Moore, Guy (2007), The Standard Model. A Primer, Cambridge University Press, ISBN 978-0-521-86036-9
Guenther, Robert D. (1990), Modern Optics, John Wiley, ISBN 0-471-60538-7
Itzykson, Claude; Zuber, Jean-Bernard (1980), Quantum Field Theory, McGraw–Hill, ISBN 0-07-032071-3
Riley, Kenneth F.; Hobson, Michael P.; Bence, Stephen J. (1997), Mathematical methods for physics and engineering, Cambridge University Press, ISBN 0-521-55506-X
Schiff, Leonard I. (1968), Quantum Mechanics (3rd ed.), McGraw–Hill
Weinberg, Steven (1995), The Quantum Theory of Fields. Volume I: Foundations, Cambridge University Press, ISBN 0-521-55001-7
Wherrett, Brian S. (1987), Group Theory for Atoms, Molecules and Solids, Prentice–Hall International, ISBN 0-13-365461-3
Zabrodin, Anton; Brezin, Édouard; Kazakov, Vladimir; Serban, Didina; Wiegmann, Paul (2006), Applications of Random Matrices in Physics (NATO Science Series II: Mathematics, Physics and Chemistry), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-1-4020-4530-1


=== Historical references ===
A. Cayley A memoir on the theory of matrices. Phil. Trans. 148 1858 17-37; Math. Papers II 475-496
Bôcher, Maxime (2004), Introduction to higher algebra, New York, NY: Dover Publications, ISBN 978-0-486-49570-5, reprint of the 1907 original edition
Cayley, Arthur (1889), The collected mathematical papers of Arthur Cayley, vol. I (1841–1853), Cambridge University Press, pp. 123–126
Dieudonné, Jean, ed. (1978), Abrégé d'histoire des mathématiques 1700-1900, Paris, FR: Hermann
Hawkins, Thomas (1975), ""Cauchy and the spectral theory of matrices"", Historia Mathematica, 2: 1–29, doi:10.1016/0315-0860(75)90032-4, ISSN 0315-0860, MR 0469635
Knobloch, Eberhard (1994), ""From Gauss to Weierstrass: determinant theory and its historical evaluations"", The intersection of history and mathematics, Science Networks Historical Studies, vol. 15, Basel, Boston, Berlin: Birkhäuser, pp. 51–66, MR 1308079
Kronecker, Leopold (1897),  Hensel, Kurt (ed.), Leopold Kronecker's Werke, Teubner
Mehra, Jagdish; Rechenberg, Helmut (1987), The Historical Development of Quantum Theory (1st ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-96284-9
Shen, Kangshen; Crossley, John N.; Lun, Anthony Wah-Cheung (1999), Nine Chapters of the Mathematical Art, Companion and Commentary (2nd ed.), Oxford University Press, ISBN 978-0-19-853936-0
Weierstrass, Karl (1915), Collected works, vol. 3


== Further reading ==
""Matrix"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Kaw, Autar K. (September 2008), Introduction to Matrix Algebra, ISBN 978-0-615-25126-4
The Matrix Cookbook (PDF), retrieved 24 March 2014
Brookes, Mike (2005), The Matrix Reference Manual, London: Imperial College, retrieved 10 Dec 2008


== External links ==

An excellent online matrix calculator that displays the intermediate steps
MacTutor: Matrices and determinants
Matrices and Linear Algebra on the Earliest Uses Pages
Earliest Uses of Symbols for Matrices and Vectors"
c8b8923e12,Pure mathematics,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
While pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius. Another example is the problem of factoring large integers, which is the basis of the RSA cryptosystem, widely used to secure internet communications.It follows that, presently, the distinction between pure and applied mathematics is more a philosophical point of view or a mathematician's preference rather than a rigid subdivision of mathematics. In particular, it is not uncommon that some members of a department of applied mathematics describe themselves as pure mathematicians.


== History ==


=== Ancient Greece ===
Ancient Greek mathematicians were among the earliest to make a distinction between pure and applied mathematics. Plato helped to create the gap between ""arithmetic"", now called number theory, and ""logistic"", now called arithmetic. Plato regarded logistic (arithmetic) as appropriate for businessmen and men of war who ""must learn the art of numbers or [they] will not know how to array [their] troops"" and arithmetic (number theory) as appropriate for philosophers ""because [they have] to arise out of the sea of change and lay hold of true being."" Euclid of Alexandria, when asked by one of his students of what use was the study of geometry, asked his slave to give the student threepence, ""since he must make gain of what he learns."" The Greek mathematician Apollonius of Perga was asked about the usefulness of some of his theorems in Book IV of Conics to which he proudly asserted,
They are worthy of acceptance for the sake of the demonstrations themselves, in the same way as we accept many other things in mathematics for this and for no other reason.
And since many of his results were not applicable to the science or engineering of his day, Apollonius further argued in the preface of the fifth book of Conics that the subject is one of those that ""...seem worthy of study for their own sake.""


=== 19th century ===
The term itself is enshrined in the full title of the Sadleirian Chair, ""Sadleirian Professor of Pure Mathematics"", founded (as a professorship) in the mid-nineteenth century. The idea of a separate discipline of pure mathematics may have emerged at that time. The generation of Gauss made no sweeping distinction of the kind, between pure and applied. In the following years, specialisation and professionalisation (particularly in the Weierstrass approach to mathematical analysis) started to make a rift more apparent.


=== 20th century ===
At the start of the twentieth century mathematicians took up the axiomatic method, strongly influenced by David Hilbert's example. The logical formulation of pure mathematics suggested by Bertrand Russell in terms of a quantifier structure of propositions seemed more and more plausible, as large parts of mathematics became axiomatised and thus subject to the simple criteria of rigorous proof.
Pure mathematics, according to a view that can be ascribed to the Bourbaki group, is what is proved. ""Pure mathematician"" became a recognized vocation, achievable through training.
The case was made that pure mathematics is useful in engineering education:
There is a training in habits of thought, points of view, and intellectual comprehension of ordinary engineering problems, which only the study of higher mathematics can give.


== Generality and abstraction ==

One central concept in pure mathematics is the idea of generality; pure mathematics often exhibits a trend towards increased generality. Uses and advantages of generality include the following:

Generalizing theorems or mathematical structures can lead to deeper understanding of the original theorems or structures
Generality can simplify the presentation of material, resulting in shorter proofs or arguments that are easier to follow.
One can use generality to avoid duplication of effort, proving a general result instead of having to prove separate cases independently, or using results from other areas of mathematics.
Generality can facilitate connections between different branches of mathematics. Category theory is one area of mathematics dedicated to exploring this commonality of structure as it plays out in some areas of math.Generality's impact on intuition is both dependent on the subject and a matter of personal preference or learning style.  Often generality is seen as a hindrance to intuition, although it can certainly function as an aid to it, especially when it provides analogies to material for which one already has good intuition.
As a prime example of generality, the Erlangen program involved an expansion of geometry to accommodate non-Euclidean geometries as well as the field of topology, and other forms of geometry, by viewing geometry as the study of a space together with a group of transformations.  The study of numbers, called algebra at the beginning undergraduate level, extends to abstract algebra at a more advanced level; and the study of functions, called calculus at the college freshman level becomes mathematical analysis and functional analysis at a more advanced level.  Each of these branches of more abstract mathematics have many sub-specialties, and there are in fact many connections between pure mathematics and applied mathematics disciplines. A steep rise in abstraction was seen mid 20th century.
In practice, however, these developments led to a sharp divergence from physics, particularly from 1950 to 1983. Later this was criticised, for example by Vladimir Arnold, as too much Hilbert, not enough Poincaré. The point does not yet seem to be settled, in that string theory pulls one way, while discrete mathematics pulls back towards proof as central.


== Pure vs. applied mathematics ==
Mathematicians have always had differing opinions regarding the distinction between pure and applied mathematics. One of the most famous (but perhaps misunderstood) modern examples of this debate can be found in G.H. Hardy's 1940 essay A Mathematician's Apology.
It is widely believed that Hardy considered applied mathematics to be ugly and dull. Although it is true that Hardy preferred pure mathematics, which he often compared to painting and poetry, Hardy saw the distinction between pure and applied mathematics to be simply that applied mathematics sought to express physical truth in a mathematical framework, whereas pure mathematics expressed truths that were independent of the physical world. Hardy made a separate distinction in mathematics between what he called ""real"" mathematics, ""which has permanent aesthetic value"", and ""the dull and elementary parts of mathematics"" that have practical use.
Hardy considered some physicists, such as Einstein and Dirac, to be among the ""real"" mathematicians, but at the time that he was writing his Apology, he considered general relativity and quantum mechanics to be ""useless"", which allowed him to hold the opinion that only ""dull"" mathematics was useful. Moreover, Hardy briefly admitted that—just as the application of matrix theory and group theory to physics had come unexpectedly—the time may come where some kinds of beautiful, ""real"" mathematics may be useful as well.
Another insightful view is offered by American mathematician Andy Magid:

I've always thought that a good model here could be drawn from ring theory. In that subject, one has the subareas of commutative ring theory and non-commutative ring theory. An uninformed observer might think that these represent a dichotomy, but in fact the latter subsumes the former: a non-commutative ring is a not-necessarily-commutative ring. If we use similar conventions, then we could refer to applied mathematics and nonapplied mathematics, where by the latter we mean not-necessarily-applied mathematics... [emphasis added]
Friedrich Engels argued in his 1878 book Anti-Dühring that ""it is not at all true that in pure mathematics the mind deals only with its own creations and imaginations. The concepts of number and figure have not been invented from any source other than the world of reality"".: 36  He further argued that ""Before one came upon the idea of deducing the form of a cylinder from the rotation of a rectangle about one of its sides, a number of real rectangles and cylinders, however imperfect in form, must have been examined. Like all other sciences, mathematics arose out of the needs of men...But, as in every department of thought, at a certain stage of development the laws, which were abstracted from the real world, become divorced from the real world, and are set up against it as something independent, as laws coming from outside, to which the world has to conform."": 37 


== See also ==
Applied mathematics
Logic
Metalogic
Metamathematics


== References ==


== External links ==

What is Pure Mathematics? – Department of Pure Mathematics, University of Waterloo
The Principles of Mathematics by Bertrand Russell"
8fbfd749cf,Ring (mathematics),"In mathematics, rings are algebraic structures that generalize fields: multiplication need not be commutative and multiplicative inverses need not exist. In other words, a ring is a set equipped with two binary operations satisfying properties analogous to those of addition and multiplication of integers. Ring elements may be numbers such as integers or complex numbers, but they may also be non-numerical objects such as polynomials, square matrices, functions, and power series.

Formally, a ring is an abelian group whose operation is called addition, with a second binary operation called multiplication that is associative, is distributive over the addition operation, and has a multiplicative identity element. (Some authors use the term ""rng"" with a missing ""i"" to refer to the more general structure that omits this last requirement; see § Notes on the definition.)
Whether a ring is commutative (that is, whether the order in which two elements are multiplied might change the result) has profound implications on its behavior. Commutative algebra, the theory of commutative rings, is a major branch of ring theory. Its development has been greatly influenced by problems and ideas of algebraic number theory and algebraic geometry. The simplest commutative rings are those that admit division by non-zero elements; such rings are called fields.
Examples of commutative rings include the set of integers with their standard addition and multiplication, the set of polynomials with their addition and multiplication, the coordinate ring of an affine algebraic variety, and the ring of integers of a number field. Examples of noncommutative rings include the ring of n × n real square matrices with n ≥ 2, group rings in representation theory, operator algebras in functional analysis, rings of differential operators, and cohomology rings in topology.
The conceptualization of rings spanned the 1870s to the 1920s, with key contributions by Dedekind, Hilbert, Fraenkel, and Noether. Rings were first formalized as a generalization of Dedekind domains that occur in number theory, and of polynomial rings and rings of invariants that occur in algebraic geometry and invariant theory. They later proved useful in other branches of mathematics such as geometry and analysis.


== Definition ==
A ring is a set R equipped with two binary operations + (addition) and ⋅ (multiplication) satisfying the following three sets of axioms, called the ring axioms
R is an abelian group under addition, meaning that:
(a + b) + c = a + (b + c) for all a, b, c in R (that is, + is associative)
a + b = b + a for all a, b in R (that is, + is commutative).
There is an element 0 in R such that a + 0 = a for all a in R (that is, 0 is the additive identity).
For each a in R there exists −a in R such that a + (−a) = 0 (that is, −a is the additive inverse of a).
R is a monoid under multiplication, meaning that:
(a · b) · c = a · (b · c) for all a, b, c in R (that is, ⋅ is associative).
There is an element 1 in R such that a · 1 = a and 1 · a = a for all a in R (that is, 1 is the multiplicative identity).
Multiplication is distributive with respect to addition, meaning that:
a · (b + c) = (a · b) + (a · c) for all a, b, c in R (left distributivity).
(b + c) · a = (b · a) + (c · a) for all a, b, c in R (right distributivity).


=== Notes on the definition ===
In the terminology of this article, a ring is defined to have a multiplicative identity, while a structure with the same axiomatic definition but without the requirement for a multiplicative identity is instead called a rng (IPA: ).  For example, the set of even integers with the usual + and ⋅ is a rng, but not a ring.  As explained in § History below, many authors apply the term ""ring"" without requiring a multiplicative identity.
The multiplication symbol ⋅ is usually omitted; for example, xy means x · y.
Although ring addition is commutative, ring multiplication is not required to be commutative: ab need not necessarily equal ba. Rings that also satisfy commutativity for multiplication (such as the ring of integers) are called commutative rings. Books on commutative algebra or algebraic geometry often adopt the convention that ring means commutative ring, to simplify terminology.
In a ring, multiplicative inverses are not required to exist. A nonzero commutative ring in which every nonzero element has a multiplicative inverse is called a field.
The additive group of a ring is the underlying set equipped with only the operation of addition. Although the definition requires that the additive group be abelian, this can be inferred from the other ring axioms. The proof makes use of the ""1"", and does not work in a rng. (For a rng, omitting the axiom of commutativity of addition leaves it inferable from the remaining rng assumptions only for elements that are products: ab + cd = cd + ab.)
Although most modern authors use the term ""ring"" as defined here, there are a few who use the term to refer to more general structures in which there is no requirement for multiplication to be associative. For these authors, every algebra is a ""ring"".


== Illustration ==

The most familiar example of a ring is the set of all integers 
  
    
      
        
          Z
        
        ,
      
    
    {\displaystyle \mathbb {Z} ,}
   consisting of the numbers

  
    
      
        …
        ,
        −
        5
        ,
        −
        4
        ,
        −
        3
        ,
        −
        2
        ,
        −
        1
        ,
        0
        ,
        1
        ,
        2
        ,
        3
        ,
        4
        ,
        5
        ,
        …
      
    
    {\displaystyle \dots ,-5,-4,-3,-2,-1,0,1,2,3,4,5,\dots }
  The axioms of a ring were elaborated as a generalization of familiar properties of addition and multiplication of integers.


=== Some properties ===
Some basic properties of a ring follow immediately from the axioms:

The additive identity is unique.
The additive inverse of each element is unique.
The multiplicative identity is unique.
For any element x in a ring R, one has x0 = 0 = 0x (zero is an absorbing element with respect to multiplication) and (–1)x = –x.
If 0 = 1 in a ring R (or more generally, 0 is a unit element), then R has only one element, and is called the zero ring.
If a ring R contains the zero ring as a subring, then R itself is the zero ring.
The binomial formula holds for any x and y satisfying xy = yx.


=== Example: Integers modulo 4 ===

Equip the set 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
        =
        
          {
          
            
              
                0
                ¯
              
            
            ,
            
              
                1
                ¯
              
            
            ,
            
              
                2
                ¯
              
            
            ,
            
              
                3
                ¯
              
            
          
          }
        
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} =\left\{{\overline {0}},{\overline {1}},{\overline {2}},{\overline {3}}\right\}}
   with the following operations:

The sum 
  
    
      
        
          
            x
            ¯
          
        
        +
        
          
            y
            ¯
          
        
      
    
    {\displaystyle {\overline {x}}+{\overline {y}}}
   in 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} }
   is the remainder when the integer x + y is divided by 4 (as x + y is always smaller than 8, this remainder is either x + y or x + y − 4). For example, 
  
    
      
        
          
            2
            ¯
          
        
        +
        
          
            3
            ¯
          
        
        =
        
          
            1
            ¯
          
        
      
    
    {\displaystyle {\overline {2}}+{\overline {3}}={\overline {1}}}
   and 
  
    
      
        
          
            3
            ¯
          
        
        +
        
          
            3
            ¯
          
        
        =
        
          
            2
            ¯
          
        
        .
      
    
    {\displaystyle {\overline {3}}+{\overline {3}}={\overline {2}}.}
  
The product 
  
    
      
        
          
            x
            ¯
          
        
        ⋅
        
          
            y
            ¯
          
        
      
    
    {\displaystyle {\overline {x}}\cdot {\overline {y}}}
   in 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} }
   is the remainder when the integer xy is divided by 4. For example, 
  
    
      
        
          
            2
            ¯
          
        
        ⋅
        
          
            3
            ¯
          
        
        =
        
          
            2
            ¯
          
        
      
    
    {\displaystyle {\overline {2}}\cdot {\overline {3}}={\overline {2}}}
   and 
  
    
      
        
          
            3
            ¯
          
        
        ⋅
        
          
            3
            ¯
          
        
        =
        
          
            1
            ¯
          
        
        .
      
    
    {\displaystyle {\overline {3}}\cdot {\overline {3}}={\overline {1}}.}
  Then 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} }
   is a ring: each axiom follows from the corresponding axiom for 
  
    
      
        
          Z
        
        .
      
    
    {\displaystyle \mathbb {Z} .}
   If x is an integer, the remainder of x when divided by 4 may be considered as an element of 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
        ,
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} ,}
   and this element is often denoted by ""x mod 4"" or 
  
    
      
        
          
            x
            ¯
          
        
        ,
      
    
    {\displaystyle {\overline {x}},}
   which is consistent with the notation for 0, 1, 2, 3. The additive inverse of any 
  
    
      
        
          
            x
            ¯
          
        
      
    
    {\displaystyle {\overline {x}}}
   in 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} }
   is 
  
    
      
        −
        
          
            x
            ¯
          
        
        =
        
          
            
              −
              x
            
            ¯
          
        
        .
      
    
    {\displaystyle -{\overline {x}}={\overline {-x}}.}
   For example, 
  
    
      
        −
        
          
            3
            ¯
          
        
        =
        
          
            
              −
              3
            
            ¯
          
        
        =
        
          
            1
            ¯
          
        
        .
      
    
    {\displaystyle -{\overline {3}}={\overline {-3}}={\overline {1}}.}
  


=== Example: 2-by-2 matrices ===
The set of 2-by-2 square matrices with entries in a field F is

  
    
      
        
          M
          
            2
          
        
        ⁡
        (
        F
        )
        =
        
          {
          
            
              
              
                
                  (
                  
                    
                      
                        a
                      
                      
                        b
                      
                    
                    
                      
                        c
                      
                      
                        d
                      
                    
                  
                  )
                
              
              |
            
             
            a
            ,
            b
            ,
            c
            ,
            d
            ∈
            F
          
          }
        
        .
      
    
    {\displaystyle \operatorname {M} _{2}(F)=\left\{\left.{\begin{pmatrix}a&b\\c&d\end{pmatrix}}\right|\ a,b,c,d\in F\right\}.}
  With the operations of matrix addition and matrix multiplication, 
  
    
      
        
          M
          
            2
          
        
        ⁡
        (
        F
        )
      
    
    {\displaystyle \operatorname {M} _{2}(F)}
   satisfies the above ring axioms. The element 
  
    
      
        
          (
          
            
              
                
                  
                    1
                  
                  
                    0
                  
                
                
                  
                    0
                  
                  
                    1
                  
                
              
            
          
          )
        
      
    
    {\displaystyle \left({\begin{smallmatrix}1&0\\0&1\end{smallmatrix}}\right)}
   is the multiplicative identity of the ring. If 
  
    
      
        A
        =
        
          (
          
            
              
                
                  
                    0
                  
                  
                    1
                  
                
                
                  
                    1
                  
                  
                    0
                  
                
              
            
          
          )
        
      
    
    {\displaystyle A=\left({\begin{smallmatrix}0&1\\1&0\end{smallmatrix}}\right)}
   and 
  
    
      
        B
        =
        
          (
          
            
              
                
                  
                    0
                  
                  
                    1
                  
                
                
                  
                    0
                  
                  
                    0
                  
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle B=\left({\begin{smallmatrix}0&1\\0&0\end{smallmatrix}}\right),}
   then 
  
    
      
        A
        B
        =
        
          (
          
            
              
                
                  
                    0
                  
                  
                    0
                  
                
                
                  
                    0
                  
                  
                    1
                  
                
              
            
          
          )
        
      
    
    {\displaystyle AB=\left({\begin{smallmatrix}0&0\\0&1\end{smallmatrix}}\right)}
   while 
  
    
      
        B
        A
        =
        
          (
          
            
              
                
                  
                    1
                  
                  
                    0
                  
                
                
                  
                    0
                  
                  
                    0
                  
                
              
            
          
          )
        
        ;
      
    
    {\displaystyle BA=\left({\begin{smallmatrix}1&0\\0&0\end{smallmatrix}}\right);}
   this example shows that the ring is noncommutative.
More generally, for any ring R, commutative or not, and any nonnegative integer n, the square matrices of dimension n with entries in R form a ring: see Matrix ring.


== History ==


=== Dedekind ===
The study of rings originated from the theory of polynomial rings and the theory of algebraic integers. In 1871, Richard Dedekind defined the concept of the ring of integers of a number field. In this context, he introduced the terms ""ideal"" (inspired by Ernst Kummer's notion of ideal number) and ""module"" and studied their properties. Dedekind did not use the term ""ring"" and did not define the concept of a ring in a general setting.


=== Hilbert ===
The term ""Zahlring"" (number ring) was coined by David Hilbert in 1892 and published in 1897. In 19th century German, the word ""Ring"" could mean ""association"", which is still used today in English in a limited sense (for example, spy ring), so if that were the etymology then it would be similar to the way ""group"" entered mathematics by being a non-technical word for ""collection of related things"". According to Harvey Cohn, Hilbert used the term for a ring that had the property of ""circling directly back"" to an element of itself (in the sense of an equivalence). Specifically, in a ring of algebraic integers, all high powers of an algebraic integer can be written as an integral combination of a fixed set of lower powers, and thus the powers ""cycle back"". For instance, if 
  
    
      
        
          a
          
            3
          
        
        −
        4
        a
        +
        1
        =
        0
      
    
    {\displaystyle a^{3}-4a+1=0}
   then:

  
    
      
        
          
            
              
                
                  a
                  
                    3
                  
                
              
              
                
                =
                4
                a
                −
                1
                ,
              
            
            
              
                
                  a
                  
                    4
                  
                
              
              
                
                =
                4
                
                  a
                  
                    2
                  
                
                −
                a
                ,
              
            
            
              
                
                  a
                  
                    5
                  
                
              
              
                
                =
                −
                
                  a
                  
                    2
                  
                
                +
                16
                a
                −
                4
                ,
              
            
            
              
                
                  a
                  
                    6
                  
                
              
              
                
                =
                16
                
                  a
                  
                    2
                  
                
                −
                8
                a
                +
                1
                ,
              
            
            
              
                
                  a
                  
                    7
                  
                
              
              
                
                =
                −
                8
                
                  a
                  
                    2
                  
                
                +
                65
                a
                −
                16
                ,
              
            
            
              
                ⋮
                 
              
              
                
                
                ⋮
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}a^{3}&=4a-1,\\a^{4}&=4a^{2}-a,\\a^{5}&=-a^{2}+16a-4,\\a^{6}&=16a^{2}-8a+1,\\a^{7}&=-8a^{2}+65a-16,\\\vdots \ &\qquad \vdots \end{aligned}}}
  and so on; in general, an is going to be an integral linear combination of 1, a, and a2.


=== Fraenkel and Noether ===
The first axiomatic definition of a ring was given by Adolf Fraenkel in 1915, but his axioms were stricter than those in the modern definition. For instance, he required every non-zero-divisor to have a multiplicative inverse. In 1921, Emmy Noether gave a modern axiomatic definition of commutative rings (with and without 1) and developed the foundations of commutative ring theory in her paper Idealtheorie in Ringbereichen.


=== Multiplicative identity and the term ""ring"" ===
Fraenkel's axioms for a ""ring"" included that of a multiplicative identity, whereas Noether's did not.Most or all books on algebra up to around 1960 followed Noether's convention of not requiring a 1 for a ""ring"". Starting in the 1960s, it became increasingly common to see books including the existence of 1 in the definition of ""ring"", especially in advanced books by notable authors such as Artin, Atiyah and MacDonald, Bourbaki, Eisenbud, and Lang. There are also books published as late as 2006 that use the term without the requirement for a 1.Gardner and Wiegandt assert that, when dealing with several objects in the category of rings (as opposed to working with a fixed ring), if one requires all rings to have a 1, then some consequences include the lack of existence of infinite direct sums of rings, and that proper direct summands of rings are not subrings. They conclude that ""in many, maybe most, branches of ring theory the requirement of the existence of a unity element is not sensible, and therefore unacceptable.""  Poonen makes the counterargument that the natural notion for rings is the direct product rather than the direct sum. He further argues that rings without a multiplicative identity are not totally associative (the product of any finite sequence of ring elements, including the empty sequence, is well-defined, independent of the order of operations) and writes ""the natural extension of associativity demands that rings should contain an empty product, so it is natural to require rings to have a 1"".Authors who follow either convention for the use of the term ""ring"" may use one of the following terms to refer to objects satisfying the other convention:

to include a requirement for a multiplicative identity: ""unital ring"", ""unitary ring"", ""unit ring"", ""ring with unity"", ""ring with identity"", ""ring with a unit"", or ""ring with 1"".
to omit a requirement for a multiplicative identity: ""rng"" or ""pseudo-ring"", although the latter may be confusing because it also has other meanings.


== Basic examples ==


=== Commutative rings ===
The prototypical example is the ring of integers with the two operations of addition and multiplication.
The rational, real and complex numbers are commutative rings of a type called fields.
A unital associative algebra over a commutative ring R is itself a ring as well as an R-module. Some examples:
The algebra R[X] of polynomials with coefficients in R.
The algebra 
  
    
      
        R
        [
        [
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ]
        ]
      
    
    {\displaystyle R[[X_{1},\dots ,X_{n}]]}
   of formal power series with coefficients in R.
The set of all continuous real-valued functions defined on the real line forms a commutative 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  -algebra. The operations are pointwise addition and multiplication of functions.
Let X be a set, and let R be a ring. Then the set of all functions from X to R forms a ring, which is commutative if R is commutative.
The ring of quadratic integers, the integral closure of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   in a quadratic extension of 
  
    
      
        
          Q
        
        .
      
    
    {\displaystyle \mathbb {Q} .}
   It is a subring of the ring of all algebraic integers.
The ring of profinite integers 
  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
        ,
      
    
    {\displaystyle {\widehat {\mathbb {Z} }},}
   the (infinite) product of the rings of p-adic integers 
  
    
      
        
          
            Z
          
          
            p
          
        
      
    
    {\displaystyle \mathbb {Z} _{p}}
   over all prime numbers p.
The Hecke ring, the ring generated by Hecke operators.
If S is a set, then the power set of S becomes a ring if we define addition to be the symmetric difference of sets and multiplication to be intersection. This is an example of a Boolean ring.


=== Noncommutative rings ===
For any ring R and any natural number n, the set of all square n-by-n matrices with entries from R, forms a ring with matrix addition and matrix multiplication as operations. For n = 1, this matrix ring is isomorphic to R itself. For n > 1 (and R not the zero ring), this matrix ring is noncommutative.
If G is an abelian group, then the endomorphisms of G form a ring, the endomorphism ring End(G) of G. The operations in this ring are addition and composition of endomorphisms. More generally, if V is a left module over a ring R, then the set of all R-linear maps forms a ring, also called the endomorphism ring and denoted by EndR(V).
The endomorphism ring of an elliptic curve. It is a commutative ring if the elliptic curve is defined over a field of characteristic zero.
If G is a group and R is a ring, the group ring of G over R is a free module over R having G as basis. Multiplication is defined by the rules that the elements of G commute with the elements of R and multiply together as they do in the group G.
The ring of differential operators (depending on the context). In fact, many rings that appear in analysis are noncommutative. For example, most Banach algebras are noncommutative.


=== Non-rings ===
The set of natural numbers 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   with the usual operations is not a ring, since 
  
    
      
        (
        
          N
        
        ,
        +
        )
      
    
    {\displaystyle (\mathbb {N} ,+)}
   is not even a group (not all the elements are invertible with respect to addition – for instance, there is no natural number which can be added to 3 to get 0 as a result). There is a natural way to enlarge it to a ring, by including negative numbers to produce the ring of integers 
  
    
      
        
          Z
        
        .
      
    
    {\displaystyle \mathbb {Z} .}
   The natural numbers (including 0) form an algebraic structure known as a semiring (which has all of the axioms of a ring excluding that of an additive inverse).
Let R be the set of all continuous functions on the real line that vanish outside a bounded interval that depends on the function, with addition as usual but with multiplication defined as convolution:  Then R is a rng, but not a ring: the Dirac delta function has the property of a multiplicative identity, but it is not a function and hence is not an element of R.


== Basic concepts ==


=== Products and powers ===
For each nonnegative integer n, given a sequence 
  
    
      
        (
        
          a
          
            1
          
        
        ,
        …
        ,
        
          a
          
            n
          
        
        )
      
    
    {\displaystyle (a_{1},\dots ,a_{n})}
   of n elements of R, one can define the product 
  
    
      
        
          P
          
            n
          
        
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          a
          
            i
          
        
      
    
    {\displaystyle P_{n}=\prod _{i=1}^{n}a_{i}}
   recursively: let P0 = 1 and let Pm = Pm−1am for 1 ≤ m ≤ n.
As a special case, one can define nonnegative integer powers of an element a of a ring: a0 = 1 and an = an−1a for n ≥ 1.  Then am+n = aman for all m, n ≥ 0.


=== Elements in a ring ===
A left zero divisor of a ring R is an element a in the ring such that there exists a nonzero element b of R such that ab = 0. A right zero divisor is defined similarly.
A nilpotent element is an element a such that an = 0 for some n > 0. One example of a nilpotent element is a nilpotent matrix. A nilpotent element in a nonzero ring is necessarily a zero divisor.
An idempotent 
  
    
      
        e
      
    
    {\displaystyle e}
   is an element such that e2 = e. One example of an idempotent element is a projection in linear algebra.
A unit is an element a having a multiplicative inverse; in this case the inverse is unique, and is denoted by a–1. The set of units of a ring is a group under ring multiplication; this group is denoted by R× or R* or U(R). For example, if R is the ring of all square matrices of size n over a field, then R× consists of the set of all invertible matrices of size n, and is called the general linear group.


=== Subring ===

A subset S of R is called a subring if any one of the following equivalent conditions holds:

the addition and multiplication of R restrict to give operations S × S → S making S a ring with the same multiplicative identity as R.
1 ∈ S; and for all x, y in S, the elements xy, x + y, and −x are in S.
S can be equipped with operations making it a ring such that the inclusion map S → R is a ring homomorphism.For example, the ring 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   of integers is a subring of the field of real numbers and also a subring of the ring of polynomials 
  
    
      
        
          Z
        
        [
        X
        ]
      
    
    {\displaystyle \mathbb {Z} [X]}
   (in both cases, 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   contains 1, which is the multiplicative identity of the larger rings). On the other hand, the subset of even integers 
  
    
      
        2
        
          Z
        
      
    
    {\displaystyle 2\mathbb {Z} }
   does not contain the identity element 1 and thus does not qualify as a subring of 
  
    
      
        
          Z
        
        ;
      
    
    {\displaystyle \mathbb {Z} ;}
   one could call 
  
    
      
        2
        
          Z
        
      
    
    {\displaystyle 2\mathbb {Z} }
   a subrng, however.
An intersection of subrings is a subring. Given a subset E of R, the smallest subring of R containing E is the intersection of all subrings of R containing E, and it is called the subring generated by E.
For a ring R, the smallest subring of R is called the characteristic subring of R. It can be generated through addition of copies of 1 and −1. It is possible that n · 1 = 1 + 1 + ... + 1 (n times) can be zero. If n is the smallest positive integer such that this occurs, then n is called the characteristic of R. In some rings, n · 1 is never zero for any positive integer n, and those rings are said to have characteristic zero.
Given a ring R, let Z(R) denote the set of all elements x in R such that x commutes with every element in R: xy = yx for any y in R. Then Z(R) is a subring of R, called the center of R. More generally, given a subset X of R, let S be the set of all elements in R that commute with every element in X. Then S is a subring of R, called the centralizer (or commutant) of X. The center is the centralizer of the entire ring R. Elements or subsets of the center are said to be central in R; they (each individually) generate a subring of the center.


=== Ideal ===

Let R be a ring.  A left ideal of R is a nonempty subset I of R such that for any x, y in I and r in R, the elements x + y and rx are in I. If R I denotes the R-span of I, that is, the set of finite sums

  
    
      
        
          r
          
            1
          
        
        
          x
          
            1
          
        
        +
        ⋯
        +
        
          r
          
            n
          
        
        
          x
          
            n
          
        
        
        
          
            such
          
        
        
        
          
            that
          
        
        
        
          r
          
            i
          
        
        ∈
        R
        
        
          
            and
          
        
        
        
          x
          
            i
          
        
        ∈
        I
        ,
      
    
    {\displaystyle r_{1}x_{1}+\cdots +r_{n}x_{n}\quad {\textrm {such}}\;{\textrm {that}}\;r_{i}\in R\;{\textrm {and}}\;x_{i}\in I,}
  then I is a left ideal if 
  
    
      
        R
        I
        ⊆
        I
        .
      
    
    {\displaystyle RI\subseteq I.}
   Similarly, a right ideal is a subset I such that 
  
    
      
        I
        R
        ⊆
        I
        .
      
    
    {\displaystyle IR\subseteq I.}
   A subset I is said to be a two-sided ideal or simply ideal if it is both a left ideal and right ideal. A one-sided or two-sided ideal is then an additive subgroup of R. If E is a subset of R, then R E is a left ideal, called the left ideal generated by E; it is the smallest left ideal containing E. Similarly, one can consider the right ideal or the two-sided ideal generated by a subset of R.
If x is in R, then Rx and xR are left ideals and right ideals, respectively; they are called the principal left ideals and right ideals generated by x. The principal ideal RxR is written as (x). For example, the set of all positive and negative multiples of 2 along with 0 form an ideal of the integers, and this ideal is generated by the integer 2. In fact, every ideal of the ring of integers is principal.
Like a group, a ring is said to be simple if it is nonzero and it has no proper nonzero two-sided ideals. A commutative simple ring is precisely a field.
Rings are often studied with special conditions set upon their ideals. For example, a ring in which there is no strictly increasing infinite chain of left ideals is called a left Noetherian ring. A ring in which there is no strictly decreasing infinite chain of left ideals is called a left Artinian ring. It is a somewhat surprising fact that a left Artinian ring is left Noetherian (the Hopkins–Levitzki theorem). The integers, however, form a Noetherian ring which is not Artinian.
For commutative rings, the ideals generalize the classical notion of divisibility and decomposition of an integer into prime numbers in algebra. A proper ideal P of R is called a prime ideal if for any elements 
  
    
      
        x
        ,
        y
        ∈
        R
      
    
    {\displaystyle x,y\in R}
   we have that 
  
    
      
        x
        y
        ∈
        P
      
    
    {\displaystyle xy\in P}
   implies either 
  
    
      
        x
        ∈
        P
      
    
    {\displaystyle x\in P}
   or 
  
    
      
        y
        ∈
        P
        .
      
    
    {\displaystyle y\in P.}
   Equivalently, P is prime if for any ideals 
  
    
      
        I
        ,
        J
      
    
    {\displaystyle I,J}
   we have that 
  
    
      
        I
        J
        ⊆
        P
      
    
    {\displaystyle IJ\subseteq P}
   implies either 
  
    
      
        I
        ⊆
        P
      
    
    {\displaystyle I\subseteq P}
   or 
  
    
      
        J
        ⊆
        P
        .
      
    
    {\displaystyle J\subseteq P.}
   This latter formulation illustrates the idea of ideals as generalizations of elements.


=== Homomorphism ===

A homomorphism from a ring (R, +, ⋅) to a ring (S, ‡, ∗) is a function f from R to S that preserves the ring operations; namely, such that, for all a, b in R the following identities hold:

  
    
      
        
          
            
              
              
                f
                (
                a
                +
                b
                )
                =
                f
                (
                a
                )
                ‡
                f
                (
                b
                )
              
            
            
              
              
                f
                (
                a
                ⋅
                b
                )
                =
                f
                (
                a
                )
                ∗
                f
                (
                b
                )
              
            
            
              
              
                f
                (
                
                  1
                  
                    R
                  
                
                )
                =
                
                  1
                  
                    S
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&f(a+b)=f(a)\ddagger f(b)\\&f(a\cdot b)=f(a)*f(b)\\&f(1_{R})=1_{S}\end{aligned}}}
  If one is working with rngs, then the third condition is dropped.
A ring homomorphism f is said to be an isomorphism if there exists an inverse homomorphism to f (that is, a ring homomorphism that is an inverse function). Any bijective ring homomorphism is a ring isomorphism. Two rings R, S are said to be isomorphic if there is an isomorphism between them and in that case one writes 
  
    
      
        R
        ≃
        S
        .
      
    
    {\displaystyle R\simeq S.}
   A ring homomorphism between the same ring is called an endomorphism, and an isomorphism between the same ring an automorphism.
Examples:

The function that maps each integer x to its remainder modulo 4 (a number in { 0, 1, 2, 3 }) is a homomorphism from the ring 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   to the quotient ring 
  
    
      
        
          Z
        
        
          /
        
        4
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /4\mathbb {Z} }
   (""quotient ring"" is defined below).
If u is a unit element in a ring R, then 
  
    
      
        R
        →
        R
        ,
        x
        ↦
        u
        x
        
          u
          
            −
            1
          
        
      
    
    {\displaystyle R\to R,x\mapsto uxu^{-1}}
   is a ring homomorphism, called an inner automorphism of R.
Let R be a commutative ring of prime characteristic p. Then 
  
    
      
        x
        →
        
          x
          
            p
          
        
      
    
    {\displaystyle x\to x^{p}}
   is a ring endomorphism of R called the Frobenius homomorphism.
The Galois group of a field extension L/K is the set of all automorphisms of L whose restrictions to n are the identity.
For any ring R, there are a unique ring homomorphism 
  
    
      
        
          Z
        
        ↦
        R
      
    
    {\displaystyle \mathbb {Z} \mapsto R}
   and a unique ring homomorphism R → 0.
An epimorphism (that is, right-cancelable morphism) of rings need not be surjective. For example, the unique map 
  
    
      
        
          Z
        
        →
        
          Q
        
      
    
    {\displaystyle \mathbb {Z} \to \mathbb {Q} }
   is an epimorphism.
An algebra homomorphism from a k-algebra to the endomorphism algebra of a vector space over k is called a representation of the algebra.Given a ring homomorphism f : R → S, the set of all elements mapped to 0 by f is called the kernel of f. The kernel is a two-sided ideal of R. The image of f, on the other hand, is not always an ideal, but it is always a subring of S.
To give a ring homomorphism from a commutative ring R to a ring A with image contained in the center of A is the same as to give a structure of an algebra over R to A (which in particular gives a structure of an A-module).


=== Quotient ring ===

The notion of quotient ring is analogous to the notion of a quotient group.  Given a ring (R, +, ⋅ ) and a two-sided ideal I of (R, +, ⋅ ), view I as subgroup of (R, +); then the quotient ring R/I is the set of cosets of I together with the operations

  
    
      
        
          
            
              
              
                
                (
                a
                +
                I
                )
                +
                (
                b
                +
                I
                )
                =
                (
                a
                +
                b
                )
                +
                I
                ,
              
            
            
              
              
                
                (
                a
                +
                I
                )
                (
                b
                +
                I
                )
                =
                (
                a
                b
                )
                +
                I
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&(a+I)+(b+I)=(a+b)+I,\\&(a+I)(b+I)=(ab)+I.\end{aligned}}}
  for all a, b in R.  The ring R/I is also called a factor ring.
As with a quotient group, there is a canonical homomorphism p : R → R/I, given by 
  
    
      
        x
        ↦
        x
        +
        I
        .
      
    
    {\displaystyle x\mapsto x+I.}
   It is surjective and satisfies the following universal property: 

If f : R → S is a ring homomorphism such that f(I) = 0, then there is a unique homomorphism 
  
    
      
        
          
            f
            ¯
          
        
        :
        R
        
          /
        
        I
        →
        S
      
    
    {\displaystyle {\overline {f}}:R/I\to S}
   such that 
  
    
      
        f
        =
        
          
            f
            ¯
          
        
        ∘
        p
        .
      
    
    {\displaystyle f={\overline {f}}\circ p.}
  For any ring homomorphism f : R → S, invoking the universal property with I = ker f produces a homomorphism 
  
    
      
        
          
            f
            ¯
          
        
        :
        R
        
          /
        
        ker
        ⁡
        f
        →
        S
      
    
    {\displaystyle {\overline {f}}:R/\ker f\to S}
   that gives an isomorphism from R/ker f to the image of f.


== Module ==

The concept of a module over a ring generalizes the concept of a vector space (over a field) by generalizing from multiplication of vectors with elements of a field (scalar multiplication) to multiplication with elements of a ring. More precisely, given a ring R, an R-module M is an abelian group equipped with an operation R × M → M (associating an element of M to every pair of an element of R and an element of M) that satisfies certain axioms. This operation is commonly denoted by juxtaposition and called multiplication. The axioms of modules are the following: for all a, b in R and all x, y in M, 

M is an abelian group under addition.

  
    
      
        
          
            
              
              
                a
                (
                x
                +
                y
                )
                =
                a
                x
                +
                a
                y
              
            
            
              
              
                
                (
                a
                +
                b
                )
                x
                =
                a
                x
                +
                b
                x
              
            
            
              
              
                1
                x
                =
                x
              
            
            
              
              
                
                (
                a
                b
                )
                x
                =
                a
                (
                b
                x
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&a(x+y)=ax+ay\\&(a+b)x=ax+bx\\&1x=x\\&(ab)x=a(bx)\end{aligned}}}
  When the ring is noncommutative these axioms define left modules; right modules are defined similarly by writing xa instead of ax. This is not only a change of notation, as the last axiom of right modules (that is x(ab) = (xa)b) becomes (ab)x = b(ax), if left multiplication (by ring elements) is used for a right module.
Basic examples of modules are ideals, including the ring itself.
Although similarly defined, the theory of modules is much more complicated than that of vector space, mainly, because, unlike vector spaces, modules are not characterized (up to an isomorphism) by a single invariant (the dimension of a vector space). In particular, not all modules have a basis.
The axioms of modules imply that (−1)x = −x, where the first minus denotes the additive inverse in the ring and the second minus the additive inverse in the module. Using this and denoting repeated addition by a multiplication by a positive integer allows identifying abelian groups with modules over the ring of integers.
Any ring homomorphism induces a structure of a module: if f : R → S is a ring homomorphism, then S is a left module over R by the multiplication: rs = f(r)s. If R is commutative or if f(R) is contained in the center of S, the ring S is called a R-algebra. In particular, every ring is an algebra over the integers.


== Constructions ==


=== Direct product ===

Let R and S be rings. Then the product R × S can be equipped with the following natural ring structure:

  
    
      
        
          
            
              
              
                
                (
                
                  r
                  
                    1
                  
                
                ,
                
                  s
                  
                    1
                  
                
                )
                +
                (
                
                  r
                  
                    2
                  
                
                ,
                
                  s
                  
                    2
                  
                
                )
                =
                (
                
                  r
                  
                    1
                  
                
                +
                
                  r
                  
                    2
                  
                
                ,
                
                  s
                  
                    1
                  
                
                +
                
                  s
                  
                    2
                  
                
                )
              
            
            
              
              
                
                (
                
                  r
                  
                    1
                  
                
                ,
                
                  s
                  
                    1
                  
                
                )
                ⋅
                (
                
                  r
                  
                    2
                  
                
                ,
                
                  s
                  
                    2
                  
                
                )
                =
                (
                
                  r
                  
                    1
                  
                
                ⋅
                
                  r
                  
                    2
                  
                
                ,
                
                  s
                  
                    1
                  
                
                ⋅
                
                  s
                  
                    2
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&(r_{1},s_{1})+(r_{2},s_{2})=(r_{1}+r_{2},s_{1}+s_{2})\\&(r_{1},s_{1})\cdot (r_{2},s_{2})=(r_{1}\cdot r_{2},s_{1}\cdot s_{2})\end{aligned}}}
  for all r1, r2 in R and s1, s2 in S. The ring R × S with the above operations of addition and multiplication and the multiplicative identity (1, 1) is called the direct product of R with S. The same construction also works for an arbitrary family of rings: if Ri are rings indexed by a set I, then 
  
    
      
        
          ∏
          
            i
            ∈
            I
          
        
        
          R
          
            i
          
        
      
    
    {\textstyle \prod _{i\in I}R_{i}}
   is a ring with componentwise addition and multiplication.
Let R be a commutative ring and 
  
    
      
        
          
            
              a
            
          
          
            1
          
        
        ,
        ⋯
        ,
        
          
            
              a
            
          
          
            n
          
        
      
    
    {\displaystyle {\mathfrak {a}}_{1},\cdots ,{\mathfrak {a}}_{n}}
   be ideals such that 
  
    
      
        
          
            
              a
            
          
          
            i
          
        
        +
        
          
            
              a
            
          
          
            j
          
        
        =
        (
        1
        )
      
    
    {\displaystyle {\mathfrak {a}}_{i}+{\mathfrak {a}}_{j}=(1)}
   whenever i ≠ j. Then the Chinese remainder theorem says there is a canonical ring isomorphism:

A ""finite"" direct product may also be viewed as a direct sum of ideals. Namely, let 
  
    
      
        
          R
          
            i
          
        
        ,
        1
        ≤
        i
        ≤
        n
      
    
    {\displaystyle R_{i},1\leq i\leq n}
   be rings, 
  
    
      
        
          R
          
            i
          
        
        →
        R
        =
        ∏
        
          R
          
            i
          
        
      
    
    {\textstyle R_{i}\to R=\prod R_{i}}
   the inclusions with the images 
  
    
      
        
          
            
              a
            
          
          
            i
          
        
      
    
    {\displaystyle {\mathfrak {a}}_{i}}
   (in particular 
  
    
      
        
          
            
              a
            
          
          
            i
          
        
      
    
    {\displaystyle {\mathfrak {a}}_{i}}
   are rings though not subrings). Then 
  
    
      
        
          
            
              a
            
          
          
            i
          
        
      
    
    {\displaystyle {\mathfrak {a}}_{i}}
   are ideals of R and

as a direct sum of abelian groups (because for abelian groups finite products are the same as direct sums). Clearly the direct sum of such ideals also defines a product of rings that is isomorphic to R. Equivalently, the above can be done through central idempotents. Assume that R has the above decomposition. Then we can write

By the conditions on 
  
    
      
        
          
            
              a
            
          
          
            i
          
        
        ,
      
    
    {\displaystyle {\mathfrak {a}}_{i},}
   one has that ei are central idempotents and eiej = 0, i ≠ j (orthogonal). Again, one can reverse the construction. Namely, if one is given a partition of 1 in orthogonal central idempotents, then let 
  
    
      
        
          
            
              a
            
          
          
            i
          
        
        =
        R
        
          e
          
            i
          
        
        ,
      
    
    {\displaystyle {\mathfrak {a}}_{i}=Re_{i},}
   which are two-sided ideals. If each ei is not a sum of orthogonal central idempotents, then their direct sum is isomorphic to R.
An important application of an infinite direct product is the construction of a projective limit of rings (see below). Another application is a restricted product of a family of rings (cf. adele ring).


=== Polynomial ring ===

Given a symbol t (called a variable) and a commutative ring R, the set of polynomials

  
    
      
        R
        [
        t
        ]
        =
        
          {
          
            
              a
              
                n
              
            
            
              t
              
                n
              
            
            +
            
              a
              
                n
                −
                1
              
            
            
              t
              
                n
                −
                1
              
            
            +
            ⋯
            +
            
              a
              
                1
              
            
            t
            +
            
              a
              
                0
              
            
            ∣
            n
            ≥
            0
            ,
            
              a
              
                j
              
            
            ∈
            R
          
          }
        
      
    
    {\displaystyle R[t]=\left\{a_{n}t^{n}+a_{n-1}t^{n-1}+\dots +a_{1}t+a_{0}\mid n\geq 0,a_{j}\in R\right\}}
  forms a commutative ring with the usual addition and multiplication, containing R as a subring. It is called the polynomial ring over R. More generally, the set 
  
    
      
        R
        
          [
          
            
              t
              
                1
              
            
            ,
            …
            ,
            
              t
              
                n
              
            
          
          ]
        
      
    
    {\displaystyle R\left[t_{1},\ldots ,t_{n}\right]}
   of all polynomials in variables 
  
    
      
        
          t
          
            1
          
        
        ,
        …
        ,
        
          t
          
            n
          
        
      
    
    {\displaystyle t_{1},\ldots ,t_{n}}
   forms a commutative ring, containing 
  
    
      
        R
        
          [
          
            t
            
              i
            
          
          ]
        
      
    
    {\displaystyle R\left[t_{i}\right]}
   as subrings.
If R is an integral domain, then R[t] is also an integral domain; its field of fractions is the field of rational functions. If R is a Noetherian ring, then R[t] is a Noetherian ring. If R is a unique factorization domain, then R[t] is a unique factorization domain. Finally, R is a field if and only if R[t] is a principal ideal domain.
Let 
  
    
      
        R
        ⊆
        S
      
    
    {\displaystyle R\subseteq S}
   be commutative rings. Given an element x of S, one can consider the ring homomorphism

  
    
      
        R
        [
        t
        ]
        →
        S
        ,
        
        f
        ↦
        f
        (
        x
        )
      
    
    {\displaystyle R[t]\to S,\quad f\mapsto f(x)}
  (that is, the substitution). If S = R[t] and x = t, then f(t) = f. Because of this, the polynomial f is often also denoted by f(t). The image of the map 
  
    
      
        f
        ↦
        f
        (
        x
        )
      
    
    {\displaystyle f\mapsto f(x)}
   is denoted by R[x]; it is the same thing as the subring of S generated by R and x.
Example: 
  
    
      
        k
        
          [
          
            
              t
              
                2
              
            
            ,
            
              t
              
                3
              
            
          
          ]
        
      
    
    {\displaystyle k\left[t^{2},t^{3}\right]}
   denotes the image of the homomorphism

  
    
      
        k
        [
        x
        ,
        y
        ]
        →
        k
        [
        t
        ]
        ,
        
        f
        ↦
        f
        
          (
          
            
              t
              
                2
              
            
            ,
            
              t
              
                3
              
            
          
          )
        
        .
      
    
    {\displaystyle k[x,y]\to k[t],\,f\mapsto f\left(t^{2},t^{3}\right).}
  In other words, it is the subalgebra of k[t] generated by t2 and t3.
Example: let f be a polynomial in one variable, that is, an element in a polynomial ring R. Then f(x + h) is an element in R[h] and f(x + h) – f(x) is divisible by h in that ring. The result of substituting zero to h in (f(x + h) – f(x)) / h is f' (x), the derivative of f at x.
The substitution is a special case of the universal property of a polynomial ring. The property states: given a ring homomorphism 
  
    
      
        ϕ
        :
        R
        →
        S
      
    
    {\displaystyle \phi :R\to S}
   and an element x in S there exists a unique ring homomorphism 
  
    
      
        
          
            ϕ
            ¯
          
        
        :
        R
        [
        t
        ]
        →
        S
      
    
    {\displaystyle {\overline {\phi }}:R[t]\to S}
   such that 
  
    
      
        
          
            ϕ
            ¯
          
        
        (
        t
        )
        =
        x
      
    
    {\displaystyle {\overline {\phi }}(t)=x}
   and 
  
    
      
        
          
            ϕ
            ¯
          
        
      
    
    {\displaystyle {\overline {\phi }}}
   restricts to ϕ. For example, choosing a basis, a symmetric algebra satisfies the universal property and so is a polynomial ring.
To give an example, let S be the ring of all functions from R to itself; the addition and the multiplication are those of functions. Let x be the identity function. Each r in R defines a constant function, giving rise to the homomorphism R → S. The universal property says that this map extends uniquely to

  
    
      
        R
        [
        t
        ]
        →
        S
        ,
        
        f
        ↦
        
          
            f
            ¯
          
        
      
    
    {\displaystyle R[t]\to S,\quad f\mapsto {\overline {f}}}
  (t maps to x) where 
  
    
      
        
          
            f
            ¯
          
        
      
    
    {\displaystyle {\overline {f}}}
   is the polynomial function defined by f. The resulting map is injective if and only if R is infinite.
Given a non-constant monic polynomial f in R[t], there exists a ring S containing R such that f is a product of linear factors in S[t].Let k be an algebraically closed field. The Hilbert's Nullstellensatz (theorem of zeros) states that there is a natural one-to-one correspondence between the set of all prime ideals in 
  
    
      
        k
        
          [
          
            
              t
              
                1
              
            
            ,
            …
            ,
            
              t
              
                n
              
            
          
          ]
        
      
    
    {\displaystyle k\left[t_{1},\ldots ,t_{n}\right]}
   and the set of closed subvarieties of kn. In particular, many local problems in algebraic geometry may be attacked through the study of the generators of an ideal in a polynomial ring. (cf. Gröbner basis.)
There are some other related constructions. A formal power series ring 
  
    
      
        R
        [
        
        [
        t
        ]
        
        ]
      
    
    {\displaystyle R[\![t]\!]}
   consists of formal power series

  
    
      
        
          ∑
          
            0
          
          
            ∞
          
        
        
          a
          
            i
          
        
        
          t
          
            i
          
        
        ,
        
        
          a
          
            i
          
        
        ∈
        R
      
    
    {\displaystyle \sum _{0}^{\infty }a_{i}t^{i},\quad a_{i}\in R}
  together with multiplication and addition that mimic those for convergent series. It contains R[t] as a subring. A formal power series ring does not have the universal property of a polynomial ring; a series may not converge after a substitution. The important advantage of a formal power series ring over a polynomial ring is that it is local (in fact, complete).


=== Matrix ring and endomorphism ring ===

Let R be a ring (not necessarily commutative). The set of all square matrices of size n with entries in R forms a ring with the entry-wise addition and the usual matrix multiplication. It is called the matrix ring and is denoted by Mn(R). Given a right R-module U, the set of all R-linear maps from U to itself forms a ring with addition that is of function and multiplication that is of composition of functions; it is called the endomorphism ring of U and is denoted by EndR(U).
As in linear algebra, a matrix ring may be canonically interpreted as an endomorphism ring: 
  
    
      
        
          End
          
            R
          
        
        ⁡
        (
        
          R
          
            n
          
        
        )
        ≃
        
          M
          
            n
          
        
        ⁡
        (
        R
        )
        .
      
    
    {\displaystyle \operatorname {End} _{R}(R^{n})\simeq \operatorname {M} _{n}(R).}
   This is a special case of the following fact: If 
  
    
      
        f
        :
        
          ⊕
          
            1
          
          
            n
          
        
        U
        →
        
          ⊕
          
            1
          
          
            n
          
        
        U
      
    
    {\displaystyle f:\oplus _{1}^{n}U\to \oplus _{1}^{n}U}
   is an R-linear map, then f may be written as a matrix with entries fij in S = EndR(U), resulting in the ring isomorphism:

  
    
      
        
          End
          
            R
          
        
        ⁡
        (
        
          ⊕
          
            1
          
          
            n
          
        
        U
        )
        →
        
          M
          
            n
          
        
        ⁡
        (
        S
        )
        ,
        
        f
        ↦
        (
        
          f
          
            i
            j
          
        
        )
        .
      
    
    {\displaystyle \operatorname {End} _{R}(\oplus _{1}^{n}U)\to \operatorname {M} _{n}(S),\quad f\mapsto (f_{ij}).}
  Any ring homomorphism R → S induces Mn(R) → Mn(S).Schur's lemma says that if U is a simple right R-module, then EndR(U) is a division ring. If 
  
    
      
        U
        =
        
          ⨁
          
            i
            =
            1
          
          
            r
          
        
        
          U
          
            i
          
          
            ⊕
            
              m
              
                i
              
            
          
        
      
    
    {\displaystyle U=\bigoplus _{i=1}^{r}U_{i}^{\oplus m_{i}}}
   is a direct sum of mi-copies of simple R-modules 
  
    
      
        
          U
          
            i
          
        
        ,
      
    
    {\displaystyle U_{i},}
   then

  
    
      
        
          End
          
            R
          
        
        ⁡
        (
        U
        )
        ≃
        
          ∏
          
            i
            =
            1
          
          
            r
          
        
        
          M
          
            
              m
              
                i
              
            
          
        
        ⁡
        (
        
          End
          
            R
          
        
        ⁡
        (
        
          U
          
            i
          
        
        )
        )
        .
      
    
    {\displaystyle \operatorname {End} _{R}(U)\simeq \prod _{i=1}^{r}\operatorname {M} _{m_{i}}(\operatorname {End} _{R}(U_{i})).}
  The Artin–Wedderburn theorem states any semisimple ring (cf. below) is of this form.
A ring R and the matrix ring Mn(R) over it are Morita equivalent: the category of right modules of R is equivalent to the category of right modules over Mn(R). In particular, two-sided ideals in R correspond in one-to-one to two-sided ideals in Mn(R).


=== Limits and colimits of rings ===
Let Ri be a sequence of rings such that Ri is a subring of Ri + 1 for all i. Then the union (or filtered colimit) of Ri is the ring 
  
    
      
        
          
            lim
            →
          
        
        ⁡
        
          R
          
            i
          
        
      
    
    {\displaystyle \varinjlim R_{i}}
   defined as follows: it is the disjoint union of all Ri's modulo the equivalence relation x ~ y if and only if x = y in Ri for sufficiently large i.
Examples of colimits:

A polynomial ring in infinitely many variables: 
  
    
      
        R
        [
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        ,
        ⋯
        ]
        =
        
          
            lim
            →
          
        
        ⁡
        R
        [
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        ,
        ⋯
        ,
        
          t
          
            m
          
        
        ]
        .
      
    
    {\displaystyle R[t_{1},t_{2},\cdots ]=\varinjlim R[t_{1},t_{2},\cdots ,t_{m}].}
  
The algebraic closure of finite fields of the same characteristic 
  
    
      
        
          
            
              
                F
              
              ¯
            
          
          
            p
          
        
        =
        
          
            lim
            →
          
        
        ⁡
        
          
            F
          
          
            
              p
              
                m
              
            
          
        
        .
      
    
    {\displaystyle {\overline {\mathbf {F} }}_{p}=\varinjlim \mathbf {F} _{p^{m}}.}
  
The field of formal Laurent series over a field k: 
  
    
      
        k
        (
        
        (
        t
        )
        
        )
        =
        
          
            lim
            →
          
        
        ⁡
        
          t
          
            −
            m
          
        
        k
        [
        
        [
        t
        ]
        
        ]
      
    
    {\displaystyle k(\!(t)\!)=\varinjlim t^{-m}k[\![t]\!]}
   (it is the field of fractions of the formal power series ring 
  
    
      
        k
        [
        
        [
        t
        ]
        
        ]
        .
      
    
    {\displaystyle k[\![t]\!].}
  )
The function field of an algebraic variety over a field k is 
  
    
      
        
          
            lim
            →
          
        
        ⁡
        k
        [
        U
        ]
      
    
    {\displaystyle \varinjlim k[U]}
   where the limit runs over all the coordinate rings k[U] of nonempty open subsets U (more succinctly it is the stalk of the structure sheaf at the generic point.)Any commutative ring is the colimit of finitely generated subrings.
A projective limit (or a filtered limit) of rings is defined as follows. Suppose we're given a family of rings Ri, i running over positive integers, say, and ring homomorphisms Rj → Ri, j ≥ i such that Ri → Ri are all the identities and Rk → Rj → Ri is Rk → Ri whenever k ≥ j ≥ i. Then 
  
    
      
        
          
            lim
            ←
          
        
        ⁡
        
          R
          
            i
          
        
      
    
    {\displaystyle \varprojlim R_{i}}
   is the subring of 
  
    
      
        
          ∏
          
            R
            
              i
            
          
        
      
    
    {\displaystyle \textstyle \prod R_{i}}
   consisting of (xn) such that xj maps to xi under Rj → Ri, j ≥ i.
For an example of a projective limit, see § Completion.


=== Localization ===
The localization generalizes the construction of the field of fractions of an integral domain to an arbitrary ring and modules. Given a (not necessarily commutative) ring R and a subset S of R, there exists a ring 
  
    
      
        R
        [
        
          S
          
            −
            1
          
        
        ]
      
    
    {\displaystyle R[S^{-1}]}
   together with the ring homomorphism 
  
    
      
        R
        →
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle R\to R\left[S^{-1}\right]}
   that ""inverts"" S; that is, the homomorphism maps elements in S to unit elements in 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        ,
      
    
    {\displaystyle R\left[S^{-1}\right],}
   and, moreover, any ring homomorphism from R that ""inverts"" S uniquely factors through 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        .
      
    
    {\displaystyle R\left[S^{-1}\right].}
   The ring 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle R\left[S^{-1}\right]}
   is called the localization of R with respect to S. For example, if R is a commutative ring and f an element in R, then the localization 
  
    
      
        R
        
          [
          
            f
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle R\left[f^{-1}\right]}
   consists of elements of the form 
  
    
      
        r
        
          /
        
        
          f
          
            n
          
        
        ,
        
        r
        ∈
        R
        ,
        
        n
        ≥
        0
      
    
    {\displaystyle r/f^{n},\,r\in R,\,n\geq 0}
   (to be precise, 
  
    
      
        R
        
          [
          
            f
            
              −
              1
            
          
          ]
        
        =
        R
        [
        t
        ]
        
          /
        
        (
        t
        f
        −
        1
        )
        .
      
    
    {\displaystyle R\left[f^{-1}\right]=R[t]/(tf-1).}
  )The localization is frequently applied to a commutative ring R with respect to the complement of a prime ideal (or a union of prime ideals) in R. In that case 
  
    
      
        S
        =
        R
        −
        
          
            p
          
        
        ,
      
    
    {\displaystyle S=R-{\mathfrak {p}},}
   one often writes 
  
    
      
        
          R
          
            
              p
            
          
        
      
    
    {\displaystyle R_{\mathfrak {p}}}
   for 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        .
      
    
    {\displaystyle R\left[S^{-1}\right].}
   
  
    
      
        
          R
          
            
              p
            
          
        
      
    
    {\displaystyle R_{\mathfrak {p}}}
   is then a local ring with the maximal ideal 
  
    
      
        
          
            p
          
        
        
          R
          
            
              p
            
          
        
        .
      
    
    {\displaystyle {\mathfrak {p}}R_{\mathfrak {p}}.}
   This is the reason for the terminology ""localization"". The field of fractions of an integral domain R is the localization of R at the prime ideal zero. If 
  
    
      
        
          
            p
          
        
      
    
    {\displaystyle {\mathfrak {p}}}
   is a prime ideal of a commutative ring R, then the field of fractions of 
  
    
      
        R
        
          /
        
        
          
            p
          
        
      
    
    {\displaystyle R/{\mathfrak {p}}}
   is the same as the residue field of the local ring 
  
    
      
        
          R
          
            
              p
            
          
        
      
    
    {\displaystyle R_{\mathfrak {p}}}
   and is denoted by 
  
    
      
        k
        (
        
          
            p
          
        
        )
        .
      
    
    {\displaystyle k({\mathfrak {p}}).}
  
If M is a left R-module, then the localization of M with respect to S is given by a change of rings 
  
    
      
        M
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        =
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        
          ⊗
          
            R
          
        
        M
        .
      
    
    {\displaystyle M\left[S^{-1}\right]=R\left[S^{-1}\right]\otimes _{R}M.}
  
The most important properties of localization are the following: when R is a commutative ring and S a multiplicatively closed subset

  
    
      
        
          
            p
          
        
        ↦
        
          
            p
          
        
        
          [
          
            S
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle {\mathfrak {p}}\mapsto {\mathfrak {p}}\left[S^{-1}\right]}
   is a bijection between the set of all prime ideals in R disjoint from S and the set of all prime ideals in 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        .
      
    
    {\displaystyle R\left[S^{-1}\right].}
  

  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
        =
        
          
            lim
            →
          
        
        ⁡
        R
        
          [
          
            f
            
              −
              1
            
          
          ]
        
        ,
      
    
    {\displaystyle R\left[S^{-1}\right]=\varinjlim R\left[f^{-1}\right],}
   f running over elements in S with partial ordering given by divisibility.
The localization is exact:  is exact over 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle R\left[S^{-1}\right]}
   whenever 
  
    
      
        0
        →
        
          M
          ′
        
        →
        M
        →
        
          M
          ″
        
        →
        0
      
    
    {\displaystyle 0\to M'\to M\to M''\to 0}
   is exact over R.
Conversely, if 
  
    
      
        0
        →
        
          M
          
            
              m
            
          
          ′
        
        →
        
          M
          
            
              m
            
          
        
        →
        
          M
          
            
              m
            
          
          ″
        
        →
        0
      
    
    {\displaystyle 0\to M'_{\mathfrak {m}}\to M_{\mathfrak {m}}\to M''_{\mathfrak {m}}\to 0}
   is exact for any maximal ideal 
  
    
      
        
          
            m
          
        
        ,
      
    
    {\displaystyle {\mathfrak {m}},}
   then 
  
    
      
        0
        →
        
          M
          ′
        
        →
        M
        →
        
          M
          ″
        
        →
        0
      
    
    {\displaystyle 0\to M'\to M\to M''\to 0}
   is exact.
A remark: localization is no help in proving a global existence. One instance of this is that if two modules are isomorphic at all prime ideals, it does not follow that they are isomorphic. (One way to explain this is that the localization allows one to view a module as a sheaf over prime ideals and a sheaf is inherently a local notion.)In category theory, a localization of a category amounts to making some morphisms isomorphisms. An element in a commutative ring R may be thought of as an endomorphism of any R-module. Thus, categorically, a localization of R with respect to a subset S of R is a functor from the category of R-modules to itself that sends elements of S viewed as endomorphisms to automorphisms and is universal with respect to this property. (Of course, R then maps to 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle R\left[S^{-1}\right]}
   and R-modules map to 
  
    
      
        R
        
          [
          
            S
            
              −
              1
            
          
          ]
        
      
    
    {\displaystyle R\left[S^{-1}\right]}
  -modules.)


=== Completion ===
Let R be a commutative ring, and let I be an ideal of R.
The completion of R at I is the projective limit 
  
    
      
        
          
            
              R
              ^
            
          
        
        =
        
          
            lim
            ←
          
        
        ⁡
        R
        
          /
        
        
          I
          
            n
          
        
        ;
      
    
    {\displaystyle {\hat {R}}=\varprojlim R/I^{n};}
   it is a commutative ring. The canonical homomorphisms from R to the quotients 
  
    
      
        R
        
          /
        
        
          I
          
            n
          
        
      
    
    {\displaystyle R/I^{n}}
   induce a homomorphism 
  
    
      
        R
        →
        
          
            
              R
              ^
            
          
        
        .
      
    
    {\displaystyle R\to {\hat {R}}.}
   The latter homomorphism is injective if R is a Noetherian integral domain and I is a proper ideal, or if R is a Noetherian local ring with maximal ideal I, by Krull's intersection theorem. The construction is especially useful when I is a maximal ideal.
The basic example is the completion of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   at the principal ideal (p) generated by a prime number p; it is called the ring of p-adic integers and is denoted 
  
    
      
        
          
            Z
          
          
            p
          
        
        .
      
    
    {\displaystyle \mathbb {Z} _{p}.}
   The completion can in this case be constructed also from the p-adic absolute value on 
  
    
      
        
          Q
        
        .
      
    
    {\displaystyle \mathbb {Q} .}
   The p-adic absolute value on 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
   is a map 
  
    
      
        x
        ↦
        
          |
        
        x
        
          |
        
      
    
    {\displaystyle x\mapsto |x|}
   from 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
   to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   given by 
  
    
      
        
          |
        
        n
        
          
            |
          
          
            p
          
        
        =
        
          p
          
            −
            
              v
              
                p
              
            
            (
            n
            )
          
        
      
    
    {\displaystyle |n|_{p}=p^{-v_{p}(n)}}
   where 
  
    
      
        
          v
          
            p
          
        
        (
        n
        )
      
    
    {\displaystyle v_{p}(n)}
   denotes the exponent of p in the prime factorization of a nonzero integer n into prime numbers (we also put 
  
    
      
        
          |
        
        0
        
          
            |
          
          
            p
          
        
        =
        0
      
    
    {\displaystyle |0|_{p}=0}
   and 
  
    
      
        
          |
        
        m
        
          /
        
        n
        
          
            |
          
          
            p
          
        
        =
        
          |
        
        m
        
          
            |
          
          
            p
          
        
        
          /
        
        
          |
        
        n
        
          
            |
          
          
            p
          
        
      
    
    {\displaystyle |m/n|_{p}=|m|_{p}/|n|_{p}}
  ). It defines a distance function on 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
   and the completion of 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
   as a metric space is denoted by 
  
    
      
        
          
            Q
          
          
            p
          
        
        .
      
    
    {\displaystyle \mathbb {Q} _{p}.}
   It is again a field since the field operations extend to the completion. The subring of 
  
    
      
        
          
            Q
          
          
            p
          
        
      
    
    {\displaystyle \mathbb {Q} _{p}}
   consisting of elements x with 
  
    
      
        
          |
        
        x
        
          
            |
          
          
            p
          
        
        ≤
        1
      
    
    {\displaystyle |x|_{p}\leq 1}
   is isomorphic to 
  
    
      
        
          
            Z
          
          
            p
          
        
        .
      
    
    {\displaystyle \mathbb {Z} _{p}.}
  
Similarly, the formal power series ring R[{[t]}] is the completion of R[t] at (t) (see also Hensel's lemma)
A complete ring has much simpler structure than a commutative ring. This owns to the Cohen structure theorem, which says, roughly, that a complete local ring tends to look like a formal power series ring or a quotient of it. On the other hand, the interaction between the integral closure and completion has been among the most important aspects that distinguish modern commutative ring theory from the classical one developed by the likes of Noether. Pathological examples found by Nagata led to the reexamination of the roles of Noetherian rings and motivated, among other things, the definition of excellent ring.


=== Rings with generators and relations ===
The most general way to construct a ring is by specifying generators and relations. Let F be a free ring (that is, free algebra over the integers) with the set X of symbols, that is, F consists of polynomials with integral coefficients in noncommuting variables that are elements of X. A free ring satisfies the universal property: any function from the set X to a ring R factors through F so that F → R is the unique ring homomorphism. Just as in the group case, every ring can be represented as a quotient of a free ring.Now, we can impose relations among symbols in X by taking a quotient. Explicitly, if E is a subset of F, then the quotient ring of F by the ideal generated by E is called the ring with generators X and relations E. If we used a ring, say, A as a base ring instead of 
  
    
      
        
          Z
        
        ,
      
    
    {\displaystyle \mathbb {Z} ,}
   then the resulting ring will be over A. For example, if 
  
    
      
        E
        =
        {
        x
        y
        −
        y
        x
        ∣
        x
        ,
        y
        ∈
        X
        }
        ,
      
    
    {\displaystyle E=\{xy-yx\mid x,y\in X\},}
   then the resulting ring will be the usual polynomial ring with coefficients in A in variables that are elements of X (It is also the same thing as the symmetric algebra over A with symbols X.)
In the category-theoretic terms, the formation 
  
    
      
        S
        ↦
        
          the free ring generated by the set 
        
        S
      
    
    {\displaystyle S\mapsto {\text{the free ring generated by the set }}S}
   is the left adjoint functor of the forgetful functor from the category of rings to Set (and it is often called the free ring functor.)

Let A, B be algebras over a commutative ring R. Then the tensor product of R-modules 
  
    
      
        A
        
          ⊗
          
            R
          
        
        B
      
    
    {\displaystyle A\otimes _{R}B}
   is an R-algebra with multiplication characterized by 
  
    
      
        (
        x
        ⊗
        u
        )
        (
        y
        ⊗
        v
        )
        =
        x
        y
        ⊗
        u
        v
        .
      
    
    {\displaystyle (x\otimes u)(y\otimes v)=xy\otimes uv.}
   


== Special kinds of rings ==


=== Domains ===
A nonzero ring with no nonzero zero-divisors is called a domain. A commutative domain is called an integral domain. The most important integral domains are principal ideal domains, PIDs for short, and fields. A principal ideal domain is an integral domain in which every ideal is principal. An important class of integral domains that contain a PID is a unique factorization domain (UFD), an integral domain in which every nonunit element is a product of prime elements (an element is prime if it generates a prime ideal.) The fundamental question in algebraic number theory is on the extent to which the ring of (generalized) integers in a number field, where an ""ideal"" admits prime factorization, fails to be a PID.
Among theorems concerning a PID, the most important one is the structure theorem for finitely generated modules over a principal ideal domain. The theorem may be illustrated by the following application to linear algebra. Let V be a finite-dimensional vector space over a field k and f : V → V a linear map with minimal polynomial q. Then, since k[t] is a unique factorization domain, q factors into powers of distinct irreducible polynomials (that is, prime elements):

Letting 
  
    
      
        t
        ⋅
        v
        =
        f
        (
        v
        )
        ,
      
    
    {\displaystyle t\cdot v=f(v),}
   we make V a k[t]-module. The structure theorem then says V is a direct sum of cyclic modules, each of which is isomorphic to the module of the form 
  
    
      
        k
        [
        t
        ]
        
          /
        
        
          (
          
            p
            
              i
            
            
              
                k
                
                  j
                
              
            
          
          )
        
        .
      
    
    {\displaystyle k[t]/\left(p_{i}^{k_{j}}\right).}
   Now, if 
  
    
      
        
          p
          
            i
          
        
        (
        t
        )
        =
        t
        −
        
          λ
          
            i
          
        
        ,
      
    
    {\displaystyle p_{i}(t)=t-\lambda _{i},}
   then such a cyclic module (for pi) has a basis in which the restriction of f is represented by a Jordan matrix. Thus, if, say, k is algebraically closed, then all pi's are of the form t – λi and the above decomposition corresponds to the Jordan canonical form of f.
In algebraic geometry, UFDs arise because of smoothness. More precisely, a point in a variety (over a perfect field) is smooth if the local ring at the point is a regular local ring. A regular local ring is a UFD.The following is a chain of class inclusions that describes the relationship between rings, domains and fields:

rngs ⊃ rings ⊃ commutative rings ⊃  integral domains ⊃ integrally closed domains ⊃ GCD domains ⊃ unique factorization domains ⊃ principal ideal domains ⊃ Euclidean domains ⊃ fields ⊃ algebraically closed fields


=== Division ring ===
A division ring is a ring such that every non-zero element is a unit. A commutative division ring is a field. A prominent example of a division ring that is not a field is the ring of quaternions. Any centralizer in a division ring is also a division ring. In particular, the center of a division ring is a field. It turned out that every finite domain (in particular finite division ring) is a field; in particular commutative (the Wedderburn's little theorem).
Every module over a division ring is a free module (has a basis); consequently, much of linear algebra can be carried out over a division ring instead of a field.
The study of conjugacy classes figures prominently in the classical theory of division rings; see, for example, the Cartan–Brauer–Hua theorem.
A cyclic algebra, introduced by L. E. Dickson, is a generalization of a quaternion algebra.


=== Semisimple rings ===

A semisimple module is a direct sum of simple modules.  A semisimple ring is a ring that is semisimple as a left module (or right module) over itself.


==== Examples ====
A division ring is semisimple (and simple).
For any division ring D and positive integer n, the matrix ring Mn(D) is semisimple (and simple).
For a field k and finite group G, the group ring kG is semisimple if and only if the characteristic of k does not divide the order of G (Maschke's theorem).
Clifford algebras are semisimple.The Weyl algebra over a field is a simple ring, but it is not semisimple.  The same holds for a ring of differential operators in many variables.


==== Properties ====
Any module over a semisimple ring is semisimple. (Proof: A free module over a semisimple ring is semisimple and any module is a quotient of a free module.)
For a ring R, the following are equivalent:

R is semisimple.
R is artinian and semiprimitive.
R is a finite direct product 
  
    
      
        
          ∏
          
            i
            =
            1
          
          
            r
          
        
        
          M
          
            
              n
              
                i
              
            
          
        
        ⁡
        (
        
          D
          
            i
          
        
        )
      
    
    {\textstyle \prod _{i=1}^{r}\operatorname {M} _{n_{i}}(D_{i})}
   where each ni is a positive integer, and each Di is a division ring (Artin–Wedderburn theorem).Semisimplicity is closely related to separability. A unital associative algebra A over a field k is said to be separable if the base extension 
  
    
      
        A
        
          ⊗
          
            k
          
        
        F
      
    
    {\displaystyle A\otimes _{k}F}
   is semisimple for every field extension F / k. If A happens to be a field, then this is equivalent to the usual definition in field theory (cf. separable extension.)


=== Central simple algebra and Brauer group ===

For a field k, a k-algebra is central if its center is k and is simple if it is a simple ring. Since the center of a simple k-algebra is a field, any simple k-algebra is a central simple algebra over its center. In this section, a central simple algebra is assumed to have finite dimension. Also, we mostly fix the base field; thus, an algebra refers to a k-algebra. The matrix ring of size n over a ring R will be denoted by Rn.
The Skolem–Noether theorem states any automorphism of a central simple algebra is inner.
Two central simple algebras A and B are said to be similar if there are integers n and m such that 
  
    
      
        A
        
          ⊗
          
            k
          
        
        
          k
          
            n
          
        
        ≈
        B
        
          ⊗
          
            k
          
        
        
          k
          
            m
          
        
        .
      
    
    {\displaystyle A\otimes _{k}k_{n}\approx B\otimes _{k}k_{m}.}
   Since 
  
    
      
        
          k
          
            n
          
        
        
          ⊗
          
            k
          
        
        
          k
          
            m
          
        
        ≃
        
          k
          
            n
            m
          
        
        ,
      
    
    {\displaystyle k_{n}\otimes _{k}k_{m}\simeq k_{nm},}
   the similarity is an equivalence relation. The similarity classes [A] with the multiplication 
  
    
      
        [
        A
        ]
        [
        B
        ]
        =
        
          [
          
            A
            
              ⊗
              
                k
              
            
            B
          
          ]
        
      
    
    {\displaystyle [A][B]=\left[A\otimes _{k}B\right]}
   form an abelian group called the Brauer group of k and is denoted by Br(k). By the Artin–Wedderburn theorem, a central simple algebra is the matrix ring of a division ring; thus, each similarity class is represented by a unique division ring.
For example, Br(k) is trivial if k is a finite field or an algebraically closed field (more generally quasi-algebraically closed field; cf. Tsen's theorem). 
  
    
      
        Br
        ⁡
        (
        
          R
        
        )
      
    
    {\displaystyle \operatorname {Br} (\mathbb {R} )}
   has order 2 (a special case of the theorem of Frobenius). Finally, if k is a nonarchimedean local field (for example, 
  
    
      
        
          
            Q
          
          
            p
          
        
      
    
    {\displaystyle \mathbb {Q} _{p}}
  ), then 
  
    
      
        Br
        ⁡
        (
        k
        )
        =
        
          Q
        
        
          /
        
        
          Z
        
      
    
    {\displaystyle \operatorname {Br} (k)=\mathbb {Q} /\mathbb {Z} }
   through the invariant map.
Now, if F is a field extension of k, then the base extension 
  
    
      
        −
        
          ⊗
          
            k
          
        
        F
      
    
    {\displaystyle -\otimes _{k}F}
   induces Br(k) → Br(F). Its kernel is denoted by Br(F/k). It consists of [A] such that 
  
    
      
        A
        
          ⊗
          
            k
          
        
        F
      
    
    {\displaystyle A\otimes _{k}F}
   is a matrix ring over F (that is, A is split by F.) If the extension is finite and Galois, then Br(F/k) is canonically isomorphic to 
  
    
      
        
          H
          
            2
          
        
        
          (
          
            Gal
            ⁡
            (
            F
            
              /
            
            k
            )
            ,
            
              k
              
                ∗
              
            
          
          )
        
        .
      
    
    {\displaystyle H^{2}\left(\operatorname {Gal} (F/k),k^{*}\right).}
  Azumaya algebras generalize the notion of central simple algebras to a commutative local ring.


=== Valuation ring ===

If K is a field, a valuation v is a group homomorphism from the multiplicative group K∗ to a totally ordered abelian group G such that, for any f, g in K with f + g nonzero, v(f + g) ≥ min{v(f), v(g)}. The valuation ring of v is the subring of K consisting of zero and all nonzero f such that v(f) ≥ 0.
Examples:

The field of formal Laurent series 
  
    
      
        k
        (
        
        (
        t
        )
        
        )
      
    
    {\displaystyle k(\!(t)\!)}
   over a field k comes with the valuation v such that v(f) is the least degree of a nonzero term in f; the valuation ring of v is the formal power series ring 
  
    
      
        k
        [
        
        [
        t
        ]
        
        ]
        .
      
    
    {\displaystyle k[\![t]\!].}
  
More generally, given a field k and a totally ordered abelian group G, let 
  
    
      
        k
        (
        
        (
        G
        )
        
        )
      
    
    {\displaystyle k(\!(G)\!)}
   be the set of all functions from G to k whose supports (the sets of points at which the functions are nonzero) are well ordered. It is a field with the multiplication given by convolution:  It also comes with the valuation v such that v(f) is the least element in the support of f. The subring consisting of elements with finite support is called the group ring of G (which makes sense even if G is not commutative). If G is the ring of integers, then we recover the previous example (by identifying f with the series whose n-th coefficient is f(n).)


== Rings with extra structure ==
A ring may be viewed as an abelian group (by using the addition operation), with extra structure: namely, ring multiplication. In the same way, there are other mathematical objects which may be considered as rings with extra structure. For example:

An associative algebra is a ring that is also a vector space over a field n such that the scalar multiplication is compatible with the ring multiplication. For instance, the set of n-by-n matrices over the real field 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   has dimension n2 as a real vector space.
A ring R is a topological ring if its set of elements R is given a topology which makes the addition map ( 
  
    
      
        +
        :
        R
        ×
        R
        →
        R
        
      
    
    {\displaystyle +:R\times R\to R\,}
  ) and the multiplication map 
  
    
      
        ⋅
        :
        R
        ×
        R
        →
        R
      
    
    {\displaystyle \cdot :R\times R\to R}
   to be both continuous as maps between topological spaces (where X × X inherits the product topology or any other product in the category). For example, n-by-n matrices over the real numbers could be given either the Euclidean topology, or the Zariski topology, and in either case one would obtain a topological ring.
A λ-ring is a commutative ring R together with operations λn: R → R that are like n-th exterior powers:
  
    
      
        
          λ
          
            n
          
        
        (
        x
        +
        y
        )
        =
        
          ∑
          
            0
          
          
            n
          
        
        
          λ
          
            i
          
        
        (
        x
        )
        
          λ
          
            n
            −
            i
          
        
        (
        y
        )
        .
      
    
    {\displaystyle \lambda ^{n}(x+y)=\sum _{0}^{n}\lambda ^{i}(x)\lambda ^{n-i}(y).}
  
For example, 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   is a λ-ring with 
  
    
      
        
          λ
          
            n
          
        
        (
        x
        )
        =
        
          
            
              (
            
            
              x
              n
            
            
              )
            
          
        
        ,
      
    
    {\displaystyle \lambda ^{n}(x)={\binom {x}{n}},}
   the binomial coefficients. The notion plays a central rule in the algebraic approach to the Riemann–Roch theorem.A totally ordered ring is a ring with a total ordering that is compatible with ring operations.


== Some examples of the ubiquity of rings ==
Many different kinds of mathematical objects can be fruitfully analyzed in terms of some associated ring.


=== Cohomology ring of a topological space ===
To any topological space X one can associate its integral cohomology ring

  
    
      
        
          H
          
            ∗
          
        
        (
        X
        ,
        
          Z
        
        )
        =
        
          ⨁
          
            i
            =
            0
          
          
            ∞
          
        
        
          H
          
            i
          
        
        (
        X
        ,
        
          Z
        
        )
        ,
      
    
    {\displaystyle H^{*}(X,\mathbb {Z} )=\bigoplus _{i=0}^{\infty }H^{i}(X,\mathbb {Z} ),}
  a graded ring. There are also homology groups 
  
    
      
        
          H
          
            i
          
        
        (
        X
        ,
        
          Z
        
        )
      
    
    {\displaystyle H_{i}(X,\mathbb {Z} )}
   of a space, and indeed these were defined first, as a useful tool for distinguishing between certain pairs of topological spaces, like the spheres and tori, for which the methods of point-set topology are not well-suited. Cohomology groups were later defined in terms of homology groups in a way which is roughly analogous to the dual of a vector space. To know each individual integral homology group is essentially the same as knowing each individual integral cohomology group, because of the universal coefficient theorem. However, the advantage of the cohomology groups is that there is a natural product, which is analogous to the observation that one can multiply pointwise a k-multilinear form and an l-multilinear form to get a (k + l)-multilinear form.
The ring structure in cohomology provides the foundation for characteristic classes of fiber bundles, intersection theory on manifolds and algebraic varieties, Schubert calculus and much more.


=== Burnside ring of a group ===
To any group is associated its Burnside ring which uses a ring to describe the various ways the group can act on a finite set. The Burnside ring's additive group is the free abelian group whose basis are the transitive actions of the group and whose addition is the disjoint union of the action. Expressing an action in terms of the basis is decomposing an action into its transitive constituents. The multiplication is easily expressed in terms of the representation ring: the multiplication in the Burnside ring is formed by writing the tensor product of two permutation modules as a permutation module. The ring structure allows a formal way of subtracting one action from another. Since the Burnside ring is contained as a finite index subring of the representation ring, one can pass easily from one to the other by extending the coefficients from integers to the rational numbers.


=== Representation ring of a group ring ===
To any group ring or Hopf algebra is associated its representation ring or ""Green ring"". The representation ring's additive group is the free abelian group whose basis are the indecomposable modules and whose addition corresponds to the direct sum. Expressing a module in terms of the basis is finding an indecomposable decomposition of the module. The multiplication is the tensor product. When the algebra is semisimple, the representation ring is just the character ring from character theory, which is more or less the Grothendieck group given a ring structure.


=== Function field of an irreducible algebraic variety ===
To any irreducible algebraic variety is associated its function field. The points of an algebraic variety correspond to valuation rings contained in the function field and containing the coordinate ring. The study of algebraic geometry makes heavy use of commutative algebra to study geometric concepts in terms of ring-theoretic properties. Birational geometry studies maps between the subrings of the function field.


=== Face ring of a simplicial complex ===
Every simplicial complex has an associated face ring, also called its Stanley–Reisner ring. This ring reflects many of the combinatorial properties of the simplicial complex, so it is of particular interest in algebraic combinatorics. In particular, the algebraic geometry of the Stanley–Reisner ring was used to characterize the numbers of faces in each dimension of simplicial polytopes.


== Category-theoretic description ==

Every ring can be thought of as a monoid in Ab, the category of abelian groups (thought of as a monoidal category under the tensor product of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
  -modules). The monoid action of a ring R on an abelian group is simply an R-module. Essentially, an R-module is a generalization of the notion of a vector space – where rather than a vector space over a field, one has a ""vector space over a ring"".
Let (A, +) be an abelian group and let End(A) be its endomorphism ring (see above). Note that, essentially, End(A) is the set of all morphisms of A, where if f is in End(A), and g is in End(A), the following rules may be used to compute f + g and f ⋅ g:

  
    
      
        
          
            
              
              
                
                (
                f
                +
                g
                )
                (
                x
                )
                =
                f
                (
                x
                )
                +
                g
                (
                x
                )
              
            
            
              
              
                
                (
                f
                ⋅
                g
                )
                (
                x
                )
                =
                f
                (
                g
                (
                x
                )
                )
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&(f+g)(x)=f(x)+g(x)\\&(f\cdot g)(x)=f(g(x)),\end{aligned}}}
  where + as in f(x) + g(x) is addition in A, and function composition is denoted from right to left. Therefore, associated to any abelian group, is a ring. Conversely, given any ring, (R, +, ⋅ ), (R, +) is an abelian group. Furthermore, for every r in R, right (or left) multiplication by r gives rise to a morphism of (R, +), by right (or left) distributivity. Let A = (R, +). Consider those endomorphisms of A, that ""factor through"" right (or left) multiplication of R. In other words, let EndR(A) be the set of all morphisms m of A, having the property that m(r ⋅ x) = r ⋅ m(x). It was seen that every r in R gives rise to a morphism of A: right multiplication by r. It is in fact true that this association of any element of R, to a morphism of A, as a function from R to EndR(A), is an isomorphism of rings. In this sense, therefore, any ring can be viewed as the endomorphism ring of some abelian X-group (by X-group, it is meant a group with X being its set of operators). In essence, the most general form of a ring, is the endomorphism group of some abelian X-group.
Any ring can be seen as a preadditive category with a single object. It is therefore natural to consider arbitrary preadditive categories to be generalizations of rings. And indeed, many definitions and theorems originally given for rings can be translated to this more general context. Additive functors between preadditive categories generalize the concept of ring homomorphism, and ideals in additive categories can be defined as sets of morphisms closed under addition and under composition with arbitrary morphisms.


== Generalization ==
Algebraists have defined structures more general than rings by weakening or dropping some of ring axioms.


=== Rng ===
A rng is the same as a ring, except that the existence of a multiplicative identity is not assumed.


=== Nonassociative ring ===
A nonassociative ring is an algebraic structure that satisfies all of the ring axioms except the associative property and the existence of a multiplicative identity. A notable example is a Lie algebra. There exists some structure theory for such algebras that generalizes the analogous results for Lie algebras and associative algebras.


=== Semiring ===
A semiring (sometimes rig) is obtained by weakening the assumption that (R, +) is an abelian group to the assumption that (R, +) is a commutative monoid, and adding the axiom that 0 ⋅ a = a ⋅ 0 = 0 for all a in R (since it no longer follows from the other axioms).
Examples:

the non-negative integers 
  
    
      
        {
        0
        ,
        1
        ,
        2
        ,
        …
        }
      
    
    {\displaystyle \{0,1,2,\ldots \}}
   with ordinary addition and multiplication;
the tropical semiring.


== Other ring-like objects ==


=== Ring object in a category ===
Let C be a category with finite products. Let pt denote a terminal object of C (an empty product). A ring object in C is an object R equipped with morphisms 
  
    
      
        R
        ×
        R
        
        
          
            
              
                →
              
              
                a
              
            
          
        
        
        R
      
    
    {\displaystyle R\times R\;{\stackrel {a}{\to }}\,R}
   (addition), 
  
    
      
        R
        ×
        R
        
        
          
            
              
                →
              
              
                m
              
            
          
        
        
        R
      
    
    {\displaystyle R\times R\;{\stackrel {m}{\to }}\,R}
   (multiplication), 
  
    
      
        pt
        
          
            
              
                →
              
              
                0
              
            
          
        
        
        R
      
    
    {\displaystyle \operatorname {pt} {\stackrel {0}{\to }}\,R}
   (additive identity), 
  
    
      
        R
        
        
          
            
              
                →
              
              
                i
              
            
          
        
        
        R
      
    
    {\displaystyle R\;{\stackrel {i}{\to }}\,R}
   (additive inverse), and 
  
    
      
        pt
        
          
            
              
                →
              
              
                1
              
            
          
        
        
        R
      
    
    {\displaystyle \operatorname {pt} {\stackrel {1}{\to }}\,R}
   (multiplicative identity) satisfying the usual ring axioms. Equivalently, a ring object is an object R equipped with a factorization of its functor of points 
  
    
      
        
          h
          
            R
          
        
        =
        Hom
        ⁡
        (
        −
        ,
        R
        )
        :
        
          C
          
            op
          
        
        →
        
          S
          e
          t
          s
        
      
    
    {\displaystyle h_{R}=\operatorname {Hom} (-,R):C^{\operatorname {op} }\to \mathbf {Sets} }
   through the category of rings: 
  
    
      
        
          C
          
            op
          
        
        →
        
          R
          i
          n
          g
          s
        
        
          
            
              
                ⟶
              
              
                
                  forgetful
                
              
            
          
        
        
          S
          e
          t
          s
        
        .
      
    
    {\displaystyle C^{\operatorname {op} }\to \mathbf {Rings} {\stackrel {\textrm {forgetful}}{\longrightarrow }}\mathbf {Sets} .}
  


=== Ring scheme ===
In algebraic geometry, a ring scheme over a base scheme S is a ring object in the category of S-schemes. One example is the ring scheme Wn over 
  
    
      
        Spec
        ⁡
        
          Z
        
      
    
    {\displaystyle \operatorname {Spec} \mathbb {Z} }
  , which for any commutative ring A returns the ring Wn(A) of p-isotypic Witt vectors of length n over A.


=== Ring spectrum ===
In algebraic topology, a ring spectrum is a spectrum X together with a multiplication 
  
    
      
        μ
        :
        X
        ∧
        X
        →
        X
      
    
    {\displaystyle \mu :X\wedge X\to X}
   and a unit map S → X from the sphere spectrum S, such that the ring axiom diagrams commute up to homotopy. In practice, it is common to define a ring spectrum as a monoid object in a good category of spectra such as the category of symmetric spectra.


== See also ==

Special types of rings:


== Notes ==


== Citations ==


== References ==


=== General references ===


=== Special references ===


=== Primary sources ===


=== Historical references ==="
7eb5e33606,Group (mathematics),"In mathematics, a group is a non-empty set and an operation that combines any two elements of the set to produce a third element of the set, in such a way that the operation is associative, an identity element exists and every element has an inverse. These three axioms hold for number systems and many other mathematical structures. For example, the integers together with the addition operation form a group. The concept of a group and the axioms that define it were elaborated for handling, in a unified way, essential structural properties of very different mathematical entities such as numbers, geometric shapes and polynomial roots. Because the concept of groups is ubiquitous in numerous areas both within and outside mathematics, some authors consider it as a central organizing principle of contemporary mathematics.In geometry groups arise naturally in the study of symmetries and geometric transformations: The symmetries of an object form a group, called the symmetry group of the object, and the transformations of a given type form a general group. Lie groups appear in  symmetry groups in geometry, and also in the Standard Model of particle physics. The Poincaré group is a Lie group consisting of the symmetries of spacetime in special relativity. Point groups describe symmetry in molecular chemistry.
The concept of a group arose in the study of polynomial equations, starting with Évariste Galois in the 1830s, who introduced the term group (French: groupe) for the symmetry group of the roots of an equation, now called a Galois group. After contributions from other fields such as number theory and geometry, the group notion was generalized and firmly established around 1870. Modern group theory—an active mathematical discipline—studies groups in their own right. To explore groups, mathematicians have devised various notions to break groups into smaller, better-understandable pieces, such as subgroups, quotient groups and simple groups. In addition to their abstract properties, group theorists also study the different ways in which a group can be expressed concretely, both from a point of view of representation theory (that is, through the representations of the group) and of computational group theory. A theory has been developed for finite groups, which culminated with the classification of finite simple groups, completed in 2004. Since the mid-1980s, geometric group theory, which studies finitely generated groups as geometric objects, has become an active area in group theory.


== Definition and illustration ==


=== First example: the integers ===
One of the more familiar groups is the set of integers 

together with addition. For any two integers 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  , the sum 
  
    
      
        a
        +
        b
      
    
    {\displaystyle a+b}
   is also an integer; this closure property says that 
  
    
      
        +
      
    
    {\displaystyle +}
   is a binary operation on 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
  . The following properties of integer addition serve as a model for the group axioms in the definition below.

For all integers 
  
    
      
        a
      
    
    {\displaystyle a}
  , 
  
    
      
        b
      
    
    {\displaystyle b}
   and 
  
    
      
        c
      
    
    {\displaystyle c}
  , one has 
  
    
      
        (
        a
        +
        b
        )
        +
        c
        =
        a
        +
        (
        b
        +
        c
        )
      
    
    {\displaystyle (a+b)+c=a+(b+c)}
  . Expressed in words, adding 
  
    
      
        a
      
    
    {\displaystyle a}
   to 
  
    
      
        b
      
    
    {\displaystyle b}
   first, and then adding the result to 
  
    
      
        c
      
    
    {\displaystyle c}
   gives the same final result as adding 
  
    
      
        a
      
    
    {\displaystyle a}
   to the sum of 
  
    
      
        b
      
    
    {\displaystyle b}
   and 
  
    
      
        c
      
    
    {\displaystyle c}
  . This property is known as associativity.
If 
  
    
      
        a
      
    
    {\displaystyle a}
   is any integer, then 
  
    
      
        0
        +
        a
        =
        a
      
    
    {\displaystyle 0+a=a}
   and 
  
    
      
        a
        +
        0
        =
        a
      
    
    {\displaystyle a+0=a}
  . Zero is called the identity element of addition because adding it to any integer returns the same integer.
For every integer 
  
    
      
        a
      
    
    {\displaystyle a}
  , there is an integer 
  
    
      
        b
      
    
    {\displaystyle b}
   such that 
  
    
      
        a
        +
        b
        =
        0
      
    
    {\displaystyle a+b=0}
   and 
  
    
      
        b
        +
        a
        =
        0
      
    
    {\displaystyle b+a=0}
  . The integer 
  
    
      
        b
      
    
    {\displaystyle b}
   is called the inverse element of the integer 
  
    
      
        a
      
    
    {\displaystyle a}
   and is denoted 
  
    
      
        −
        a
      
    
    {\displaystyle -a}
  .The integers, together with the operation 
  
    
      
        +
      
    
    {\displaystyle +}
  , form a mathematical object belonging to a broad class sharing similar structural aspects. To appropriately understand these structures as a collective, the following definition is developed.


=== Definition ===

A group is a non-empty set 
  
    
      
        G
      
    
    {\displaystyle G}
   together with a binary operation on 
  
    
      
        G
      
    
    {\displaystyle G}
  , here denoted ""
  
    
      
        ⋅
      
    
    {\displaystyle \cdot }
  "", that combines any two elements 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   of 
  
    
      
        G
      
    
    {\displaystyle G}
   to form an element of 
  
    
      
        G
      
    
    {\displaystyle G}
  , denoted 
  
    
      
        a
        ⋅
        b
      
    
    {\displaystyle a\cdot b}
  , such that the following three requirements, known as group axioms, are satisfied:
Associativity
For all 
  
    
      
        a
      
    
    {\displaystyle a}
  , 
  
    
      
        b
      
    
    {\displaystyle b}
  , 
  
    
      
        c
      
    
    {\displaystyle c}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
  , one has 
  
    
      
        (
        a
        ⋅
        b
        )
        ⋅
        c
        =
        a
        ⋅
        (
        b
        ⋅
        c
        )
      
    
    {\displaystyle (a\cdot b)\cdot c=a\cdot (b\cdot c)}
  .
Identity element
There exists an element 
  
    
      
        e
      
    
    {\displaystyle e}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
   such that, for every 
  
    
      
        a
      
    
    {\displaystyle a}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
  , one has 
  
    
      
        e
        ⋅
        a
        =
        a
      
    
    {\displaystyle e\cdot a=a}
   and 
  
    
      
        a
        ⋅
        e
        =
        a
      
    
    {\displaystyle a\cdot e=a}
  .
Such an element is unique (see below). It is called the identity element of the group.
Inverse element
For each 
  
    
      
        a
      
    
    {\displaystyle a}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
  , there exists an element 
  
    
      
        b
      
    
    {\displaystyle b}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
   such that 
  
    
      
        a
        ⋅
        b
        =
        e
      
    
    {\displaystyle a\cdot b=e}
   and 
  
    
      
        b
        ⋅
        a
        =
        e
      
    
    {\displaystyle b\cdot a=e}
  , where 
  
    
      
        e
      
    
    {\displaystyle e}
   is the identity element.
For each 
  
    
      
        a
      
    
    {\displaystyle a}
  , the element 
  
    
      
        b
      
    
    {\displaystyle b}
   is unique (see below); it is called the inverse of 
  
    
      
        a
      
    
    {\displaystyle a}
   and is commonly denoted 
  
    
      
        
          a
          
            −
            1
          
        
      
    
    {\displaystyle a^{-1}}
  .


=== Notation and terminology ===
Formally, the group is the ordered pair of a set and a binary operation on this set that satisfies the group axioms. The set is called the underlying set of the group, and the operation is called the group operation or the group law.
A group and its underlying set are thus two different mathematical objects. To avoid cumbersome notation, it is common to abuse notation by using the same symbol to denote both. This reflects also an informal way of thinking: that the group is the same as the set except that it has been enriched by additional structure provided by the operation.
For example, consider the set of real numbers 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , which has the operations of addition 
  
    
      
        a
        +
        b
      
    
    {\displaystyle a+b}
   and multiplication 
  
    
      
        a
        b
      
    
    {\displaystyle ab}
  . Formally, 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is a set, 
  
    
      
        (
        
          R
        
        ,
        +
        )
      
    
    {\displaystyle (\mathbb {R} ,+)}
   is a group, and 
  
    
      
        (
        
          R
        
        ,
        +
        ,
        ⋅
        )
      
    
    {\displaystyle (\mathbb {R} ,+,\cdot )}
   is a field. But it is common to write 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   to denote any of these three objects.
The additive group of the field 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is the group whose underlying set is 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   and whose operation is addition. The multiplicative group of the field 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is the group 
  
    
      
        
          
            R
          
          
            ×
          
        
      
    
    {\displaystyle \mathbb {R} ^{\times }}
   whose underlying set is the set of nonzero real numbers 
  
    
      
        
          R
        
        ∖
        {
        0
        }
      
    
    {\displaystyle \mathbb {R} \smallsetminus \{0\}}
   and whose operation is multiplication.
More generally, one speaks of an additive group whenever the group operation is notated as addition; in this case, the identity is typically denoted 
  
    
      
        0
      
    
    {\displaystyle 0}
  , and the inverse of an element 
  
    
      
        x
      
    
    {\displaystyle x}
   is denoted 
  
    
      
        −
        x
      
    
    {\displaystyle -x}
  . Similarly, one speaks of a multiplicative group whenever the group operation is notated as multiplication; in this case, the identity is typically denoted 
  
    
      
        1
      
    
    {\displaystyle 1}
  , and the inverse of an element 
  
    
      
        x
      
    
    {\displaystyle x}
   is denoted 
  
    
      
        
          x
          
            −
            1
          
        
      
    
    {\displaystyle x^{-1}}
  . In a multiplicative group, the operation symbol is usually omitted entirely, so that the operation is denoted by juxtaposition, 
  
    
      
        a
        b
      
    
    {\displaystyle ab}
   instead of 
  
    
      
        a
        ⋅
        b
      
    
    {\displaystyle a\cdot b}
  .
The definition of a group does not require that 
  
    
      
        a
        ⋅
        b
        =
        b
        ⋅
        a
      
    
    {\displaystyle a\cdot b=b\cdot a}
   for all elements 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
  . If this additional condition holds, then the operation is said to be commutative, and the group is called an abelian group. It is a common convention that for an abelian group either additive or multiplicative notation may be used, but for a nonabelian group only multiplicative notation is used.
Several other notations are commonly used for groups whose elements are not numbers. For a group whose elements are functions, the operation is often function composition 
  
    
      
        f
        ∘
        g
      
    
    {\displaystyle f\circ g}
  ; then the identity may be denoted id. In the more specific cases of geometric transformation groups, symmetry groups, permutation groups, and automorphism groups, the symbol 
  
    
      
        ∘
      
    
    {\displaystyle \circ }
   is often omitted, as for multiplicative groups. Many other variants of notation may be encountered.


=== Second example: a symmetry group ===
Two figures in the plane are congruent if one can be changed into the other using a combination of rotations, reflections, and translations. Any figure is congruent to itself. However, some figures are congruent to themselves in more than one way, and these extra congruences are called symmetries. A square has eight symmetries. These are:

the identity operation leaving everything unchanged, denoted id;
rotations of the square around its center by 90°, 180°, and 270° clockwise, denoted by 
  
    
      
        
          r
          
            1
          
        
      
    
    {\displaystyle r_{1}}
  , 
  
    
      
        
          r
          
            2
          
        
      
    
    {\displaystyle r_{2}}
   and 
  
    
      
        
          r
          
            3
          
        
      
    
    {\displaystyle r_{3}}
  , respectively;
reflections about the horizontal and vertical middle line (
  
    
      
        
          f
          
            
              v
            
          
        
      
    
    {\displaystyle f_{\mathrm {v} }}
   and 
  
    
      
        
          f
          
            
              h
            
          
        
      
    
    {\displaystyle f_{\mathrm {h} }}
  ), or through the two diagonals (
  
    
      
        
          f
          
            
              d
            
          
        
      
    
    {\displaystyle f_{\mathrm {d} }}
   and 
  
    
      
        
          f
          
            
              c
            
          
        
      
    
    {\displaystyle f_{\mathrm {c} }}
  ).
These symmetries are functions. Each sends a point in the square to the corresponding point under the symmetry. For example, 
  
    
      
        
          r
          
            1
          
        
      
    
    {\displaystyle r_{1}}
   sends a point to its rotation 90° clockwise around the square's center, and 
  
    
      
        
          f
          
            
              h
            
          
        
      
    
    {\displaystyle f_{\mathrm {h} }}
   sends a point to its reflection across the square's vertical middle line. Composing two of these symmetries gives another symmetry. These symmetries determine a group called the dihedral group of degree four, denoted 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
  . The underlying set of the group is the above set of symmetries, and the group operation is function composition. Two symmetries are combined by composing them as functions, that is, applying the first one to the square, and the second one to the result of the first application. The result of performing first 
  
    
      
        a
      
    
    {\displaystyle a}
   and then 
  
    
      
        b
      
    
    {\displaystyle b}
   is written symbolically from right to left as 
  
    
      
        b
        ∘
        a
      
    
    {\displaystyle b\circ a}
   (""apply the symmetry 
  
    
      
        b
      
    
    {\displaystyle b}
   after performing the symmetry 
  
    
      
        a
      
    
    {\displaystyle a}
  ""). This is the usual notation for composition of functions.
The group table lists the results of all such compositions possible. For example, rotating by 270° clockwise (
  
    
      
        
          r
          
            3
          
        
      
    
    {\displaystyle r_{3}}
  ) and then reflecting horizontally (
  
    
      
        
          f
          
            
              h
            
          
        
      
    
    {\displaystyle f_{\mathrm {h} }}
  ) is the same as performing a reflection along the diagonal (
  
    
      
        
          f
          
            
              d
            
          
        
      
    
    {\displaystyle f_{\mathrm {d} }}
  ). Using the above symbols, highlighted in blue in the group table:

Given this set of symmetries and the described operation, the group axioms can be understood as follows.
Binary operation: Composition is a binary operation. That is, 
  
    
      
        a
        ∘
        b
      
    
    {\displaystyle a\circ b}
   is a symmetry for any two symmetries 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  . For example,

that is, rotating 270° clockwise after reflecting horizontally equals reflecting along the counter-diagonal (
  
    
      
        
          f
          
            
              c
            
          
        
      
    
    {\displaystyle f_{\mathrm {c} }}
  ). Indeed, every other combination of two symmetries still gives a symmetry, as can be checked using the group table.
Associativity: The associativity axiom deals with composing more than two symmetries: Starting with three elements 
  
    
      
        a
      
    
    {\displaystyle a}
  , 
  
    
      
        b
      
    
    {\displaystyle b}
   and 
  
    
      
        c
      
    
    {\displaystyle c}
   of 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
  , there are two possible ways of using these three symmetries in this order to determine a symmetry of the square. One of these ways is to first compose 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   into a single symmetry, then to compose that symmetry with 
  
    
      
        c
      
    
    {\displaystyle c}
  . The other way is to first compose 
  
    
      
        b
      
    
    {\displaystyle b}
   and 
  
    
      
        c
      
    
    {\displaystyle c}
  , then to compose the resulting symmetry with 
  
    
      
        a
      
    
    {\displaystyle a}
  . These two ways must give always the same result, that is,

For example, 
  
    
      
        (
        
          f
          
            
              d
            
          
        
        ∘
        
          f
          
            
              v
            
          
        
        )
        ∘
        
          r
          
            2
          
        
        =
        
          f
          
            
              d
            
          
        
        ∘
        (
        
          f
          
            
              v
            
          
        
        ∘
        
          r
          
            2
          
        
        )
      
    
    {\displaystyle (f_{\mathrm {d} }\circ f_{\mathrm {v} })\circ r_{2}=f_{\mathrm {d} }\circ (f_{\mathrm {v} }\circ r_{2})}
   can be checked using the group table:

Identity element: The identity element is 
  
    
      
        
          i
          d
        
      
    
    {\displaystyle \mathrm {id} }
  , as it does not change any symmetry 
  
    
      
        a
      
    
    {\displaystyle a}
   when composed with it either on the left or on the right.
Inverse element: Each symmetry has an inverse: 
  
    
      
        
          i
          d
        
      
    
    {\displaystyle \mathrm {id} }
  , the reflections 
  
    
      
        
          f
          
            
              h
            
          
        
      
    
    {\displaystyle f_{\mathrm {h} }}
  , 
  
    
      
        
          f
          
            
              v
            
          
        
      
    
    {\displaystyle f_{\mathrm {v} }}
  , 
  
    
      
        
          f
          
            
              d
            
          
        
      
    
    {\displaystyle f_{\mathrm {d} }}
  , 
  
    
      
        
          f
          
            
              c
            
          
        
      
    
    {\displaystyle f_{\mathrm {c} }}
   and the 180° rotation 
  
    
      
        
          r
          
            2
          
        
      
    
    {\displaystyle r_{2}}
   are their own inverse, because performing them twice brings the square back to its original orientation. The rotations 
  
    
      
        
          r
          
            3
          
        
      
    
    {\displaystyle r_{3}}
   and 
  
    
      
        
          r
          
            1
          
        
      
    
    {\displaystyle r_{1}}
   are each other's inverses, because rotating 90° and then rotation 270° (or vice versa) yields a rotation over 360° which leaves the square unchanged. This is easily verified on the table.
In contrast to the group of integers above, where the order of the operation is immaterial, it does matter in 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
  , as, for example, 
  
    
      
        
          f
          
            
              h
            
          
        
        ∘
        
          r
          
            1
          
        
        =
        
          f
          
            
              c
            
          
        
      
    
    {\displaystyle f_{\mathrm {h} }\circ r_{1}=f_{\mathrm {c} }}
   but 
  
    
      
        
          r
          
            1
          
        
        ∘
        
          f
          
            
              h
            
          
        
        =
        
          f
          
            
              d
            
          
        
      
    
    {\displaystyle r_{1}\circ f_{\mathrm {h} }=f_{\mathrm {d} }}
  . In other words, 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   is not abelian.


== History ==

The modern concept of an abstract group developed out of several fields of mathematics. The original motivation for group theory was the quest for solutions of polynomial equations of degree higher than 4. The 19th-century French mathematician Évariste Galois, extending prior work of Paolo Ruffini and Joseph-Louis Lagrange, gave a criterion for the solvability of a particular polynomial equation in terms of the symmetry group of its roots (solutions). The elements of such a Galois group correspond to certain permutations of the roots. At first, Galois's ideas were rejected by his contemporaries, and published only posthumously. More general permutation groups were investigated in particular by Augustin Louis Cauchy. Arthur Cayley's On the theory of groups, as depending on the symbolic equation 
  
    
      
        
          θ
          
            n
          
        
        =
        1
      
    
    {\displaystyle \theta ^{n}=1}
   (1854) gives the first abstract definition of a finite group.Geometry was a second field in which groups were used systematically, especially symmetry groups as part of Felix Klein's 1872 Erlangen program. After novel geometries such as hyperbolic and projective geometry had emerged, Klein used group theory to organize them in a more coherent way. Further advancing these ideas, Sophus Lie founded the study of Lie groups in 1884.The third field contributing to group theory was number theory. Certain abelian group structures had been used implicitly in Carl Friedrich Gauss's number-theoretical work Disquisitiones Arithmeticae (1798), and more explicitly by Leopold Kronecker. In 1847, Ernst Kummer made early attempts to prove Fermat's Last Theorem  by developing groups describing factorization into prime numbers.The convergence of these various sources into a uniform theory of groups started with Camille Jordan's Traité des substitutions et des équations algébriques (1870). Walther von Dyck (1882) introduced the idea of specifying a group by means of generators and relations, and was also the first to give an axiomatic definition of an ""abstract group"", in the terminology of the time. As of the 20th century, groups gained wide recognition by the pioneering work of Ferdinand Georg Frobenius and William Burnside, who worked on representation theory of finite groups, Richard Brauer's modular representation theory and Issai Schur's papers. The theory of Lie groups, and more generally locally compact groups was studied by Hermann Weyl, Élie Cartan and many others. Its algebraic counterpart, the theory of algebraic groups, was first shaped by Claude Chevalley (from the late 1930s) and later by the work of Armand Borel and Jacques Tits.The University of Chicago's 1960–61 Group Theory Year brought together group theorists such as Daniel Gorenstein, John G. Thompson and Walter Feit, laying the foundation of a collaboration that, with input from numerous other mathematicians, led to the classification of finite simple groups, with the final step taken by Aschbacher and Smith in 2004. This project exceeded previous mathematical endeavours by its sheer size, in both length of proof and number of researchers. Research concerning this classification proof is ongoing. Group theory remains a highly active mathematical branch, impacting many other fields, as the examples below illustrate.


== Elementary consequences of the group axioms ==
Basic facts about all groups that can be obtained directly from the group axioms are commonly subsumed under elementary group theory. For example, repeated applications of the associativity axiom show that the unambiguity of

generalizes to more than three factors. Because this implies that parentheses can be inserted anywhere within such a series of terms, parentheses are usually omitted.


=== Uniqueness of identity element ===
The group axioms imply that the identity element is unique: If 
  
    
      
        e
      
    
    {\displaystyle e}
   and 
  
    
      
        f
      
    
    {\displaystyle f}
   are identity elements of a group, then 
  
    
      
        e
        =
        e
        ⋅
        f
        =
        f
      
    
    {\displaystyle e=e\cdot f=f}
  . Therefore, it is customary to speak of the identity.


=== Uniqueness of inverses ===
The group axioms also imply that the inverse of each element is unique: if a group element 
  
    
      
        a
      
    
    {\displaystyle a}
   has both 
  
    
      
        b
      
    
    {\displaystyle b}
   and 
  
    
      
        c
      
    
    {\displaystyle c}
   as inverses, then

  
    
      
        
          
            
              
                b
              
              
                
                =
                b
                ⋅
                e
              
              
              
                
                  (
                
                e
                
                   is the identity element)
                
              
            
            
              
              
                
                =
                b
                ⋅
                (
                a
                ⋅
                c
                )
              
              
              
                
                  (
                
                c
                
                   is an inverse)
                
              
            
            
              
              
                
                =
                (
                b
                ⋅
                a
                )
                ⋅
                c
              
              
              
                
                  (associativity)
                
              
            
            
              
              
                
                =
                e
                ⋅
                c
              
              
              
                
                  (
                
                b
                
                   is an inverse)
                
              
            
            
              
              
                
                =
                c
              
              
              
                
                  (
                
                e
                
                   is the identity element)
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}b&=b\cdot e&&{\text{(}}e{\text{ is the identity element)}}\\&=b\cdot (a\cdot c)&&{\text{(}}c{\text{ is an inverse)}}\\&=(b\cdot a)\cdot c&&{\text{(associativity)}}\\&=e\cdot c&&{\text{(}}b{\text{ is an inverse)}}\\&=c&&{\text{(}}e{\text{ is the identity element)}}\end{aligned}}}
  Therefore, it is customary to speak of the inverse of an element.


=== Division ===
Given elements 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   of a group 
  
    
      
        G
      
    
    {\displaystyle G}
  , there is a unique solution 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
   to the equation 
  
    
      
        a
        ⋅
        x
        =
        b
      
    
    {\displaystyle a\cdot x=b}
  , namely 
  
    
      
        
          a
          
            −
            1
          
        
        ⋅
        b
      
    
    {\displaystyle a^{-1}\cdot b}
  . It follows that for each 
  
    
      
        a
      
    
    {\displaystyle a}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
  , the function 
  
    
      
        G
        →
        G
      
    
    {\displaystyle G\to G}
   that maps each 
  
    
      
        x
      
    
    {\displaystyle x}
   to 
  
    
      
        a
        ⋅
        x
      
    
    {\displaystyle a\cdot x}
   is a bijection; it is called left multiplication by 
  
    
      
        a
      
    
    {\displaystyle a}
   or left translation by 
  
    
      
        a
        .
      
    
    {\displaystyle a.}
  
Similarly, given 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  , the unique solution to 
  
    
      
        x
        ⋅
        a
        =
        b
      
    
    {\displaystyle x\cdot a=b}
   is 
  
    
      
        b
        ⋅
        
          a
          
            −
            1
          
        
      
    
    {\displaystyle b\cdot a^{-1}}
  . For each 
  
    
      
        a
      
    
    {\displaystyle a}
  , the function 
  
    
      
        G
        →
        G
      
    
    {\displaystyle G\to G}
   that maps each 
  
    
      
        x
      
    
    {\displaystyle x}
   to 
  
    
      
        x
        ⋅
        a
      
    
    {\displaystyle x\cdot a}
   is a bijection called right multiplication by 
  
    
      
        a
      
    
    {\displaystyle a}
   or right translation by 
  
    
      
        a
        .
      
    
    {\displaystyle a.}
  


=== Equivalent definition with relaxed axioms ===
The individual group axioms may be ""weakened"" to assert only the existence of a left identity and left inverses. From these one-sided axioms, one can prove that the left identity is also a right identity and a left inverse is also a right inverse for the same element. Since they define exactly the same structures as groups, collectively the axioms are not weaker.In particular, assuming associativity and the existence of a left identity 
  
    
      
        e
      
    
    {\displaystyle e}
   (that is, 
  
    
      
        e
        ⋅
        f
        =
        f
      
    
    {\displaystyle e\cdot f=f}
  ) and a left inverse 
  
    
      
        
          f
          
            −
            1
          
        
      
    
    {\displaystyle f^{-1}}
   for each element 
  
    
      
        f
      
    
    {\displaystyle f}
   (that is, 
  
    
      
        
          f
          
            −
            1
          
        
        ⋅
        f
        =
        e
      
    
    {\displaystyle f^{-1}\cdot f=e}
  ), one can  show that every left inverse is also a right inverse of the same element as follows.
Indeed, one has

  
    
      
        
          
            
              
                f
                ⋅
                
                  f
                  
                    −
                    1
                  
                
              
              
                
                =
                e
                ⋅
                (
                f
                ⋅
                
                  f
                  
                    −
                    1
                  
                
                )
              
              
              
                
                  (left identity)
                
              
            
            
              
              
                
                =
                (
                (
                
                  f
                  
                    −
                    1
                  
                
                
                  )
                  
                    −
                    1
                  
                
                ⋅
                
                  f
                  
                    −
                    1
                  
                
                )
                ⋅
                (
                f
                ⋅
                
                  f
                  
                    −
                    1
                  
                
                )
              
              
              
                
                  (left inverse)
                
              
            
            
              
              
                
                =
                (
                
                  f
                  
                    −
                    1
                  
                
                
                  )
                  
                    −
                    1
                  
                
                ⋅
                (
                (
                
                  f
                  
                    −
                    1
                  
                
                ⋅
                f
                )
                ⋅
                
                  f
                  
                    −
                    1
                  
                
                )
              
              
              
                
                  (associativity)
                
              
            
            
              
              
                
                =
                (
                
                  f
                  
                    −
                    1
                  
                
                
                  )
                  
                    −
                    1
                  
                
                ⋅
                (
                e
                ⋅
                
                  f
                  
                    −
                    1
                  
                
                )
              
              
              
                
                  (left inverse)
                
              
            
            
              
              
                
                =
                (
                
                  f
                  
                    −
                    1
                  
                
                
                  )
                  
                    −
                    1
                  
                
                ⋅
                
                  f
                  
                    −
                    1
                  
                
              
              
              
                
                  (left identity)
                
              
            
            
              
              
                
                =
                e
              
              
              
                
                  (left inverse)
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f\cdot f^{-1}&=e\cdot (f\cdot f^{-1})&&{\text{(left identity)}}\\&=((f^{-1})^{-1}\cdot f^{-1})\cdot (f\cdot f^{-1})&&{\text{(left inverse)}}\\&=(f^{-1})^{-1}\cdot ((f^{-1}\cdot f)\cdot f^{-1})&&{\text{(associativity)}}\\&=(f^{-1})^{-1}\cdot (e\cdot f^{-1})&&{\text{(left inverse)}}\\&=(f^{-1})^{-1}\cdot f^{-1}&&{\text{(left identity)}}\\&=e&&{\text{(left inverse)}}\end{aligned}}}
  Similarly, the left identity is also a right identity:

  
    
      
        
          
            
              
                f
                ⋅
                e
              
              
                
                =
                f
                ⋅
                (
                
                  f
                  
                    −
                    1
                  
                
                ⋅
                f
                )
              
              
              
                
                  (left inverse)
                
              
            
            
              
              
                
                =
                (
                f
                ⋅
                
                  f
                  
                    −
                    1
                  
                
                )
                ⋅
                f
              
              
              
                
                  (associativity)
                
              
            
            
              
              
                
                =
                e
                ⋅
                f
              
              
              
                
                  (right inverse)
                
              
            
            
              
              
                
                =
                f
              
              
              
                
                  (left identity)
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f\cdot e&=f\cdot (f^{-1}\cdot f)&&{\text{(left inverse)}}\\&=(f\cdot f^{-1})\cdot f&&{\text{(associativity)}}\\&=e\cdot f&&{\text{(right inverse)}}\\&=f&&{\text{(left identity)}}\end{aligned}}}
  
These proofs require all three axioms (associativity, existence of left identity and existence of left inverse). For a structure with a looser definition (like a semigroup) one may have, for example, that a left identity is not necessarily a right identity.


== Basic concepts ==

When studying sets, one uses concepts such as subset, function, and quotient by an equivalence relation. When studying groups, one uses instead subgroups, homomorphisms, and quotient groups. These are the analogues that take the group structure into account.


=== Group homomorphisms ===

Group homomorphisms are functions that respect group structure; they may be used to relate two groups. A homomorphism from a group 
  
    
      
        (
        G
        ,
        ⋅
        )
      
    
    {\displaystyle (G,\cdot )}
   to a group 
  
    
      
        (
        H
        ,
        ∗
        )
      
    
    {\displaystyle (H,*)}
   is a function 
  
    
      
        φ
        :
        G
        →
        H
      
    
    {\displaystyle \varphi \colon G\to H}
   such that

It would be natural to require also that 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   respect identities, 
  
    
      
        φ
        (
        
          1
          
            G
          
        
        )
        =
        
          1
          
            H
          
        
      
    
    {\displaystyle \varphi (1_{G})=1_{H}}
  , and inverses, 
  
    
      
        φ
        (
        
          a
          
            −
            1
          
        
        )
        =
        φ
        (
        a
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle \varphi (a^{-1})=\varphi (a)^{-1}}
   for all 
  
    
      
        a
      
    
    {\displaystyle a}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
  . However, these additional requirements need not be included in the definition of homomorphisms, because they are already implied by the requirement of respecting the group operation.The identity homomorphism of a group 
  
    
      
        G
      
    
    {\displaystyle G}
   is the homomorphism 
  
    
      
        
          ι
          
            G
          
        
        :
        G
        →
        G
      
    
    {\displaystyle \iota _{G}\colon G\to G}
   that maps each element of 
  
    
      
        G
      
    
    {\displaystyle G}
   to itself. An inverse homomorphism of a homomorphism 
  
    
      
        φ
        :
        G
        →
        H
      
    
    {\displaystyle \varphi \colon G\to H}
   is a homomorphism 
  
    
      
        ψ
        :
        H
        →
        G
      
    
    {\displaystyle \psi \colon H\to G}
   such that 
  
    
      
        ψ
        ∘
        φ
        =
        
          ι
          
            G
          
        
      
    
    {\displaystyle \psi \circ \varphi =\iota _{G}}
   and 
  
    
      
        φ
        ∘
        ψ
        =
        
          ι
          
            H
          
        
      
    
    {\displaystyle \varphi \circ \psi =\iota _{H}}
  , that is, such that 
  
    
      
        ψ
        
          
            (
          
        
        φ
        (
        g
        )
        
          
            )
          
        
        =
        g
      
    
    {\displaystyle \psi {\bigl (}\varphi (g){\bigr )}=g}
   for all 
  
    
      
        g
      
    
    {\displaystyle g}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
   and such that 
  
    
      
        φ
        
          
            (
          
        
        ψ
        (
        h
        )
        
          
            )
          
        
        =
        h
      
    
    {\displaystyle \varphi {\bigl (}\psi (h){\bigr )}=h}
   for all 
  
    
      
        h
      
    
    {\displaystyle h}
   in 
  
    
      
        H
      
    
    {\displaystyle H}
  . An isomorphism is a homomorphism that has an inverse homomorphism; equivalently, it is a bijective homomorphism. Groups 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        H
      
    
    {\displaystyle H}
   are called isomorphic if there exists an isomorphism 
  
    
      
        φ
        :
        G
        →
        H
      
    
    {\displaystyle \varphi \colon G\to H}
  . In this case, 
  
    
      
        H
      
    
    {\displaystyle H}
   can be obtained from 
  
    
      
        G
      
    
    {\displaystyle G}
   simply by renaming its elements according to the function 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  ; then any statement true for 
  
    
      
        G
      
    
    {\displaystyle G}
   is true for 
  
    
      
        H
      
    
    {\displaystyle H}
  , provided that any specific elements mentioned in the statement are also renamed.
The collection of all groups, together with the homomorphisms between them, form a category, the category of groups.


=== Subgroups ===

Informally, a subgroup is a group 
  
    
      
        H
      
    
    {\displaystyle H}
   contained within a bigger one, 
  
    
      
        G
      
    
    {\displaystyle G}
  : it has a subset of the elements of 
  
    
      
        G
      
    
    {\displaystyle G}
  , with the same operation. Concretely, this means that the identity element of 
  
    
      
        G
      
    
    {\displaystyle G}
   must be contained in 
  
    
      
        H
      
    
    {\displaystyle H}
  , and whenever 
  
    
      
        
          h
          
            1
          
        
      
    
    {\displaystyle h_{1}}
   and 
  
    
      
        
          h
          
            2
          
        
      
    
    {\displaystyle h_{2}}
   are both in 
  
    
      
        H
      
    
    {\displaystyle H}
  , then so are 
  
    
      
        
          h
          
            1
          
        
        ⋅
        
          h
          
            2
          
        
      
    
    {\displaystyle h_{1}\cdot h_{2}}
   and 
  
    
      
        
          h
          
            1
          
          
            −
            1
          
        
      
    
    {\displaystyle h_{1}^{-1}}
  , so the elements of 
  
    
      
        H
      
    
    {\displaystyle H}
  , equipped with the group operation on 
  
    
      
        G
      
    
    {\displaystyle G}
   restricted to 
  
    
      
        H
      
    
    {\displaystyle H}
  , indeed form a group.  In this case, the inclusion map 
  
    
      
        H
        →
        G
      
    
    {\displaystyle H\to G}
   is a homomorphism.
In the example of symmetries of a square, the identity and the rotations constitute a subgroup 
  
    
      
        R
        =
        {
        
          i
          d
        
        ,
        
          r
          
            1
          
        
        ,
        
          r
          
            2
          
        
        ,
        
          r
          
            3
          
        
        }
      
    
    {\displaystyle R=\{\mathrm {id} ,r_{1},r_{2},r_{3}\}}
  , highlighted in red in the group table of the example: any two rotations composed are still a rotation, and a rotation can be undone by (i.e., is inverse to) the complementary rotations  270° for 90°, 180° for 180°, and 90° for 270°. The subgroup test provides a necessary and sufficient condition for a nonempty subset H of a group G to be a subgroup: it is sufficient to check that 
  
    
      
        
          g
          
            −
            1
          
        
        ⋅
        h
        ∈
        H
      
    
    {\displaystyle g^{-1}\cdot h\in H}
   for all elements 
  
    
      
        g
      
    
    {\displaystyle g}
   and 
  
    
      
        h
      
    
    {\displaystyle h}
   in 
  
    
      
        H
      
    
    {\displaystyle H}
  . Knowing a group's subgroups is important in understanding the group as a whole.Given any subset 
  
    
      
        S
      
    
    {\displaystyle S}
   of a group 
  
    
      
        G
      
    
    {\displaystyle G}
  , the subgroup generated by 
  
    
      
        S
      
    
    {\displaystyle S}
   consists of all products of elements of 
  
    
      
        S
      
    
    {\displaystyle S}
   and their inverses. It is the smallest subgroup of 
  
    
      
        G
      
    
    {\displaystyle G}
   containing 
  
    
      
        S
      
    
    {\displaystyle S}
  . In the example of symmetries of a square, the subgroup generated by 
  
    
      
        
          r
          
            2
          
        
      
    
    {\displaystyle r_{2}}
   and 
  
    
      
        
          f
          
            
              v
            
          
        
      
    
    {\displaystyle f_{\mathrm {v} }}
   consists of these two elements, the identity element 
  
    
      
        
          i
          d
        
      
    
    {\displaystyle \mathrm {id} }
  , and the element 
  
    
      
        
          f
          
            
              h
            
          
        
        =
        
          f
          
            
              v
            
          
        
        ⋅
        
          r
          
            2
          
        
      
    
    {\displaystyle f_{\mathrm {h} }=f_{\mathrm {v} }\cdot r_{2}}
  . Again, this is a subgroup, because combining any two of these four elements or their inverses (which are, in this particular case, these same elements) yields an element of this subgroup.
An injective homomorphism 
  
    
      
        ϕ
        :
        
          G
          ′
        
        →
        G
      
    
    {\displaystyle \phi \colon G'\to G}
   factors canonically as an isomorphism followed by an inclusion, 
  
    
      
        
          G
          ′
        
        
        
          
            
              
                →
              
              
                ∼
              
            
          
        
        
        H
        ↪
        G
      
    
    {\displaystyle G'\;{\stackrel {\sim }{\to }}\;H\hookrightarrow G}
   for some subgroup H of G.
Injective homomorphisms are the monomorphisms in the category of groups.


=== Cosets ===

In many situations it is desirable to consider two group elements the same if they differ by an element of a given subgroup. For example, in the symmetry group of a square, once any reflection is performed, rotations alone cannot return the square to its original position, so one can think of the reflected positions of the square as all being equivalent to each other, and as inequivalent to the unreflected positions; the rotation operations are irrelevant to the question whether a reflection has been performed. Cosets are used to formalize this insight: a subgroup 
  
    
      
        H
      
    
    {\displaystyle H}
   determines left and right cosets, which can be thought of as translations of 
  
    
      
        H
      
    
    {\displaystyle H}
   by an arbitrary group element 
  
    
      
        g
      
    
    {\displaystyle g}
  . In symbolic terms, the left and right cosets of 
  
    
      
        H
      
    
    {\displaystyle H}
  , containing an element 
  
    
      
        g
      
    
    {\displaystyle g}
  , are

The left cosets of any subgroup 
  
    
      
        H
      
    
    {\displaystyle H}
   form a partition of 
  
    
      
        G
      
    
    {\displaystyle G}
  ; that is, the union of all left cosets is equal to 
  
    
      
        G
      
    
    {\displaystyle G}
   and two left cosets are either equal or have an empty intersection. The first case 
  
    
      
        
          g
          
            1
          
        
        H
        =
        
          g
          
            2
          
        
        H
      
    
    {\displaystyle g_{1}H=g_{2}H}
   happens precisely when 
  
    
      
        
          g
          
            1
          
          
            −
            1
          
        
        ⋅
        
          g
          
            2
          
        
        ∈
        H
      
    
    {\displaystyle g_{1}^{-1}\cdot g_{2}\in H}
  , i.e., when the two elements differ by an element of 
  
    
      
        H
      
    
    {\displaystyle H}
  . Similar considerations apply to the right cosets of 
  
    
      
        H
      
    
    {\displaystyle H}
  . The left cosets of 
  
    
      
        H
      
    
    {\displaystyle H}
   may or may not be the same as its right cosets. If they are (that is, if all 
  
    
      
        g
      
    
    {\displaystyle g}
   in 
  
    
      
        G
      
    
    {\displaystyle G}
   satisfy 
  
    
      
        g
        H
        =
        H
        g
      
    
    {\displaystyle gH=Hg}
  ), then 
  
    
      
        H
      
    
    {\displaystyle H}
   is said to be a normal subgroup.
In 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
  , the group of symmetries of a square, with its subgroup 
  
    
      
        R
      
    
    {\displaystyle R}
   of rotations, the left cosets 
  
    
      
        g
        R
      
    
    {\displaystyle gR}
   are either equal to 
  
    
      
        R
      
    
    {\displaystyle R}
  , if 
  
    
      
        g
      
    
    {\displaystyle g}
   is an element of 
  
    
      
        R
      
    
    {\displaystyle R}
   itself, or otherwise equal to 
  
    
      
        U
        =
        
          f
          
            
              c
            
          
        
        R
        =
        {
        
          f
          
            
              c
            
          
        
        ,
        
          f
          
            
              d
            
          
        
        ,
        
          f
          
            
              v
            
          
        
        ,
        
          f
          
            
              h
            
          
        
        }
      
    
    {\displaystyle U=f_{\mathrm {c} }R=\{f_{\mathrm {c} },f_{\mathrm {d} },f_{\mathrm {v} },f_{\mathrm {h} }\}}
   (highlighted in green in the group table of 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
  ). The subgroup 
  
    
      
        R
      
    
    {\displaystyle R}
   is normal, because 
  
    
      
        
          f
          
            
              c
            
          
        
        R
        =
        U
        =
        R
        
          f
          
            
              c
            
          
        
      
    
    {\displaystyle f_{\mathrm {c} }R=U=Rf_{\mathrm {c} }}
   and similarly for the other elements of the group. (In fact, in the case of 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
  , the cosets generated by reflections are all equal: 
  
    
      
        
          f
          
            
              h
            
          
        
        R
        =
        
          f
          
            
              v
            
          
        
        R
        =
        
          f
          
            
              d
            
          
        
        R
        =
        
          f
          
            
              c
            
          
        
        R
      
    
    {\displaystyle f_{\mathrm {h} }R=f_{\mathrm {v} }R=f_{\mathrm {d} }R=f_{\mathrm {c} }R}
  .)


=== Quotient groups ===

Suppose that 
  
    
      
        N
      
    
    {\displaystyle N}
   is a normal subgroup of a group 
  
    
      
        G
      
    
    {\displaystyle G}
  , and

denotes its set of cosets.
Then there is a unique group law on 
  
    
      
        G
        
          /
        
        N
      
    
    {\displaystyle G/N}
   for which the map 
  
    
      
        G
        →
        G
        
          /
        
        N
      
    
    {\displaystyle G\to G/N}
   sending each element 
  
    
      
        g
      
    
    {\displaystyle g}
   to 
  
    
      
        g
        N
      
    
    {\displaystyle gN}
   is a homomorphism.
Explicitly, the product of two cosets 
  
    
      
        g
        N
      
    
    {\displaystyle gN}
   and 
  
    
      
        h
        N
      
    
    {\displaystyle hN}
   is 
  
    
      
        (
        g
        h
        )
        N
      
    
    {\displaystyle (gh)N}
  , the coset 
  
    
      
        e
        N
        =
        N
      
    
    {\displaystyle eN=N}
   serves as the identity of 
  
    
      
        G
        
          /
        
        N
      
    
    {\displaystyle G/N}
  , and the inverse of 
  
    
      
        g
        N
      
    
    {\displaystyle gN}
   in the quotient group is 
  
    
      
        (
        g
        N
        
          )
          
            −
            1
          
        
        =
        
          (
          
            g
            
              −
              1
            
          
          )
        
        N
      
    
    {\displaystyle (gN)^{-1}=\left(g^{-1}\right)N}
  .
The group 
  
    
      
        G
        
          /
        
        N
      
    
    {\displaystyle G/N}
  , read as ""
  
    
      
        G
      
    
    {\displaystyle G}
   modulo 
  
    
      
        N
      
    
    {\displaystyle N}
  "", is called a quotient group or factor group.
The quotient group can alternatively be characterized by a universal property.

The elements of the quotient group 
  
    
      
        
          
            D
          
          
            4
          
        
        
          /
        
        R
      
    
    {\displaystyle \mathrm {D} _{4}/R}
   are 
  
    
      
        R
      
    
    {\displaystyle R}
   and 
  
    
      
        U
        =
        
          f
          
            
              v
            
          
        
        R
      
    
    {\displaystyle U=f_{\mathrm {v} }R}
  . The group operation on the quotient is shown in the table. For example, 
  
    
      
        U
        ⋅
        U
        =
        
          f
          
            
              v
            
          
        
        R
        ⋅
        
          f
          
            
              v
            
          
        
        R
        =
        (
        
          f
          
            
              v
            
          
        
        ⋅
        
          f
          
            
              v
            
          
        
        )
        R
        =
        R
      
    
    {\displaystyle U\cdot U=f_{\mathrm {v} }R\cdot f_{\mathrm {v} }R=(f_{\mathrm {v} }\cdot f_{\mathrm {v} })R=R}
  . Both the subgroup 
  
    
      
        R
        =
        {
        
          i
          d
        
        ,
        
          r
          
            1
          
        
        ,
        
          r
          
            2
          
        
        ,
        
          r
          
            3
          
        
        }
      
    
    {\displaystyle R=\{\mathrm {id} ,r_{1},r_{2},r_{3}\}}
   and the quotient 
  
    
      
        
          
            D
          
          
            4
          
        
        
          /
        
        R
      
    
    {\displaystyle \mathrm {D} _{4}/R}
   are abelian, but 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   is not.  Sometimes a group can be reconstructed from a subgroup and quotient (plus some additional data), by the semidirect product construction; 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   is an example.
The first isomorphism theorem implies that any surjective homomorphism 
  
    
      
        ϕ
        :
        G
        →
        H
      
    
    {\displaystyle \phi \colon G\to H}
   factors canonically as a quotient homomorphism followed by an isomorphism: 
  
    
      
        G
        →
        G
        
          /
        
        ker
        ⁡
        ϕ
        
        
          
            
              
                →
              
              
                ∼
              
            
          
        
        
        H
      
    
    {\displaystyle G\to G/\ker \phi \;{\stackrel {\sim }{\to }}\;H}
  .
Surjective homomorphisms are the epimorphisms in the category of groups.


=== Presentations ===

Every group is isomorphic to a quotient of a free group, in many ways.
For example, the dihedral group 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   is generated by the right rotation 
  
    
      
        
          r
          
            1
          
        
      
    
    {\displaystyle r_{1}}
   and the reflection 
  
    
      
        
          f
          
            
              v
            
          
        
      
    
    {\displaystyle f_{\mathrm {v} }}
   in a vertical line (every element of 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   is a finite product of copies of these and their inverses).
Hence there is a surjective homomorphism φ from the free group 
  
    
      
        ⟨
        r
        ,
        f
        ⟩
      
    
    {\displaystyle \langle r,f\rangle }
   on two generators to 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   sending 
  
    
      
        r
      
    
    {\displaystyle r}
   to 
  
    
      
        
          r
          
            1
          
        
      
    
    {\displaystyle r_{1}}
   and 
  
    
      
        f
      
    
    {\displaystyle f}
   to 
  
    
      
        
          f
          
            1
          
        
      
    
    {\displaystyle f_{1}}
  .
Elements in 
  
    
      
        ker
        ⁡
        ϕ
      
    
    {\displaystyle \ker \phi }
   are called relations; examples include 
  
    
      
        
          r
          
            4
          
        
        ,
        
          r
          
            2
          
        
        ,
        (
        r
        ⋅
        f
        
          )
          
            2
          
        
      
    
    {\displaystyle r^{4},r^{2},(r\cdot f)^{2}}
  .
In fact, it turns out that 
  
    
      
        ker
        ⁡
        ϕ
      
    
    {\displaystyle \ker \phi }
   is the smallest normal subgroup of 
  
    
      
        ⟨
        r
        ,
        f
        ⟩
      
    
    {\displaystyle \langle r,f\rangle }
   containing these three elements; in other words, all relations are consequences of these three.
The quotient of the free group by this normal subgroup is denoted 
  
    
      
        ⟨
        r
        ,
        f
        ∣
        
          r
          
            4
          
        
        =
        
          f
          
            2
          
        
        =
        (
        r
        ⋅
        f
        
          )
          
            2
          
        
        =
        1
        ⟩
      
    
    {\displaystyle \langle r,f\mid r^{4}=f^{2}=(r\cdot f)^{2}=1\rangle }
  .
This is called a presentation of 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   by generators and relations, because the first isomorphism theorem for φ yields an isomorphism 
  
    
      
        ⟨
        r
        ,
        f
        ∣
        
          r
          
            4
          
        
        =
        
          f
          
            2
          
        
        =
        (
        r
        ⋅
        f
        
          )
          
            2
          
        
        =
        1
        ⟩
        →
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \langle r,f\mid r^{4}=f^{2}=(r\cdot f)^{2}=1\rangle \to \mathrm {D} _{4}}
  .A presentation of a group can be used to construct the Cayley graph, a graphical depiction of a discrete group.


== Examples and applications ==

Examples and applications of groups abound. A starting point is the group 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   of integers with addition as group operation, introduced above. If instead of addition multiplication is considered, one obtains multiplicative groups. These groups are predecessors of important constructions in abstract algebra.
Groups are also applied in many other mathematical areas. Mathematical objects are often examined by associating groups to them and studying the properties of the corresponding groups. For example, Henri Poincaré founded what is now called algebraic topology by introducing the fundamental group. By means of this connection, topological properties such as proximity and continuity translate into properties of groups. For example, elements of the fundamental group are represented by loops. The second image shows some loops in a plane minus a point. The blue loop is considered null-homotopic (and thus irrelevant), because it can be continuously shrunk to a point. The presence of the hole prevents the orange loop from being shrunk to a point. The fundamental group of the plane with a point deleted turns out to be infinite cyclic, generated by the orange loop (or any other loop winding once around the hole). This way, the fundamental group detects the hole.
In more recent applications, the influence has also been reversed to motivate geometric constructions by a group-theoretical background. In a similar vein, geometric group theory employs geometric concepts, for example in the study of hyperbolic groups. Further branches crucially applying groups include algebraic geometry and number theory.In addition to the above theoretical applications, many practical applications of groups exist. Cryptography relies on the combination of the abstract group theory approach together with algorithmical knowledge obtained in computational group theory, in particular when implemented for finite groups. Applications of group theory are not restricted to mathematics; sciences such as physics, chemistry and computer science benefit from the concept.


=== Numbers ===
Many number systems, such as the integers and the rationals, enjoy a naturally given group structure. In some cases, such as with the rationals, both addition and multiplication operations give rise to group structures. Such number systems are predecessors to more general algebraic structures known as rings and fields. Further abstract algebraic concepts such as modules, vector spaces and algebras also form groups.


==== Integers ====
The group of integers 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   under addition, denoted 
  
    
      
        
          (
          
            
              Z
            
            ,
            +
          
          )
        
      
    
    {\displaystyle \left(\mathbb {Z} ,+\right)}
  , has been described above. The integers, with the operation of multiplication instead of addition, 
  
    
      
        
          (
          
            
              Z
            
            ,
            ⋅
          
          )
        
      
    
    {\displaystyle \left(\mathbb {Z} ,\cdot \right)}
   do not form a group. The associativity and identity axioms are satisfied, but inverses do not exist: for example, 
  
    
      
        a
        =
        2
      
    
    {\displaystyle a=2}
   is an integer, but the only solution to the equation 
  
    
      
        a
        ⋅
        b
        =
        1
      
    
    {\displaystyle a\cdot b=1}
   in this case is 
  
    
      
        b
        =
        
          
            
              1
              2
            
          
        
      
    
    {\displaystyle b={\tfrac {1}{2}}}
  , which is a rational number, but not an integer. Hence not every element of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   has a (multiplicative) inverse.


==== Rationals ====
The desire for the existence of multiplicative inverses suggests considering fractions

Fractions of integers (with 
  
    
      
        b
      
    
    {\displaystyle b}
   nonzero) are known as rational numbers. The set of all such irreducible fractions is commonly denoted 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  . There is still a minor obstacle for 
  
    
      
        
          (
          
            
              Q
            
            ,
            ⋅
          
          )
        
      
    
    {\displaystyle \left(\mathbb {Q} ,\cdot \right)}
  , the rationals with multiplication, being a group: because zero does not have a multiplicative inverse (i.e., there is no 
  
    
      
        x
      
    
    {\displaystyle x}
   such that 
  
    
      
        x
        ⋅
        0
        =
        1
      
    
    {\displaystyle x\cdot 0=1}
  ), 
  
    
      
        
          (
          
            
              Q
            
            ,
            ⋅
          
          )
        
      
    
    {\displaystyle \left(\mathbb {Q} ,\cdot \right)}
   is still not a group.
However, the set of all nonzero rational numbers 
  
    
      
        
          Q
        
        ∖
        
          {
          0
          }
        
        =
        
          {
          
            q
            ∈
            
              Q
            
            ∣
            q
            ≠
            0
          
          }
        
      
    
    {\displaystyle \mathbb {Q} \smallsetminus \left\{0\right\}=\left\{q\in \mathbb {Q} \mid q\neq 0\right\}}
   does form an abelian group under multiplication, also denoted 
  
    
      
        
          
            Q
          
          
            ×
          
        
      
    
    {\displaystyle \mathbb {Q} ^{\times }}
  . Associativity and identity element axioms follow from the properties of integers. The closure requirement still holds true after removing zero, because the product of two nonzero rationals is never zero. Finally, the inverse of 
  
    
      
        a
        
          /
        
        b
      
    
    {\displaystyle a/b}
   is 
  
    
      
        b
        
          /
        
        a
      
    
    {\displaystyle b/a}
  , therefore the axiom of the inverse element is satisfied.
The rational numbers (including zero) also form a group under addition. Intertwining addition and multiplication operations yields more complicated structures called rings and – if division by other than zero is possible, such as in 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
   – fields, which occupy a central position in abstract algebra. Group theoretic arguments therefore underlie parts of the theory of those entities.


=== Modular arithmetic ===

Modular arithmetic for a modulus 
  
    
      
        n
      
    
    {\displaystyle n}
   defines any two elements 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   that differ by a multiple of 
  
    
      
        n
      
    
    {\displaystyle n}
   to be equivalent, denoted by 
  
    
      
        a
        ≡
        b
        
          
          (
          mod
          
          n
          )
        
      
    
    {\displaystyle a\equiv b{\pmod {n}}}
  . Every integer is equivalent to one of the integers from 
  
    
      
        0
      
    
    {\displaystyle 0}
   to 
  
    
      
        n
        −
        1
      
    
    {\displaystyle n-1}
  , and the operations of modular arithmetic modify normal arithmetic by replacing the result of any operation by its equivalent representative. Modular addition, defined in this way for the integers from 
  
    
      
        0
      
    
    {\displaystyle 0}
   to 
  
    
      
        n
        −
        1
      
    
    {\displaystyle n-1}
  , forms a group, denoted as 
  
    
      
        
          
            Z
          
          
            n
          
        
      
    
    {\displaystyle \mathrm {Z} _{n}}
   or 
  
    
      
        (
        
          Z
        
        
          /
        
        n
        
          Z
        
        ,
        +
        )
      
    
    {\displaystyle (\mathbb {Z} /n\mathbb {Z} ,+)}
  , with 
  
    
      
        0
      
    
    {\displaystyle 0}
   as the identity element and 
  
    
      
        n
        −
        a
      
    
    {\displaystyle n-a}
   as the inverse element of 
  
    
      
        a
      
    
    {\displaystyle a}
  .
A familiar example is addition of hours on the face of a clock, where 12 rather than 0 is chosen as the representative of the identity. If the hour hand is on 
  
    
      
        9
      
    
    {\displaystyle 9}
   and is advanced 
  
    
      
        4
      
    
    {\displaystyle 4}
   hours, it ends up on 
  
    
      
        1
      
    
    {\displaystyle 1}
  , as shown in the illustration. This is expressed by saying that 
  
    
      
        9
        +
        4
      
    
    {\displaystyle 9+4}
   is congruent to 
  
    
      
        1
      
    
    {\displaystyle 1}
   ""modulo 
  
    
      
        12
      
    
    {\displaystyle 12}
  "" or, in symbols,

For any prime number 
  
    
      
        p
      
    
    {\displaystyle p}
  , there is also the multiplicative group of integers modulo 
  
    
      
        p
      
    
    {\displaystyle p}
  . Its elements can be represented by 
  
    
      
        1
      
    
    {\displaystyle 1}
   to 
  
    
      
        p
        −
        1
      
    
    {\displaystyle p-1}
  . The group operation, multiplication modulo 
  
    
      
        p
      
    
    {\displaystyle p}
  , replaces the usual product by its representative, the remainder of division by 
  
    
      
        p
      
    
    {\displaystyle p}
  . For example, for 
  
    
      
        p
        =
        5
      
    
    {\displaystyle p=5}
  , the four group elements can be represented by 
  
    
      
        1
        ,
        2
        ,
        3
        ,
        4
      
    
    {\displaystyle 1,2,3,4}
  . In this group, 
  
    
      
        4
        ⋅
        4
        ≡
        1
        
          mod
          
            5
          
        
      
    
    {\displaystyle 4\cdot 4\equiv 1{\bmod {5}}}
  , because the usual product 
  
    
      
        16
      
    
    {\displaystyle 16}
   is equivalent to 
  
    
      
        1
      
    
    {\displaystyle 1}
  : when divided by 
  
    
      
        5
      
    
    {\displaystyle 5}
   it yields a remainder of 
  
    
      
        1
      
    
    {\displaystyle 1}
  . The primality of 
  
    
      
        p
      
    
    {\displaystyle p}
   ensures that the usual product of two representatives is not divisible by 
  
    
      
        p
      
    
    {\displaystyle p}
  , and therefore that the modular product is nonzero. The identity element is represented by 
  
    
      
        1
      
    
    {\displaystyle 1}
  , and associativity follows from the corresponding property of the integers. Finally, the inverse element axiom requires that given an integer 
  
    
      
        a
      
    
    {\displaystyle a}
   not divisible by 
  
    
      
        p
      
    
    {\displaystyle p}
  , there exists an integer 
  
    
      
        b
      
    
    {\displaystyle b}
   such that 

that is, such that 
  
    
      
        p
      
    
    {\displaystyle p}
   evenly divides 
  
    
      
        a
        ⋅
        b
        −
        1
      
    
    {\displaystyle a\cdot b-1}
  . The inverse 
  
    
      
        b
      
    
    {\displaystyle b}
   can be found by using Bézout's identity and the fact that the greatest common divisor 
  
    
      
        gcd
        (
        a
        ,
        p
        )
      
    
    {\displaystyle \gcd(a,p)}
   equals 
  
    
      
        1
      
    
    {\displaystyle 1}
  . In the case 
  
    
      
        p
        =
        5
      
    
    {\displaystyle p=5}
   above, the inverse of the element represented by 
  
    
      
        4
      
    
    {\displaystyle 4}
   is that represented by 
  
    
      
        4
      
    
    {\displaystyle 4}
  , and the inverse of the element represented by 
  
    
      
        3
      
    
    {\displaystyle 3}
   is represented by 
  
    
      
        2
      
    
    {\displaystyle 2}
  , as 
  
    
      
        3
        ⋅
        2
        =
        6
        ≡
        1
        
          mod
          
            5
          
        
      
    
    {\displaystyle 3\cdot 2=6\equiv 1{\bmod {5}}}
  . Hence all group axioms are fulfilled. This example is similar to 
  
    
      
        
          (
          
            
              Q
            
            ∖
            
              {
              0
              }
            
            ,
            ⋅
          
          )
        
      
    
    {\displaystyle \left(\mathbb {Q} \smallsetminus \left\{0\right\},\cdot \right)}
   above: it consists of exactly those elements in the ring 
  
    
      
        
          Z
        
        
          /
        
        p
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /p\mathbb {Z} }
   that have a multiplicative inverse. These groups, denoted 
  
    
      
        
          
            F
          
          
            p
          
          
            ×
          
        
      
    
    {\displaystyle \mathbb {F} _{p}^{\times }}
  , are crucial to public-key cryptography.


=== Cyclic groups ===

A cyclic group is a group all of whose elements are powers of a particular element 
  
    
      
        a
      
    
    {\displaystyle a}
  . In multiplicative notation, the elements of the group are

where 
  
    
      
        
          a
          
            2
          
        
      
    
    {\displaystyle a^{2}}
   means 
  
    
      
        a
        ⋅
        a
      
    
    {\displaystyle a\cdot a}
  , 
  
    
      
        
          a
          
            −
            3
          
        
      
    
    {\displaystyle a^{-3}}
   stands for 
  
    
      
        
          a
          
            −
            1
          
        
        ⋅
        
          a
          
            −
            1
          
        
        ⋅
        
          a
          
            −
            1
          
        
        =
        (
        a
        ⋅
        a
        ⋅
        a
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle a^{-1}\cdot a^{-1}\cdot a^{-1}=(a\cdot a\cdot a)^{-1}}
  , etc. Such an element 
  
    
      
        a
      
    
    {\displaystyle a}
   is called a generator or a primitive element of the group. In additive notation, the requirement for an element to be primitive is that each element of the group can be written as

In the groups 
  
    
      
        (
        
          Z
        
        
          /
        
        n
        
          Z
        
        ,
        +
        )
      
    
    {\displaystyle (\mathbb {Z} /n\mathbb {Z} ,+)}
   introduced above, the element 
  
    
      
        1
      
    
    {\displaystyle 1}
   is primitive, so these groups are cyclic. Indeed, each element is expressible as a sum all of whose terms are 
  
    
      
        1
      
    
    {\displaystyle 1}
  . Any cyclic group with 
  
    
      
        n
      
    
    {\displaystyle n}
   elements is isomorphic to this group. A second example for cyclic groups is the group of 
  
    
      
        n
      
    
    {\displaystyle n}
  th complex roots of unity, given by complex numbers 
  
    
      
        z
      
    
    {\displaystyle z}
   satisfying 
  
    
      
        
          z
          
            n
          
        
        =
        1
      
    
    {\displaystyle z^{n}=1}
  . These numbers can be visualized as the vertices on a regular 
  
    
      
        n
      
    
    {\displaystyle n}
  -gon, as shown in blue in the image for 
  
    
      
        n
        =
        6
      
    
    {\displaystyle n=6}
  . The group operation is multiplication of complex numbers. In the picture, multiplying with 
  
    
      
        z
      
    
    {\displaystyle z}
   corresponds to a counter-clockwise rotation by 60°. From field theory, the group 
  
    
      
        
          
            F
          
          
            p
          
          
            ×
          
        
      
    
    {\displaystyle \mathbb {F} _{p}^{\times }}
   is cyclic for prime 
  
    
      
        p
      
    
    {\displaystyle p}
  : for example, if 
  
    
      
        p
        =
        5
      
    
    {\displaystyle p=5}
  , 
  
    
      
        3
      
    
    {\displaystyle 3}
   is a generator since 
  
    
      
        
          3
          
            1
          
        
        =
        3
      
    
    {\displaystyle 3^{1}=3}
  , 
  
    
      
        
          3
          
            2
          
        
        =
        9
        ≡
        4
      
    
    {\displaystyle 3^{2}=9\equiv 4}
  , 
  
    
      
        
          3
          
            3
          
        
        ≡
        2
      
    
    {\displaystyle 3^{3}\equiv 2}
  , and 
  
    
      
        
          3
          
            4
          
        
        ≡
        1
      
    
    {\displaystyle 3^{4}\equiv 1}
  .
Some cyclic groups have an infinite number of elements. In these groups, for every non-zero element 
  
    
      
        a
      
    
    {\displaystyle a}
  , all the powers of 
  
    
      
        a
      
    
    {\displaystyle a}
   are distinct; despite the name ""cyclic group"", the powers of the elements do not cycle. An infinite cyclic group is isomorphic to 
  
    
      
        (
        
          Z
        
        ,
        +
        )
      
    
    {\displaystyle (\mathbb {Z} ,+)}
  , the group of integers under addition introduced above. As these two prototypes are both abelian, so are all cyclic groups.
The study of finitely generated abelian groups is quite mature, including the fundamental theorem of finitely generated abelian groups; and reflecting this state of affairs, many group-related notions, such as center and commutator, describe the extent to which a given group is not abelian.


=== Symmetry groups ===

Symmetry groups are groups consisting of symmetries of given mathematical objects, principally geometric entities, such as the symmetry group of the square given as an introductory example above, although they also arise in algebra such as the symmetries among the roots of polynomial equations dealt with in Galois theory (see below). Conceptually, group theory can be thought of as the study of symmetry. Symmetries in mathematics greatly simplify the study of geometrical or analytical objects. A group is said to act on another mathematical object X if every group element can be associated to some operation on X and the composition of these operations follows the group law. For example, an element of the (2,3,7) triangle group acts on a triangular tiling of the hyperbolic plane by permuting the triangles. By a group action, the group pattern is connected to the structure of the object being acted on.
In chemical fields, such as crystallography, space groups and point groups describe molecular symmetries and crystal symmetries. These symmetries underlie the chemical and physical behavior of these systems, and group theory enables simplification of quantum mechanical analysis of these properties. For example, group theory is used to show that optical transitions between certain quantum levels cannot occur simply because of the symmetry of the states involved.Group theory helps predict the changes in physical properties that occur when a material undergoes a phase transition, for example, from a cubic to a tetrahedral crystalline form. An example is ferroelectric materials, where the change from a paraelectric to a ferroelectric state occurs at the Curie temperature and is related to a change from the high-symmetry paraelectric state to the lower symmetry ferroelectric state, accompanied by a so-called soft phonon mode, a vibrational lattice mode that goes to zero frequency at the transition.Such spontaneous symmetry breaking has found further application in elementary particle physics, where its occurrence is related to the appearance of Goldstone bosons.
Finite symmetry groups such as the Mathieu groups are used in coding theory, which is in turn applied in error correction of transmitted data, and in CD players. Another application is differential Galois theory, which characterizes functions having antiderivatives of a prescribed form, giving group-theoretic criteria for when solutions of certain differential equations are well-behaved. Geometric properties that remain stable under group actions are investigated in (geometric) invariant theory.


=== General linear group and representation theory ===

Matrix groups consist of matrices together with matrix multiplication. The general linear group 
  
    
      
        
          G
          L
        
        (
        n
        ,
        
          R
        
        )
      
    
    {\displaystyle \mathrm {GL} (n,\mathbb {R} )}
   consists of all invertible 
  
    
      
        n
      
    
    {\displaystyle n}
  -by-
  
    
      
        n
      
    
    {\displaystyle n}
   matrices with real entries. Its subgroups are referred to as matrix groups or linear groups. The dihedral group example mentioned above can be viewed as a (very small) matrix group. Another important matrix group is the special orthogonal group 
  
    
      
        
          S
          O
        
        (
        n
        )
      
    
    {\displaystyle \mathrm {SO} (n)}
  . It describes all possible rotations in 
  
    
      
        n
      
    
    {\displaystyle n}
   dimensions. Rotation matrices in this group are used in computer graphics.Representation theory is both an application of the group concept and important for a deeper understanding of groups. It studies the group by its group actions on other spaces. A broad class of group representations are linear representations in which the group acts on a vector space, such as the three-dimensional Euclidean space 
  
    
      
        
          
            R
          
          
            3
          
        
      
    
    {\displaystyle \mathbb {R} ^{3}}
  . A representation of a group 
  
    
      
        G
      
    
    {\displaystyle G}
   on an 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional real vector space is simply a group homomorphism

  
    
      
        ρ
        :
        G
        →
        
          G
          L
        
        (
        n
        ,
        
          R
        
        )
      
    
    {\displaystyle \rho :G\to \mathrm {GL} (n,\mathbb {R} )}
  
from the group to the general linear group. This way, the group operation, which may be abstractly given, translates to the multiplication of matrices making it accessible to explicit computations.A group action gives further means to study the object being acted on. On the other hand, it also yields information about the group. Group representations are an organizing principle in the theory of finite groups, Lie groups, algebraic groups and topological groups, especially (locally) compact groups.


=== Galois groups ===

Galois groups were developed to help solve polynomial equations by capturing their symmetry features. For example, the solutions of the quadratic equation 
  
    
      
        a
        
          x
          
            2
          
        
        +
        b
        x
        +
        c
        =
        0
      
    
    {\displaystyle ax^{2}+bx+c=0}
   are given by

Each solution can be obtained by replacing the 
  
    
      
        ±
      
    
    {\displaystyle \pm }
   sign by 
  
    
      
        +
      
    
    {\displaystyle +}
   or 
  
    
      
        −
      
    
    {\displaystyle -}
  ; analogous formulae are known for cubic and quartic equations, but do not exist in general for degree 5 and higher. In the quadratic formula, changing the sign (permuting the resulting two solutions) can be viewed as a (very simple) group operation. Analogous Galois groups act on the solutions of higher-degree polynomials and are closely related to the existence of formulas for their solution. Abstract properties of these groups (in particular their solvability) give a criterion for the ability to express the solutions of these polynomials using solely addition, multiplication, and roots similar to the formula above.Modern Galois theory generalizes the above type of Galois groups by shifting to field theory and considering field extensions formed as the splitting field of a polynomial. This theory establishes—via the fundamental theorem of Galois theory—a precise relationship between fields and groups, underlining once again the ubiquity of groups in mathematics.


== Finite groups ==

A group is called finite if it has a finite number of elements. The number of elements is called the order of the group. An important class is the symmetric groups 
  
    
      
        
          
            S
          
          
            N
          
        
      
    
    {\displaystyle \mathrm {S} _{N}}
  , the groups of permutations of 
  
    
      
        N
      
    
    {\displaystyle N}
   objects. For example, the symmetric group on 3 letters 
  
    
      
        
          
            S
          
          
            3
          
        
      
    
    {\displaystyle \mathrm {S} _{3}}
   is the group of all possible reorderings of the objects. The three letters ABC can be reordered into ABC, ACB, BAC, BCA, CAB, CBA, forming in total 6 (factorial of 3) elements. The group operation is composition of these reorderings, and the identity element is the reordering operation that leaves the order unchanged. This class is fundamental insofar as any finite group can be expressed as a subgroup of a symmetric group 
  
    
      
        
          
            S
          
          
            N
          
        
      
    
    {\displaystyle \mathrm {S} _{N}}
   for a suitable integer 
  
    
      
        N
      
    
    {\displaystyle N}
  , according to Cayley's theorem. Parallel to the group of symmetries of the square above, 
  
    
      
        
          
            S
          
          
            3
          
        
      
    
    {\displaystyle \mathrm {S} _{3}}
   can also be interpreted as the group of symmetries of an equilateral triangle.
The order of an element 
  
    
      
        a
      
    
    {\displaystyle a}
   in a group 
  
    
      
        G
      
    
    {\displaystyle G}
   is the least positive integer 
  
    
      
        n
      
    
    {\displaystyle n}
   such that 
  
    
      
        
          a
          
            n
          
        
        =
        e
      
    
    {\displaystyle a^{n}=e}
  , where 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a^{n}}
   represents

that is, application of the operation ""
  
    
      
        ⋅
      
    
    {\displaystyle \cdot }
  "" to 
  
    
      
        n
      
    
    {\displaystyle n}
   copies of 
  
    
      
        a
      
    
    {\displaystyle a}
  . (If ""
  
    
      
        ⋅
      
    
    {\displaystyle \cdot }
  "" represents multiplication, then 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a^{n}}
   corresponds to the 
  
    
      
        n
      
    
    {\displaystyle n}
  th power of 
  
    
      
        a
      
    
    {\displaystyle a}
  .) In infinite groups, such an 
  
    
      
        n
      
    
    {\displaystyle n}
   may not exist, in which case the order of 
  
    
      
        a
      
    
    {\displaystyle a}
   is said to be infinity. The order of an element equals the order of the cyclic subgroup generated by this element.
More sophisticated counting techniques, for example, counting cosets, yield more precise statements about finite groups: Lagrange's Theorem states that for a finite group 
  
    
      
        G
      
    
    {\displaystyle G}
   the order of any finite subgroup 
  
    
      
        H
      
    
    {\displaystyle H}
   divides the order of 
  
    
      
        G
      
    
    {\displaystyle G}
  . The Sylow theorems give a partial converse.
The dihedral group 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   of symmetries of a square is a finite group of order 8. In this group, the order of 
  
    
      
        
          r
          
            1
          
        
      
    
    {\displaystyle r_{1}}
   is 4, as is the order of the subgroup 
  
    
      
        R
      
    
    {\displaystyle R}
   that this element generates. The order of the reflection elements 
  
    
      
        
          f
          
            
              v
            
          
        
      
    
    {\displaystyle f_{\mathrm {v} }}
   etc. is 2. Both orders divide 8, as predicted by Lagrange's theorem. The groups 
  
    
      
        
          
            F
          
          
            p
          
          
            ×
          
        
      
    
    {\displaystyle \mathbb {F} _{p}^{\times }}
   of multiplication modulo a prime 
  
    
      
        p
      
    
    {\displaystyle p}
   have order 
  
    
      
        p
        −
        1
      
    
    {\displaystyle p-1}
  .


=== Finite abelian groups ===
Any finite abelian group is isomorphic to a product of finite cyclic groups; this statement is part of the fundamental theorem of finitely generated abelian groups.
Any group of prime order 
  
    
      
        p
      
    
    {\displaystyle p}
   is isomorphic to the cyclic group 
  
    
      
        
          
            Z
          
          
            p
          
        
      
    
    {\displaystyle \mathrm {Z} _{p}}
   (a consequence of Lagrange's theorem).  
Any group of order 
  
    
      
        
          p
          
            2
          
        
      
    
    {\displaystyle p^{2}}
   is abelian, isomorphic to 
  
    
      
        
          
            Z
          
          
            
              p
              
                2
              
            
          
        
      
    
    {\displaystyle \mathrm {Z} _{p^{2}}}
   or 
  
    
      
        
          
            Z
          
          
            p
          
        
        ×
        
          
            Z
          
          
            p
          
        
      
    
    {\displaystyle \mathrm {Z} _{p}\times \mathrm {Z} _{p}}
  .
But there exist nonabelian groups of order 
  
    
      
        
          p
          
            3
          
        
      
    
    {\displaystyle p^{3}}
  ; the dihedral group 
  
    
      
        
          
            D
          
          
            4
          
        
      
    
    {\displaystyle \mathrm {D} _{4}}
   of order 
  
    
      
        
          2
          
            3
          
        
      
    
    {\displaystyle 2^{3}}
   above is an example.


=== Simple groups ===
When a group 
  
    
      
        G
      
    
    {\displaystyle G}
   has a normal subgroup 
  
    
      
        N
      
    
    {\displaystyle N}
   other than 
  
    
      
        {
        1
        }
      
    
    {\displaystyle \{1\}}
   and 
  
    
      
        G
      
    
    {\displaystyle G}
   itself, questions about 
  
    
      
        G
      
    
    {\displaystyle G}
   can sometimes be reduced to questions about 
  
    
      
        N
      
    
    {\displaystyle N}
   and 
  
    
      
        G
        
          /
        
        N
      
    
    {\displaystyle G/N}
  .  A nontrivial group is called simple if it has no such normal subgroup.  Finite simple groups are to finite groups as prime numbers are to positive integers: they serve as building blocks, in a sense made precise by the Jordan–Hölder theorem.


=== Classification of finite simple groups ===

Computer algebra systems have been used to list all groups of order up to 2000.
But classifying all finite groups is a problem considered too hard to be solved.
The classification of all finite simple groups was a major achievement in contemporary group theory.  There are several infinite families of such groups, as well as 26 ""sporadic groups"" that do not belong to any of the families.  The largest sporadic group is called the monster group.   The monstrous moonshine conjectures, proved by Richard Borcherds, relate the monster group to certain modular functions.The gap between the classification of simple groups and the classification of all groups lies in the extension problem.


== Groups with additional structure ==
An equivalent definition of group consists of replacing the ""there exist"" part of the group axioms by operations whose result is the element that must exist. So, a group is a set 
  
    
      
        G
      
    
    {\displaystyle G}
   equipped with a binary operation 
  
    
      
        G
        ×
        G
        →
        G
      
    
    {\displaystyle G\times G\rightarrow G}
   (the group operation), a unary operation 
  
    
      
        G
        →
        G
      
    
    {\displaystyle G\rightarrow G}
   (which provides the inverse) and a nullary operation, which has no operand and results in the identity element. Otherwise, the group axioms are exactly the same. This variant of the definition avoids existential quantifiers and is used in computing with groups and for computer-aided proofs.
This way of defining groups lends itself to generalizations such as the notion of group object in a category.  Briefly, this is an object with morphisms that mimic the group axioms.


=== Topological groups ===

Some topological spaces may be endowed with a group law. In order for the group law and the topology to interweave well, the group operations must be continuous functions; informally, 
  
    
      
        g
        ⋅
        h
      
    
    {\displaystyle g\cdot h}
   and 
  
    
      
        
          g
          
            −
            1
          
        
      
    
    {\displaystyle g^{-1}}
   must not vary wildly if 
  
    
      
        g
      
    
    {\displaystyle g}
   and 
  
    
      
        h
      
    
    {\displaystyle h}
   vary only a little. Such groups are called topological groups, and they are the group objects in the category of topological spaces. The most basic examples are the group of real numbers under addition and the group of nonzero real numbers under multiplication. Similar examples can be formed from any other topological field, such as the field of complex numbers or the field of p-adic numbers. These examples are locally compact, so they have Haar measures and can be studied via harmonic analysis. Other locally compact topological groups include the group of points of an algebraic group over a local field or adele ring; these are basic to number theory Galois groups of infinite algebraic field extensions are equipped with the Krull topology, which plays a role in infinite Galois theory. A generalization used in algebraic geometry is the étale fundamental group.


=== Lie groups ===

A Lie group is a group that also has the structure of a differentiable manifold; informally, this means that it looks locally like a Euclidean space of some fixed dimension. Again, the definition requires the additional structure, here the manifold structure, to be compatible: the multiplication and inverse maps are required to be smooth.
A standard example is the general linear group introduced above: it is an open subset of the space of all 
  
    
      
        n
      
    
    {\displaystyle n}
  -by-
  
    
      
        n
      
    
    {\displaystyle n}
   matrices, because it is given by the inequality

where 
  
    
      
        A
      
    
    {\displaystyle A}
   denotes an 
  
    
      
        n
      
    
    {\displaystyle n}
  -by-
  
    
      
        n
      
    
    {\displaystyle n}
   matrix.Lie groups are of fundamental importance in modern physics: Noether's theorem links continuous symmetries to conserved quantities. Rotation, as well as translations in space and time, are basic symmetries of the laws of mechanics. They can, for instance, be used to construct simple models—imposing, say, axial symmetry on a situation will typically lead to significant simplification in the equations one needs to solve to provide a physical description. Another example is the group of Lorentz transformations, which relate measurements of time and velocity of two observers in motion relative to each other. They can be deduced in a purely group-theoretical way, by expressing the transformations as a rotational symmetry of Minkowski space. The latter serves—in the absence of significant gravitation—as a model of spacetime in special relativity. The full symmetry group of Minkowski space, i.e., including translations, is known as the Poincaré group. By the above, it plays a pivotal role in special relativity and, by implication, for quantum field theories. Symmetries that vary with location are central to the modern description of physical interactions with the help of gauge theory. An important example of a gauge theory is the Standard Model, which describes three of the four known fundamental forces and classifies all known elementary particles.


== Generalizations ==
More general structures may be defined by relaxing some of the axioms defining a group.  The table gives a list of several structures generalizing groups.
For example, if the requirement that every element has an inverse is eliminated, the resulting algebraic structure is called a monoid. The natural numbers 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   (including zero) under addition form a monoid, as do the nonzero integers under multiplication 
  
    
      
        (
        
          Z
        
        ∖
        {
        0
        }
        ,
        ⋅
        )
      
    
    {\displaystyle (\mathbb {Z} \smallsetminus \{0\},\cdot )}
  . Adjoining inverses of all elements of the monoid 
  
    
      
        (
        
          Z
        
        ∖
        {
        0
        }
        ,
        ⋅
        )
      
    
    {\displaystyle (\mathbb {Z} \smallsetminus \{0\},\cdot )}
   produces a group 
  
    
      
        (
        
          Q
        
        ∖
        {
        0
        }
        ,
        ⋅
        )
      
    
    {\displaystyle (\mathbb {Q} \smallsetminus \{0\},\cdot )}
  , and likewise adjoining inverses to any (abelian) monoid M produces a group known as the Grothendieck group of M.
A group can be thought of as a small category with one object x in which every morphism is an isomorphism: given such a category, the set 
  
    
      
        Hom
        ⁡
        (
        x
        ,
        x
        )
      
    
    {\displaystyle \operatorname {Hom} (x,x)}
   is a group; conversely, given a group G, one can build a small category with one object x in which  
  
    
      
        Hom
        ⁡
        (
        x
        ,
        x
        )
        ≃
        G
      
    
    {\displaystyle \operatorname {Hom} (x,x)\simeq G}
  .
More generally, a groupoid is any small category in which every morphism is an isomorphism.  
In a groupoid, the set of all morphisms in the category is usually not a group, because the composition is only partially defined: fg is defined only when the source of f matches the target of g.
Groupoids arise in topology (for instance, the fundamental groupoid) and in the theory of stacks.
Finally, it is possible to generalize any of these concepts by replacing the binary operation with an n-ary operation (i.e., an operation taking n arguments, for some nonnegative integer n). With the proper generalization of the group axioms, this gives a notion of n-ary group.


== See also ==
List of group theory topics


== Notes ==


== Citations ==


== References ==


=== General references ===
Artin, Michael (2018), Algebra, Prentice Hall, ISBN 978-0-13-468960-9, Chapter 2 contains an undergraduate-level exposition of the notions covered in this article.
Cook, Mariana R. (2009), Mathematicians: An Outer View of the Inner World, Princeton, N.J.: Princeton University Press, ISBN 978-0-691-13951-7
Hall, G. G. (1967), Applied Group Theory, American Elsevier Publishing Co., Inc., New York, MR 0219593, an elementary introduction.
Herstein, Israel Nathan (1996), Abstract Algebra (3rd ed.), Upper Saddle River, NJ: Prentice Hall Inc., ISBN 978-0-13-374562-7, MR 1375019.
Herstein, Israel Nathan (1975), Topics in Algebra (2nd ed.), Lexington, Mass.: Xerox College Publishing, MR 0356988.
Lang, Serge (2002), Algebra, Graduate Texts in Mathematics, vol. 211 (Revised third ed.), New York: Springer-Verlag, ISBN 978-0-387-95385-4, MR 1878556
Lang, Serge (2005), Undergraduate Algebra (3rd ed.), Berlin, New York: Springer-Verlag, ISBN 978-0-387-22025-3.
Ledermann, Walter (1953), Introduction to the Theory of Finite Groups, Oliver and Boyd, Edinburgh and London, MR 0054593.
Ledermann, Walter (1973), Introduction to Group Theory, New York: Barnes and Noble, OCLC 795613.
Robinson, Derek John Scott (1996), A Course in the Theory of Groups, Berlin, New York: Springer-Verlag, ISBN 978-0-387-94461-6.


=== Special references ===
Artin, Emil (1998), Galois Theory, New York: Dover Publications, ISBN 978-0-486-62342-9.
Aschbacher, Michael (2004), ""The status of the classification of the finite simple groups"" (PDF), Notices of the American Mathematical Society, 51 (7): 736–740.
Awodey, Steve (2010), Category Theory, Oxford University Press, ISBN 978-0-19-958736-0
Behler, Florian; Wickleder, Mathias S.; Christoffers, Jens (2014), ""Biphenyl and bimesityl tetrasulfonic acid – new linker molecules for coordination polymers"", Arkivoc, 2015 (2): 64–75, doi:10.3998/ark.5550190.p008.911
Bersuker, Isaac (2006), The Jahn–Teller Effect, Cambridge University Press, ISBN 0-521-82212-2.
Besche, Hans Ulrich; Eick, Bettina; O'Brien, E. A. (2001), ""The groups of order at most 2000"", Electronic Research Announcements of the American Mathematical Society, 7: 1–4, doi:10.1090/S1079-6762-01-00087-7, MR 1826989.
Bishop, David H. L. (1993), Group Theory and Chemistry, New York: Dover Publications, ISBN 978-0-486-67355-4.
Borel, Armand (1991), Linear Algebraic Groups, Graduate Texts in Mathematics, vol. 126 (2nd ed.), Berlin, New York: Springer-Verlag, ISBN 978-0-387-97370-8, MR 1102012.
Carter, Roger W. (1989), Simple Groups of Lie Type, New York: John Wiley & Sons, ISBN 978-0-471-50683-6.
Chancey, C. C.; O'Brien, M. C. M. (2021), The Jahn–Teller Effect in C60 and Other Icosahedral Complexes, Princeton University Press, ISBN 978-0-691-22534-0
Conway, John Horton; Delgado Friedrichs, Olaf; Huson, Daniel H.; Thurston, William P. (2001), ""On three-dimensional space groups"", Beiträge zur Algebra und Geometrie, 42 (2): 475–507, arXiv:math.MG/9911185, MR 1865535.
Coornaert, M.; Delzant, T.; Papadopoulos, A. (1990), Géométrie et théorie des groupes [Geometry and Group Theory], Lecture Notes in Mathematics (in French), vol. 1441, Berlin, New York: Springer-Verlag, ISBN 978-3-540-52977-4, MR 1075994.
Denecke, Klaus; Wismath, Shelly L. (2002), Universal Algebra and Applications in Theoretical Computer Science, London: CRC Press, ISBN 978-1-58488-254-1.
Dove, Martin T (2003), Structure and Dynamics: An Atomic View of Materials, Oxford University Press, p. 265, ISBN 0-19-850678-3.
Dudek, Wiesław A. (2001), ""On some old and new problems in n-ary groups"" (PDF), Quasigroups and Related Systems, 8: 15–36, MR 1876783.
Eliel, Ernest; Wilen, Samuel; Mander, Lewis (1994), Stereochemistry of Organic Compounds, Wiley, ISBN 978-0-471-01670-0
Ellis, Graham (2019), ""6.4 Triangle groups"", An Invitation to Computational Homotopy, Oxford University Press, pp. 441–444, doi:10.1093/oso/9780198832973.001.0001, ISBN 978-0-19-883298-0, MR 3971587.
Frucht, R. (1939), ""Herstellung von Graphen mit vorgegebener abstrakter Gruppe [Construction of graphs with prescribed group]"", Compositio Mathematica (in German), 6: 239–50, archived from the original on 2008-12-01.
Fulton, William; Harris, Joe (1991), Representation Theory: A First Course, Graduate Texts in Mathematics, Readings in Mathematics, vol. 129, New York: Springer-Verlag, ISBN 978-0-387-97495-8, MR 1153249
Goldstein, Herbert (1980), Classical Mechanics (2nd ed.), Reading, MA: Addison-Wesley Publishing, pp. 588–596, ISBN 0-201-02918-9.
Gollmann, Dieter (2011), Computer Security (2nd ed.), West Sussex, England: John Wiley & Sons, Ltd., ISBN 978-0-470-74115-3
Hatcher, Allen (2002), Algebraic Topology, Cambridge University Press, ISBN 978-0-521-79540-1.
Husain, Taqdir (1966), Introduction to Topological Groups, Philadelphia: W.B. Saunders Company, ISBN 978-0-89874-193-3
Jahn, H.; Teller, E. (1937), ""Stability of polyatomic molecules in degenerate electronic states. I. Orbital degeneracy"", Proceedings of the Royal Society A, 161 (905): 220–235, Bibcode:1937RSPSA.161..220J, doi:10.1098/rspa.1937.0142.
Kuipers, Jack B. (1999), Quaternions and Rotation Sequences: A Primer with Applications to Orbits, Aerospace, and Virtual Reality, Princeton University Press, Bibcode:1999qrsp.book.....K, ISBN 978-0-691-05872-6, MR 1670862.
Kuga, Michio (1993), Galois' Dream: Group Theory and Differential Equations, Boston, MA: Birkhäuser Boston, ISBN 978-0-8176-3688-3, MR 1199112.
Kurzweil, Hans; Stellmacher, Bernd (2004), The Theory of Finite Groups, Universitext, Berlin, New York: Springer-Verlag, ISBN 978-0-387-40510-0, MR 2014408.
Lay, David (2003), Linear Algebra and Its Applications, Addison-Wesley, ISBN 978-0-201-70970-4.
Mac Lane, Saunders (1998), Categories for the Working Mathematician (2nd ed.), Berlin, New York: Springer-Verlag, ISBN 978-0-387-98403-2.
Magnus, Wilhelm; Karrass, Abraham; Solitar, Donald (2004) [1966], Combinatorial Group Theory: Presentations of Groups in Terms of Generators and Relations, Courier, ISBN 978-0-486-43830-6
MathSciNet (2021), List of papers reviewed on MathSciNet on ""Group theory and its generalizations"" (MSC code 20), published in 2020, retrieved 14 May 2021
Michler, Gerhard (2006), Theory of Finite Simple Groups, Cambridge University Press, ISBN 978-0-521-86625-5.
Milne, James S. (1980), Étale Cohomology, Princeton University Press, ISBN 978-0-691-08238-7
Mumford, David; Fogarty, J.; Kirwan, F. (1994), Geometric Invariant Theory, vol. 34 (3rd ed.), Berlin, New York: Springer-Verlag, ISBN 978-3-540-56963-3, MR 1304906.
Naber, Gregory L. (2003), The Geometry of Minkowski Spacetime, New York: Dover Publications, ISBN 978-0-486-43235-9, MR 2044239.
Neukirch, Jürgen (1999), Algebraic Number Theory, Grundlehren der mathematischen Wissenschaften, vol. 322, Berlin: Springer-Verlag, ISBN 978-3-540-65399-8, MR 1697859, Zbl 0956.11021
Romanowska, A. B.; Smith, J. D. H. (2002), Modes, World Scientific, ISBN 978-981-02-4942-7.
Ronan, Mark (2007), Symmetry and the Monster: The Story of One of the Greatest Quests of Mathematics, Oxford University Press, ISBN 978-0-19-280723-6.
Rosen, Kenneth H. (2000), Elementary Number Theory and its Applications (4th ed.), Addison-Wesley, ISBN 978-0-201-87073-2, MR 1739433.
Rudin, Walter (1990), Fourier Analysis on Groups, Wiley Classics, Wiley-Blackwell, ISBN 0-471-52364-X.
Seress, Ákos (1997), ""An Introduction to Computational Group Theory"" (PDF), Notices of the American Mathematical Society, 44 (6): 671–679, MR 1452069.
Serre, Jean-Pierre (1977), Linear Representations of Finite Groups, Berlin, New York: Springer-Verlag, ISBN 978-0-387-90190-9, MR 0450380.
Schwartzman, Steven (1994), The Words of Mathematics: An Etymological Dictionary of Mathematical Terms Used in English, Mathematical Association of America, ISBN 978-0-88385-511-9.
Shatz, Stephen S. (1972), Profinite Groups, Arithmetic, and Geometry, Princeton University Press, ISBN 978-0-691-08017-8, MR 0347778
Simons, Jack (2003), An Introduction to Theoretical Chemistry, Cambridge University Press, ISBN 978-0-521-53047-7
Solomon, Ronald (2018), ""The classification of finite simple groups: A progress report"", Notices of the AMS, 65 (6): 1, doi:10.1090/noti1689
Stewart, Ian (2015), Galois Theory (4th ed.), CRC Press, ISBN 978-1-4822-4582-0
Suzuki, Michio (1951), ""On the lattice of subgroups of finite groups"", Transactions of the American Mathematical Society, 70 (2): 345–371, doi:10.2307/1990375, JSTOR 1990375.
Warner, Frank (1983), Foundations of Differentiable Manifolds and Lie Groups, Berlin, New York: Springer-Verlag, ISBN 978-0-387-90894-6.
Weibel, Charles A. (1994), An introduction to homological algebra, Cambridge Studies in Advanced Mathematics, vol. 38, Cambridge University Press, ISBN 978-0-521-55987-4, MR 1269324, OCLC 36131259
Weinberg, Steven (1972), Gravitation and Cosmology, New York: John Wiley & Sons, ISBN 0-471-92567-5.
Welsh, Dominic (1989), Codes and Cryptography, Oxford: Clarendon Press, ISBN 978-0-19-853287-3.
Weyl, Hermann (1952), Symmetry, Princeton University Press, ISBN 978-0-691-02374-8.
Zee, A. (2010), Quantum Field Theory in a Nutshell (second ed.), Princeton, N.J.: Princeton University Press, ISBN 978-0-691-14034-6, OCLC 768477138


=== Historical references ===

Borel, Armand (2001), Essays in the History of Lie Groups and Algebraic Groups, Providence, R.I.: American Mathematical Society, ISBN 978-0-8218-0288-5
Cayley, Arthur (1889), The Collected Mathematical Papers of Arthur Cayley, vol. II (1851–1860), Cambridge University Press.
O'Connor, John J.; Robertson, Edmund F., ""The development of group theory"", MacTutor History of Mathematics archive, University of St Andrews
Curtis, Charles W. (2003), Pioneers of Representation Theory: Frobenius, Burnside, Schur, and Brauer, History of Mathematics, Providence, R.I.: American Mathematical Society, ISBN 978-0-8218-2677-5.
von Dyck, Walther (1882), ""Gruppentheoretische Studien (Group-theoretical studies)"", Mathematische Annalen (in German), 20 (1): 1–44, doi:10.1007/BF01443322, S2CID 179178038, archived from the original on 2014-02-22.
Galois, Évariste (1908),  Tannery, Jules (ed.), Manuscrits de Évariste Galois [Évariste Galois' Manuscripts] (in French), Paris: Gauthier-Villars (Galois work was first published by Joseph Liouville in 1843).
Jordan, Camille (1870), Traité des substitutions et des équations algébriques [Study of Substitutions and Algebraic Equations] (in French), Paris: Gauthier-Villars.
Kleiner, Israel (1986), ""The evolution of group theory: A brief survey"", Mathematics Magazine, 59 (4): 195–215, doi:10.2307/2690312, JSTOR 2690312, MR 0863090.
Lie, Sophus (1973), Gesammelte Abhandlungen. Band 1 [Collected papers. Volume 1] (in German), New York: Johnson Reprint Corp., MR 0392459.
Mackey, George Whitelaw (1976), The Theory of Unitary Group Representations, University of Chicago Press, MR 0396826
Smith, David Eugene (1906), History of Modern Mathematics, Mathematical Monographs, No. 1.
Weyl, Hermann (1950) [1931], The Theory of Groups and Quantum Mechanics, translated by Robertson, H. P., Dover, ISBN 978-0-486-60269-1.
Wussing, Hans (2007), The Genesis of the Abstract Group Concept: A Contribution to the History of the Origin of Abstract Group Theory, New York: Dover Publications, ISBN 978-0-486-45868-7.


== External links ==
Weisstein, Eric W., ""Group"", MathWorld"
0144bcf2d2,Series (mathematics),"In mathematics, a series is, roughly speaking, the operation of adding infinitely many quantities, one after the other, to a given starting quantity.  The study of series is a major part of calculus and its generalization, mathematical analysis. Series are used in most areas of mathematics, even for studying finite structures (such as in combinatorics) through generating functions. In addition to their ubiquity in mathematics, infinite series are also widely used in other quantitative disciplines such as physics, computer science, statistics and finance.
For a long time, the idea that such a potentially infinite summation could produce a finite result was considered paradoxical. This paradox was resolved using the concept of a limit during the 17th century. Zeno's paradox of Achilles and the tortoise illustrates this counterintuitive property of infinite sums: Achilles runs after a tortoise, but when he reaches the position of the tortoise at the beginning of the race, the tortoise has reached a second position; when he reaches this second position, the tortoise is at a third position, and so on. Zeno concluded that Achilles could never reach the tortoise, and thus that movement does not exist. Zeno divided the race into infinitely many sub-races, each requiring a finite amount of time, so that the total time for Achilles to catch the tortoise is given by a series. The resolution of the paradox is that, although the series has an infinite number of terms, it has a finite sum, which gives the time necessary for Achilles to catch up with the tortoise.
In modern terminology, any (ordered) infinite sequence 
  
    
      
        (
        
          a
          
            1
          
        
        ,
        
          a
          
            2
          
        
        ,
        
          a
          
            3
          
        
        ,
        …
        )
      
    
    {\displaystyle (a_{1},a_{2},a_{3},\ldots )}
   of terms (that is, numbers, functions, or anything that can be added) defines a series, which is the operation of adding the ai one after the other. To emphasize that there are an infinite number of terms, a series may be called an infinite series. Such a series is represented (or denoted) by an expression like

or, using the summation sign,

The infinite sequence of additions implied by a series cannot be effectively carried on (at least in a finite amount of time). However, if the set to which the terms and their finite sums belong has a notion of limit, it is sometimes possible to assign a value to a series, called the sum of the series. This value is the limit as n tends to infinity (if the limit exists) of the finite sums of the n first terms of the series, which are called the nth partial sums of the series. That is,

When this limit exists, one says that the series is convergent or summable, or that the sequence 
  
    
      
        (
        
          a
          
            1
          
        
        ,
        
          a
          
            2
          
        
        ,
        
          a
          
            3
          
        
        ,
        …
        )
      
    
    {\displaystyle (a_{1},a_{2},a_{3},\ldots )}
   is summable. In this case, the limit is called the sum of the series. Otherwise, the series is said to be divergent.The notation 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            ∞
          
        
        
          a
          
            i
          
        
      
    
    {\textstyle \sum _{i=1}^{\infty }a_{i}}
   denotes both the series—that is the implicit process of adding the terms one after the other indefinitely—and, if the series is convergent, the sum of the series—the result of the process. This is a generalization of the similar convention of denoting by 
  
    
      
        a
        +
        b
      
    
    {\displaystyle a+b}
   both the addition—the process of adding—and its result—the sum of a and b.
Generally, the terms of a series come from a ring, often the field 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   of the real numbers or the field 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
   of the complex numbers. In this case, the set of all series is itself a ring (and even an associative algebra), in which the addition consists of adding the series term by term, and the multiplication is the Cauchy product.


== Basic properties ==
An infinite series or simply a series is an infinite sum, represented by an infinite expression of the form
where 
  
    
      
        (
        
          a
          
            n
          
        
        )
      
    
    {\displaystyle (a_{n})}
   is any ordered sequence of terms, such as numbers, functions, or anything else that can be added (an abelian group).  This is an expression that is obtained from the list of terms 
  
    
      
        
          a
          
            0
          
        
        ,
        
          a
          
            1
          
        
        ,
        …
      
    
    {\displaystyle a_{0},a_{1},\dots }
   by laying them side by side, and conjoining them with the symbol ""+"".  A series may also be represented by using summation notation, such as

If an abelian group A of terms has a concept of limit (e.g., if it is a metric space), then some series, the convergent series, can be interpreted as having a value in A, called the sum of the series. This includes the common cases from calculus, in which the group is the field of real numbers or the field of complex numbers. Given a series 
  
    
      
        s
        =
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          a
          
            n
          
        
      
    
    {\textstyle s=\sum _{n=0}^{\infty }a_{n}}
  , its kth partial sum is
By definition, the series 
  
    
      
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          a
          
            n
          
        
      
    
    {\textstyle \sum _{n=0}^{\infty }a_{n}}
   converges to the limit L (or simply sums to L), if the sequence of its partial sums has a limit L. In this case, one usually writes

A series is said to be convergent if it converges to some limit, or divergent when it does not. The value of this limit, if it exists, is then the value of the series.


=== Convergent series ===

A series Σan is said to converge or to be convergent when the sequence (sk) of partial sums has a finite limit. If the limit of sk is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the value (or sum) of the series

An easy way that an infinite series can converge is if all the an are zero for n sufficiently large.  Such a series can be identified with a finite sum, so it is only infinite in a trivial sense.
Working out the properties of the series that converge, even if infinitely many terms are nonzero, is the essence of the study of series. Consider the example

It is possible to ""visualize"" its convergence on the real number line: we can imagine a line of length 2, with successive segments marked off of lengths 1, 1/2, 1/4, etc. There is always room to mark the next segment, because the amount of line remaining is always the same as the last segment marked: When we have marked off 1/2, we still have a piece of length 1/2 unmarked, so we can certainly mark the next 1/4. This argument does not prove that the sum is equal to 2 (although it is), but it does prove that it is at most 2.  In other words, the series has an upper bound. Given that the series converges, proving that it is equal to 2 requires only elementary algebra. If the series is denoted S, it can be seen that

Therefore,

The idiom can be extended to other, equivalent notions of series. For instance, a recurring decimal, as in

encodes the series

Since these series always converge to real numbers (because of what is called the completeness property of the real numbers), to talk about the series in this way is the same as to talk about the numbers for which they stand. In particular, the decimal expansion 0.111... can be identified with 1/9. This leads to an argument that 9 × 0.111... = 0.999... = 1, which only relies on the fact that the limit laws for series preserve the arithmetic operations; for more detail on this argument, see 0.999....


== Examples of numerical series ==

A geometric series is one where each successive term is produced by multiplying the previous term by a constant number (called the common ratio in this context). For example: In general, the geometric series  converges if and only if 
  
    
      
        
          |
        
        z
        
          |
        
        <
        1
      
    
    {\textstyle |z|<1}
  , in which case it converges to 
  
    
      
        
          
            1
            
              1
              −
              z
            
          
        
      
    
    {\textstyle {1 \over 1-z}}
  .
The harmonic series is the series  The harmonic series is divergent.
An alternating series is a series where terms alternate signs.  Examples:  (alternating harmonic series) and 
A telescoping series  converges if the sequence bn converges to a limit L—as n goes to infinity. The value of the series is then b1 − L.
An arithmetico-geometric series is a generalization of the geometric series, which has coefficients of the common ratio equal to the terms in an arithmetic sequence.  Example: 
The p-series  converges if p > 1 and diverges for p ≤ 1, which can be shown with the integral criterion described below in convergence tests. As a function of p, the sum of this series is Riemann's zeta function.
Hypergeometric series:  and their generalizations (such as basic hypergeometric series and elliptic hypergeometric series) frequently appear in integrable systems and mathematical physics.
There are some elementary series whose convergence is not yet known/proven. For example, it is unknown whether the Flint Hills series    converges or not. The convergence depends on how well 
  
    
      
        π
      
    
    {\displaystyle \pi }
   can be approximated with rational numbers (which is unknown as of yet). More specifically, the values of n with large numerical contributions to the sum are the numerators of the continued fraction convergents of 
  
    
      
        π
      
    
    {\displaystyle \pi }
  , a sequence beginning with 1, 3, 22, 333, 355, 103993, ... (sequence A046947 in the OEIS). These are integers that are close to 
  
    
      
        n
        π
      
    
    {\displaystyle n\pi }
   for some integer n, so that 
  
    
      
        sin
        ⁡
        n
        π
      
    
    {\displaystyle \sin n\pi }
   is close to 0 and its reciprocal is large.


=== Pi ===


=== Natural logarithm of 2 ===


=== Natural logarithm base e ===


== Calculus and partial summation as an operation on sequences ==
Partial summation takes as input a sequence, (an), and gives as output another sequence, (SN). It is thus a unary operation on sequences. Further, this function is linear, and thus is a linear operator on the vector space of sequences, denoted Σ. The inverse operator is the finite difference operator, denoted Δ. These behave as discrete analogues of integration and differentiation, only for series (functions of a natural number) instead of functions of a real variable. For example, the sequence (1, 1, 1, ...) has series (1, 2, 3, 4, ...) as its partial summation, which is analogous to the fact that 
  
    
      
        
          ∫
          
            0
          
          
            x
          
        
        1
        
        d
        t
        =
        x
        .
      
    
    {\textstyle \int _{0}^{x}1\,dt=x.}
  
In computer science, it is known as prefix sum.


== Properties of series ==
Series are classified not only by whether they converge or diverge, but also by the properties of the terms an (absolute or conditional convergence); type of convergence of the series (pointwise, uniform); the class of the term an (whether it is a real number, arithmetic progression, trigonometric function); etc.


=== Non-negative terms ===
When an is a non-negative real number for every n, the sequence SN of partial sums is non-decreasing. It follows that a series Σan with non-negative terms converges if and only if the sequence SN of partial sums is bounded.
For example, the series

is convergent, because the inequality

and a telescopic sum argument implies that the partial sums are bounded by 2. The exact value of the original series is the Basel problem.


=== Grouping ===
When you group a series reordering of the series does not happen, so Riemann series theorem does not apply. A new series will have its partial sums as subsequence of original series, which means if the original series converges, so does the new series. But for divergent series that is not true, for example 1-1+1-1+... grouped every two elements will create 0+0+0+... series, which is convergent. On the other hand, divergence of the new series means the original series can be only divergent which is sometimes useful, like in Oresme proof.


=== Absolute convergence ===

A series

converges absolutely if the series of absolute values

converges. This is sufficient to guarantee not only that the original series converges to a limit, but also that any reordering of it converges to the same limit.


=== Conditional convergence ===

A series of real or complex numbers is said to be conditionally convergent (or semi-convergent) if it is convergent but not absolutely convergent. A famous example is the alternating series

which is convergent (and its sum is equal to 
  
    
      
        ln
        ⁡
        2
      
    
    {\displaystyle \ln 2}
  ), but the series formed by taking the absolute value of each term is the divergent harmonic series. The Riemann series theorem says that any conditionally convergent series can be reordered to make a divergent series, and moreover, if the 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   are real and 
  
    
      
        S
      
    
    {\displaystyle S}
   is any real number, that one can find a reordering so that the reordered series converges with sum equal to 
  
    
      
        S
      
    
    {\displaystyle S}
  .
Abel's test is an important tool for handling semi-convergent series. If a series has the form

where the partial sums 
  
    
      
        
          B
          
            n
          
        
        =
        
          b
          
            0
          
        
        +
        ⋯
        +
        
          b
          
            n
          
        
      
    
    {\displaystyle B_{n}=b_{0}+\cdots +b_{n}}
   are bounded, 
  
    
      
        
          λ
          
            n
          
        
      
    
    {\displaystyle \lambda _{n}}
   has bounded variation, and 
  
    
      
        lim
        
          λ
          
            n
          
        
        
          b
          
            n
          
        
      
    
    {\displaystyle \lim \lambda _{n}b_{n}}
   exists:

then the series 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   is convergent. This applies to the point-wise convergence of many trigonometric series, as in

with 
  
    
      
        0
        <
        x
        <
        2
        π
      
    
    {\displaystyle 0<x<2\pi }
  .  Abel's method consists in writing 
  
    
      
        
          b
          
            n
            +
            1
          
        
        =
        
          B
          
            n
            +
            1
          
        
        −
        
          B
          
            n
          
        
      
    
    {\displaystyle b_{n+1}=B_{n+1}-B_{n}}
  , and in performing a transformation similar to integration by parts (called summation by parts), that relates the given series 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   to the absolutely convergent series


=== Evaluation of truncation errors ===
The evaluation of truncation errors is an important procedure in numerical analysis (especially validated numerics and computer-assisted proof).


==== Alternating series ====
When conditions of the alternating series test are satisfied by 
  
    
      
        S
        :=
        
          ∑
          
            m
            =
            0
          
          
            ∞
          
        
        (
        −
        1
        
          )
          
            m
          
        
        
          u
          
            m
          
        
      
    
    {\textstyle S:=\sum _{m=0}^{\infty }(-1)^{m}u_{m}}
  , there is an exact error evaluation. Set 
  
    
      
        
          s
          
            n
          
        
      
    
    {\displaystyle s_{n}}
   to be the partial sum 
  
    
      
        
          s
          
            n
          
        
        :=
        
          ∑
          
            m
            =
            0
          
          
            n
          
        
        (
        −
        1
        
          )
          
            m
          
        
        
          u
          
            m
          
        
      
    
    {\textstyle s_{n}:=\sum _{m=0}^{n}(-1)^{m}u_{m}}
   of the given alternating series 
  
    
      
        S
      
    
    {\displaystyle S}
  . Then the next inequality holds:


==== Taylor series ====
Taylor's theorem is a statement that includes the evaluation of the error term when the Taylor series is truncated.


==== Hypergeometric series ====
By using the ratio, we can obtain the evaluation of the error term when the hypergeometric series is truncated.


==== Matrix exponential ====
For the matrix exponential:

the following error evaluation holds (scaling and squaring method):


== Convergence tests ==

There exist many tests that can be used to determine whether particular series converge or diverge.

n-th term test: If 
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        ≠
        0
      
    
    {\textstyle \lim _{n\to \infty }a_{n}\neq 0}
  , then the series diverges; if 
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        =
        0
      
    
    {\textstyle \lim _{n\to \infty }a_{n}=0}
  , then the test is inconclusive.
Comparison test 1 (see Direct comparison test): If 
  
    
      
        ∑
        
          b
          
            n
          
        
      
    
    {\textstyle \sum b_{n}}
   is an absolutely convergent series such that 
  
    
      
        
          |
          
            a
            
              n
            
          
          |
        
        ≤
        C
        
          |
          
            b
            
              n
            
          
          |
        
      
    
    {\displaystyle \left\vert a_{n}\right\vert \leq C\left\vert b_{n}\right\vert }
   for some number 
  
    
      
        C
      
    
    {\displaystyle C}
   and for sufficiently large 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   converges absolutely as well. If 
  
    
      
        ∑
        
          |
          
            b
            
              n
            
          
          |
        
      
    
    {\textstyle \sum \left\vert b_{n}\right\vert }
   diverges, and 
  
    
      
        
          |
          
            a
            
              n
            
          
          |
        
        ≥
        
          |
          
            b
            
              n
            
          
          |
        
      
    
    {\displaystyle \left\vert a_{n}\right\vert \geq \left\vert b_{n}\right\vert }
    for all sufficiently large 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   also fails to converge absolutely (though it could still be conditionally convergent, for example, if the 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   alternate in sign).
Comparison test 2 (see Limit comparison test): If 
  
    
      
        ∑
        
          b
          
            n
          
        
      
    
    {\textstyle \sum b_{n}}
   is an absolutely convergent series such that 
  
    
      
        
          |
          
            
              
                a
                
                  n
                  +
                  1
                
              
              
                a
                
                  n
                
              
            
          
          |
        
        ≤
        
          |
          
            
              
                b
                
                  n
                  +
                  1
                
              
              
                b
                
                  n
                
              
            
          
          |
        
      
    
    {\displaystyle \left\vert {\frac {a_{n+1}}{a_{n}}}\right\vert \leq \left\vert {\frac {b_{n+1}}{b_{n}}}\right\vert }
   for sufficiently large 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   converges absolutely as well. If 
  
    
      
        ∑
        
          |
          
            b
            
              n
            
          
          |
        
      
    
    {\textstyle \sum \left|b_{n}\right|}
   diverges, and 
  
    
      
        
          |
          
            
              
                a
                
                  n
                  +
                  1
                
              
              
                a
                
                  n
                
              
            
          
          |
        
        ≥
        
          |
          
            
              
                b
                
                  n
                  +
                  1
                
              
              
                b
                
                  n
                
              
            
          
          |
        
      
    
    {\displaystyle \left\vert {\frac {a_{n+1}}{a_{n}}}\right\vert \geq \left\vert {\frac {b_{n+1}}{b_{n}}}\right\vert }
   for all sufficiently large 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   also fails to converge absolutely (though it could still be conditionally convergent, for example, if the 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   alternate in sign).
Ratio test: If there exists a constant 
  
    
      
        C
        <
        1
      
    
    {\displaystyle C<1}
   such that 
  
    
      
        
          |
          
            
              
                a
                
                  n
                  +
                  1
                
              
              
                a
                
                  n
                
              
            
          
          |
        
        <
        C
      
    
    {\displaystyle \left\vert {\frac {a_{n+1}}{a_{n}}}\right\vert <C}
   for all sufficiently large 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   converges absolutely. When the ratio is less than 
  
    
      
        1
      
    
    {\displaystyle 1}
  , but not less than a constant less than 
  
    
      
        1
      
    
    {\displaystyle 1}
  , convergence is possible but this test does not establish it.
Root test: If there exists a constant 
  
    
      
        C
        <
        1
      
    
    {\displaystyle C<1}
   such that 
  
    
      
        
          
            |
            
              a
              
                n
              
            
            |
          
          
            
              1
              n
            
          
        
        ≤
        C
      
    
    {\displaystyle \left\vert a_{n}\right\vert ^{\frac {1}{n}}\leq C}
   for all sufficiently large 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   converges absolutely.
Integral test: if 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   is a positive monotone decreasing function defined on the interval 
  
    
      
        [
        1
        ,
        ∞
        )
      
    
    {\displaystyle [1,\infty )}
    with 
  
    
      
        f
        (
        n
        )
        =
        
          a
          
            n
          
        
      
    
    {\displaystyle f(n)=a_{n}}
   for all 
  
    
      
        n
      
    
    {\displaystyle n}
  , then 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   converges if and only if the integral 
  
    
      
        
          ∫
          
            1
          
          
            ∞
          
        
        f
        (
        x
        )
        
        d
        x
      
    
    {\textstyle \int _{1}^{\infty }f(x)\,dx}
   is finite.
Cauchy's condensation test: If 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   is non-negative and non-increasing, then the two series 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\textstyle \sum a_{n}}
   and 
  
    
      
        ∑
        
          2
          
            k
          
        
        
          a
          
            (
            
              2
              
                k
              
            
            )
          
        
      
    
    {\textstyle \sum 2^{k}a_{(2^{k})}}
   are of the same nature: both convergent, or both divergent.
Alternating series test: A series of the form 
  
    
      
        ∑
        (
        −
        1
        
          )
          
            n
          
        
        
          a
          
            n
          
        
      
    
    {\textstyle \sum (-1)^{n}a_{n}}
   (with 
  
    
      
        
          a
          
            n
          
        
        >
        0
      
    
    {\displaystyle a_{n}>0}
  ) is called alternating. Such a series converges if the sequence 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   is monotone decreasing and converges to 
  
    
      
        0
      
    
    {\displaystyle 0}
  . The converse is in general not true.
For some specific types of series there are more specialized convergence tests, for instance for Fourier series there is the Dini test.


== Series of functions ==

A series of real- or complex-valued functions

converges pointwise on a set E, if the series converges for each x in E as an ordinary series of real or complex numbers.  Equivalently, the partial sums

converge to ƒ(x) as N → ∞ for each x ∈ E.
A stronger notion of convergence of a series of functions is the uniform convergence. A series converges uniformly if it converges pointwise to the function ƒ(x), and the error in approximating the limit by the Nth partial sum,

can be made minimal independently of x by choosing a sufficiently large N.
Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit.  For example, if a series of continuous functions converges uniformly, then the limit function is also continuous.  Similarly, if the ƒn are integrable on a closed and bounded interval I and converge uniformly, then the series is also integrable on I and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.
More sophisticated types of convergence of a series of functions can also be defined.  In measure theory, for instance, a series of functions converges almost everywhere if it converges pointwise except on a certain set of measure zero.  Other modes of convergence depend on a different metric space structure on the space of functions under consideration.  For instance, a series of functions converges in mean on a set E to a limit function ƒ provided

as N → ∞.


=== Power series ===
A power series is a series of the form

The Taylor series at a point c of a function is a power series that, in many cases, converges to the function in a neighborhood of c. For example, the series

is the Taylor series of 
  
    
      
        
          e
          
            x
          
        
      
    
    {\displaystyle e^{x}}
   at the origin and converges to it for every x.
Unless it converges only at x=c, such a series converges on a certain open disc of convergence centered at the point c in the complex plane, and may also converge at some of the points of the boundary of the disc.  The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients an.  The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets.
Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required.


=== Formal power series ===

While many uses of power series refer to their sums, it is also possible to treat power series as formal sums, meaning that no addition operations are actually performed, and the symbol ""+"" is an abstract symbol of conjunction which is not necessarily interpreted as corresponding to addition. In this setting, the sequence of coefficients itself is of interest, rather than the convergence of the series. Formal power series are used in combinatorics to describe and study sequences that are otherwise difficult to handle, for example, using the method of generating functions.  The Hilbert–Poincaré series is a formal power series used to study graded algebras.
Even if the limit of the power series is not considered, if the terms support appropriate structure then it is possible to define operations such as addition, multiplication, derivative, antiderivative for power series ""formally"", treating the symbol ""+"" as if it corresponded to addition. In the most common setting, the terms come from a commutative ring, so that the formal power series can be added term-by-term and multiplied via the Cauchy product. In this case the algebra of formal power series is the total algebra of the monoid of natural numbers over the underlying term ring.  If the underlying term ring is a differential algebra, then the algebra of formal power series is also a differential algebra, with differentiation performed term-by-term.


=== Laurent series ===

Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents.  A Laurent series is thus any series of the form

If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points.  The series converges uniformly on compact subsets of the interior of the annulus of convergence.


=== Dirichlet series ===
A Dirichlet series is one of the form

where s is a complex number.  For example, if all an are equal to 1, then the Dirichlet series is the Riemann zeta function

Like the zeta function, Dirichlet series in general play an important role in analytic number theory.  Generally a Dirichlet series converges if the real part of s is greater than a number called the abscissa of convergence.  In many cases, a Dirichlet series can be extended to an analytic function outside the domain of convergence by analytic continuation.  For example, the Dirichlet series for the zeta function converges absolutely when Re(s) > 1, but the zeta function can be extended to a holomorphic function defined on 
  
    
      
        
          C
        
        ∖
        {
        1
        }
      
    
    {\displaystyle \mathbb {C} \setminus \{1\}}
   with a simple pole at 1.
This series can be directly generalized to general Dirichlet series.


=== Trigonometric series ===

A series of functions in which the terms are trigonometric functions is called a trigonometric series:

The most important example of a trigonometric series is the Fourier series of a function.


== History of the theory of infinite series ==


=== Development of infinite series ===
Greek mathematician Archimedes produced the first known summation of an infinite series with a
method that is still used in the area of calculus today. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of π.Mathematicians from Kerala, India studied infinite series around 1350 CE.In the 17th century, James Gregory worked in the new decimal system on infinite series and published several Maclaurin series. In 1715, a general method for constructing the Taylor series for all functions for which they exist was provided by Brook Taylor. Leonhard Euler in the 18th century, developed the theory of hypergeometric series and q-series.


=== Convergence criteria ===
The investigation of the validity of infinite series is considered to begin with Gauss in the 19th century. Euler had already considered the hypergeometric series

on which Gauss published a memoir in 1812. It established simpler criteria of convergence, and the questions of remainders and the range of convergence.
Cauchy (1821) insisted on strict tests of convergence; he showed that if two series are convergent their product is not necessarily so, and with him begins the discovery of effective criteria. The terms convergence and divergence had been introduced long before by Gregory (1668). Leonhard Euler and Gauss had given various criteria, and Colin Maclaurin had anticipated some of Cauchy's discoveries. Cauchy advanced the theory of power series by his expansion of a complex function in such a form.
Abel (1826) in his memoir on the binomial series

corrected certain of Cauchy's conclusions, and gave a completely scientific summation of the series for complex values of 
  
    
      
        m
      
    
    {\displaystyle m}
   and 
  
    
      
        x
      
    
    {\displaystyle x}
  . He showed the necessity of considering the subject of continuity in questions of convergence.
Cauchy's methods led to special rather than general criteria, and
the same may be said of Raabe (1832), who made the first elaborate investigation of the subject, of De Morgan (from 1842), whose
logarithmic test DuBois-Reymond (1873) and Pringsheim (1889) have
shown to fail within a certain region; of Bertrand (1842), Bonnet
(1843), Malmsten (1846, 1847, the latter without integration); Stokes (1847), Paucker (1852), Chebyshev (1852), and Arndt
(1853).
General criteria began with Kummer (1835), and have been studied by Eisenstein (1847), Weierstrass in his various
contributions to the theory of functions, Dini (1867),
DuBois-Reymond (1873), and many others. Pringsheim's memoirs (1889) present the most complete general theory.


=== Uniform convergence ===
The theory of uniform convergence was treated by Cauchy (1821), his limitations being pointed out by Abel, but the first to attack it
successfully were Seidel and Stokes (1847–48). Cauchy took up the
problem again (1853), acknowledging Abel's criticism, and reaching
the same conclusions which Stokes had already found. Thomae used the
doctrine (1866), but there was great delay in recognizing the importance of distinguishing between uniform and non-uniform
convergence, in spite of the demands of the theory of functions.


=== Semi-convergence ===
A series is said to be semi-convergent (or conditionally convergent) if it is convergent but not absolutely convergent.
Semi-convergent series were studied by Poisson (1823), who also gave a general form for the remainder of the Maclaurin formula. The most important solution of the problem is due, however, to Jacobi (1834), who attacked the question of the remainder from a different standpoint and reached a different formula. This expression was also worked out, and another one given, by Malmsten (1847). Schlömilch (Zeitschrift, Vol.I, p. 192, 1856) also improved Jacobi's remainder, and showed the relation between the remainder and Bernoulli's function

Genocchi (1852) has further contributed to the theory.
Among the early writers was Wronski, whose ""loi suprême"" (1815) was hardly recognized until Cayley (1873) brought it into
prominence.


=== Fourier series ===
Fourier series were being investigated
as the result of physical considerations at the same time that
Gauss, Abel, and Cauchy were working out the theory of infinite
series. Series for the expansion of sines and cosines, of multiple
arcs in powers of the sine and cosine of the arc had been treated by
Jacob Bernoulli (1702) and his brother Johann Bernoulli (1701) and still
earlier by Vieta. Euler and Lagrange simplified the subject,
as did Poinsot, Schröter, Glaisher, and Kummer.
Fourier (1807) set for himself a different problem, to
expand a given function of x in terms of the sines or cosines of
multiples of x, a problem which he embodied in his Théorie analytique de la chaleur (1822). Euler had already given the formulas for determining the coefficients in the series;
Fourier was the first to assert and attempt to prove the general
theorem. Poisson (1820–23) also attacked the problem from a
different standpoint. Fourier did not, however, settle the question
of convergence of his series, a matter left for Cauchy (1826) to
attempt and for Dirichlet (1829) to handle in a thoroughly
scientific manner (see convergence of Fourier series). Dirichlet's treatment (Crelle, 1829), of trigonometric series was the subject of criticism and improvement by
Riemann (1854), Heine, Lipschitz, Schläfli, and
du Bois-Reymond. Among other prominent contributors to the theory of
trigonometric and Fourier series were Dini, Hermite, Halphen,
Krause, Byerly and Appell.


== Generalizations ==


=== Asymptotic series ===
Asymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain.  In general they do not converge, but they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers.


=== Divergent series ===

Under many circumstances, it is desirable to assign a limit to a series which fails to converge in the usual sense.  A summability method is such an assignment of a limit to a subset of the set of divergent series which properly extends the classical notion of convergence.  Summability methods include Cesàro summation, (C,k) summation, Abel summation, and Borel summation, in increasing order of generality (and hence applicable to increasingly divergent series).
A variety of general results concerning possible summability methods are known.  The Silverman–Toeplitz theorem characterizes matrix summability methods, which are methods for summing a divergent series by applying an infinite matrix to the vector of coefficients.  The most general method for summing a divergent series is non-constructive, and concerns Banach limits.


=== Summations over arbitrary index sets ===
Definitions may be given for sums over an arbitrary index set 
  
    
      
        I
        .
      
    
    {\displaystyle I.}
    There are two main differences with the usual notion of series: first, there is no specific order given on the set 
  
    
      
        I
      
    
    {\displaystyle I}
  ; second, this set 
  
    
      
        I
      
    
    {\displaystyle I}
   may be uncountable.  The notion of convergence needs to be strengthened, because the concept of conditional convergence depends on the ordering of the index set.
If 
  
    
      
        a
        :
        I
        ↦
        G
      
    
    {\displaystyle a:I\mapsto G}
   is a function from an index set 
  
    
      
        I
      
    
    {\displaystyle I}
   to a set 
  
    
      
        G
        ,
      
    
    {\displaystyle G,}
   then the ""series"" associated to 
  
    
      
        a
      
    
    {\displaystyle a}
   is the formal sum of the elements 
  
    
      
        a
        (
        x
        )
        ∈
        G
      
    
    {\displaystyle a(x)\in G}
   over the index elements 
  
    
      
        x
        ∈
        I
      
    
    {\displaystyle x\in I}
   denoted by the

When the index set is the natural numbers 
  
    
      
        I
        =
        
          N
        
        ,
      
    
    {\displaystyle I=\mathbb {N} ,}
   the function 
  
    
      
        a
        :
        
          N
        
        ↦
        G
      
    
    {\displaystyle a:\mathbb {N} \mapsto G}
   is a sequence denoted by 
  
    
      
        a
        (
        n
        )
        =
        
          a
          
            n
          
        
        .
      
    
    {\displaystyle a(n)=a_{n}.}
   A series indexed on the natural numbers is an ordered formal sum and so we rewrite 
  
    
      
        
          ∑
          
            n
            ∈
            
              N
            
          
        
      
    
    {\textstyle \sum _{n\in \mathbb {N} }}
   as 
  
    
      
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
      
    
    {\textstyle \sum _{n=0}^{\infty }}
   in order to emphasize the ordering induced by the natural numbers. Thus, we obtain the common notation for a series indexed by the natural numbers


==== Families of non-negative numbers ====
When summing a family 
  
    
      
        
          {
          
            
              a
              
                i
              
            
            :
            i
            ∈
            I
          
          }
        
      
    
    {\displaystyle \left\{a_{i}:i\in I\right\}}
   of non-negative real numbers, define

When the supremum is finite then the set of 
  
    
      
        i
        ∈
        I
      
    
    {\displaystyle i\in I}
   such that 
  
    
      
        
          a
          
            i
          
        
        >
        0
      
    
    {\displaystyle a_{i}>0}
   is countable.  Indeed, for every 
  
    
      
        n
        ≥
        1
        ,
      
    
    {\displaystyle n\geq 1,}
   the cardinality 
  
    
      
        
          |
          
            A
            
              n
            
          
          |
        
      
    
    {\displaystyle \left|A_{n}\right|}
   of the set 
  
    
      
        
          A
          
            n
          
        
        =
        
          {
          
            i
            ∈
            I
            :
            
              a
              
                i
              
            
            >
            1
            
              /
            
            n
          
          }
        
      
    
    {\displaystyle A_{n}=\left\{i\in I:a_{i}>1/n\right\}}
   is finite because

If 
  
    
      
        I
      
    
    {\displaystyle I}
   is countably infinite and enumerated as 
  
    
      
        I
        =
        
          {
          
            
              i
              
                0
              
            
            ,
            
              i
              
                1
              
            
            ,
            …
          
          }
        
      
    
    {\displaystyle I=\left\{i_{0},i_{1},\ldots \right\}}
   then the above defined sum satisfies

provided the value 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
   is allowed for the sum of the series.
Any sum over non-negative reals can be understood as the integral of a non-negative function with respect to the counting measure, which accounts for the many similarities between the two constructions.


==== Abelian topological groups ====
Let 
  
    
      
        a
        :
        I
        →
        X
      
    
    {\displaystyle a:I\to X}
   be a map, also denoted by 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
        ,
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I},}
   from some non-empty set 
  
    
      
        I
      
    
    {\displaystyle I}
   into a Hausdorff abelian topological group 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   
Let 
  
    
      
        Finite
        ⁡
        (
        I
        )
      
    
    {\displaystyle \operatorname {Finite} (I)}
   be the collection of all finite subsets of 
  
    
      
        I
        ,
      
    
    {\displaystyle I,}
   with 
  
    
      
        Finite
        ⁡
        (
        I
        )
      
    
    {\displaystyle \operatorname {Finite} (I)}
   viewed as a directed set, ordered under inclusion 
  
    
      
        
        ⊆
        
      
    
    {\displaystyle \,\subseteq \,}
   with union as join. 
The family 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
        ,
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I},}
   is said to be unconditionally summable if the following limit, which is denoted by 
  
    
      
        
          ∑
          
            i
            ∈
            I
          
        
        
          a
          
            i
          
        
      
    
    {\displaystyle \sum _{i\in I}a_{i}}
   and is called the sum of 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
        ,
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I},}
   exists in 
  
    
      
        X
        :
      
    
    {\displaystyle X:}
  

Saying that the sum 
  
    
      
        S
        :=
        
          ∑
          
            i
            ∈
            I
          
        
        
          a
          
            i
          
        
      
    
    {\displaystyle S:=\sum _{i\in I}a_{i}}
   is the limit of finite partial sums means that for every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of the origin in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   there exists a finite subset 
  
    
      
        
          A
          
            0
          
        
      
    
    {\displaystyle A_{0}}
   of 
  
    
      
        I
      
    
    {\displaystyle I}
   such that

Because 
  
    
      
        Finite
        ⁡
        (
        I
        )
      
    
    {\displaystyle \operatorname {Finite} (I)}
   is not totally ordered, this is not a limit of a sequence of partial sums, but rather of a net.For every neighborhood 
  
    
      
        W
      
    
    {\displaystyle W}
   of the origin in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   there is a smaller neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   such that 
  
    
      
        V
        −
        V
        ⊆
        W
        .
      
    
    {\displaystyle V-V\subseteq W.}
    It follows that the finite partial sums of an unconditionally summable family 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
        ,
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I},}
   form a Cauchy net, that is, for every neighborhood 
  
    
      
        W
      
    
    {\displaystyle W}
   of the origin in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   there exists a finite subset 
  
    
      
        
          A
          
            0
          
        
      
    
    {\displaystyle A_{0}}
   of 
  
    
      
        I
      
    
    {\displaystyle I}
   such that

which implies that 
  
    
      
        
          a
          
            i
          
        
        ∈
        W
      
    
    {\displaystyle a_{i}\in W}
   for every 
  
    
      
        i
        ∈
        I
        ∖
        
          A
          
            0
          
        
      
    
    {\displaystyle i\in I\setminus A_{0}}
   (by taking 
  
    
      
        
          A
          
            1
          
        
        :=
        
          A
          
            0
          
        
        ∪
        {
        i
        }
      
    
    {\displaystyle A_{1}:=A_{0}\cup \{i\}}
   and 
  
    
      
        
          A
          
            2
          
        
        :=
        
          A
          
            0
          
        
      
    
    {\displaystyle A_{2}:=A_{0}}
  ). 
When 
  
    
      
        X
      
    
    {\displaystyle X}
   is complete, a family 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I}}
   is unconditionally summable in 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if the finite sums satisfy the latter Cauchy net condition. When 
  
    
      
        X
      
    
    {\displaystyle X}
   is complete and 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
        ,
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I},}
   is unconditionally summable in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then for every subset 
  
    
      
        J
        ⊆
        I
        ,
      
    
    {\displaystyle J\subseteq I,}
   the corresponding subfamily 
  
    
      
        
          
            (
            
              a
              
                j
              
            
            )
          
          
            j
            ∈
            J
          
        
        ,
      
    
    {\displaystyle \left(a_{j}\right)_{j\in J},}
   is also unconditionally summable in 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
  
When the sum of a family of non-negative numbers, in the extended sense defined before, is finite, then it coincides with the sum in the topological group 
  
    
      
        X
        =
        
          R
        
        .
      
    
    {\displaystyle X=\mathbb {R} .}
  
If a family 
  
    
      
        
          
            (
            
              a
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \left(a_{i}\right)_{i\in I}}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   is unconditionally summable then for every neighborhood 
  
    
      
        W
      
    
    {\displaystyle W}
   of the origin in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   there is a finite subset 
  
    
      
        
          A
          
            0
          
        
        ⊆
        I
      
    
    {\displaystyle A_{0}\subseteq I}
   such that 
  
    
      
        
          a
          
            i
          
        
        ∈
        W
      
    
    {\displaystyle a_{i}\in W}
   for every index 
  
    
      
        i
      
    
    {\displaystyle i}
   not in 
  
    
      
        
          A
          
            0
          
        
        .
      
    
    {\displaystyle A_{0}.}
    If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a first-countable space then it follows that the set of 
  
    
      
        i
        ∈
        I
      
    
    {\displaystyle i\in I}
   such that 
  
    
      
        
          a
          
            i
          
        
        ≠
        0
      
    
    {\displaystyle a_{i}\neq 0}
   is countable.  This need not be true in a general abelian topological group (see examples below).


==== Unconditionally convergent series ====
Suppose that 
  
    
      
        I
        =
        
          N
        
        .
      
    
    {\displaystyle I=\mathbb {N} .}
    If a family 
  
    
      
        
          a
          
            n
          
        
        ,
        n
        ∈
        
          N
        
        ,
      
    
    {\displaystyle a_{n},n\in \mathbb {N} ,}
   is unconditionally summable in a Hausdorff abelian topological group 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then the series in the usual sense converges and has the same sum,

By nature, the definition of unconditional summability is insensitive to the order of the summation.  When 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\displaystyle \sum a_{n}}
   is unconditionally summable, then the series remains convergent after any permutation 
  
    
      
        σ
        :
        
          N
        
        →
        
          N
        
      
    
    {\displaystyle \sigma :\mathbb {N} \to \mathbb {N} }
   of the set 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   of indices, with the same sum,

Conversely, if every permutation of a series 
  
    
      
        ∑
        
          a
          
            n
          
        
      
    
    {\displaystyle \sum a_{n}}
   converges, then the series is unconditionally convergent.  When 
  
    
      
        X
      
    
    {\displaystyle X}
   is complete then unconditional convergence is also equivalent to the fact that all subseries are convergent; if 
  
    
      
        X
      
    
    {\displaystyle X}
   is a Banach space, this is equivalent to say that for every sequence of signs 
  
    
      
        
          ε
          
            n
          
        
        =
        ±
        1
      
    
    {\displaystyle \varepsilon _{n}=\pm 1}
  , the series

converges in 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
  


==== Series in topological vector spaces ====
If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a topological vector space (TVS) and 
  
    
      
        
          
            (
            
              x
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \left(x_{i}\right)_{i\in I}}
   is a (possibly uncountable) family in 
  
    
      
        X
      
    
    {\displaystyle X}
   then this family is summable if the limit 
  
    
      
        
          lim
          
            A
            ∈
            Finite
            ⁡
            (
            I
            )
          
        
        
          x
          
            A
          
        
      
    
    {\displaystyle \lim _{A\in \operatorname {Finite} (I)}x_{A}}
   of the net 
  
    
      
        
          
            (
            
              x
              
                A
              
            
            )
          
          
            A
            ∈
            Finite
            ⁡
            (
            I
            )
          
        
      
    
    {\displaystyle \left(x_{A}\right)_{A\in \operatorname {Finite} (I)}}
   exists in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   where 
  
    
      
        Finite
        ⁡
        (
        I
        )
      
    
    {\displaystyle \operatorname {Finite} (I)}
   is the directed set of all finite subsets of 
  
    
      
        I
      
    
    {\displaystyle I}
   directed by inclusion 
  
    
      
        
        ⊆
        
      
    
    {\displaystyle \,\subseteq \,}
   and 
  
    
      
        
          x
          
            A
          
        
        :=
        
          ∑
          
            i
            ∈
            A
          
        
        
          x
          
            i
          
        
        .
      
    
    {\textstyle x_{A}:=\sum _{i\in A}x_{i}.}
  
It is called absolutely summable if in addition, for every continuous seminorm 
  
    
      
        p
      
    
    {\displaystyle p}
   on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   the family 
  
    
      
        
          
            (
            
              p
              
                (
                
                  x
                  
                    i
                  
                
                )
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \left(p\left(x_{i}\right)\right)_{i\in I}}
   is summable.
If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a normable space and if 
  
    
      
        
          
            (
            
              x
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \left(x_{i}\right)_{i\in I}}
   is an absolutely summable family in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then necessarily all but a countable collection of 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  ’s are zero. Hence, in normed spaces, it is usually only ever necessary to consider series with countably many terms.
Summable families play an important role in the theory of nuclear spaces.


===== Series in Banach and seminormed spaces =====
The notion of series can be easily extended to the case of a seminormed space. 
If 
  
    
      
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{n}}
   is a sequence of elements of a normed space 
  
    
      
        X
      
    
    {\displaystyle X}
   and if 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   then the series 
  
    
      
        ∑
        
          x
          
            n
          
        
      
    
    {\displaystyle \sum x_{n}}
   converges to 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   if the sequence of partial sums of the series 
  
    
      
        
          
            (
            
              
                ∑
                
                  n
                  =
                  0
                
                
                  N
                
              
              
                x
                
                  n
                
              
            
            )
          
          
            N
            =
            1
          
          
            ∞
          
        
      
    
    {\textstyle \left(\sum _{n=0}^{N}x_{n}\right)_{N=1}^{\infty }}
   converges to 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
  ; to wit,

More generally, convergence of series can be defined in any abelian Hausdorff topological group. 
Specifically, in this case, 
  
    
      
        ∑
        
          x
          
            n
          
        
      
    
    {\displaystyle \sum x_{n}}
   converges to 
  
    
      
        x
      
    
    {\displaystyle x}
   if the sequence of partial sums converges to 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
  
If 
  
    
      
        (
        X
        ,
        
          |
        
        ⋅
        
          |
        
        )
      
    
    {\displaystyle (X,|\cdot |)}
   is a seminormed space, then the notion of absolute convergence becomes: 
A series 
  
    
      
        
          ∑
          
            i
            ∈
            I
          
        
        
          x
          
            i
          
        
      
    
    {\textstyle \sum _{i\in I}x_{i}}
   of vectors in 
  
    
      
        X
      
    
    {\displaystyle X}
   converges absolutely if

in which case all but at most countably many of the values 
  
    
      
        
          |
          
            x
            
              i
            
          
          |
        
      
    
    {\displaystyle \left|x_{i}\right|}
   are necessarily zero.
If a countable series of vectors in a Banach space converges absolutely then it converges unconditionally, but the converse only holds in finite-dimensional Banach spaces (theorem of Dvoretzky & Rogers (1950)).


==== Well-ordered sums ====
Conditionally convergent series can be considered if 
  
    
      
        I
      
    
    {\displaystyle I}
   is a well-ordered set, for example, an ordinal number 
  
    
      
        
          α
          
            0
          
        
        .
      
    
    {\displaystyle \alpha _{0}.}
  
In this case, define by transfinite recursion:

and for a limit ordinal 
  
    
      
        α
        ,
      
    
    {\displaystyle \alpha ,}
  

if this limit exists.  If all limits exist up to 
  
    
      
        
          α
          
            0
          
        
        ,
      
    
    {\displaystyle \alpha _{0},}
   then the series converges.


==== Examples ====
Given a function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   into an abelian topological group 
  
    
      
        Y
        ,
      
    
    {\displaystyle Y,}
   define for every 
  
    
      
        a
        ∈
        X
        ,
      
    
    {\displaystyle a\in X,}
    a function whose support is a singleton 
  
    
      
        {
        a
        }
        .
      
    
    {\displaystyle \{a\}.}
    Then  in the topology of pointwise convergence (that is, the sum is taken in the infinite product group 
  
    
      
        
          Y
          
            X
          
        
      
    
    {\displaystyle Y^{X}}
  ).
In the definition of partitions of unity, one constructs sums of functions over arbitrary index set 
  
    
      
        I
        ,
      
    
    {\displaystyle I,}
    While, formally, this requires a notion of sums of uncountable series, by construction there are, for every given 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   only finitely many nonzero terms in the sum, so issues regarding convergence of such sums do not arise.  Actually, one usually assumes more: the family of functions is locally finite, that is, for every 
  
    
      
        x
      
    
    {\displaystyle x}
   there is a neighborhood of 
  
    
      
        x
      
    
    {\displaystyle x}
   in which all but a finite number of functions vanish.  Any regularity property of the 
  
    
      
        
          φ
          
            i
          
        
        ,
      
    
    {\displaystyle \varphi _{i},}
   such as continuity, differentiability, that is preserved under finite sums will be preserved for the sum of any subcollection of this family of functions.
On the first uncountable ordinal 
  
    
      
        
          ω
          
            1
          
        
      
    
    {\displaystyle \omega _{1}}
   viewed as a topological space in the order topology, the constant function 
  
    
      
        f
        :
        
          [
          
            0
            ,
            
              ω
              
                1
              
            
          
          )
        
        →
        
          [
          
            0
            ,
            
              ω
              
                1
              
            
          
          ]
        
      
    
    {\displaystyle f:\left[0,\omega _{1}\right)\to \left[0,\omega _{1}\right]}
   given by 
  
    
      
        f
        (
        α
        )
        =
        1
      
    
    {\displaystyle f(\alpha )=1}
   satisfies  (in other words, 
  
    
      
        
          ω
          
            1
          
        
      
    
    {\displaystyle \omega _{1}}
   copies of 1 is 
  
    
      
        
          ω
          
            1
          
        
      
    
    {\displaystyle \omega _{1}}
  ) only if one takes a limit over all countable partial sums, rather than finite partial sums.  This space is not separable.


== See also ==
Continued fraction
Convergence tests
Convergent series
Divergent series
Infinite compositions of analytic functions
Infinite expression
Infinite product
Iterated binary operation
List of mathematical series
Prefix sum
Sequence transformation
Series expansion


== References ==


== Bibliography ==


== External links ==

""Series"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Infinite Series Tutorial
""Series-TheBasics"". Paul's Online Math Notes.
""Show-Me Collection of Series"" (PDF). Leslie Green."
454a0b4885,Philosophy of mathematics,"The philosophy of mathematics is the branch of philosophy that studies the assumptions, foundations, and implications of mathematics. It aims to understand the nature and methods of mathematics, and find out the place of mathematics in people's lives. The logical and structural nature of mathematics makes this branch of philosophy broad and unique. 
The philosophy of mathematics has two major themes: mathematical realism and mathematical anti-realism.


== History ==

The origin of mathematics is of arguments and disagreements. Whether the birth of mathematics was by chance or induced by necessity during the development of similar subjects, such as physics, remains an area of contention.Many thinkers have contributed their ideas concerning the nature of mathematics. Today, some philosophers of mathematics aim to give accounts of this form of inquiry and its products as they stand, while others emphasize a role for themselves that goes beyond simple interpretation to critical analysis. There are traditions of mathematical philosophy in both Western philosophy and Eastern philosophy. Western philosophies of mathematics go as far back as Pythagoras, who described the theory ""everything is mathematics"" (mathematicism), Plato, who paraphrased Pythagoras, and studied the ontological status of mathematical objects, and Aristotle, who studied logic and issues related to infinity (actual versus potential).
Greek philosophy on mathematics was strongly influenced by their study of geometry. For example, at one time, the Greeks held the opinion that 1 (one) was not a number, but rather a unit of arbitrary length. A number was defined as a multitude. Therefore, 3, for example, represented a certain multitude of units, and was thus ""truly"" a number. At another point, a similar argument was made that 2 was not a number but a fundamental notion of a pair. These views come from the heavily geometric straight-edge-and-compass viewpoint of the Greeks: just as lines drawn in a geometric problem are measured in proportion to the first arbitrarily drawn line, so too are the numbers on a number line measured in proportion to the arbitrary first ""number"" or ""one"".These earlier Greek ideas of numbers were later upended by the discovery of the irrationality of the square root of two. Hippasus, a disciple of Pythagoras, showed that the diagonal of a unit square was incommensurable with its (unit-length) edge: in other words he proved there was no existing (rational) number that accurately depicts the proportion of the diagonal of the unit square to its edge. This caused a significant re-evaluation of Greek philosophy of mathematics. According to legend, fellow Pythagoreans were so traumatized by this discovery that they murdered Hippasus to stop him from spreading his heretical idea. Simon Stevin was one of the first in Europe to challenge Greek ideas in the 16th century. Beginning with Leibniz, the focus shifted strongly to the relationship between mathematics and logic. This perspective dominated the philosophy of mathematics through the time of Frege and of Russell, but was brought into question by developments in the late 19th and early 20th centuries.


=== Contemporary philosophy ===
A perennial issue in the philosophy of mathematics concerns the relationship between logic and mathematics at their joint foundations. While 20th-century philosophers continued to ask the questions mentioned at  the outset of this article, the philosophy of mathematics in the 20th century was characterized by a predominant interest in formal logic, set theory (both naive set theory and axiomatic set theory), and foundational issues.
It is a profound puzzle that on the one hand mathematical truths seem to have a compelling inevitability, but on the other hand the source of their ""truthfulness"" remains elusive. Investigations into this issue are known as the foundations of mathematics program.
At the start of the 20th century, philosophers of mathematics were already beginning to divide into various schools of thought about all these questions, broadly distinguished by their pictures of mathematical epistemology and ontology. Three schools, formalism, intuitionism, and logicism, emerged at this time, partly in response to the increasingly widespread worry that mathematics as it stood, and analysis in particular, did not live up to the standards of certainty and rigor that had been taken for granted. Each school addressed the issues that came to the fore at that time, either attempting to resolve them or claiming that mathematics is not entitled to its status as our most trusted knowledge.
Surprising and counter-intuitive developments in formal logic and set theory early in the 20th century led to new questions concerning what was traditionally called the foundations of mathematics. As the century unfolded, the initial focus of concern expanded to an open exploration of the fundamental axioms of mathematics, the axiomatic approach having been taken for granted since the time of Euclid around 300 BCE as the natural basis for mathematics. Notions of axiom, proposition and proof, as well as the notion of a proposition being true of a mathematical object (see Assignment), were formalized, allowing them to be treated mathematically. The Zermelo–Fraenkel axioms for set theory were formulated which provided a conceptual framework in which much mathematical discourse would be interpreted. In mathematics, as in physics, new and unexpected ideas had arisen and significant changes were coming. With Gödel numbering, propositions could be interpreted as referring to themselves or other propositions, enabling inquiry into the consistency of mathematical theories. This reflective critique in which the theory under review ""becomes itself the object of a mathematical study"" led Hilbert to call such study metamathematics or proof theory.At the middle of the century, a new mathematical theory was created by Samuel Eilenberg and Saunders Mac Lane, known as category theory, and it became a new contender for the natural language of mathematical thinking. As the 20th century progressed, however, philosophical opinions diverged as to just how well-founded were the questions about foundations that were raised at the century's beginning. Hilary Putnam summed up one common view of the situation in the last third of the century by saying:

When philosophy discovers something wrong with science, sometimes science has to be changed—Russell's paradox comes to mind, as does Berkeley's attack on the actual infinitesimal—but more often it is philosophy that has to be changed. I do not think that the difficulties that philosophy finds with classical mathematics today are genuine difficulties; and I think that the philosophical interpretations of mathematics that we are being offered on every hand are wrong, and that ""philosophical interpretation"" is just what mathematics doesn't need.: 169–170 

Philosophy of mathematics today proceeds along several different lines of inquiry, by philosophers of mathematics, logicians, and mathematicians, and there are many schools of thought on the subject. The schools are addressed separately in the next section, and their assumptions explained.


== Major themes ==


=== Mathematical realism ===
Mathematical realism, like realism in general, holds that mathematical entities exist independently of the human mind. Thus, humans do not invent mathematics, but rather discover it, and any other intelligent beings in the universe would presumably do the same. In this point of view, there is really one sort of mathematics that can be discovered; triangles, for example, are real entities, not the creations of the human mind.
Many working mathematicians have been mathematical realists; they see themselves as discoverers of naturally occurring objects. Examples include Paul Erdős and Kurt Gödel. Gödel believed in an objective mathematical reality that could be perceived in a manner analogous to sense perception. Certain principles (e.g., for any two objects, there is a collection of objects consisting of precisely those two objects) could be directly seen to be true, but the continuum hypothesis conjecture might prove undecidable just on the basis of such principles. Gödel suggested that quasi-empirical methodology could be used to provide sufficient evidence to be able to reasonably assume such a conjecture.
Within realism, there are distinctions depending on what sort of existence one takes mathematical entities to have, and how we know about them. Major forms of mathematical realism include Platonism and Aristotelianism.


=== Mathematical anti-realism ===

Mathematical anti-realism generally holds that mathematical statements have truth-values, but that they do not do so by corresponding to a special realm of immaterial or non-empirical entities. Major forms of mathematical anti-realism include formalism and fictionalism.


== Contemporary schools of thought ==


=== Artistic ===
The view that claims that mathematics is the aesthetic combination of assumptions, and then also claims that mathematics is an art. A famous mathematician who claims that is the British G. H. Hardy. For Hardy, in his book, A Mathematician's Apology, the definition of mathematics was more like the aesthetic combination of concepts.


=== Platonism ===

Mathematical Platonism is the form of realism that suggests that mathematical entities are abstract, have no spatiotemporal or causal properties, and are eternal and unchanging. This is often claimed to be the view most people have of numbers. The term Platonism is used because such a view is seen to parallel Plato's Theory of Forms and a ""World of Ideas"" (Greek: eidos (εἶδος)) described in Plato's allegory of the cave: the everyday world can only imperfectly approximate an unchanging, ultimate reality. Both Plato's cave and Platonism have meaningful, not just superficial connections, because Plato's ideas were preceded and probably influenced by the hugely popular Pythagoreans of ancient Greece, who believed that the world was, quite literally, generated by numbers.
A major question considered in mathematical Platonism is: Precisely where and how do the mathematical entities exist, and how do we know about them? Is there a world, completely separate from our physical one, that is occupied by the mathematical entities? How can we gain access to this separate world and discover truths about the entities? One proposed answer is the Ultimate Ensemble, a theory that postulates that all structures that exist mathematically also exist physically in their own universe.

Kurt Gödel's Platonism postulates a special kind of mathematical intuition that lets us perceive mathematical objects directly. (This view bears resemblances to many things Husserl said about mathematics, and supports Kant's idea that mathematics is synthetic a priori.) Davis and Hersh have suggested in their 1999 book The Mathematical Experience that most mathematicians act as though they are Platonists, even though, if pressed to defend the position carefully, they may retreat to formalism.
Full-blooded Platonism is a modern variation of Platonism, which is in reaction to the fact that different sets of mathematical entities can be proven to exist depending on the axioms and inference rules employed (for instance, the law of the excluded middle, and the axiom of choice). It holds that all mathematical entities exist. They may be provable, even if they cannot all be derived from a single consistent set of axioms.Set-theoretic realism (also set-theoretic Platonism) a position defended by Penelope Maddy, is the view that set theory is about a single universe of sets. This position (which is also known as naturalized Platonism because it is a naturalized version of mathematical Platonism) has been criticized by Mark Balaguer on the basis of Paul Benacerraf's epistemological problem.  A similar view, termed Platonized naturalism, was later defended by the Stanford–Edmonton School: according to this view, a more traditional kind of Platonism is consistent with naturalism; the more traditional kind of Platonism they defend is distinguished by general principles that assert the existence of abstract objects.


=== Mathematicism ===

Max Tegmark's mathematical universe hypothesis (or mathematicism) goes further than Platonism in asserting that not only do all mathematical objects exist, but nothing else does. Tegmark's sole postulate is: All structures that exist mathematically also exist physically. That is, in the sense that ""in those [worlds] complex enough to contain self-aware substructures [they] will subjectively perceive themselves as existing in a physically 'real' world"".


=== Logicism ===

Logicism is the thesis that mathematics is reducible to logic, and hence nothing but a part of logic.: 41  Logicists hold that mathematics can be known a priori, but suggest that our knowledge of mathematics is just part of our knowledge of logic in general, and is thus analytic, not requiring any special faculty of mathematical intuition. In this view, logic is the proper foundation of mathematics, and all mathematical statements are necessary logical truths.
Rudolf Carnap (1931) presents the logicist thesis in two parts:
The concepts of mathematics can be derived from logical concepts through explicit definitions.
The theorems of mathematics can be derived from logical axioms through purely logical deduction.Gottlob Frege was the founder of logicism. In his seminal Die Grundgesetze der Arithmetik (Basic Laws of Arithmetic) he built up arithmetic from a system of logic with a general principle of comprehension, which he called ""Basic Law V"" (for concepts F and G, the extension of F equals the extension of G if and only if for all objects a, Fa equals Ga), a principle that he took to be acceptable as part of logic.

Frege's construction was flawed. Bertrand Russell discovered that Basic Law V is inconsistent (this is Russell's paradox). Frege abandoned his logicist program soon after this, but it was continued by Russell and Whitehead. They attributed the paradox to ""vicious circularity"" and built up what they called ramified type theory to deal with it. In this system, they were eventually able to build up much of modern mathematics but in an altered, and excessively complex form (for example, there were different natural numbers in each type, and there were infinitely many types). They also had to make several compromises in order to develop much of mathematics, such as the ""axiom of reducibility"". Even Russell said that this axiom did not really belong to logic.
Modern logicists (like Bob Hale, Crispin Wright, and perhaps others) have returned to a program closer to Frege's. They have abandoned Basic Law V in favor of abstraction principles such as Hume's principle (the number of objects falling under the concept F equals the number of objects falling under the concept G if and only if the extension of F and the extension of G can be put into one-to-one correspondence). Frege required Basic Law V to be able to give an explicit definition of the numbers, but all the properties of numbers can be derived from Hume's principle. This would not have been enough for Frege because (to paraphrase him) it does not exclude the possibility that the number 3 is in fact Julius Caesar. In addition, many of the weakened principles that they have had to adopt to replace Basic Law V no longer seem so obviously analytic, and thus purely logical.


=== Formalism ===

Formalism holds that mathematical statements may be thought of as statements about the consequences of certain string manipulation rules. For example, in the ""game"" of Euclidean geometry (which is seen as consisting of some strings called ""axioms"", and some ""rules of inference"" to generate new strings from given ones), one can prove that the Pythagorean theorem holds (that is, one can generate the string corresponding to the Pythagorean theorem). According to formalism, mathematical truths are not about numbers and sets and triangles and the like—in fact, they are not ""about"" anything at all.
Another version of formalism is often known as deductivism. In deductivism, the Pythagorean theorem is not an absolute truth, but a relative one: if one assigns meaning to the strings in such a way that the rules of the game become true (i.e., true statements are assigned to the axioms and the rules of inference are truth-preserving), then one must accept the theorem, or, rather, the interpretation one has given it must be a true statement. The same is held to be true for all other mathematical statements. Thus, formalism need not mean that mathematics is nothing more than a meaningless symbolic game. It is usually hoped that there exists some interpretation in which the rules of the game hold. (Compare this position to structuralism.) But it does allow the working mathematician to continue in his or her work and leave such problems to the philosopher or scientist. Many formalists would say that in practice, the axiom systems to be studied will be suggested by the demands of science or other areas of mathematics.

A major early proponent of formalism was David Hilbert, whose program was intended to be a complete and consistent axiomatization of all of mathematics. Hilbert aimed to show the consistency of mathematical systems from the assumption that the ""finitary arithmetic"" (a subsystem of the usual arithmetic of the positive integers, chosen to be philosophically uncontroversial) was consistent. Hilbert's goals of creating a system of mathematics that is both complete and consistent were seriously undermined by the second of Gödel's incompleteness theorems, which states that sufficiently expressive consistent axiom systems can never prove their own consistency. Since any such axiom system would contain the finitary arithmetic as a subsystem, Gödel's theorem implied that it would be impossible to prove the system's consistency relative to that (since it would then prove its own consistency, which Gödel had shown was impossible). Thus, in order to show that any axiomatic system of mathematics is in fact consistent, one needs to first assume the consistency of a system of mathematics that is in a sense stronger than the system to be proven consistent.
Hilbert was initially a deductivist, but, as may be clear from above, he considered certain metamathematical methods to yield intrinsically meaningful results and was a realist with respect to the finitary arithmetic. Later, he held the opinion that there was no other meaningful mathematics whatsoever, regardless of interpretation.
Other formalists, such as Rudolf Carnap, Alfred Tarski, and Haskell Curry, considered mathematics to be the investigation of formal axiom systems. Mathematical logicians study formal systems but are just as often realists as they are formalists.
Formalists are relatively tolerant and inviting to new approaches to logic, non-standard number systems, new set theories etc. The more games we study, the better. However, in all three of these examples, motivation is drawn from existing mathematical or philosophical concerns. The ""games"" are usually not arbitrary.
The main critique of formalism is that the actual mathematical ideas that occupy mathematicians are far removed from the string manipulation games mentioned above. Formalism is thus silent on the question of which axiom systems ought to be studied, as none is more meaningful than another from a formalistic point of view.
Recently, some formalist mathematicians have proposed that all of our formal mathematical knowledge should be systematically encoded in computer-readable formats, so as to facilitate automated proof checking of mathematical proofs and the use of interactive theorem proving in the development of mathematical theories and computer software. Because of their close connection with computer science, this idea is also advocated by mathematical intuitionists and constructivists in the ""computability"" tradition—see QED project for a general overview.


=== Conventionalism ===

The French mathematician Henri Poincaré was among the first to articulate a conventionalist view. Poincaré's use of non-Euclidean geometries in his work on differential equations convinced him that Euclidean geometry should not be regarded as a priori truth. He held that axioms in geometry should be chosen for the results they produce, not for their apparent coherence with human intuitions about the physical world.


=== Intuitionism ===

In mathematics, intuitionism is a program of methodological reform whose motto is that ""there are no non-experienced mathematical truths"" (L. E. J. Brouwer). From this springboard, intuitionists seek to reconstruct what they consider to be the corrigible portion of mathematics in accordance with Kantian concepts of being, becoming, intuition, and knowledge. Brouwer, the founder of the movement, held that mathematical objects arise from the a priori forms of the volitions that inform the perception of empirical objects.A major force behind intuitionism was L. E. J. Brouwer, who rejected the usefulness of formalized logic of any sort for mathematics. His student Arend Heyting postulated an intuitionistic logic, different from the classical Aristotelian logic; this logic does not contain the law of the excluded middle and therefore frowns upon proofs by contradiction. The axiom of choice is also rejected in most intuitionistic set theories, though in some versions it is accepted.
In intuitionism, the term ""explicit construction"" is not cleanly defined, and that has led to criticisms. Attempts have been made to use the concepts of Turing machine or computable function to fill this gap, leading to the claim that only questions regarding the behavior of finite algorithms are meaningful and should be investigated in mathematics. This has led to the study of the computable numbers, first introduced by Alan Turing. Not surprisingly, then, this approach to mathematics is sometimes associated with theoretical computer science.


==== Constructivism ====

Like intuitionism, constructivism involves the regulative principle that only mathematical entities which can be explicitly constructed in a certain sense should be admitted to mathematical discourse. In this view, mathematics is an exercise of the human intuition, not a game played with meaningless symbols. Instead, it is about entities that we can create directly through mental activity. In addition, some adherents of these schools reject non-constructive proofs, such as using proof by contradiction when showing the existence of an object or when trying to establish the truth of some proposition. Important work was done by Errett Bishop, who managed to prove versions of the most important theorems in real analysis as constructive analysis in his 1967 Foundations of Constructive Analysis.


==== Finitism ====

Finitism is an extreme form of constructivism, according to which a mathematical object does not exist unless it can be constructed from natural numbers in a finite number of steps. In her book Philosophy of Set Theory, Mary Tiles characterized those who allow countably infinite objects as classical finitists, and those who deny even countably infinite objects as strict finitists.
The most famous proponent of finitism was Leopold Kronecker, who said:

God created the natural numbers, all else is the work of man.
Ultrafinitism is an even more extreme version of finitism, which rejects not only infinities but finite quantities that cannot feasibly be constructed with available resources. Another variant of finitism is Euclidean arithmetic, a system developed by John Penn Mayberry in his book The Foundations of Mathematics in the Theory of Sets. Mayberry's system is Aristotelian in general inspiration and, despite his strong rejection of any role for operationalism or feasibility in the foundations of mathematics, comes to somewhat similar conclusions, such as, for instance, that super-exponentiation is not a legitimate finitary function.


=== Structuralism ===

Structuralism is a position holding that mathematical theories describe structures, and that mathematical objects are exhaustively defined by their places in such structures, consequently having no intrinsic properties. For instance, it would maintain that all that needs to be known about the number 1 is that it is the first whole number after 0. Likewise all the other whole numbers are defined by their places in a structure, the number line. Other examples of mathematical objects might include lines and planes in geometry, or elements and operations in abstract algebra.
Structuralism is an epistemologically realistic view in that it holds that mathematical statements have an objective truth value. However, its central claim only relates to what kind of entity a mathematical object is, not to what kind of existence mathematical objects or structures have (not, in other words, to their ontology). The kind of existence mathematical objects have would clearly be dependent on that of the structures in which they are embedded; different sub-varieties of structuralism make different ontological claims in this regard.The ante rem structuralism (""before the thing"") has a similar ontology to Platonism.  Structures are held to have a real but abstract and immaterial existence. As such, it faces the standard epistemological problem of explaining the interaction between such abstract structures and flesh-and-blood mathematicians (see Benacerraf's identification problem).
The in re structuralism (""in the thing"") is the equivalent of Aristotelian realism. Structures are held to exist inasmuch as some concrete system exemplifies them. This incurs the usual issues that some perfectly legitimate structures might accidentally happen not to exist, and that a finite physical world might not be ""big"" enough to accommodate some otherwise legitimate structures.
The post rem structuralism (""after the thing"") is anti-realist about structures in a way that parallels nominalism. Like nominalism, the post rem approach denies the existence of abstract mathematical objects with properties other than their place in a relational structure.  According to this view mathematical systems exist, and have structural features in common. If something is true of a structure, it will be true of all systems exemplifying the structure. However, it is merely instrumental to talk of structures being ""held in common"" between systems: they in fact have no independent existence.


=== Embodied mind theories ===
Embodied mind theories hold that mathematical thought is a natural outgrowth of the human cognitive apparatus which finds itself in our physical universe. For example, the abstract concept of number springs from the experience of counting discrete objects (requiring the human senses such as sight for detecting the objects, touch; and signalling from the brain). It is held that mathematics is not universal and does not exist in any real sense, other than in human brains. Humans construct, but do not discover, mathematics.
The cognitive processes of pattern-finding and distinguishing objects are also subject to neuroscience; if mathematics is considered to be relevant to a natural world (such as from realism or a degree of it, as opposed to pure solipsism).
Its actual relevance to reality, while accepted to be a trustworthy approximation (it is also suggested the evolution of perceptions, the body, and the senses may have been necessary for survival) is not necessarily accurate to a full realism (and is still subject to flaws such as illusion, assumptions (consequently; the foundations and axioms in which mathematics have been formed by humans), generalisations, deception, and hallucinations). As such, this may also raise questions for the modern scientific method for its compatibility with general mathematics; as while relatively reliable, it is still limited by what can be measured by empiricism which may not be as reliable as previously assumed (see also: 'counterintuitive' concepts in such as quantum nonlocality, and action at a distance).
Another issue is that one numeral system may not necessarily be applicable to problem solving. Subjects such as complex numbers or imaginary numbers require specific changes to more commonly used axioms of mathematics; otherwise they cannot be adequately understood.
Alternatively, computer programmers may use hexadecimal for its 'human-friendly' representation of binary-coded values, rather than decimal (convenient for counting because humans have ten fingers). The axioms or logical rules behind mathematics also vary through time (such as the adaption and invention of zero).
As perceptions from the human brain are subject to illusions, assumptions, deceptions, (induced) hallucinations, cognitive errors or assumptions in a general context, it can be questioned whether they are accurate or strictly indicative of truth (see also: philosophy of being), and the nature of empiricism itself in relation to the universe and whether it is independent to the senses and the universe. 
The human mind has no special claim on reality or approaches to it built out of math. If such constructs as Euler's identity are true then they are true as a map of the human mind and cognition.
Embodied mind theorists thus explain the effectiveness of mathematics—mathematics was constructed by the brain in order to be effective in this universe.
The most accessible, famous, and infamous treatment of this perspective is Where Mathematics Comes From, by George Lakoff and Rafael E. Núñez. In addition, mathematician Keith Devlin has investigated similar concepts with his book The Math Instinct, as has neuroscientist Stanislas Dehaene with his book The Number Sense. For more on the philosophical ideas that inspired this perspective, see cognitive science of mathematics.


==== Aristotelian realism ====

Aristotelian realism holds that mathematics studies properties such as symmetry, continuity and order that can be literally realized in the physical world (or in any other world there might be). It contrasts with Platonism in holding that the objects of mathematics, such as numbers, do not exist in an ""abstract"" world but can be physically realized. For example, the number 4 is realized in the relation between a heap of parrots and the universal ""being a parrot"" that divides the heap into so many parrots. Aristotelian realism is defended by James Franklin and the Sydney School in the philosophy of mathematics and is close to the view of Penelope Maddy that when an egg carton is opened, a set of three eggs is perceived (that is, a mathematical entity realized in the physical world). A problem for Aristotelian realism is what account to give of higher infinities, which may not be realizable in the physical world.
The Euclidean arithmetic developed by John Penn Mayberry in his book The Foundations of Mathematics in the Theory of Sets also falls into the Aristotelian realist tradition. Mayberry, following Euclid, considers numbers to be simply ""definite multitudes of units"" realized in nature—such as ""the members of the London Symphony Orchestra"" or ""the trees in Birnam wood"". Whether or not there are definite multitudes of units for which Euclid's Common Notion 5 (the whole is greater than the part) fails and which would consequently be reckoned as infinite is for Mayberry essentially a question about Nature and does not entail any transcendental suppositions.


==== Psychologism ====

Psychologism in the philosophy of mathematics is the position that mathematical concepts and/or truths are grounded in, derived from or explained by psychological facts (or laws).
John Stuart Mill seems to have been an advocate of a type of logical psychologism, as were many 19th-century German logicians such as Sigwart and Erdmann as well as a number of psychologists, past and present: for example, Gustave Le Bon. Psychologism was famously criticized by Frege in his The Foundations of Arithmetic, and many of his works and essays, including his review of Husserl's Philosophy of Arithmetic. Edmund Husserl, in the first volume of his Logical Investigations, called ""The Prolegomena of Pure Logic"", criticized psychologism thoroughly and sought to distance himself from it. The ""Prolegomena"" is considered a more concise, fair, and thorough refutation of psychologism than the criticisms made by Frege, and also it is considered today by many as being a memorable refutation for its decisive blow to psychologism. Psychologism was also criticized by Charles Sanders Peirce and Maurice Merleau-Ponty.


==== Empiricism ====

Mathematical empiricism is a form of realism that denies that mathematics can be known a priori at all. It says that we discover mathematical facts by empirical research, just like facts in any of the other sciences. It is not one of the classical three positions advocated in the early 20th century, but primarily arose in the middle of the century. However, an important early proponent of a view like this was John Stuart Mill. Mill's view was widely criticized, because, according to critics, such as A.J. Ayer, it makes statements like ""2 + 2 = 4"" come out as uncertain, contingent truths, which we can only learn by observing instances of two pairs coming together and forming a quartet.
Karl Popper was another philosopher to point out empirical aspects of mathematics, observing that ""most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."" Popper also noted he would ""admit a system as empirical or scientific only if it is capable of being tested by experience.""Contemporary mathematical empiricism, formulated by W. V. O. Quine and Hilary Putnam, is primarily supported by the indispensability argument: mathematics is indispensable to all empirical sciences, and if we want to believe in the reality of the phenomena described by the sciences, we ought also believe in the reality of those entities required for this description. That is, since physics needs to talk about electrons to say why light bulbs behave as they do, then electrons must exist. Since physics needs to talk about numbers in offering any of its explanations, then numbers must exist. In keeping with Quine and Putnam's overall philosophies, this is a naturalistic argument. It argues for the existence of mathematical entities as the best explanation for experience, thus stripping mathematics of being distinct from the other sciences.
Putnam strongly rejected the term ""Platonist"" as implying an over-specific ontology that was not necessary to mathematical practice in any real sense. He advocated a form of ""pure realism"" that rejected mystical notions of truth and accepted much quasi-empiricism in mathematics. This grew from the increasingly popular assertion in the late 20th century that no one foundation of mathematics could be ever proven to exist. It is also sometimes called ""postmodernism in mathematics"" although that term is considered overloaded by some and insulting by others. Quasi-empiricism argues that in doing their research, mathematicians test hypotheses as well as prove theorems. A mathematical argument can transmit falsity from the conclusion to the premises just as well as it can transmit truth from the premises to the conclusion. Putnam has argued that any theory of mathematical realism would include quasi-empirical methods. He proposed that an alien species doing mathematics might well rely on quasi-empirical methods primarily, being willing often to forgo rigorous and axiomatic proofs, and still be doing mathematics—at perhaps a somewhat greater risk of failure of their calculations. He gave a detailed argument for this in New Directions. Quasi-empiricism was also developed by Imre Lakatos.
The most important criticism of empirical views of mathematics is approximately the same as that raised against Mill. If mathematics is just as empirical as the other sciences, then this suggests that its results are just as fallible as theirs, and just as contingent. In Mill's case the empirical justification comes directly, while in Quine's case it comes indirectly, through the coherence of our scientific theory as a whole, i.e. consilience after E.O. Wilson. Quine suggests that mathematics seems completely certain because the role it plays in our web of belief is extraordinarily central, and that it would be extremely difficult for us to revise it, though not impossible.
For a philosophy of mathematics that attempts to overcome some of the shortcomings of Quine and Gödel's approaches by taking aspects of each see Penelope Maddy's Realism in Mathematics. Another example of a realist theory is the embodied mind theory.
For experimental evidence suggesting that human infants can do elementary arithmetic, see Brian Butterworth.


=== Fictionalism ===

Mathematical fictionalism was brought to fame in 1980 when Hartry Field published Science Without Numbers, which rejected and in fact reversed Quine's indispensability argument. Where Quine suggested that mathematics was indispensable for our best scientific theories, and therefore should be accepted as a body of truths talking about independently existing entities, Field suggested that mathematics was dispensable, and therefore should be considered as a body of falsehoods not talking about anything real. He did this by giving a complete axiomatization of Newtonian mechanics with no reference to numbers or functions at all. He started with the ""betweenness"" of Hilbert's axioms to characterize space without coordinatizing it, and then added extra relations between points to do the work formerly done by vector fields. Hilbert's geometry is mathematical, because it talks about abstract points, but in Field's theory, these points are the concrete points of physical space, so no special mathematical objects at all are needed.
Having shown how to do science without using numbers, Field proceeded to rehabilitate mathematics as a kind of useful fiction. He showed that mathematical physics is a conservative extension of his non-mathematical physics (that is, every physical fact provable in mathematical physics is already provable from Field's system), so that mathematics is a reliable process whose physical applications are all true, even though its own statements are false. Thus, when doing mathematics, we can see ourselves as telling a sort of story, talking as if numbers existed. For Field, a statement like ""2 + 2 = 4"" is just as fictitious as ""Sherlock Holmes lived at 221B Baker Street""—but both are true according to the relevant fictions.
Another fictionalist, Mary Leng, expresses the perspective succinctly by dismissing any seeming connection between mathematics and the physical world as ""a happy coincidence"". This rejection separates fictionalism from other forms of anti-realism, which see mathematics itself as artificial but still bounded or fitted to reality in some way.By this account, there are no metaphysical or epistemological problems special to mathematics. The only worries left are the general worries about non-mathematical physics, and about fiction in general. Field's approach has been very influential, but is widely rejected. This is in part because of the requirement of strong fragments of second-order logic to carry out his reduction, and because the statement of conservativity seems to require quantification over abstract models or deductions.


=== Social constructivism ===

Social constructivism sees mathematics primarily as a social construct, as a product of culture, subject to correction and change. Like the other sciences, mathematics is viewed as an empirical endeavor whose results are constantly evaluated and may be discarded. However, while on an empiricist view the evaluation is some sort of comparison with ""reality"", social constructivists emphasize that the direction of mathematical research is dictated by the fashions of the social group performing it or by the needs of the society financing it. However, although such external forces may change the direction of some mathematical research, there are strong internal constraints—the mathematical traditions, methods, problems, meanings and values into which mathematicians are enculturated—that work to conserve the historically-defined discipline.
This runs counter to the traditional beliefs of working mathematicians, that mathematics is somehow pure or objective. But social constructivists argue that mathematics is in fact grounded by much uncertainty: as mathematical practice evolves, the status of previous mathematics is cast into doubt, and is corrected to the degree it is required or desired by the current mathematical community. This can be seen in the development of analysis from reexamination of the calculus of Leibniz and Newton. They argue further that finished mathematics is often accorded too much status, and folk mathematics not enough, due to an overemphasis on axiomatic proof and peer review as practices.
The social nature of mathematics is highlighted in its subcultures. Major discoveries can be made in one branch of mathematics and be relevant to another, yet the relationship goes undiscovered for lack of social contact between mathematicians. Social constructivists argue each speciality forms its own epistemic community and often has great difficulty communicating, or motivating the investigation of unifying conjectures that might relate different areas of mathematics. Social constructivists see the process of ""doing mathematics"" as actually creating the meaning, while social realists see a deficiency either of human capacity to abstractify, or of human's cognitive bias, or of mathematicians' collective intelligence as preventing the comprehension of a real universe of mathematical objects. Social constructivists sometimes reject the search for foundations of mathematics as bound to fail, as pointless or even meaningless.
Contributions to this school have been made by Imre Lakatos and Thomas Tymoczko, although it is not clear that either would endorse the title. More recently Paul Ernest has explicitly formulated a social constructivist philosophy of mathematics. Some consider the work of Paul Erdős as a whole to have advanced this view (although he personally rejected it) because of his uniquely broad collaborations, which prompted others to see and study ""mathematics as a social activity"", e.g., via the Erdős number. Reuben Hersh has also promoted the social view of mathematics, calling it a ""humanistic"" approach, similar to but not quite the same as that associated with Alvin White; one of Hersh's co-authors, Philip J. Davis, has expressed sympathy for the social view as well.


=== Beyond the traditional schools ===


==== Unreasonable effectiveness ====
Rather than focus on narrow debates about the true nature of mathematical truth, or even on practices unique to mathematicians such as the proof, a growing movement from the 1960s to the 1990s began to question the idea of seeking foundations or finding any one right answer to why mathematics works. The starting point for this was Eugene Wigner's famous 1960 paper ""The Unreasonable Effectiveness of Mathematics in the Natural Sciences"", in which he argued that the happy coincidence of mathematics and physics being so well matched seemed to be unreasonable and hard to explain.


==== Popper's two senses of number statements ====
Realist and constructivist theories are normally taken to be contraries. However, Karl Popper argued that a number statement such as ""2 apples + 2 apples = 4 apples"" can be taken in two senses. In one sense it is irrefutable and logically true. In the second sense it is factually true and falsifiable. Another way of putting this is to say that a single number statement can express two propositions: one of which can be explained on constructivist lines; the other on realist lines.


==== Philosophy of language ====
Innovations in the philosophy of language during the 20th century renewed interest in whether mathematics is, as is often said, the language of science. Although some mathematicians and philosophers would accept the statement ""mathematics is a language"" (most consider that the language of mathematics is a part of mathematics to which mathematics cannot be reduced), linguists believe that the implications of such a statement must be considered. For example, the tools of linguistics are not generally applied to the symbol systems of mathematics, that is, mathematics is studied in a markedly different way from other languages. If mathematics is a language, it is a different type of language from natural languages. Indeed, because of the need for clarity and specificity, the language of mathematics is far more constrained than natural languages studied by linguists. However, the methods developed by Frege and Tarski for the study of mathematical language have been extended greatly by Tarski's student Richard Montague and other linguists working in formal semantics to show that the distinction between mathematical language and natural language may not be as great as it seems.
Mohan Ganesalingam has analysed mathematical language using tools from formal linguistics. Ganesalingam notes that some features of natural language are not necessary when analysing mathematical language (such as tense), but many of the same analytical tools can be used (such as context-free grammars). One important difference is that mathematical objects have clearly defined types, which can be explicitly defined in a text: ""Effectively, we are allowed to introduce a word in one part of a sentence, and declare its part of speech in another; and this operation has no analogue in natural language."": 251 


== Arguments ==


=== Indispensability argument for realism ===

This argument, associated with Willard Quine and Hilary Putnam, is considered by Stephen Yablo to be one of the most challenging arguments in favor of the acceptance of the existence of abstract mathematical entities, such as numbers and sets. The form of the argument is as follows.

One must have ontological commitments to all entities that are indispensable to the best scientific theories, and to those entities only (commonly referred to as ""all and only"").
Mathematical entities are indispensable to the best scientific theories. Therefore,
One must have ontological commitments to mathematical entities.The justification for the first premise is the most controversial. Both Putnam and Quine invoke naturalism to justify the exclusion of all non-scientific entities, and hence to defend the ""only"" part of ""all and only"". The assertion that ""all"" entities postulated in scientific theories, including numbers, should be accepted as real is justified by confirmation holism. Since theories are not confirmed in a piecemeal fashion, but as a whole, there is no justification for excluding any of the entities referred to in well-confirmed theories. This puts the nominalist who wishes to exclude the existence of sets and non-Euclidean geometry, but to include the existence of quarks and other undetectable entities of physics, for example, in a difficult position.


=== Epistemic argument against realism ===
The anti-realist ""epistemic argument"" against Platonism has been made by Paul Benacerraf and Hartry Field. Platonism posits that mathematical objects are abstract entities. By general agreement, abstract entities cannot interact causally with concrete, physical entities (""the truth-values of our mathematical assertions depend on facts involving Platonic entities that reside in a realm outside of space-time""). Whilst our knowledge of concrete, physical objects is based on our ability to perceive them, and therefore to causally interact with them, there is no parallel account of how mathematicians come to have knowledge of abstract objects. Another way of making the point is that if the Platonic world were to disappear, it would make no difference to the ability of mathematicians to generate proofs, etc., which is already fully accountable in terms of physical processes in their brains.
Field developed his views into fictionalism. Benacerraf also developed the philosophy of mathematical structuralism, according to which there are no mathematical objects. Nonetheless, some versions of structuralism are compatible with some versions of realism.
The argument hinges on the idea that a satisfactory naturalistic account of thought processes in terms of brain processes can be given for mathematical reasoning along with everything else. One line of defense is to maintain that this is false, so that mathematical reasoning uses some special intuition that involves contact with the Platonic realm. A modern form of this argument is given by Sir Roger Penrose.Another line of defense is to maintain that abstract objects are relevant to mathematical reasoning in a way that is non-causal, and not analogous to perception. This argument is developed by Jerrold Katz in his 2000 book Realistic Rationalism.
A more radical defense is denial of physical reality, i.e. the mathematical universe hypothesis. In that case, a mathematician's knowledge of mathematics is one mathematical object making contact with another.


== Aesthetics ==
Many practicing mathematicians have been drawn to their subject because of a sense of beauty they perceive in it. One sometimes hears the sentiment that mathematicians would like to leave philosophy to the philosophers and get back to mathematics—where, presumably, the beauty lies.
In his work on the divine proportion, H.E. Huntley relates the feeling of reading and understanding someone else's proof of a theorem of mathematics to that of a viewer of a masterpiece of art—the reader of a proof has a similar sense of exhilaration at understanding as the original author of the proof, much as, he argues, the viewer of a masterpiece has a sense of exhilaration similar to the original painter or sculptor. Indeed, one can study mathematical and scientific writings as literature.
Philip J. Davis and Reuben Hersh have commented that the sense of mathematical beauty is universal amongst practicing mathematicians. By way of example, they provide two proofs of the irrationality of √2. The first is the traditional proof by contradiction, ascribed to Euclid; the second is a more direct proof involving the fundamental theorem of arithmetic that, they argue, gets to the heart of the issue. Davis and Hersh argue that mathematicians find the second proof more aesthetically appealing because it gets closer to the nature of the problem.
Paul Erdős was well known for his notion of a hypothetical ""Book"" containing the most elegant or beautiful mathematical proofs. There is not universal agreement that a result has one ""most elegant"" proof; Gregory Chaitin has argued against this idea.
Philosophers have sometimes criticized mathematicians' sense of beauty or elegance as being, at best, vaguely stated. By the same token, however, philosophers of mathematics have sought to characterize what makes one proof more desirable than another when both are logically sound.
Another aspect of aesthetics concerning mathematics is mathematicians' views towards the possible uses of mathematics for purposes deemed unethical or inappropriate. The best-known exposition of this view occurs in G. H. Hardy's book A Mathematician's Apology, in which Hardy argues that pure mathematics is superior in beauty to applied mathematics precisely because it cannot be used for war and similar ends.


== Journals ==
Philosophia Mathematica journal
The Philosophy of Mathematics Education Journal homepage


== See also ==


=== Related works ===


=== Historical topics ===
History and philosophy of science
History of mathematics
History of philosophy


== Notes ==


== Further reading ==
Aristotle, ""Prior Analytics"", Hugh Tredennick (trans.), pp. 181–531 in Aristotle, Volume 1, Loeb Classical Library, William Heinemann, London, UK, 1938.
Benacerraf, Paul; Putnam, Hilary, eds. (1983). Philosophy of Mathematics, Selected Readings (2nd ed.). Cambridge University Press. ISBN 9781107268135.
Berkeley, George (1734), The Analyst; or, a Discourse Addressed to an Infidel Mathematician. Wherein It is examined whether the Object, Principles, and Inferences of the modern Analysis are more distinctly conceived, or more evidently deduced, than Religious Mysteries and Points of Faith, London & Dublin. Online text, David R. Wilkins (ed.), Eprint.
Bourbaki, N. (2013) [1994]. Elements of the History of Mathematics. Translated by Meldrum, John. Springer. ISBN 9783642616938. OCLC 1076247011.
Chandrasekhar, Subrahmanyan (1987). Truth and Beauty. Aesthetics and Motivations in Science. University of Chicago Press. ISBN 9780226100876. OCLC 1023891429.
Colyvan, Mark (2004), ""Indispensability Arguments in the Philosophy of Mathematics"", Stanford Encyclopedia of Philosophy, Edward N. Zalta (ed.), Eprint.
Davis, Philip J.; Hersh, Reuben (1981). The Mathematical Experience. Mariner Books.
Devlin, Keith (2005). The Math Instinct: Why You're a Mathematical Genius (Along with Lobsters, Birds, Cats, and Dogs). Thunder's Mouth Press. ISBN 9781560256724. OCLC 636363534.
Dummett, Michael (1991). Frege, Philosophy of Mathematics. Harvard University Press. ISBN 9780674319356.
Dummett, Michael (1991). Frege and Other Philosophers. Oxford University Press. ISBN 9780191520051.
Dummett, Michael (1993). Origins of Analytical Philosophy. Harvard University Press. ISBN 9780674644724.
Ernest, Paul (1998). Social Constructivism as a Philosophy of Mathematics. State University of New York Press. ISBN 9780791435885.
George, Alexandre, ed. (1994). Mathematics and Mind. Oxford University Press. ISBN 9780195079296.
Hadamard, Jacques (1954). The Psychology of Invention in the Mathematical Field (2nd ed.). Dover. ISBN 9780486201078.
Hardy, G.H. (1992) [1940]. A Mathematician's Apology. Cambridge University Press. ISBN 9780521427067.
Hart, W.D. (1996).  Wilbur Dyre Hart (ed.). The Philosophy of Mathematics. Oxford University Press. ISBN 9780198751199.
Hendricks, Vincent F.; Leitgeb, Hannes, eds. (2006). Philosophy of Mathematics: 5 Questions. Automatic Press. ISBN 9788799101351. / VIP, . キャッシング対策局【審査・在籍確認・増額・おまとめ・借り換え】
Huntley, H.E. (1970). The Divine Proportion: A Study in Mathematical Beauty. Dover. ISBN 9780486222547.
Irvine, A., ed. (2009). The Philosophy of Mathematics. Handbook of the Philosophy of Science. North-Holland Elsevier. ISBN 9780080930589.
Klein, Jacob (2013) [1968]. Greek Mathematical Thought and the Origin of Algebra. Translated by Brann, Eva. Dover. ISBN 9780486319810. OCLC 841505651.
Kline, Morris (2012) [1959]. Mathematics and the Physical World. Dover. ISBN 9780486136318. OCLC 868272162.
Kline, Morris (1990) [1972]. Mathematical Thought from Ancient to Modern Times. Oxford University Press. ISBN 9780195061352.
König, Julius (Gyula) (1905). ""Über die Grundlagen der Mengenlehre und das Kontinuumproblem"". Mathematische Annalen. 61: 156–160. doi:10.1007/BF01457735. S2CID 123696953. Reprinted, ""On the Foundations of Set Theory and the Continuum Problem"", Stefan Bauer-Mengelberg (trans.), pp. 145–149 in Jean van Heijenoort (ed., 1967).
Körner, Stephan (1960). The Philosophy of Mathematics, An Introduction. Harper Books. OCLC 1054045322.
Lakoff, George; Núñez, Rafael E. (2000). Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being. Basic Books. ISBN 9780465037704.
Lakatos, Imre (1976).  Worrall, J.; Zahar, E. (eds.). Proofs and Refutations:The Logic of Mathematical Discovery. Cambridge University Press.
Lakatos, Imre (1978).  Worrall, J.; Currie, G. (eds.). Mathematics, Science and Epistemology: Philosophical Papers. Vol. 2. Cambridge University Press. ISBN 9780521280303.
Lakatos, Imre (1968). Problems in the Philosophy of Mathematics. North Holland. OCLC 254371777.
Leibniz, G.W. (1966).  Parkinson, G.H.R. (ed.). Logical Papers (1666–1690). Oxford University Press. ISBN 9780198243069.
Maddy, Penelope (1997). Naturalism in Mathematics. Oxford University Press. ISBN 9780191518973. OCLC 1200106111.
Maziarz, Edward A.; Greenwood, Thomas (1995). Greek Mathematical Philosophy. Barnes and Noble Books. ISBN 9781566199544.
Mount, Matthew, Classical Greek Mathematical Philosophy,.
Parsons, Charles (2014). Philosophy of Mathematics in the Twentieth Century: Selected Essays. Harvard University Press. ISBN 978-0-674-72806-6.
Peirce, Benjamin (1870), ""Linear Associative Algebra"", § 1.  See Peirce, B. (1881). ""Linear Associative Algebra"". American Journal of Mathematics. 4 (1): 97–229. doi:10.2307/2369153. JSTOR 2369153.
Peirce, C.S., Collected Papers of Charles Sanders Peirce, vols. 1-6, Charles Hartshorne and Paul Weiss (eds.), vols. 7-8, Arthur W. Burks (ed.), Harvard University Press, Cambridge, MA, 1931 – 1935, 1958.  Cited as CP (volume).(paragraph).
Peirce, C.S., various pieces on mathematics and logic, many readable online through links at the Charles Sanders Peirce bibliography, especially under Books authored or edited by Peirce, published in his lifetime and the two sections following it.
Plato, ""The Republic, Volume 1"", Paul Shorey (trans.), pp. 1–535 in Plato, Volume 5, Loeb Classical Library, William Heinemann, London, UK, 1930.
Plato, ""The Republic, Volume 2"", Paul Shorey (trans.), pp. 1–521 in Plato, Volume 6, Loeb Classical Library, William Heinemann, London, UK, 1935.
Resnik, Michael D. (1980). Frege and the Philosophy of Mathematics. Cornell University. ISBN 9780801412936.
Resnik, Michael (1997). Mathematics as a Science of Patterns. Clarendon Press. ISBN 978-0-19-825014-2.
Robinson, Gilbert de B. (1959). The Foundations of Geometry (4th ed.). University of Toronto Press. ISBN 9780802011039.
Raymond, Eric S. (1993). ""The Utility of Mathematics"".
Smullyan, Raymond M. (1993). Recursion Theory for Metamathematics. Oxford University Press. ISBN 9780195082326.
Russell, Bertrand (1993) [1919]. Introduction to Mathematical Philosophy. Routledge. ISBN 9780486277240. OCLC 1097317975.
Shapiro, Stewart (2000). Thinking About Mathematics: The Philosophy of Mathematics. Oxford University Press. ISBN 9780192893062.
Strohmeier, John; Westbrook, Peter (1999). Divine Harmony, The Life and Teachings of Pythagoras. Berkeley Hills Books. ISBN 9780985424114.
Styazhkin, N.I. (1975) [1969]. History of Mathematical Logic from Leibniz to Peano. MIT Press. ISBN 9780262690492.
Tait, William W. (1986). ""Truth and Proof: The Platonism of Mathematics"". Synthese. 69 (3): 341–370. doi:10.1007/BF00413978. JSTOR 20116347. S2CID 10240391.  Reprinted in Hart 1996, pp. 142–167
Tarski, A. (1983). Logic, Semantics, Metamathematics: Papers from 1923 to 1938 (2nd ed.). Hackett. ISBN 0-915144-76-X.
Ulam, S.M. (2022) [1990].  Bednarek, A.R.; Ulam, Françoise (eds.). Analogies Between Analogies: The Mathematical Reports of S.M. Ulam and His Los Alamos Collaborators. University of California Press. ISBN 9780520302303.
van Heijenoort, Jean, ed. (2002) [1967]. From Frege To Gödel: A Source Book in Mathematical Logic, 1879–1931. Harvard University Press. ISBN 9780674324497.
Wigner, E. P. (1960). ""The unreasonable effectiveness of mathematics in the natural sciences. Richard Courant lecture in mathematical sciences delivered at New York University, May 11, 1959"". Communications on Pure and Applied Mathematics. 13 (1): 1–14. Bibcode:1960CPAM...13....1W. doi:10.1002/cpa.3160130102. S2CID 6112252.
Wilder, Raymond L. (1980). Mathematics as a Cultural System. Pergamon. ISBN 9780080257969.
Witzany, Guenther (2011). ""Can mathematics explain the evolution of human language?"". Communicative and Integrative Biology. 4 (5): 516–520. CiteSeerX 10.1.1.1043.1595. doi:10.4161/cib.16426. PMC 3204117. PMID 22046452.


== External links ==

Philosophy of mathematics at PhilPapers
Philosophy of mathematics at the Indiana Philosophy Ontology Project
Horsten, Leon. ""Philosophy of Mathematics"".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
""Philosophy of mathematics"". Internet Encyclopedia of Philosophy.
Mathematical Structuralism, Internet Encyclopaedia of Philosophy
Abstractionism, Internet Encyclopaedia of Philosophy
""Ludwig Wittgenstein: Later Philosophy of Mathematics"". Internet Encyclopedia of Philosophy.
The London Philosophy Study Guide Archived 2009-09-23 at the Wayback Machine offers many suggestions on what to read, depending on the student's familiarity with the subject:
Philosophy of Mathematics Archived 2009-06-20 at the Wayback Machine
Mathematical Logic Archived 2009-01-25 at the Wayback Machine
Set Theory & Further Logic Archived 2009-02-27 at the Wayback Machine
R.B. Jones' philosophy of mathematics page
Philosophy of mathematics at Curlie
Corfield, David. ""The Philosophy of Real Mathematics – Blog"".
Peirce, C.S. (1998). ""22. New Elements (Καινα Στοιχεία)"".  In Peirce Edition Project (ed.). The Essential Peirce, Selected Philosophical Writings. Vol. 2 (1893–1913). Indiana University Press. pp. 300–324. ISBN 9780253007810."
6affed7363,Moment (mathematics),"In mathematics, the moments of a function are certain quantitative measures related to the shape of the function's graph. If the function represents mass density, then the zeroth moment is the total mass, the first moment (normalized by total mass) is the center of mass, and the second moment is the moment of inertia. If the function is a probability distribution, then the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis. The mathematical concept is closely related to the concept of moment in physics.
For a distribution of mass or probability on a bounded interval, the collection of all the moments (of all orders, from 0 to ∞) uniquely determines the distribution (Hausdorff moment problem).  The same is not true on unbounded intervals (Hamburger moment problem).
In the mid-nineteenth century, Pafnuty Chebyshev became the first person to think systematically in terms of the moments of random variables.


== Significance of the moments ==
The n-th raw moment (i.e., moment about zero) of a distribution is defined bywhereThe n-th moment of a real-valued continuous function f(x) of a real variable about a value c is the integralIt is possible to define moments for random variables in a more general fashion than moments for real-valued functions — see moments in metric spaces. The moment of a function, without further explanation, usually refers to the above expression with c = 0.
For the second and higher moments, the central moment (moments about the mean, with c being the mean) are usually used rather than the moments about zero, because they provide clearer information about the distribution's shape.
Other moments may also be defined. For example, the nth inverse moment about zero is 
  
    
      
        E
        ⁡
        
          [
          
            X
            
              −
              n
            
          
          ]
        
      
    
    {\displaystyle \operatorname {E} \left[X^{-n}\right]}
   and the n-th logarithmic moment about zero is 
  
    
      
        E
        ⁡
        
          [
          
            
              ln
              
                n
              
            
            ⁡
            (
            X
            )
          
          ]
        
        .
      
    
    {\displaystyle \operatorname {E} \left[\ln ^{n}(X)\right].}
  
The n-th moment about zero of a probability density function f(x) is the expected value of Xn and is called a raw moment or crude moment. The moments about its mean μ are called central moments; these describe the shape of the function, independently of translation.
If f is a probability density function, then the value of the integral above is called the n-th moment of the probability distribution. More generally, if F is a cumulative probability distribution function of any probability distribution, which may not have a density function, then the n-th moment of the probability distribution is given by the Riemann–Stieltjes integralwhere X is a random variable that has this cumulative distribution F, and E is the expectation operator or mean.
Whenthe moment is said not to exist. If the n-th moment about any point exists, so does the (n − 1)-th moment (and thus, all lower-order moments) about every point.
The zeroth moment of any probability density function is 1, since the area under any probability density function must be equal to one.


=== Standardized moments ===

The normalised n-th central moment or standardised moment is the n-th central moment divided by σn; the normalised n-th central moment of the random variable X is 
These normalised central moments are dimensionless quantities, which represent the distribution independently of any linear change of scale.
For an electric signal, the first moment is its DC level, and the second moment is proportional to its average power.


=== Notable moments ===


==== Mean ====

The first raw moment is the mean, usually denoted 
  
    
      
        μ
        ≡
        E
        ⁡
        [
        X
        ]
        .
      
    
    {\displaystyle \mu \equiv \operatorname {E} [X].}
  


==== Variance ====

The second central moment is the variance. The positive square root of the variance is the standard deviation 
  
    
      
        σ
        ≡
        
          
            (
            
              E
              ⁡
              
                [
                
                  (
                  x
                  −
                  μ
                  
                    )
                    
                      2
                    
                  
                
                ]
              
            
            )
          
          
            
              1
              2
            
          
        
        .
      
    
    {\displaystyle \sigma \equiv \left(\operatorname {E} \left[(x-\mu )^{2}\right]\right)^{\frac {1}{2}}.}
  


==== Skewness ====

The third central moment is the measure of the lopsidedness of the distribution; any symmetric distribution will have a third central moment, if defined, of zero. The normalised third central moment is called the skewness, often γ. A distribution that is skewed to the left (the tail of the distribution is longer on the left) will have a negative skewness. A distribution that is skewed to the right (the tail of the distribution is longer on the right), will have a positive skewness.
For distributions that are not too different from the normal distribution, the median will be somewhere near μ − γσ/6; the mode about μ − γσ/2.


==== Kurtosis ====

The fourth central moment is a measure of the heaviness of the tail of the distribution. Since it is the expectation of a fourth power, the fourth central moment, where defined, is always nonnegative; and except for a point distribution, it is always strictly positive. The fourth central moment of a normal distribution is 3σ4.
The kurtosis κ is defined to be the standardized fourth central moment. (Equivalently, as in the next section, excess kurtosis is the fourth cumulant divided by the square of the second cumulant.) If a distribution has heavy tails, the kurtosis will be high (sometimes called leptokurtic); conversely, light-tailed distributions (for example, bounded distributions such as the uniform) have low kurtosis (sometimes called platykurtic).
The kurtosis can be positive without limit, but κ must be greater than or equal to γ2 + 1; equality only holds for binary distributions. For unbounded skew distributions not too far from normal, κ tends to be somewhere in the area of γ2 and 2γ2.
The inequality can be proven by consideringwhere T = (X − μ)/σ. This is the expectation of a square, so it is non-negative for all a; however it is also a quadratic polynomial in a. Its discriminant must be non-positive, which gives the required relationship.


=== Higher moments ===
High-order moments are moments beyond 4th-order moments. 
As with variance, skewness, and kurtosis, these are higher-order statistics, involving non-linear combinations of the data, and can be used for description or estimation of further shape parameters. The higher the moment, the harder it is to estimate, in the sense that larger samples are required in order to obtain estimates of similar quality. This is due to the excess degrees of freedom consumed by the higher orders. Further, they can be subtle to interpret, often being most easily understood in terms of lower order moments – compare the higher-order derivatives of jerk and jounce in physics. For example, just as the 4th-order moment (kurtosis) can be interpreted as ""relative importance of tails as compared to shoulders in contribution to dispersion"" (for a given amount of dispersion, higher kurtosis corresponds to thicker tails, while lower kurtosis corresponds to broader shoulders), the 5th-order moment can be interpreted as measuring ""relative importance of tails as compared to center (mode and shoulders) in contribution to skewness"" (for a given amount of skewness, higher 5th moment corresponds to higher skewness in the tail portions and little skewness of mode, while lower 5th moment corresponds to more skewness in shoulders).


=== Mixed moments ===
Mixed moments are moments involving multiple variables.
The value 
  
    
      
        E
        [
        
          X
          
            k
          
        
        ]
      
    
    {\displaystyle E[X^{k}]}
   is called the moment of order 
  
    
      
        k
      
    
    {\displaystyle k}
   (moments are also defined for non-integral 
  
    
      
        k
      
    
    {\displaystyle k}
  ). The moments of the joint distribution of random variables 
  
    
      
        
          X
          
            1
          
        
        .
        .
        .
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1}...X_{n}}
   are defined similarly. For any integers 
  
    
      
        
          k
          
            i
          
        
        ≥
        0
      
    
    {\displaystyle k_{i}\geq 0}
  , the mathematical expectation 
  
    
      
        E
        [
        
          
            
              X
              
                1
              
            
          
          
            
              k
              
                1
              
            
          
        
        ⋯
        
          
            
              X
              
                n
              
            
          
          
            
              k
              
                n
              
            
          
        
        ]
      
    
    {\displaystyle E[{X_{1}}^{k_{1}}\cdots {X_{n}}^{k_{n}}]}
   is called a mixed moment of order 
  
    
      
        k
      
    
    {\displaystyle k}
   (where 
  
    
      
        k
        =
        
          k
          
            1
          
        
        +
        .
        .
        .
        +
        
          k
          
            n
          
        
      
    
    {\displaystyle k=k_{1}+...+k_{n}}
  ), and 
  
    
      
        E
        [
        (
        
          X
          
            1
          
        
        −
        E
        [
        
          X
          
            1
          
        
        ]
        
          )
          
            
              k
              
                1
              
            
          
        
        ⋯
        (
        
          X
          
            n
          
        
        −
        E
        [
        
          X
          
            n
          
        
        ]
        
          )
          
            
              k
              
                n
              
            
          
        
        ]
      
    
    {\displaystyle E[(X_{1}-E[X_{1}])^{k_{1}}\cdots (X_{n}-E[X_{n}])^{k_{n}}]}
   is called a central mixed moment of order 
  
    
      
        k
      
    
    {\displaystyle k}
  . The mixed moment 
  
    
      
        E
        [
        (
        
          X
          
            1
          
        
        −
        E
        [
        
          X
          
            1
          
        
        ]
        )
        (
        
          X
          
            2
          
        
        −
        E
        [
        
          X
          
            2
          
        
        ]
        )
        ]
      
    
    {\displaystyle E[(X_{1}-E[X_{1}])(X_{2}-E[X_{2}])]}
   is called the covariance and is one of the basic characteristics of dependency between random variables.
Some examples are covariance, coskewness and cokurtosis.  While there is a unique covariance, there are multiple co-skewnesses and co-kurtoses.


== Properties of moments ==


=== Transformation of center ===
Since

where 
  
    
      
        
          
            
              (
            
            
              n
              i
            
            
              )
            
          
        
      
    
    {\textstyle {\binom {n}{i}}}
   is the binomial coefficient, it follows that the moments about b can be calculated from the moments about a by:


=== The moment of a convolution of function ===

The moment of a convolution 
  
    
      
        h
        (
        t
        )
        =
        (
        f
        ∗
        g
        )
        (
        t
        )
        =
        
          ∫
          
            −
            ∞
          
          
            ∞
          
        
        f
        (
        τ
        )
        g
        (
        t
        −
        τ
        )
        
        d
        τ
      
    
    {\textstyle h(t)=(f*g)(t)=\int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau }
   reads 

where 
  
    
      
        
          μ
          
            n
          
        
        [
        
        ⋅
        
        ]
      
    
    {\displaystyle \mu _{n}[\,\cdot \,]}
   denotes the 
  
    
      
        n
      
    
    {\displaystyle n}
  -th moment of the function given in the brackets. This identity follows by the convolution theorem for moment generating function and applying the chain rule for differentiating a product.


== Cumulants ==

The first raw moment and the second and third unnormalized central moments are additive in the sense that if X and Y are independent random variables then

(These can also hold for variables that satisfy weaker conditions than independence. The first always holds; if the second holds, the variables are called uncorrelated).
In fact, these are the first three cumulants and all cumulants share this additivity property.


== Sample moments ==
For all k, the k-th raw moment of a population can be estimated using the k-th raw sample  moment

applied to a sample X1, ..., Xn drawn from the population.
It can be shown that the expected value of the raw sample moment is equal to the k-th raw moment of the population, if that moment exists, for any sample size n. It is thus an unbiased estimator. This contrasts with the situation for central moments, whose computation uses up a degree of freedom by using the sample mean. So for example an unbiased estimate of the population variance (the second central moment) is given by

in which the previous denominator n has been replaced by the degrees of freedom n − 1, and in which 
  
    
      
        
          
            
              X
              ¯
            
          
        
      
    
    {\displaystyle {\bar {X}}}
   refers to the sample mean. This estimate of the population moment is greater than the unadjusted observed sample moment by a factor of 
  
    
      
        
          
            
              n
              
                n
                −
                1
              
            
          
        
        ,
      
    
    {\displaystyle {\tfrac {n}{n-1}},}
   and it is referred to as the ""adjusted sample variance"" or sometimes simply the ""sample variance"".


== Problem of moments ==

Problems of determining a probability distribution from its sequence of moments are called problem of moments. Such problems were first discussed by P.L. Chebyshev (1874) in connection with research on limit theorems. In order that the probability distribution of a random variable 
  
    
      
        X
      
    
    {\displaystyle X}
   be uniquely defined by its moments 
  
    
      
        
          α
          
            k
          
        
        =
        E
        
          X
          
            k
          
        
      
    
    {\displaystyle \alpha _{k}=EX^{k}}
   it is sufficient, for example, that Carleman's condition be satisfied:

A similar result even holds for moments of random vectors. The problem of moments seeks characterizations of sequences 
  
    
      
        
          
            
              
                μ
                
                  n
                
              
            
            ′
          
          :
          n
          =
          1
          ,
          2
          ,
          3
          ,
          …
        
      
    
    {\displaystyle {{\mu _{n}}':n=1,2,3,\dots }}
  that are sequences of moments of some function f, all moments 
  
    
      
        
          α
          
            k
          
        
        (
        n
        )
      
    
    {\displaystyle \alpha _{k}(n)}
   of which are finite, and for each integer 
  
    
      
        k
        ≥
        1
      
    
    {\displaystyle k\geq 1}
   let

where 
  
    
      
        
          α
          
            k
          
        
      
    
    {\displaystyle \alpha _{k}}
   is finite. Then there is a sequence 
  
    
      
        
          
            
              μ
              
                n
              
            
          
          ′
        
      
    
    {\displaystyle {\mu _{n}}'}
   that weakly converges to a distribution function 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   having 
  
    
      
        
          α
          
            k
          
        
      
    
    {\displaystyle \alpha _{k}}
   as its moments. If the moments determine 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   uniquely, then the sequence 
  
    
      
        
          
            
              μ
              
                n
              
            
          
          ′
        
      
    
    {\displaystyle {\mu _{n}}'}
   weakly converges to 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  .


== Partial moments ==
Partial moments are sometimes referred to as ""one-sided moments."" The n-th order lower and upper partial moments with respect to a reference point r may be expressed as

If the integral function do not converge, the partial moment does not exist.
Partial moments are normalized by being raised to the power 1/n. The upside potential ratio may be expressed as a ratio of a first-order upper partial moment to a normalized second-order lower partial moment. They have been used in the definition of some financial metrics, such as the Sortino ratio, as they focus purely on upside or downside.


== Central moments in metric spaces ==
Let (M, d) be a metric space, and let B(M) be the Borel σ-algebra on M, the σ-algebra generated by the d-open subsets of M. (For technical reasons, it is also convenient to assume that M is a separable space with respect to the metric d.) Let 1 ≤ p ≤ ∞.
The  p-th central moment of a measure μ on the measurable space (M, B(M)) about a given point x0 ∈ M is defined to be

μ is said to have finite p-th central moment if the p-th central moment of μ about x0 is finite for some x0 ∈ M.
This terminology for measures carries over to random variables in the usual way: if (Ω, Σ, P) is a probability space and X : Ω → M is a random variable, then the p-th central moment of X about x0 ∈ M is defined to be

and X has finite p-th central moment if the p-th central moment of X about x0 is finite for some x0 ∈ M.


== See also ==


== References ==
 Text was copied from Moment at the Encyclopedia of Mathematics, which is released under a Creative Commons Attribution-Share Alike 3.0 (Unported) (CC-BY-SA 3.0) license and the GNU Free Documentation License.


== Further reading ==
Spanos, Aris (1999). Probability Theory and Statistical Inference. New York: Cambridge University Press. pp. 109–130. ISBN 0-521-42408-9.
Walker, Helen M. (1929). Studies in the history of statistical method, with special reference to certain educational problems. Baltimore, Williams & Wilkins Co. p. 71.


== External links ==
""Moment"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Moments at Mathworld"
82d4196fb4,Mathematical logic,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.


== Subfields and scope ==
The Handbook of Mathematical Logic in 1977 makes a rough division of contemporary mathematical logic into four areas:

set theory
model theory
recursion theory, and
proof theory and constructive mathematics (considered as parts of a single area).Additionally, sometimes the field of computational complexity theory is also included as part of mathematical logic. Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp.  Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.
The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.


== History ==
Mathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics.  ""Mathematical logic, also called  'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last Nineteenth Century with the aid of an artificial notation and a rigorously deductive method.""  Before this emergence, logic was studied with rhetoric, with calculationes, through the syllogism, and with philosophy.  The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.


=== Early history ===

Theories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world.  Greek methods, particularly Aristotelian logic (or term logic) as found in the Organon, found wide application and acceptance in Western science and mathematics for millennia. The Stoics, especially Chrysippus, began the development of predicate logic. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.


=== 19th century ===
In the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic.  Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics. Charles Sanders Peirce later built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.
Gottlob Frege presented an independent development of logic with quantifiers in his Begriffsschrift, published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century.  The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.
From 1890 to 1905, Ernst Schröder published Vorlesungen über die Algebra der Logik in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.


==== Foundational theories ====
Concerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.
In logic, the term arithmetic refers to the theory of the natural numbers. Giuseppe Peano published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the  recursive definitions of addition and multiplication from the successor function and mathematical induction.
In the mid-19th century, flaws in Euclid's axioms for geometry became known.  In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826, mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert developed a complete set of axioms for geometry, building on previous work by Pasch.  The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line.  This would prove to be a major area of research in the first half of the 20th century.
The 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate.  Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817, but remained relatively unknown.
Cauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34).  In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers, a definition still employed in contemporary texts.Georg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities. Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895.


=== 20th century ===
In the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.
In 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's Entscheidungsproblem, posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.


==== Set theory and paradoxes ====
Ernst Zermelo gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof. This paper led to the general acceptance of the axiom of choice in the mathematics community.
Skepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard  discovered Richard's paradox.Zermelo provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.
In 1910, the first volume of Principia Mathematica by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. Principia Mathematica is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics.Fraenkel proved that the axiom of choice cannot be proved from the axioms of Zermelo's set theory with urelements. Later work by Paul Cohen showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.


==== Symbolic logic ====
Leopold Löwenheim and Thoralf Skolem obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.
In his doctoral thesis, Kurt Gödel proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.
In 1931, Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems, which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic.  Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.Gödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory.  Gödel gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.The first textbook on symbolic logic for the layman was written by Lewis Carroll, author of Alice in Wonderland, in 1896.


==== Beginnings of the other branches ====
Alfred Tarski developed the basics of model theory.
Beginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish Éléments de mathématique, a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words bijection, injection, and surjection, and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.
The study of computability came to be known as recursion theory or computability theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.
Numerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene introduced the concepts of relative computability, foreshadowed by Turing, and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Georg Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.


== Formal logical systems ==
At its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.  The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties.  Stronger classical logics such as second-order logic or infinitary logic are also studied, along with Non-classical logics such as intuitionistic logic.


=== First-order logic ===

First-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.
Early results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.
Gödel's completeness theorem established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.
Gödel's incompleteness theorems establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the Gödel sentence holds for the natural numbers but cannot be proved.
Here a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called ""sufficiently strong."" When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be reached.


=== Other classical logics ===
Many logics besides first-order logic are studied.  These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.
The most well studied infinitary logic is 
  
    
      
        
          L
          
            
              ω
              
                1
              
            
            ,
            ω
          
        
      
    
    {\displaystyle L_{\omega _{1},\omega }}
  . In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of 
  
    
      
        
          L
          
            
              ω
              
                1
              
            
            ,
            ω
          
        
      
    
    {\displaystyle L_{\omega _{1},\omega }}
   such as

  
    
      
        (
        x
        =
        0
        )
        ∨
        (
        x
        =
        1
        )
        ∨
        (
        x
        =
        2
        )
        ∨
        ⋯
        .
      
    
    {\displaystyle (x=0)\lor (x=1)\lor (x=2)\lor \cdots .}
  Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type.  The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.
Another type of logics are fixed-point logics that allow inductive definitions, like one writes for primitive recursive functions.
One can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic.
Lindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the downward Löwenheim–Skolem theorem is first-order logic.


=== Nonclassical and modal logic ===

Modal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability and set-theoretic forcing.Intuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true.  Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.


=== Algebraic logic ===
Algebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.


== Set theory ==

Set theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo, was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.
Other formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF).  Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.
Two famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo, was proved independent of ZF by Fraenkel, but has come to be widely accepted by mathematicians.  It states that given a collection of nonempty sets there is a single set C that contains exactly one element from each set in the collection. The set C is said to ""choose"" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.
The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory. This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear.Contemporary research in set theory includes the study of large cardinals and determinacy.  Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC.  Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line.  Determinacy refers to the possible existence of winning strategies for certain two-player games (the games are said to be determined). The existence of these strategies implies structural properties of the real line and other Polish spaces.


== Model theory ==

Model theory studies the models of various formal theories.  Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.
The set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.
The method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic. A modern subfield developing from this is concerned with o-minimal structures.
Morley's categoricity theorem, proved by Michael D. Morley, states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.
A trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.  Many special cases of this conjecture have been established.


== Recursion theory ==

Recursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability.  Recursion theory also includes the study of generalized computability and definability.  Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems.  More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.
Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.
Contemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.


=== Algorithmically unsolvable problems ===
An important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.
There are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959.  The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.
Hilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970.


== Proof theory and constructive mathematics ==

Proof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.  Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.
The study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems.  An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods.Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.   The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.  Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or translate) classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.
Recent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.


== Applications ==
""Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski).  Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls)."" ""Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).""


== Connections with computer science ==

The study of computability theory in computer science is closely related to the study of computability in mathematical logic.  There is a difference of emphasis, however.  Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.
The theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard correspondence between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.
Computer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.
Descriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.


== Foundations of mathematics ==

In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.
Cantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated ""God made the integers; all else is the work of man,"" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying ""No one shall expel us from the Paradise that Cantor has created.""
Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.  In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining point to mean a point on a fixed sphere and line to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.
With the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term finitary to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.
A second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of constructive. At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. 
In the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a part of philosophy of mathematics . This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to intuit the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true.  Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.


== See also ==
Argument
Informal logic
Knowledge representation and reasoning
Logic
List of computability and complexity topics
List of first-order theories
List of logic symbols
List of mathematical logic topics
List of set theory topics
Mereology
Propositional calculus
Well-formed formula


== Notes ==


== References ==


=== Undergraduate texts ===
Walicki, Michał (2011). Introduction to Mathematical Logic. Singapore: World Scientific Publishing. ISBN 9789814343879.
Boolos, George; Burgess, John; Jeffrey, Richard (2002). Computability and Logic (4th ed.). Cambridge University Press. ISBN 9780521007580.
Crossley, J.N.; Ash, C.J.; Brickhill, C.J.; Stillwell, J.C.; Williams, N.H. (1972). What is mathematical logic?. London, Oxford, New York City: Oxford University Press. ISBN 9780198880875. Zbl 0251.02001.
Enderton, Herbert (2001). A mathematical introduction to logic (2nd ed.). Boston MA: Academic Press. ISBN 978-0-12-238452-3.
Fisher, Alec (1982). Formal Number Theory and Computability: A Workbook. (suitable as a first course for independent study) (1st ed.). Oxford University Press. ISBN 978-0-19-853188-3.
Hamilton, A.G. (1988). Logic for Mathematicians (2nd ed.). Cambridge University Press. ISBN 978-0-521-36865-0.
Ebbinghaus, H.-D.; Flum, J.; Thomas, W. (1994). Mathematical Logic (2nd ed.). New York City: Springer. ISBN 9780387942582.
Katz, Robert (1964). Axiomatic Analysis. Boston MA: D. C. Heath and Company.
Mendelson, Elliott (1997). Introduction to Mathematical Logic (4th ed.). London: Chapman & Hall. ISBN 978-0-412-80830-2.
Rautenberg, Wolfgang (2010). A Concise Introduction to Mathematical Logic (3rd ed.). New York City: Springer. doi:10.1007/978-1-4419-1221-3. ISBN 9781441912206.
Schwichtenberg, Helmut (2003–2004). Mathematical Logic (PDF). Munich: Mathematisches Institut der Universität München. Retrieved 2016-02-24.
Shawn Hedman, A first course in logic: an introduction to model theory, proof theory, computability, and complexity, Oxford University Press, 2004, ISBN 0-19-852981-3. Covers logics in close relation with computability theory and complexity theory
van Dalen, Dirk (2013). Logic and Structure. Universitext. Berlin: Springer. doi:10.1007/978-1-4471-4558-5. ISBN 978-1-4471-4557-8.


=== Graduate texts ===
Hinman, Peter G. (2005). Fundamentals of mathematical logic. A K Peters, Ltd. ISBN 1-56881-262-0.
Andrews, Peter B. (2002). An Introduction to Mathematical Logic and Type Theory: To Truth Through Proof (2nd ed.). Boston: Kluwer Academic Publishers. ISBN 978-1-4020-0763-7.
Barwise, Jon, ed. (1989). Handbook of Mathematical Logic. Studies in Logic and the Foundations of Mathematics. Amsterdam: Elsevier. ISBN 9780444863881.
Hodges, Wilfrid (1997). A shorter model theory. Cambridge University Press. ISBN 9780521587136.
Jech, Thomas (2003). Set Theory: Millennium Edition. Springer Monographs in Mathematics. Berlin, New York: Springer. ISBN 9783540440857.
Kleene, Stephen Cole.(1952), Introduction to Metamathematics. New York: Van Nostrand. (Ishi Press: 2009 reprint).
Kleene, Stephen Cole. (1967),  Mathematical Logic. John Wiley. Dover reprint, 2002. ISBN 0-486-42533-9.
Shoenfield, Joseph R. (2001) [1967]. Mathematical Logic (2nd ed.). A K Peters. ISBN 9781568811352.
Troelstra, Anne Sjerp; Schwichtenberg, Helmut (2000). Basic Proof Theory. Cambridge Tracts in Theoretical Computer Science (2nd ed.). Cambridge University Press. ISBN 978-0-521-77911-1.


=== Research papers, monographs, texts, and surveys ===
Augusto, Luis M. (2017). Logical consequences. Theory and applications: An introduction. London: College Publications. ISBN 978-1-84890-236-7.
Boehner, Philotheus (1950). Medieval Logic. Manchester.
Cohen, Paul J. (1966). Set Theory and the Continuum Hypothesis. Menlo Park CA: W. A. Benjamin.
Cohen, Paul J. (2008) [1966]. Set theory and the continuum hypothesis. Mineola NY: Dover Publications. ISBN 9780486469218.
J.D. Sneed, The Logical Structure of Mathematical Physics. Reidel, Dordrecht, 1971 (revised edition 1979).
Davis, Martin (1973). ""Hilbert's tenth problem is unsolvable"". The American Mathematical Monthly. 80 (3): 233–269. doi:10.2307/2318447. JSTOR 2318447. Reprinted as an appendix in Martin Davis (1985). Computability and Unsolvability. Dover. ISBN 9780486614717.
Felscher, Walter (2000). ""Bolzano, Cauchy, Epsilon, Delta"". The American Mathematical Monthly. 107 (9): 844–862. doi:10.2307/2695743. JSTOR 2695743.
Ferreirós, José (2001). ""The Road to Modern Logic-An Interpretation"" (PDF). Bulletin of Symbolic Logic. 7 (4): 441–484. doi:10.2307/2687794. hdl:11441/38373. JSTOR 2687794. S2CID 43258676.
Hamkins, Joel David; Löwe, Benedikt (2007). ""The modal logic of forcing"". Transactions of the American Mathematical Society. 360 (4): 1793–1818. arXiv:math/0509616. doi:10.1090/s0002-9947-07-04297-3. S2CID 14724471.
Katz, Victor J. (1998). A History of Mathematics. Addison–Wesley. ISBN 9780321016188.
Morley, Michael (1965). ""Categoricity in Power"". Transactions of the American Mathematical Society. 114 (2): 514–538. doi:10.2307/1994188. JSTOR 1994188.
Soare, Robert I. (1996). ""Computability and recursion"". Bulletin of Symbolic Logic. 2 (3): 284–321. CiteSeerX 10.1.1.35.5803. doi:10.2307/420992. JSTOR 420992. S2CID 5894394.
Solovay, Robert M. (1976). ""Provability Interpretations of Modal Logic"". Israel Journal of Mathematics. 25 (3–4): 287–304. doi:10.1007/BF02757006. S2CID 121226261.
Woodin, W. Hugh (2001). ""The Continuum Hypothesis, Part I"" (PDF). Notices of the American Mathematical Society. 48 (6).


=== Classical papers, texts, and collections ===
Banach, Stefan; Tarski, Alfred (1924). ""Sur la décomposition des ensembles de points en parties respectivement congruentes"" (PDF). Fundamenta Mathematicae (in French). 6: 244–277. doi:10.4064/fm-6-1-244-277.Bochenski, Jozef Maria, ed. (1959). A Precis of Mathematical Logic. Synthese Library, Vol. 1. Translated by Otto Bird. Dordrecht: Springer. doi:10.1007/978-94-017-0592-9. ISBN 9789048183296.

Burali-Forti, Cesare (1897). A question on transfinite numbers. Reprinted in van Heijenoort 1976, pp. 104–111Cantor, Georg (1874). ""Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen"" (PDF). Journal für die Reine und Angewandte Mathematik. 1874 (77): 258–262. doi:10.1515/crll.1874.77.258. S2CID 199545885.
Carroll, Lewis (1896). Symbolic Logic. Kessinger Legacy Reprints. ISBN 9781163444955.

Dedekind, Richard (1872). Stetigkeit und irrationale Zahlen (in German). English translation as: ""Consistency and irrational numbers"".
Dedekind, Richard (1888). Was sind und was sollen die Zahlen?. Two English translations:
1963 (1901). Essays on the Theory of Numbers. Beman, W. W., ed. and trans. Dover.
1996. In From Kant to Hilbert: A Source Book in the Foundations of Mathematics, 2 vols, Ewald, William B., ed., Oxford University Press: 787–832.
Fraenkel, Abraham A. (1922). ""Der Begriff 'definit' und die Unabhängigkeit des Auswahlsaxioms"". Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-mathematische Klasse (in German). pp. 253–257. Reprinted in English translation as ""The notion of 'definite' and the independence of the axiom of choice"" in van Heijenoort 1976, pp. 284–289.
Frege, Gottlob (1879), Begriffsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen Denkens. Halle a. S.: Louis Nebert. Translation: Concept Script, a formal language of pure thought modelled upon that of arithmetic, by S. Bauer-Mengelberg in van Heijenoort 1976.
Frege, Gottlob (1884), Die Grundlagen der Arithmetik: eine logisch-mathematische Untersuchung über den Begriff der Zahl. Breslau: W. Koebner. Translation: J. L. Austin, 1974. The Foundations of Arithmetic: A logico-mathematical enquiry into the concept of number, 2nd ed. Blackwell.
Gentzen, Gerhard (1936). ""Die Widerspruchsfreiheit der reinen Zahlentheorie"". Mathematische Annalen. 112: 132–213. doi:10.1007/BF01565428. S2CID 122719892. Reprinted in English translation in Gentzen's Collected works, M. E. Szabo, ed., North-Holland, Amsterdam, 1969.
Gödel, Kurt (1929). Über die Vollständigkeit des Logikkalküls [Completeness of the logical calculus]. doctoral dissertation. University Of Vienna.
Gödel, Kurt (1930). ""Die Vollständigkeit der Axiome des logischen Funktionen-kalküls"" [The completeness of the axioms of the calculus of logical functions]. Monatshefte für Mathematik und Physik (in German). 37: 349–360. doi:10.1007/BF01696781. S2CID 123343522.
Gödel, Kurt (1931). ""Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I"" [On Formally Undecidable Propositions of Principia Mathematica and Related Systems]. Monatshefte für Mathematik und Physik (in German). 38 (1): 173–198. doi:10.1007/BF01700692. S2CID 197663120.
Gödel, Kurt (1958). ""Über eine bisher noch nicht benützte Erweiterung des finiten Standpunktes"". Dialectica (in German). 12 (3–4): 280–287. doi:10.1111/j.1746-8361.1958.tb01464.x. Reprinted in English translation in Gödel's Collected Works, vol II, Solomon Feferman et al., eds. Oxford University Press, 1993.
van Heijenoort, Jean, ed. (1976) [1967]. From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931 (3rd ed.). Cambridge MA: Harvard University Press. ISBN 9780674324497. (pbk.).
Hilbert, David (1899). Grundlagen der Geometrie (in German). Leipzig: Teubner. English 1902 edition (The Foundations of Geometry) republished 1980, Open Court, Chicago.
Hilbert, David (1929). ""Probleme der Grundlegung der Mathematik"". Mathematische Annalen. 102: 1–9. doi:10.1007/BF01782335. S2CID 122870563. Lecture given at the International Congress of Mathematicians, 3 September 1928. Published in English translation as ""The Grounding of Elementary Number Theory"", in Mancosu 1998, pp. 266–273.
Hilbert, David; Bernays, Paul (1934). Grundlagen der Mathematik. I. Die Grundlehren der mathematischen Wissenschaften. Vol. 40. Berlin, New York City: Springer. ISBN 9783540041344. JFM 60.0017.02. MR 0237246.
Kleene, Stephen Cole (1943). ""Recursive Predicates and Quantifiers"". Transactions of the American Mathematical Society. 53 (1): 41–73. doi:10.2307/1990131. JSTOR 1990131.
Lobachevsky, Nikolai (1840). Geometrishe Untersuchungen zur Theorie der Parellellinien (in German). Reprinted in English translation as Robert Bonola, ed. (1955). ""Geometric Investigations on the Theory of Parallel Lines"". Non-Euclidean Geometry. Dover. ISBN 0-486-60027-0.
Löwenheim, Leopold (1915). ""Über Möglichkeiten im Relativkalkül"". Mathematische Annalen (in German). 76 (4): 447–470. doi:10.1007/BF01458217. ISSN 0025-5831. S2CID 116581304. Translated as ""On possibilities in the calculus of relatives"" in Jean van Heijenoort (1967). A Source Book in Mathematical Logic, 1879–1931. Harvard Univ. Press. pp. 228–251.
Mancosu, Paolo, ed. (1998). From Brouwer to Hilbert. The Debate on the Foundations of Mathematics in the 1920s. Oxford University Press.
Pasch, Moritz (1882). Vorlesungen über neuere Geometrie.
Peano, Giuseppe (1889). Arithmetices principia, nova methodo exposita (in Lithuanian). Excerpt reprinted in English translation as ""The principles of arithmetic, presented by a new method""in van Heijenoort 1976, pp. 83–97.
Richard, Jules (1905). ""Les principes des mathématiques et le problème des ensembles"". Revue Générale des Sciences Pures et Appliquées (in French). 16: 541. Reprinted in English translation as ""The principles of mathematics and the problems of sets"" in van Heijenoort 1976, pp. 142–144.
Skolem, Thoralf (1920). ""Logisch-kombinatorische Untersuchungen über die Erfüllbarkeit oder Beweisbarkeit mathematischer Sätze nebst einem Theoreme über dichte Mengen"". Videnskapsselskapet Skrifter, I. Matematisk-naturvidenskabelig Klasse (in German). 6: 1–36.Soare, Robert Irving (22 December 2011). ""Computability Theory and Applications: The Art of Classical Computability"" (PDF). Department of Mathematics. University of Chicago. Retrieved 23 August 2017.
Swineshead, Richard (1498). Calculationes Suiseth Anglici (in Lithuanian). Papie: Per Franciscum Gyrardengum.

Tarski, Alfred (1948). A decision method for elementary algebra and geometry. Santa Monica CA: RAND Corporation.
Turing, Alan M. (1939). ""Systems of Logic Based on Ordinals"". Proceedings of the London Mathematical Society. 45 (2): 161–228. doi:10.1112/plms/s2-45.1.161. hdl:21.11116/0000-0001-91CE-3.
Weyl, Hermann (1918). Das Kontinuum. Kritische Untersuchungen über die Grund lagen der Analysis (in German). Leipzig.
Zermelo, Ernst (1904). ""Beweis, daß jede Menge wohlgeordnet werden kann"". Mathematische Annalen (in German). 59 (4): 514–516. doi:10.1007/BF01445300. S2CID 124189935. Reprinted in English translation as ""Proof that every set can be well-ordered"" in van Heijenoort 1976, pp. 139–141.
Zermelo, Ernst (1908a). ""Neuer Beweis für die Möglichkeit einer Wohlordnung"". Mathematische Annalen (in German). 65: 107–128. doi:10.1007/BF01450054. ISSN 0025-5831. S2CID 119924143. Reprinted in English translation as ""A new proof of the possibility of a well-ordering"" in van Heijenoort 1976, pp. 183–198.
Zermelo, Ernst (1908b). ""Untersuchungen über die Grundlagen der Mengenlehre"". Mathematische Annalen. 65 (2): 261–281. doi:10.1007/BF01449999. S2CID 120085563.


== External links ==
""Mathematical logic"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Polyvalued logic and Quantity Relation Logic
forall x: an introduction to formal logic, a free textbook by P. D. Magnus.
A Problem Course in Mathematical Logic, a free textbook by Stefan Bilaniuk.
Detlovs, Vilnis, and Podnieks, Karlis (University of Latvia), Introduction to Mathematical Logic. (hyper-textbook).
In the Stanford Encyclopedia of Philosophy:
Classical Logic by Stewart Shapiro.
First-order Model Theory by Wilfrid Hodges.
In the London Philosophy Study Guide:
Mathematical Logic
Set Theory & Further Logic
Philosophy of Mathematics
School of Mathematics, University of Manchester, Prof. Jeff Paris’s Mathematical Logic (course material and unpublished papers)"
052cb12dd6,Sign (mathematics),"In mathematics, the sign of a real number is its property of being either positive, negative, or zero. Depending on local conventions, zero may be considered as being neither positive nor negative (having no sign or a unique third sign), or it may be considered both positive and negative (having both signs). Whenever not specifically mentioned, this article adheres to the first convention.
In some contexts, it makes sense to consider a signed zero (such as floating-point representations of real numbers within computers). In mathematics and physics, the phrase ""change of sign"" is associated with the generation of the additive inverse (negation, or multiplication by −1) of any object that allows for this construction, and is not restricted to real numbers. It applies among other objects to vectors, matrices, and complex numbers, which are not prescribed to be only either positive, negative, or zero. The word ""sign"" is also often used to indicate other binary aspects of mathematical objects that resemble positivity and negativity, such as odd and even (sign of a permutation), sense of orientation or rotation (cw/ccw), one sided limits, and other concepts described in § Other meanings below.


== Sign of a number ==
Numbers from various number systems, like integers, rationals, complex numbers, quaternions, octonions, ... may have multiple attributes, that fix certain properties of a number. A number system that bears the structure of an ordered ring contains a unique number that when added with any number leaves the latter unchanged. This unique number is known as the system's additive identity element. For example, the integers has the structure of an ordered ring. This number is generally denoted as 0. Because of the total order in this ring, there are numbers greater than zero, called the positive numbers. Another property required for a ring to be ordered is that, for each positive number, there exists a unique corresponding number less than 0 whose sum with the original positive number is 0. These numbers less than 0 are called the negative numbers. The numbers in each such pair are their respective additive inverses. This attribute of a number, being exclusively either zero (0), positive (+), or negative (−), is called its sign, and is often encoded to the real numbers 0, 1, and −1, respectively (similar to the way the sign function is defined). Since rational and real numbers are also ordered rings (in fact ordered fields), the sign attribute also applies to these number systems.
When a minus sign is used in between two numbers, it represents the binary operation of subtraction. When a minus sign is written before a single number, it represents the unary operation of yielding the additive inverse (sometimes called negation) of the operand. Abstractly then, the difference of two number is the sum of the minuend with the additive inverse of the subtrahend. While 0 is its own additive inverse (−0 = 0), the additive inverse of a positive number is negative, and the additive inverse of a negative number is positive. A double application of this operation is written as −(−3) = 3. The plus sign is predominantly used in algebra to denote the binary operation of addition, and only rarely to emphasize the positivity of an expression.
In common numeral notation (used in arithmetic and elsewhere), the sign of a number is often made explicit by placing a plus or a minus sign before the number. For example, +3 denotes ""positive three"", and −3 denotes ""negative three"" (algebraically: the additive inverse of 3). Without specific context (or when no explicit sign is given), a number is interpreted per default as positive. This notation establishes a strong association of the minus sign ""−"" with negative numbers, and the plus sign ""+"" with positive numbers.


=== Sign of zero ===
Within the convention of zero being neither positive nor negative, a specific sign-value 0 may be assigned to the number value 0. This is exploited in the 
  
    
      
        sgn
      
    
    {\displaystyle \operatorname {sgn} }
  -function, as defined for real numbers. In arithmetic, +0 and −0 both denote the same number 0. There is generally no danger of confusing the value with its sign, although the convention of assigning both signs to 0 does not immediately allow for this discrimination.
In some contexts, especially in computing, it is useful to consider signed versions of zero, with signed zeros referring to different, discrete number representations (see signed number representations for more).
The symbols +0 and −0 rarely appear as substitutes for 0+ and 0−, used in calculus and mathematical analysis for one-sided limits (right-sided limit and left-sided limit, respectively). This notation refers to the behaviour of a function as its real input variable approaches 0 along positive (resp., negative) values; the two limits need not exist or agree.


=== Terminology for signs ===
When 0 is said to be neither positive nor negative, the following phrases may refer to the sign of a number:

A number is positive if it is greater than zero.
A number is negative if it is less than zero.
A number is non-negative if it is greater than or equal to zero.
A number is non-positive if it is less than or equal to zero.When 0 is said to be both positive and negative, modified phrases are used to refer to the sign of a number:

A number is strictly positive if it is greater than zero.
A number is strictly negative if it is less than zero.
A number is positive if it is greater than or equal to zero.
A number is negative if it is less than or equal to zero.For example, the absolute value of a real number is always ""non-negative"", but is not necessarily ""positive"" in the first interpretation, whereas in the second interpretation, it is called ""positive""—though not necessarily ""strictly positive"".
The same terminology is sometimes used for functions that yield real or other signed values. For example, a function would be called a positive function if its values are positive for all arguments of its domain, or a non-negative function if all of its values are non-negative.


=== Complex numbers ===
Complex numbers are impossible to order, so they cannot carry the structure of an ordered ring, and, accordingly, cannot be partitioned into positive and negative complex numbers. They do, however, share an attribute with the reals, which is called absolute value or magnitude. Magnitudes are always non-negative real numbers, and to any non-zero number there belongs a positive real number, its absolute value. 
For example, the absolute value of −3 and the absolute value of 3 are both equal to 3. This is written in symbols as |−3| = 3 and |3| = 3.
In general, any arbitrary real value can be specified by its magnitude and its sign. Using the standard encoding, any real value is given by the product of the magnitude and the sign in standard encoding. This relation can be generalized to define a sign for complex numbers.
Since the real and complex numbers both form a field and contain the positive reals, they also contain the reciprocals of the magnitudes of all non-zero numbers. This means that any non-zero number may be multiplied with the reciprocal of its magnitude, that is, divided by its magnitude. It is immediate that the quotient of any non-zero real number by its magnitude yields exactly its sign. By analogy, the sign of a complex number z can be defined as the quotient of z and its magnitude |z|. The sign of a complex number is the exponential of the product of its argument with the imaginary unit. represents in some sense its complex argument. This is to be compared to the sign of real numbers, except with 
  
    
      
        
          e
          
            i
            π
          
        
        =
        −
        1.
      
    
    {\displaystyle e^{i\pi }=-1.}
   For the definition of a complex sign-function. see § Complex sign function below.


=== Sign functions ===

When dealing with numbers, it is often convenient to have their sign available as a number. This is accomplished by functions that extract the sign of any number, and map it to a predefined value before making it available for further calculations. For example, it might be advantageous to formulate an intricate algorithm for positive values only, and take care of the sign only afterwards.


==== Real sign function ====
The sign function or signum function extracts the sign of a real number, by mapping the set of real numbers to the set of the three reals 
  
    
      
        {
        −
        1
        ,
        
        0
        ,
        
        1
        }
        .
      
    
    {\displaystyle \{-1,\;0,\;1\}.}
   It can be defined as follows:
Thus sgn(x) is 1 when x is positive, and sgn(x) is −1 when x is negative. For non-zero values of x, this function can also be defined by the formula

where |x| is the absolute value of x.


==== Complex sign function ====
While a real number has a 1-dimensional direction, a complex number has a 2-dimensional direction. The complex sign function requires the magnitude of its argument z = x + iy, which can be calculated as

Analogous to above, the complex sign function extracts the complex sign of a complex number by mapping the set of non-zero complex numbers to the set of unimodular complex numbers, and 0 to 0: 
  
    
      
        {
        z
        ∈
        
          C
        
        :
        
          |
        
        z
        
          |
        
        =
        1
        }
        ∪
        {
        0
        }
        .
      
    
    {\displaystyle \{z\in \mathbb {C} :|z|=1\}\cup \{0\}.}
   It may be defined as follows:
Let z be also expressed by its magnitude and one of its arguments φ as z = |z|⋅eiφ, then
This definition may also be recognized as a normalized vector, that is, a vector whose direction is unchanged, and whose length is fixed to unity. If the original value was R,θ in polar form, then sign(R, θ) is 1 θ. Extension of sign() or signum() to any number of dimensions is obvious, but this has already been defined as normalizing a vector.


== Signs per convention ==

In situations where there are exactly two possibilities on equal footing for an attribute, these are often labelled by convention as plus and minus, respectively. In some contexts, the choice of this assignment (i.e., which range of values is considered positive and which negative) is natural, whereas in other contexts, the choice is arbitrary, making an explicit sign convention necessary, the only requirement being consistent use of the convention.


=== Sign of an angle ===

In many contexts, it is common to associate a sign with the measure of an angle, particularly an oriented angle or an angle of rotation. In such a situation, the sign indicates whether the angle is in the clockwise or counterclockwise direction. Though different conventions can be used, it is common in mathematics to have counterclockwise angles count as positive, and clockwise angles count as negative.It is also possible to associate a sign to an angle of rotation in three dimensions, assuming that the axis of rotation has been oriented. Specifically, a right-handed rotation around an oriented axis typically counts as positive, while a left-handed rotation counts as negative.


=== Sign of a change ===
When a quantity x changes over time, the change in the value of x is typically defined by the equation

Using this convention, an increase in x counts as positive change, while a decrease of x counts as negative change. In calculus, this same convention is used in the definition of the derivative. As a result, any increasing function has positive derivative, while any decreasing function has negative derivative.


=== Sign of a direction ===
In analytic geometry and physics, it is common to label certain directions as positive or negative. For a basic example, the number line is usually drawn with positive numbers to the right, and negative numbers to the left:

As a result, when discussing linear motion, displacement or velocity, a motion to the right is usually thought of as being positive, while similar motion to the left is thought of as being negative.
On the Cartesian plane, the rightward and upward directions are usually thought of as positive, with rightward being the positive x-direction, and upward being the positive y-direction. If a displacement or velocity vector is separated into its vector components, then the horizontal part will be positive for motion to the right and negative for motion to the left, while the vertical part will be positive for motion upward and negative for motion downward.


=== Signedness in computing ===

In computing, an integer value may be either signed or unsigned, depending on whether the computer is keeping track of a sign for the number. By restricting an integer variable to non-negative values only, one more bit can be used for storing the value of a number. Because of the way integer arithmetic is done within computers, signed number representations usually do not store the sign as a single independent bit, instead using e.g. two's complement.
In contrast, real numbers are stored and manipulated as floating point values. The floating point values are represented using three separate values, mantissa, exponent, and sign. Given this separate sign bit, it is possible to represent both positive and negative zero. Most programming languages normally treat positive zero and negative zero as equivalent values, albeit, they provide means by which the distinction can be detected.


=== Other meanings ===

In addition to the sign of a real number, the word sign is also used in various related ways throughout mathematics and other sciences:

Words up to sign mean that, for a quantity q, it is known that either q = Q or q = −Q for certain Q. It is often expressed as q = ±Q. For real numbers, it means that only the absolute value |q| of the quantity is known. For complex numbers and vectors, a quantity known up to sign is a stronger condition than a quantity with known magnitude: aside Q and −Q, there are many other possible values of q such that |q| = |Q|.
The sign of a permutation is defined to be positive if the permutation is even, and negative if the permutation is odd.
In graph theory, a signed graph is a graph in which each edge has been marked with a positive or negative sign.
In mathematical analysis, a signed measure is a generalization of the concept of measure in which the measure of a set may have positive or negative values.
In a signed-digit representation, each digit of a number may have a positive or negative sign.
The ideas of signed area and signed volume are sometimes used when it is convenient for certain areas or volumes to count as negative. This is particularly true in the theory of determinants. In an (abstract) oriented vector space, each ordered basis for the vector space can be classified as either positively or negatively oriented.
In physics, any electric charge comes with a sign, either positive or negative. By convention, a positive charge is a charge with the same sign as that of a proton, and a negative charge is a charge with the same sign as that of an electron.


== See also ==
Plus–minus sign
Positive element
Signed distance
Signedness
Symmetry in mathematics


== References =="
f90e25903f,Division (mathematics),"Division is one of the four basic operations of arithmetic, the ways that numbers are combined to make new numbers. The other operations are addition, subtraction, and multiplication.
At an elementary level the division of two natural numbers is, among other possible interpretations, the process of calculating the number of times one number is contained within another.: 7  This number of times need not be an integer. For example, if 20 apples are divided evenly between 4 people, everyone receives 5 apples (see picture).
The division with remainder or Euclidean division of two natural numbers provides an integer quotient, which is the number of times the second number is completely contained in the first number, and a remainder, which is the part of the first number that remains, when in the course of computing the quotient, no further full chunk of the size of the second number can be allocated. For example, if 21 apples are divided between 4 people, everyone receives 5 apples again, and 1 apple remains.
For division to always yield one number rather than a quotient plus a remainder, the natural numbers must be extended to rational numbers or real numbers. In these enlarged number systems, division is the inverse operation to multiplication, that is a = c / b means a × b = c, as long as b is not zero. If b = 0, then this is a division by zero, which is not defined.: 246  In the 21-apples example, everyone would receive 5 apple and a quarter of an apple, thus avoiding any leftover.
Both forms of division appear in various algebraic structures, different ways of defining mathematical structure. Those in which a Euclidean division (with remainder) is defined are called Euclidean domains and include polynomial rings in one indeterminate (which define multiplication and addition over single-variabled formulas). Those in which a division (with a single result) by all nonzero elements is defined are called fields and division rings. In a ring the elements by which division is always possible are called the units (for example, 1 and −1 in the ring of integers). Another generalization of division to algebraic structures is the quotient group, in which the result of ""division"" is a group rather than a number.


== Introduction ==
The simplest way of viewing division is in terms of quotition and partition: from the quotition perspective, 20 / 5 means the number of 5s that must be added to get 20. In terms of partition, 20 / 5 means the size of each of 5 parts into which a set of size 20 is divided. For example, 20 apples divide into five groups of four apples, meaning that twenty divided by five is equal to four. This is denoted as 20 / 5 = 4, or 20/5 = 4. What is being divided is called the dividend, which is divided by the divisor, and the result is called the quotient. In the example, 20 is the dividend, 5 is the divisor, and 4 is the quotient.
Unlike the other basic operations, when dividing natural numbers there is sometimes a remainder that will not go evenly into the dividend; for example, 10 / 3 leaves a remainder of 1, as 10 is not a multiple of 3. Sometimes this remainder is added to the quotient as a fractional part, so 10 / 3 is equal to 3+1/3 or 3.33..., but in the context of integer division, where numbers have no fractional part, the remainder is kept separately (or exceptionally, discarded or rounded). When the remainder is kept as a fraction, it leads to a rational number. The set of all rational numbers is created by extending the integers with all possible results of divisions of integers.
Unlike multiplication and addition, division is not commutative, meaning that a / b is not always equal to b / a. Division is also not, in general, associative, meaning that when dividing multiple times, the order of division can change the result. For example, (24 / 6) / 2 = 2, but 24 / (6 / 2) = 8 (where the use of parentheses indicates that the operations inside parentheses are performed before the operations outside parentheses).
Division is traditionally considered as left-associative. That is, if there are multiple divisions in a row, the order of calculation goes from left to right:

  
    
      
        a
        
          /
        
        b
        
          /
        
        c
        =
        (
        a
        
          /
        
        b
        )
        
          /
        
        c
        =
        a
        
          /
        
        (
        b
        ×
        c
        )
        
        ≠
        
        a
        
          /
        
        (
        b
        
          /
        
        c
        )
        =
        (
        a
        ×
        c
        )
        
          /
        
        b
        .
      
    
    {\displaystyle a/b/c=(a/b)/c=a/(b\times c)\;\neq \;a/(b/c)=(a\times c)/b.}
  Division is right-distributive over addition and subtraction, in the sense that 

  
    
      
        
          
            
              a
              ±
              b
            
            c
          
        
        =
        (
        a
        ±
        b
        )
        
          /
        
        c
        =
        (
        a
        
          /
        
        c
        )
        ±
        (
        b
        
          /
        
        c
        )
        =
        
          
            a
            c
          
        
        ±
        
          
            b
            c
          
        
        .
      
    
    {\displaystyle {\frac {a\pm b}{c}}=(a\pm b)/c=(a/c)\pm (b/c)={\frac {a}{c}}\pm {\frac {b}{c}}.}
  This is the same for multiplication, as 
  
    
      
        (
        a
        +
        b
        )
        ×
        c
        =
        a
        ×
        c
        +
        b
        ×
        c
      
    
    {\displaystyle (a+b)\times c=a\times c+b\times c}
  . However, division is not left-distributive, as

  
    
      
        
          
            a
            
              b
              +
              c
            
          
        
        =
        a
        
          /
        
        (
        b
        +
        c
        )
        
        ≠
        
        (
        a
        
          /
        
        b
        )
        +
        (
        a
        
          /
        
        c
        )
        =
        
          
            
              a
              c
              +
              a
              b
            
            
              b
              c
            
          
        
        .
      
    
    {\displaystyle {\frac {a}{b+c}}=a/(b+c)\;\neq \;(a/b)+(a/c)={\frac {ac+ab}{bc}}.}
     For example  
  
    
      
        
          
            12
            
              2
              +
              4
            
          
        
        =
        
          
            12
            6
          
        
        =
        2
        ,
      
    
    {\displaystyle {\frac {12}{2+4}}={\frac {12}{6}}=2,}
   but 
  
    
      
        
          
            12
            2
          
        
        +
        
          
            12
            4
          
        
        =
        6
        +
        3
        =
        9.
      
    
    {\displaystyle {\frac {12}{2}}+{\frac {12}{4}}=6+3=9.}
  This is unlike the case in multiplication, which is both left-distributive and right-distributive, and thus distributive.


== Notation ==

Division is often shown in algebra and science by placing the dividend over the divisor with a horizontal line, also called a fraction bar, between them. For example, ""a divided by b"" can written as:

  
    
      
        
          
            a
            b
          
        
      
    
    {\displaystyle {\frac {a}{b}}}
  which can also be read out loud as ""divide a by b"" or ""a over b"". A way to express division all on one line is to write the dividend (or numerator), then a slash, then the divisor (or denominator), as follows:

  
    
      
        a
        
          /
        
        b
      
    
    {\displaystyle a/b}
  This is the usual way of specifying division in most computer programming languages, since it can easily be typed as a simple sequence of ASCII characters. (It is also the only notation used for quotient objects in abstract algebra.) Some mathematical software, such as MATLAB and GNU Octave, allows the operands to be written in the reverse order by using the backslash as the division operator:

  
    
      
        b
        ∖
        a
      
    
    {\displaystyle b\backslash a}
  A typographical variation halfway between these two forms uses a solidus (fraction slash), but elevates the dividend and lowers the divisor:

  
    
      
        
          

          
          
            a
          
        
        
        
          /
        
        
          

          
          
            b
          
        
      
    
    {\displaystyle {}^{a}\!/{}_{b}}
  Any of these forms can be used to display a fraction. A fraction is a division expression where both dividend and divisor are integers (typically called the numerator and denominator), and there is no implication that the division must be evaluated further. A second way to show division is to use the division sign (÷, also known as obelus though the term has additional meanings), common in arithmetic, in this manner:

  
    
      
        a
        ÷
        b
      
    
    {\displaystyle a\div b}
  This form is infrequent except in elementary arithmetic. ISO 80000-2-9.6 states it should not be used. This division sign is also used alone to represent the division operation itself, as for instance as a label on a key of a calculator. The obelus was introduced by Swiss mathematician Johann Rahn in 1659 in Teutsche Algebra.: 211  The ÷ symbol is used to indicate subtraction in some European countries, so its use may be misunderstood.
In some non-English-speaking countries, a colon is used to denote division:

  
    
      
        a
        :
        b
      
    
    {\displaystyle a:b}
  This notation was introduced by Gottfried Wilhelm Leibniz in his 1684 Acta eruditorum.: 295  Leibniz disliked having separate symbols for ratio and division. However, in English usage the colon is restricted to expressing the related concept of ratios.
Since the 19th century, US textbooks have used 
  
    
      
        b
        )
        a
      
    
    {\displaystyle b)a}
   or 
  
    
      
        b
        
          
            
              )
              a
            
            ¯
          
        
      
    
    {\displaystyle b{\overline {)a}}}
   to denote a divided by b, especially when discussing long division. The history of this notation is not entirely clear because it evolved over time.


== Computing ==


=== Manual methods ===
Division is often introduced through the notion of ""sharing out"" a set of objects, for example a pile of lollies, into a number of equal portions. Distributing the objects several at a time in each round of sharing to each portion leads to the idea of 'chunking' –  a form of division where one repeatedly subtracts multiples of the divisor from the dividend itself.
By allowing one to subtract more multiples than what the partial remainder allows at a given stage, more flexible methods, such as the bidirectional variant of chunking, can be developed as well.
More systematically and more efficiently, two integers can be divided with pencil and paper with the method of short division, if the divisor is small, or long division, if the divisor is larger. If the dividend has a fractional part (expressed as a decimal fraction), one can continue the procedure past the ones place as far as desired. If the divisor has a fractional part, one can restate the problem by moving the decimal to the right in both numbers until the divisor has no fraction, which can make the problem easier to solve (e.g., 10/2.5 = 100/25 = 4).
Division can be calculated with an abacus.Logarithm tables can be used to divide two numbers, by subtracting the two numbers' logarithms, then looking up the antilogarithm of the result.
Division can be calculated with a slide rule by aligning the divisor on the C scale with the dividend on the D scale. The quotient can be found on the D scale where it is aligned with the left index on the C scale. The user is responsible, however, for mentally keeping track of the decimal point.


=== By computer ===
Modern calculators and computers compute division either by methods similar to long division, or by faster methods; see Division algorithm.
In modular arithmetic (modulo a prime number) and for real numbers, nonzero numbers have a multiplicative inverse. In these cases, a division by x may be computed as the product by the multiplicative inverse of x. This approach is often associated with the faster methods in computer arithmetic.


== Division in different contexts ==


=== Euclidean division ===

Euclidean division is the mathematical formulation of the outcome of the usual process of division of integers. It asserts that, given two integers, a, the dividend, and b, the divisor, such that b ≠ 0, there are unique integers q, the quotient, and r, the remainder,  such that a = bq + r and 0 ≤ r < |b|, where |b| denotes the absolute value of b.


=== Of integers ===
Integers are not closed under division. Apart from division by zero being undefined, the quotient is not an integer unless the dividend is an integer multiple of the divisor. For example, 26 cannot be divided by 11 to give an integer. Such a case uses one of five approaches:

Say that 26 cannot be divided by 11; division becomes a partial function.
Give an approximate answer as a floating-point number. This is the approach usually taken in numerical computation.
Give the answer as a fraction representing a rational number, so the result of the division of 26 by 11 is 
  
    
      
        
          
            
              26
              11
            
          
        
      
    
    {\displaystyle {\tfrac {26}{11}}}
   (or as a mixed number, so 
  
    
      
        
          
            
              26
              11
            
          
        
        =
        2
        
          
            
              4
              11
            
          
        
        .
      
    
    {\displaystyle {\tfrac {26}{11}}=2{\tfrac {4}{11}}.}
  ) Usually the resulting fraction should be simplified: the result of the division of 52 by 22 is also 
  
    
      
        
          
            
              26
              11
            
          
        
      
    
    {\displaystyle {\tfrac {26}{11}}}
  . This simplification may be done by factoring out the greatest common divisor.
Give the answer as an integer quotient and a remainder, so 
  
    
      
        
          
            
              26
              11
            
          
        
        =
        2
        
          
             remainder 
          
        
        4.
      
    
    {\displaystyle {\tfrac {26}{11}}=2{\mbox{ remainder }}4.}
   To make the distinction with the previous case, this division, with two integers as result, is sometimes called Euclidean division, because it is the basis of the Euclidean algorithm.
Give the integer quotient as the answer, so 
  
    
      
        
          
            
              26
              11
            
          
        
        =
        2.
      
    
    {\displaystyle {\tfrac {26}{11}}=2.}
   This is the floor function applied to case 2 or 3. It is sometimes called integer division, and denoted by ""//"".Dividing integers in a computer program requires special care. Some programming languages treat integer division as in case 5 above, so the answer is an integer. Other languages, such as MATLAB and every computer algebra system return a rational number as the answer, as in case 3 above. These languages also provide functions to get the results of the other cases, either directly or from the result of case 3.
Names and symbols used for integer division include div, /, \, and %. Definitions vary regarding integer division when the dividend or the divisor is negative: rounding may be toward zero (so called T-division) or toward −∞ (F-division); rarer styles can occur – see modulo operation for the details.
Divisibility rules can sometimes be used to quickly determine whether one integer divides exactly into another.


=== Of rational numbers ===
The result of dividing two rational numbers is another rational number when the divisor is not 0. The division of two rational numbers p/q and r/s can be computed as

All four quantities are integers, and only p may be 0. This definition ensures that division is the inverse operation of multiplication.


=== Of real numbers ===
Division of two real numbers results in another real number (when the divisor is nonzero). It is defined such that a/b = c if and only if a = cb and b ≠ 0.


=== Of complex numbers ===
Dividing two complex numbers (when the divisor is nonzero) results in another complex number, which is found using the conjugate of the denominator:

This process of multiplying and dividing by 
  
    
      
        r
        −
        i
        s
      
    
    {\displaystyle r-is}
   is called 'realisation' or (by analogy) rationalisation. All four quantities p, q, r, s are real numbers, and r and s may not both be 0.
Division for complex numbers expressed in polar form is simpler than the definition above:

Again all four quantities p, q, r, s are real numbers, and r may not be 0.


=== Of polynomials ===
One can define the division operation for polynomials in one variable over a field. Then, as in the case of integers, one has a remainder. See Euclidean division of polynomials, and, for hand-written computation, polynomial long division or synthetic division.


=== Of matrices ===
One can define a division operation for matrices. The usual way to do this is to define A / B = AB−1, where B−1 denotes the inverse of B, but it is far more common to write out AB−1 explicitly to avoid confusion. An elementwise division can also be defined in terms of the Hadamard product.


==== Left and right division ====
Because matrix multiplication is not commutative, one can also define a left division or so-called backslash-division as A \ B = A−1B. For this to be well defined, B−1 need not exist, however A−1 does need to exist. To avoid confusion, division as defined by A / B = AB−1 is sometimes called right division or slash-division in this context.
Note that with left and right division defined this way, A / (BC) is in general not the same as (A / B) / C, nor is (AB) \ C the same as A \ (B \ C). However, it holds that A / (BC) = (A / C) / B and (AB) \ C = B \ (A \ C).


==== Pseudoinverse ====
To avoid problems when A−1 and/or B−1 do not exist, division can also be defined as multiplication by the pseudoinverse. That is, A / B = AB+ and A \ B = A+B, where A+ and B+ denote the pseudoinverses of A and B.


=== Abstract algebra ===
In abstract algebra, given a magma with binary operation ∗ (which could nominally be termed multiplication), left division of b by a (written a \ b) is typically defined as the solution x to the equation a ∗ x = b, if this exists and is unique.  Similarly, right division of b by a (written b / a) is the solution y to the equation y ∗ a = b.  Division in this sense does not require ∗ to have any particular properties (such as commutativity, associativity, or an identity element).
""Division"" in the sense of ""cancellation"" can be done in any magma by an element with the cancellation property.  Examples include matrix algebras and quaternion algebras.  A quasigroup is a structure in which division is always possible, even without an identity element and hence inverses.  In an integral domain, where not every element need have an inverse, division by a cancellative element a can still be performed on elements of the form ab or ca by left or right cancellation, respectively.  If a ring is finite and every nonzero element is cancellative, then by an application of the pigeonhole principle, every nonzero element of the ring is invertible, and division by any nonzero element is possible. To learn about when algebras (in the technical sense) have a division operation, refer to the page on division algebras. In particular Bott periodicity can be used to show that any real normed division algebra must be isomorphic to either the real numbers R, the complex numbers C, the quaternions H, or the octonions O.


=== Calculus ===
The derivative of the quotient of two functions is given by the quotient rule:


== Division by zero ==

Division of any number by zero in most mathematical systems is undefined, because zero multiplied by any finite number always results in a product of zero. Entry of such an expression into most calculators produces an error message. However, in certain higher level mathematics division by zero is possible by the zero ring and algebras such as wheels. In these algebras, the meaning of division is different from traditional definitions.


== See also ==

400AD Sunzi division algorithm
Division by two
Galley division
Inverse element
Order of operations
Repeating decimal


== Notes ==


== References ==


== External links ==

Planetmath division
Division on a Japanese abacus selected from Abacus: Mystery of the Bead
Chinese Short Division Techniques on a Suan Pan
Rules of divisibility"
5866aa8204,List of mathematics awards,"This list of mathematics awards is an index to articles about notable awards for mathematics. The list is organized by the region and country of the organization that sponsors the award, but awards may be open to mathematicians from around the world. Some of the awards are limited to work in a particular field, such as topology or analysis, while others are given for any type of mathematical contribution.


== International ==


== Americas ==


== Asia ==


== Europe ==


== Oceania ==


== See also ==
Lists of awards
Lists of science and technology awards"
b6f7f735bd,Recreational mathematics,"Recreational mathematics is mathematics carried out for recreation (entertainment) rather than as a strictly research and application-based professional activity or as a part of a student's formal education. Although it is not necessarily limited to being an endeavor for amateurs, many topics in this field require no knowledge of advanced mathematics. Recreational mathematics involves mathematical puzzles and games, often appealing to children and untrained adults, inspiring their further study of the subject.The Mathematical Association of America (MAA) includes recreational mathematics as one of its seventeen Special Interest Groups, commenting:

Recreational mathematics is not easily defined because it is more than mathematics done as a diversion or playing games that involve mathematics. Recreational mathematics is inspired by deep ideas that are hidden in puzzles, games, and other forms of play. The aim of the SIGMAA on Recreational Mathematics (SIGMAA-Rec) is to bring together enthusiasts and researchers in the myriad of topics that fall under recreational math. We will share results and ideas from our work, show that real, deep mathematics is there awaiting those who look, and welcome those who wish to become involved in this branch of mathematics.
Mathematical competitions (such as those sponsored by mathematical associations) are also categorized under recreational mathematics.


== Topics ==
Some of the more well-known topics in recreational mathematics are Rubik's Cubes, magic squares, fractals, logic puzzles and mathematical chess problems, but this area of mathematics includes the aesthetics and culture of mathematics, peculiar or amusing stories and coincidences about mathematics, and the personal lives of mathematicians.


=== Mathematical games ===
Mathematical games are multiplayer games whose rules, strategies, and outcomes can be studied and explained using mathematics. The players of the game may not need to use explicit mathematics in order to play mathematical games. For example, Mancala is studied in the mathematical field of combinatorial game theory, but no mathematics is necessary in order to play it.


=== Mathematical puzzles ===
Mathematical puzzles require mathematics in order to solve them. They have specific rules, as do multiplayer games, but mathematical puzzles do not usually involve competition between two or more players. Instead, in order to solve such a puzzle, the solver must find a solution that satisfies the given conditions.
Logic puzzles and classical ciphers are common examples of mathematical puzzles. Cellular automata and fractals are also considered mathematical puzzles, even though the solver only interacts with them by providing a set of initial conditions.
As they often include or require game-like features or thinking, mathematical puzzles are sometimes also called mathematical games.


=== Mathemagics ===
Magic tricks based on mathematical principles can produce self-working but surprising effects. For instance, a mathemagician might use the combinatorial properties of a deck of playing cards to guess a volunteer's selected card, or Hamming codes to identify whether a volunteer is lying.


=== Other activities ===
Other curiosities and pastimes of non-trivial mathematical interest include:

patterns in juggling
the sometimes profound algorithmic and geometrical characteristics of origami
patterns and process in creating string figures such as Cat's cradles, etc.
fractal-generating software


== Online blogs, podcasts, and YouTube channels ==
There are many blogs and audio or video series devoted to recreational mathematics.  Among the notable are the following:

Cut-the-knot by Alexander Bogomolny
Futility Closet by Greg Ross
Numberphile by Brady Haran
Mathologer by Burkard Polster
3Blue1Brown by Grant Sanderson
The videos of Vi Hart
Stand-Up Maths by Matt Parker


== Publications ==
The journal Eureka published by the mathematical society of the University of Cambridge is one of the oldest publications in recreational mathematics. It has been published 60 times since 1939 and authors have included many famous mathematicians and scientists such as Martin Gardner, John Conway, Roger Penrose, Ian Stewart, Timothy Gowers, Stephen Hawking and Paul Dirac.
The Journal of Recreational Mathematics was the largest publication on this topic from its founding in 1968 until 2014 when it ceased publication.
Mathematical Games (1956 to 1981) was the title of a long-running Scientific American column on recreational mathematics by Martin Gardner.  He inspired several generations of mathematicians and scientists through his interest in mathematical recreations. ""Mathematical Games"" was succeeded by 25 ""Metamagical Themas"" columns (1981-1983), a similarly distinguished, but shorter-running, column by Douglas Hofstadter, then by 78 ""Mathematical Recreations"" and ""Computer Recreations"" columns (1984 to 1991) by A. K. Dewdney, then by 96 ""Mathematical Recreations"" columns (1991 to 2001) by Ian Stewart, and most recently ""Puzzling Adventures"" by Dennis Shasha.
The Recreational Mathematics Magazine, published by the Ludus Association, is electronic and semiannual, and focuses on results that provide amusing, witty but nonetheless original and scientifically profound mathematical nuggets. The issues are published in the exact moments of the equinox.


== People ==
Prominent practitioners and advocates of recreational mathematics have included professional and amateur mathematicians:


== See also ==
List of recreational number theory topics


== References ==


== Further reading ==
W. W. Rouse Ball and H.S.M. Coxeter (1987). Mathematical Recreations and Essays, Thirteenth Edition, Dover. ISBN 0-486-25357-0.
Henry E. Dudeney (1967). 536 Puzzles and Curious Problems. Charles Scribner's sons. ISBN 0-684-71755-7.
Sam Loyd (1959. 2 Vols.). in Martin Gardner: The Mathematical Puzzles of Sam Loyd. Dover. OCLC 5720955.
Raymond M. Smullyan (1991). The Lady or the Tiger? And Other Logic Puzzles. Oxford University Press. ISBN 0-19-286136-0.


== External links ==

Recreational Mathematics from MathWorld at Wolfram Research
The Unreasonable Utility of Recreational Mathematics by David Singmaster"
7d14fb72c1,Module (mathematics),"In mathematics, a module is a generalization of the notion of vector space in which the field of scalars is replaced by a ring. The concept of module generalizes also the notion of abelian group, since the abelian groups are exactly the modules over the ring of integers.
Like a vector space, a module is an additive abelian group, and scalar multiplication is distributive over the operation of addition between elements of the ring or module and is compatible with the ring multiplication.
Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.


== Introduction and definition ==


=== Motivation ===
In a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law.  In a module, the scalars need only be a ring, so the module concept represents a significant generalization.  In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules.  In non-commutative algebra, the distinction between left ideals, ideals, and modules becomes more pronounced, though some ring-theoretic conditions can be expressed either about left ideals or left modules.
Much of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a ""well-behaved"" ring, such as a principal ideal domain. However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis, and even those that do, free modules, need not have a unique rank if the underlying ring does not satisfy the invariant basis number condition, unlike vector spaces, which always have a (possibly infinite) basis whose cardinality is then unique. (These last two assertions require the axiom of choice in general, but not in the case of finite-dimensional spaces, or certain well-behaved infinite-dimensional spaces such as Lp spaces.)


=== Formal definition ===
Suppose that R is a ring, and 1 is its multiplicative identity.
A left R-module M consists of an abelian group (M, +) and an operation · : R × M → M such that for all r, s in R and x, y in M, we have

  
    
      
        r
        ⋅
        (
        x
        +
        y
        )
        =
        r
        ⋅
        x
        +
        r
        ⋅
        y
      
    
    {\displaystyle r\cdot (x+y)=r\cdot x+r\cdot y}
  

  
    
      
        (
        r
        +
        s
        )
        ⋅
        x
        =
        r
        ⋅
        x
        +
        s
        ⋅
        x
      
    
    {\displaystyle (r+s)\cdot x=r\cdot x+s\cdot x}
  

  
    
      
        (
        r
        s
        )
        ⋅
        x
        =
        r
        ⋅
        (
        s
        ⋅
        x
        )
      
    
    {\displaystyle (rs)\cdot x=r\cdot (s\cdot x)}
  

  
    
      
        1
        ⋅
        x
        =
        x
        .
      
    
    {\displaystyle 1\cdot x=x.}
  The operation · is called scalar multiplication.  Often the symbol · is omitted, but in this article we use it and reserve juxtaposition for multiplication in R. One may write RM to emphasize that M is a left R-module.  A right R-module MR is defined similarly in terms of an operation · : M × R → M.
Authors who do not require rings to be unital omit condition 4 in the definition above; they would call the structures defined above ""unital left R-modules"". In this article, consistent with the glossary of ring theory, all rings and modules are assumed to be unital.An (R,S)-bimodule is an abelian group together with both a left scalar multiplication · by elements of R and a right scalar multiplication ∗ by elements of S, making it simultaneously a left R-module and a right S-module, satisfying the additional condition (r · x) ∗ s = r ⋅ (x ∗ s) for all r in R, x in M, and s in S.
If R is commutative, then left R-modules are the same as right R-modules and are simply called R-modules.


== Examples ==
If K is a field, then K-vector spaces (vector spaces over K) and K-modules are identical.
If K is a field, and K[x] a univariate polynomial ring, then a K[x]-module M is a K-module with an additional action of x on M that commutes with the action of K on M. In other words, a K[x]-module is a K-vector space M combined with a linear map from M to M. Applying the structure theorem for finitely generated modules over a principal ideal domain to this example shows the existence of the rational and Jordan canonical forms.
The concept of a Z-module agrees with the notion of an abelian group. That is, every abelian group is a module over the ring of integers Z in a unique way. For n > 0, let n ⋅ x = x + x + ... + x (n summands), 0 ⋅ x = 0, and (−n) ⋅ x = −(n ⋅ x). Such a module need not have a basis—groups containing torsion elements do not. (For example, in the group of integers modulo 3, one cannot find even one element which satisfies the definition of a linearly independent set since when an integer such as 3 or 6 multiplies an element, the result is 0. However, if a finite field is considered as a module over the same finite field taken as a ring, it is a vector space and does have a basis.)
The decimal fractions (including negative ones) form a module over the integers. Only singletons are linearly independent sets, but there is no singleton that can serve as a basis, so the module has no basis and no rank.
If R is any ring and n a natural number, then the cartesian product Rn is both a left and right R-module over R if we use the component-wise operations. Hence when n = 1, R is an R-module, where the scalar multiplication is just ring multiplication. The case n = 0 yields the trivial R-module {0} consisting only of its identity element. Modules of this type are called free and if R has invariant basis number (e.g. any commutative ring or field) the number n is then the rank of the free module.
If Mn(R) is the ring of n × n matrices over a ring R, M is an Mn(R)-module, and ei is the n × n matrix with 1 in the (i, i)-entry (and zeros elsewhere), then eiM is an R-module, since reim = eirm ∈ eiM. So M breaks up as the direct sum of R-modules, M = e1M ⊕ ... ⊕ enM. Conversely, given an R-module M0, then M0⊕n is an Mn(R)-module. In fact, the category of R-modules and the category of Mn(R)-modules are equivalent. The special case is that the module M is just R as a module over itself, then Rn is an Mn(R)-module.
If S is a nonempty set, M is a left R-module, and MS is the collection of all functions f : S → M, then with addition and scalar multiplication in MS defined pointwise by (f + g)(s) = f(s) + g(s) and (rf)(s) = rf(s), MS is a left R-module.  The right R-module case is analogous.  In particular, if R is commutative then the collection of R-module homomorphisms h : M → N (see below) is an R-module (and in fact a submodule of NM).
If X is a smooth manifold, then the smooth functions from X to the real numbers form a ring C∞(X). The set of all smooth vector fields defined on X form a module over C∞(X), and so do the tensor fields and the differential forms on X.  More generally, the sections of any vector bundle form a projective module over C∞(X), and by Swan's theorem, every projective module is isomorphic to the module of sections of some bundle; the category of C∞(X)-modules and the category of vector bundles over X are equivalent.
If R is any ring and I is any left ideal in R, then I is a left R-module, and analogously right ideals in R are right R-modules.
If R is a ring, we can define the opposite ring Rop which has the same underlying set and the same addition operation, but the opposite multiplication:  if ab = c in R, then ba = c in Rop.  Any left R-module M can then be seen to be a right module over Rop, and any right module over R can be considered a left module over Rop.
Modules over a Lie algebra are (associative algebra) modules over its universal enveloping algebra.
If R and S are rings with a ring homomorphism φ : R → S, then every S-module M is an R-module by defining rm = φ(r)m. In particular, S itself is such an R-module.


== Submodules and homomorphisms ==
Suppose M is a left R-module and N is a subgroup of M.  Then N is a submodule (or more explicitly an R-submodule) if for any n in N and any r in R, the product r ⋅ n (or n ⋅ r for a right R-module) is in N.
If X is any subset of an R-module M, then the submodule spanned by X is defined to be 
  
    
      
        ⟨
        X
        ⟩
        =
        
        
          ⋂
          
            N
            ⊇
            X
          
        
        N
      
    
    {\textstyle \langle X\rangle =\,\bigcap _{N\supseteq X}N}
   where N runs over the submodules of M which contain X, or explicitly 
  
    
      
        
          {
          
            
              ∑
              
                i
                =
                1
              
              
                k
              
            
            
              r
              
                i
              
            
            
              x
              
                i
              
            
            ∣
            
              r
              
                i
              
            
            ∈
            R
            ,
            
              x
              
                i
              
            
            ∈
            X
          
          }
        
      
    
    {\textstyle \left\{\sum _{i=1}^{k}r_{i}x_{i}\mid r_{i}\in R,x_{i}\in X\right\}}
  , which is important in the definition of tensor products.The set of submodules of a given module M, together with the two binary operations + and ∩, forms a lattice which satisfies the modular law:
Given submodules U, N1, N2 of M such that N1 ⊂ N2, then the following two submodules are equal: (N1 + U) ∩ N2 = N1 + (U ∩ N2).
If M and N are left R-modules, then a map f : M → N is a homomorphism of R-modules if for any m, n in M and r, s in R,

  
    
      
        f
        (
        r
        ⋅
        m
        +
        s
        ⋅
        n
        )
        =
        r
        ⋅
        f
        (
        m
        )
        +
        s
        ⋅
        f
        (
        n
        )
      
    
    {\displaystyle f(r\cdot m+s\cdot n)=r\cdot f(m)+s\cdot f(n)}
  .This, like any homomorphism of mathematical objects, is just a mapping which preserves the structure of the objects.  Another name for a homomorphism of R-modules is an R-linear map.
A bijective module homomorphism f : M → N is called a module isomorphism, and the two modules M and N are called isomorphic. Two isomorphic modules are identical for all practical purposes, differing solely in the notation for their elements.
The kernel of a module homomorphism f : M → N is the submodule of M consisting of all elements that are sent to zero by f, and the image of f is the submodule of N consisting of values f(m) for all elements m of M. The isomorphism theorems familiar from groups and vector spaces are also valid for R-modules.
Given a ring R, the set of all left R-modules together with their module homomorphisms forms an abelian category, denoted by R-Mod (see category of modules).


== Types of modules ==

Finitely generated
An R-module M is finitely generated if there exist finitely many elements x1, ..., xn in M such that every element of M is a linear combination of those elements with coefficients from the ring R.
Cyclic
A module is called a cyclic module if it is generated by one element.
Free
A free R-module is a module that has a basis, or equivalently, one that is isomorphic to a direct sum of copies of the ring R. These are the modules that behave very much like vector spaces.
Projective
Projective modules are direct summands of free modules and share many of their desirable properties.
Injective
Injective modules are defined dually to projective modules.
Flat
A module is called flat if taking the tensor product of it with any exact sequence of R-modules preserves exactness.
Torsionless
A module is called torsionless if it embeds into its algebraic dual.
Simple
A simple module S is a module that is not {0} and whose only submodules are {0} and S.  Simple modules are sometimes called irreducible.
Semisimple
A semisimple module is a direct sum (finite or not) of simple modules.  Historically these modules are also called completely reducible.
Indecomposable
An indecomposable module is a non-zero module that cannot be written as a direct sum of two non-zero submodules. Every simple module is indecomposable, but there are indecomposable modules which are not simple (e.g. uniform modules).
Faithful
A faithful module M is one where the action of each r ≠ 0 in R on M is nontrivial (i.e. r ⋅ x ≠ 0 for some x in M).  Equivalently, the annihilator of M is the zero ideal.
Torsion-free
A torsion-free module is a module over a ring such that 0 is the only element annihilated by a regular element (non zero-divisor) of the ring, equivalently rm = 0 implies r = 0 or m = 0.
Noetherian
A Noetherian module is a module which satisfies the ascending chain condition on submodules, that is, every increasing chain of submodules becomes stationary after finitely many steps.  Equivalently, every submodule is finitely generated.
Artinian
An Artinian module is a module which satisfies the descending chain condition on submodules, that is, every decreasing chain of submodules becomes stationary after finitely many steps.
Graded
A graded module is a module with a decomposition as a direct sum M = ⨁x Mx over a graded ring R = ⨁x Rx such that RxMy ⊂ Mx+y for all x and y.
Uniform
A uniform module is a module in which all pairs of nonzero submodules have nonzero intersection.


== Further notions ==


=== Relation to representation theory ===
A representation of a group G over a field k is a module over the group ring k[G].
If M is a left R-module, then the action of an element r in R is defined to be the map M → M that sends each x to rx (or xr in the case of a right module), and is necessarily a group endomorphism of the abelian group (M, +).  The set of all group endomorphisms of M is denoted EndZ(M) and forms a ring under addition and composition, and sending a ring element r of R to its action actually defines a ring homomorphism from R to EndZ(M).
Such a ring homomorphism R → EndZ(M) is called a representation of R over the abelian group M; an alternative and equivalent way of defining left R-modules is to say that a left R-module is an abelian group M together with a representation of R over it. Such a representation R → EndZ(M) may also be called a ring action of R on M.
A representation is called faithful if and only if the map R → EndZ(M) is injective. In terms of modules, this means that if r is an element of R such that rx = 0 for all x in M, then r = 0. Every abelian group is a faithful module over the integers or over some ring of integers modulo n, Z/nZ.


=== Generalizations ===
A ring R corresponds to a preadditive category R with a single object. With this understanding, a left R-module is just a covariant additive functor from R to the category Ab of abelian groups, and right R-modules are contravariant additive functors. This suggests that, if C is any preadditive category, a covariant additive functor from C to Ab should be considered a generalized left module over C. These functors form a functor category C-Mod which is the natural generalization of the module category R-Mod.
Modules over commutative rings can be generalized in a different direction: take a ringed space (X, OX) and consider the sheaves of OX-modules (see sheaf of modules). These form a category OX-Mod, and play an important role in modern algebraic geometry. If X has only a single point, then this is a module category in the old sense over the commutative ring OX(X).
One can also consider modules over a semiring. Modules over rings are abelian groups, but modules over semirings are only commutative monoids. Most applications of modules are still possible. In particular, for any semiring S, the matrices over S form a semiring over which the tuples of elements from S are a module (in this generalized sense only). This allows a further generalization of the concept of vector space incorporating the semirings from theoretical computer science.
Over near-rings, one can consider near-ring modules, a nonabelian generalization of modules.


== See also ==
Group ring
Algebra (ring theory)
Module (model theory)
Module spectrum
Annihilator


== Notes ==


== References ==
F.W. Anderson and K.R. Fuller: Rings and Categories of Modules, Graduate Texts in Mathematics, Vol. 13, 2nd Ed., Springer-Verlag, New York, 1992, ISBN 0-387-97845-3, ISBN 3-540-97845-3
Nathan Jacobson. Structure of rings. Colloquium publications, Vol. 37, 2nd Ed., AMS Bookstore, 1964, ISBN 978-0-8218-1037-8


== External links ==
""Module"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
module at the nLab"
b69eaf9f3f,Spline (mathematics),"In mathematics, a spline is a special function defined piecewise by polynomials.
In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding Runge's phenomenon for higher degrees.
In the computer science subfields of computer-aided design and computer graphics, the term spline more frequently refers to a piecewise polynomial (parametric) curve. Splines are popular curves in these subfields because of the simplicity of their construction, their ease and accuracy of evaluation, and their capacity to approximate complex shapes through curve fitting and interactive curve design.
The term spline comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes.


== Introduction ==
The term ""spline"" is used to refer to a wide class of functions that are used in applications requiring data interpolation and/or smoothing. The data may be either one-dimensional or multi-dimensional. Spline functions for interpolation are normally determined as the minimizers of suitable measures of roughness (for example integral squared curvature) subject to the interpolation constraints. Smoothing splines may be viewed as generalizations of interpolation splines where the functions are determined to minimize a weighted combination of the average squared approximation error over observed data and the roughness measure. For a number of meaningful definitions of the roughness measure, the spline functions are found to be finite dimensional in nature, which is the primary reason for their utility in computations and representation. For the rest of this section, we focus entirely on one-dimensional, polynomial splines and use the term ""spline"" in this restricted sense.


== Definition ==
We begin by limiting our discussion to polynomials in one variable.  In this case, a spline is a piecewise polynomial function.
This function, call it S, takes values from an interval [a,b] and maps them to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , the set of real numbers,

  
    
      
        S
        :
        [
        a
        ,
        b
        ]
        →
        
          R
        
        .
      
    
    {\displaystyle S:[a,b]\to \mathbb {R} .}
  We want S to be piecewise defined.  To accomplish this, let the interval [a,b] be covered by k ordered, disjoint subintervals,  

  
    
      
        [
        
          t
          
            i
          
        
        ,
        
          t
          
            i
            +
            1
          
        
        ]
        
          
             , 
          
        
        i
        =
        0
        ,
        …
        ,
        k
        −
        1
      
    
    {\displaystyle [t_{i},t_{i+1}]{\mbox{ , }}i=0,\ldots ,k-1}
  

  
    
      
        [
        a
        ,
        b
        ]
        =
        [
        
          t
          
            0
          
        
        ,
        
          t
          
            1
          
        
        )
        ∪
        [
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        )
        ∪
        ⋯
        ∪
        [
        
          t
          
            k
            −
            2
          
        
        ,
        
          t
          
            k
            −
            1
          
        
        )
        ∪
        [
        
          t
          
            k
            −
            1
          
        
        ,
        
          t
          
            k
          
        
        )
        ∪
        [
        
          t
          
            k
          
        
        ]
      
    
    {\displaystyle [a,b]=[t_{0},t_{1})\cup [t_{1},t_{2})\cup \cdots \cup [t_{k-2},t_{k-1})\cup [t_{k-1},t_{k})\cup [t_{k}]}
  

  
    
      
        a
        =
        
          t
          
            0
          
        
        ≤
        
          t
          
            1
          
        
        ≤
        ⋯
        ≤
        
          t
          
            k
            −
            1
          
        
        ≤
        
          t
          
            k
          
        
        =
        b
      
    
    {\displaystyle a=t_{0}\leq t_{1}\leq \cdots \leq t_{k-1}\leq t_{k}=b}
  On each of these k ""pieces"" of [a,b], we want to define a polynomial, call it Pi.

  
    
      
        
          P
          
            i
          
        
        :
        [
        
          t
          
            i
          
        
        ,
        
          t
          
            i
            +
            1
          
        
        ]
        →
        
          R
        
      
    
    {\displaystyle P_{i}:[t_{i},t_{i+1}]\to \mathbb {R} }
  .On the ith subinterval of [a,b], S is defined by Pi,

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            0
          
        
        (
        t
        )
        
          
             , 
          
        
        
          t
          
            0
          
        
        ≤
        t
        <
        
          t
          
            1
          
        
        ,
      
    
    {\displaystyle S(t)=P_{0}(t){\mbox{ , }}t_{0}\leq t<t_{1},}
  

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            1
          
        
        (
        t
        )
        
          
             , 
          
        
        
          t
          
            1
          
        
        ≤
        t
        <
        
          t
          
            2
          
        
        ,
      
    
    {\displaystyle S(t)=P_{1}(t){\mbox{ , }}t_{1}\leq t<t_{2},}
  

  
    
      
        ⋮
      
    
    {\displaystyle \vdots }
  

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            k
            −
            1
          
        
        (
        t
        )
        
          
             , 
          
        
        
          t
          
            k
            −
            1
          
        
        ≤
        t
        ≤
        
          t
          
            k
          
        
        .
      
    
    {\displaystyle S(t)=P_{k-1}(t){\mbox{ , }}t_{k-1}\leq t\leq t_{k}.}
  The given k+1 points ti are called knots. The vector

  
    
      
        
          
            t
          
        
        =
        (
        
          t
          
            0
          
        
        ,
        …
        ,
        
          t
          
            k
          
        
        )
      
    
    {\displaystyle {\mathbf {t} }=(t_{0},\dots ,t_{k})}
   is called a knot vector for the spline.
If the knots are equidistantly distributed in the interval [a,b] we say the spline is uniform, otherwise we say it is non-uniform.
If the polynomial pieces Pi each have degree at most n, then the spline is said to be of degree 
  
    
      
        ≤
        n
      
    
    {\displaystyle \leq n}
   (or of
order n+1).
If 
  
    
      
        S
        ∈
        
          C
          
            
              r
              
                i
              
            
          
        
      
    
    {\displaystyle S\in C^{r_{i}}}
   in a neighborhood of ti, then the spline is said to be
of smoothness (at least) 
  
    
      
        
          C
          
            
              r
              
                i
              
            
          
        
      
    
    {\displaystyle C^{r_{i}}}
   at ti. That is,
at ti the two polynomial pieces Pi-1 and Pi share common
derivative values from the derivative of order 0 (the function value)
up through the derivative of order ri (in other words, the two adjacent polynomial pieces connect with loss of smoothness of at most n - ri)

  
    
      
        
          P
          
            i
            −
            1
          
          
            (
            0
            )
          
        
        (
        t
        )
        =
        
          P
          
            i
          
          
            (
            0
            )
          
        
        (
        t
        )
      
    
    {\displaystyle P_{i-1}^{(0)}(t)=P_{i}^{(0)}(t)}
  

  
    
      
        
          P
          
            i
            −
            1
          
          
            (
            1
            )
          
        
        (
        t
        )
        =
        
          P
          
            i
          
          
            (
            1
            )
          
        
        (
        t
        )
      
    
    {\displaystyle P_{i-1}^{(1)}(t)=P_{i}^{(1)}(t)}
  

  
    
      
        ⋮
      
    
    {\displaystyle \vdots }
  

  
    
      
        
          P
          
            i
            −
            1
          
          
            (
            
              r
              
                i
              
            
            )
          
        
        (
        t
        )
        =
        
          P
          
            i
          
          
            (
            
              r
              
                i
              
            
            )
          
        
        (
        t
        )
      
    
    {\displaystyle P_{i-1}^{(r_{i})}(t)=P_{i}^{(r_{i})}(t)}
  .A vector

  
    
      
        
          
            r
          
        
        =
        (
        
          r
          
            1
          
        
        ,
        …
        ,
        
          r
          
            k
            −
            1
          
        
        )
      
    
    {\displaystyle {\mathbf {r} }=(r_{1},\dots ,r_{k-1})}
   such that the spline has smoothness 
  
    
      
        
          C
          
            
              r
              
                i
              
            
          
        
      
    
    {\displaystyle C^{r_{i}}}
   at ti for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        k
        −
        1
      
    
    {\displaystyle i=1,\ldots ,k-1}
   is called a smoothness vector for the spline.
Given a knot vector 
  
    
      
        
          
            t
          
        
      
    
    {\displaystyle {\mathbf {t} }}
  , a degree n, and a smoothness vector 
  
    
      
        
          
            r
          
        
      
    
    {\displaystyle {\mathbf {r} }}
   for 
  
    
      
        
          
            t
          
        
      
    
    {\displaystyle {\mathbf {t} }}
  , one can consider the set of all splines of degree 
  
    
      
        ≤
        n
      
    
    {\displaystyle \leq n}
   having knot vector 

  
    
      
        
          
            t
          
        
      
    
    {\displaystyle {\mathbf {t} }}
   and smoothness vector 
  
    
      
        
          
            r
          
        
      
    
    {\displaystyle {\mathbf {r} }}
  . Equipped with the operation of adding two functions (pointwise addition) and taking real multiples of functions, this set becomes a real vector space. This spline space is commonly denoted by 
  
    
      
        
          S
          
            n
          
          
            
              r
            
          
        
        (
        
          
            t
          
        
        )
      
    
    {\displaystyle S_{n}^{\mathbf {r} }({\mathbf {t} })}
  .
In the mathematical study of polynomial splines the question of what happens when two knots,
say ti and ti+1,
are moved together has an easy answer. The polynomial piece
Pi(t)
disappears, and the pieces
Pi−1(t) and Pi+1(t)
join with the sum of the continuity losses for
ti and ti+1.
That is,

  
    
      
        S
        (
        t
        )
        ∈
        
          C
          
            n
            −
            
              j
              
                i
              
            
            −
            
              j
              
                i
                +
                1
              
            
          
        
        [
        
          t
          
            i
          
        
        =
        
          t
          
            i
            +
            1
          
        
        ]
        ,
      
    
    {\displaystyle S(t)\in C^{n-j_{i}-j_{i+1}}[t_{i}=t_{i+1}],}
   where 
  
    
      
        
          j
          
            i
          
        
        =
        n
        −
        
          r
          
            i
          
        
      
    
    {\displaystyle j_{i}=n-r_{i}}
  This leads to a more general understanding of a knot vector.
The continuity loss at any point can be considered to be the result of
multiple knots located at that point, and a spline type can be completely
characterized by its degree n and its extended knot vector

  
    
      
        (
        
          t
          
            0
          
        
        ,
        
          t
          
            1
          
        
        ,
        ⋯
        ,
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        ,
        ⋯
        ,
        
          t
          
            2
          
        
        ,
        
          t
          
            3
          
        
        ,
        ⋯
        ,
        
          t
          
            k
            −
            2
          
        
        ,
        
          t
          
            k
            −
            1
          
        
        ,
        ⋯
        ,
        
          t
          
            k
            −
            1
          
        
        ,
        
          t
          
            k
          
        
        )
      
    
    {\displaystyle (t_{0},t_{1},\cdots ,t_{1},t_{2},\cdots ,t_{2},t_{3},\cdots ,t_{k-2},t_{k-1},\cdots ,t_{k-1},t_{k})}
  where ti is repeated ji times
for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        k
        −
        1
      
    
    {\displaystyle i=1,\dots ,k-1}
  .
A parametric curve on the interval [a,b]

  
    
      
        G
        (
        t
        )
        =
        (
        X
        (
        t
        )
        ,
        Y
        (
        t
        )
        )
        
          
             , 
          
        
        t
        ∈
        [
        a
        ,
        b
        ]
      
    
    {\displaystyle G(t)=(X(t),Y(t)){\mbox{ , }}t\in [a,b]}
  is a spline curve if both X and Y are spline functions
of the same degree with the same extended knot vectors on that interval.


== Examples ==
Suppose the interval [a,b] is [0,3] and the subintervals
are [0,1], [1,2], and [2,3]. Suppose the polynomial pieces are
to be of degree 2, and the pieces on [0,1] and [1,2] must join in value and first derivative
(at t=1)
while the pieces on [1,2] and [2,3] join simply in value (at t = 2).
This would define a type of spline S(t) for which

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            0
          
        
        (
        t
        )
        =
        −
        1
        +
        4
        t
        −
        
          t
          
            2
          
        
        
          
             , 
          
        
        0
        ≤
        t
        <
        1
      
    
    {\displaystyle S(t)=P_{0}(t)=-1+4t-t^{2}{\mbox{ , }}0\leq t<1}
  

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            1
          
        
        (
        t
        )
        =
        2
        t
        
          
             , 
          
        
        1
        ≤
        t
        <
        2
      
    
    {\displaystyle S(t)=P_{1}(t)=2t{\mbox{ , }}1\leq t<2}
  

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            2
          
        
        (
        t
        )
        =
        2
        −
        t
        +
        
          t
          
            2
          
        
        
          
             , 
          
        
        2
        ≤
        t
        ≤
        3
      
    
    {\displaystyle S(t)=P_{2}(t)=2-t+t^{2}{\mbox{ , }}2\leq t\leq 3}
  would be a member of that type, and also

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            0
          
        
        (
        t
        )
        =
        −
        2
        −
        2
        
          t
          
            2
          
        
        
          
             , 
          
        
        0
        ≤
        t
        <
        1
      
    
    {\displaystyle S(t)=P_{0}(t)=-2-2t^{2}{\mbox{ , }}0\leq t<1}
  

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            1
          
        
        (
        t
        )
        =
        1
        −
        6
        t
        +
        
          t
          
            2
          
        
        
          
             , 
          
        
        1
        ≤
        t
        <
        2
      
    
    {\displaystyle S(t)=P_{1}(t)=1-6t+t^{2}{\mbox{ , }}1\leq t<2}
  

  
    
      
        S
        (
        t
        )
        =
        
          P
          
            2
          
        
        (
        t
        )
        =
        −
        1
        +
        t
        −
        2
        
          t
          
            2
          
        
        
          
             , 
          
        
        2
        ≤
        t
        ≤
        3
      
    
    {\displaystyle S(t)=P_{2}(t)=-1+t-2t^{2}{\mbox{ , }}2\leq t\leq 3}
  would be a member of that type.
(Note: while the polynomial piece 2t is not quadratic, the result is still called a quadratic spline. This demonstrates that the degree of a spline is the maximum degree of its polynomial parts.)
The extended knot vector for this type of spline would be (0, 1, 2, 2, 3).
The simplest spline has degree 0. It is also called a step function.
The next most simple spline has degree 1. It is also called a linear spline. A closed linear spline (i.e, the first knot and the last are the same) in the plane is just a polygon.
A common spline is the natural cubic spline of degree 3 with continuity C2.
The word ""natural"" means that the second derivatives of
the spline polynomials
are set equal to zero at the endpoints of the interval of interpolation

  
    
      
        
          S
          ″
        
        (
        a
        )
        
        =
        
          S
          ″
        
        (
        b
        )
        =
        0.
      
    
    {\displaystyle S''(a)\,=S''(b)=0.}
  This forces the spline to be a straight line outside of the interval, while not disrupting its smoothness.


=== Algorithm for computing natural cubic splines ===
Cubic splines are of the form 
  
    
      
        
          
            S
          
          
            j
          
        
        
          (
          x
          )
        
        =
        
          a
          
            j
          
        
        +
        
          b
          
            j
          
        
        
          (
          
            x
            −
            
              x
              
                j
              
            
          
          )
        
        +
        
          c
          
            j
          
        
        
          
            
              (
              
                x
                −
                
                  x
                  
                    j
                  
                
              
              )
            
          
          
            2
          
        
        +
        
          d
          
            j
          
        
        
          
            
              (
              
                x
                −
                
                  x
                  
                    j
                  
                
              
              )
            
          
          
            3
          
        
      
    
    {\displaystyle {S}_{j}\left(x\right)=a_{j}+b_{j}\left(x-x_{j}\right)+c_{j}{\left(x-x_{j}\right)}^{2}+d_{j}{\left(x-x_{j}\right)}^{3}}
  .
Given set of coordinates 
  
    
      
        C
        =
        
          [
          
            
              (
              
                
                  
                    x
                  
                  
                    0
                  
                
                ,
                
                  
                    y
                  
                  
                    0
                  
                
              
              )
            
            ,
            
              (
              
                
                  
                    x
                  
                  
                    1
                  
                
                ,
                
                  
                    y
                  
                  
                    1
                  
                
              
              )
            
            ,
            .
            .
            .
            .
            ,
            
              (
              
                
                  
                    x
                  
                  
                    n
                  
                
                ,
                
                  
                    y
                  
                  
                    n
                  
                
              
              )
            
          
          ]
        
      
    
    {\displaystyle C=\left[\left({x}_{0},{y}_{0}\right),\left({x}_{1},{y}_{1}\right),....,\left({x}_{n},{y}_{n}\right)\right]}
   we wish to find set of 
  
    
      
        n
        
      
    
    {\displaystyle n\,}
   splines 
  
    
      
        
          
            S
          
          
            i
          
        
        
          (
          x
          )
        
      
    
    {\displaystyle {S}_{i}\left(x\right)}
   for 
  
    
      
        i
        =
        0
        ,
        …
        ,
        n
        −
        1.
      
    
    {\displaystyle i=0,\ldots ,n-1.}
  
These must satisfy:

  
    
      
        
          S
          
            i
          
        
        
          (
          
            x
            
              i
            
          
          )
        
        =
        
          y
          
            i
          
        
        =
        
          S
          
            i
            −
            1
          
        
        
          (
          
            x
            
              i
            
          
          )
        
        ,
        i
        =
        1
        ,
        …
        ,
        n
        −
        1.
      
    
    {\displaystyle S_{i}\left(x_{i}\right)=y_{i}=S_{i-1}\left(x_{i}\right),i=1,\ldots ,n-1.}
  

  
    
      
        
          S
          
            0
          
        
        
          (
          
            x
            
              0
            
          
          )
        
        =
        
          y
          
            0
          
        
        .
      
    
    {\displaystyle S_{0}\left(x_{0}\right)=y_{0}.}
  

  
    
      
        
          S
          
            n
            −
            1
          
        
        
          (
          
            x
            
              n
            
          
          )
        
        =
        
          y
          
            n
          
        
        .
      
    
    {\displaystyle S_{n-1}\left(x_{n}\right)=y_{n}.}
  

  
    
      
        
          
            
              S
              ′
            
          
          
            i
          
        
        
          (
          
            x
            
              i
            
          
          )
        
        =
        
          
            
              S
              ′
            
          
          
            i
            −
            1
          
        
        
          (
          
            x
            
              i
            
          
          )
        
        ,
        i
        =
        1
        ,
        …
        ,
        n
        −
        1.
      
    
    {\displaystyle {S'}_{i}\left(x_{i}\right)={S'}_{i-1}\left(x_{i}\right),i=1,\ldots ,n-1.}
  

  
    
      
        
          
            
              S
              ″
            
          
          
            i
          
        
        
          (
          
            x
            
              i
            
          
          )
        
        =
        
          
            
              S
              ″
            
          
          
            i
            −
            1
          
        
        
          (
          
            x
            
              i
            
          
          )
        
        ,
        i
        =
        1
        ,
        …
        ,
        n
        −
        1.
      
    
    {\displaystyle {S''}_{i}\left(x_{i}\right)={S''}_{i-1}\left(x_{i}\right),i=1,\ldots ,n-1.}
  

  
    
      
        
          
            
              S
              ″
            
          
          
            0
          
        
        
          (
          
            x
            
              0
            
          
          )
        
        =
        
          
            
              S
              ″
            
          
          
            n
            −
            1
          
        
        
          (
          
            x
            
              n
            
          
          )
        
        =
        0
      
    
    {\displaystyle {S''}_{0}\left(x_{0}\right)={S''}_{n-1}\left(x_{n}\right)=0}
  .Let us define one cubic spline 
  
    
      
        S
        
      
    
    {\displaystyle S\,}
   as a 5-tuple 
  
    
      
        (
        a
        ,
        b
        ,
        c
        ,
        d
        ,
        
          x
          
            t
          
        
        )
        
      
    
    {\displaystyle (a,b,c,d,x_{t})\,}
   where 
  
    
      
        a
        ,
        b
        ,
        c
        
      
    
    {\displaystyle a,b,c\,}
   and 
  
    
      
        d
        
      
    
    {\displaystyle d\,}
   correspond to coefficients in the form shown earlier and 
  
    
      
        
          x
          
            t
          
        
        
      
    
    {\displaystyle x_{t}\,}
   is equal to 
  
    
      
        
          x
          
            j
          
        
        .
        
      
    
    {\displaystyle x_{j}.\,}
  
Algorithm for computing Natural Cubic Splines:
Input: set of coordinates 
  
    
      
        C
        
      
    
    {\displaystyle C\,}
  , with 
  
    
      
        
          |
          C
          |
        
        =
        n
        +
        1
      
    
    {\displaystyle \left|C\right|=n+1}
  
Output: set splines which is composed of n 5-tuples.

Create new array a of size n + 1 and for 
  
    
      
        i
        =
        0
        ,
        …
        ,
        n
      
    
    {\displaystyle i=0,\ldots ,n}
   set 
  
    
      
        
          a
          
            i
          
        
        =
        
          y
          
            i
          
        
        
      
    
    {\displaystyle a_{i}=y_{i}\,}
  
Create new arrays b and d each of size n.
Create new array h of size n and for 
  
    
      
        i
        =
        0
        ,
        …
        ,
        n
        −
        1
      
    
    {\displaystyle i=0,\ldots ,n-1}
   set 
  
    
      
        
          h
          
            i
          
        
        =
        
          x
          
            i
            +
            1
          
        
        −
        
          x
          
            i
          
        
        
      
    
    {\displaystyle h_{i}=x_{i+1}-x_{i}\,}
  
Create new array α of size n and for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
        −
        1
      
    
    {\displaystyle i=1,\ldots ,n-1}
   set 
  
    
      
        
          
            α
          
          
            i
          
        
        =
        
          
            3
            
              
                h
              
              
                i
              
            
          
        
        
          (
          
            
              
                a
              
              
                i
                +
                1
              
            
            −
            
              
                a
              
              
                i
              
            
          
          )
        
        −
        
          
            3
            
              
                h
              
              
                i
                −
                1
              
            
          
        
        
          (
          
            
              
                a
              
              
                i
              
            
            −
            
              
                a
              
              
                i
                −
                1
              
            
          
          )
        
      
    
    {\displaystyle {\alpha }_{i}={\frac {3}{{h}_{i}}}\left({a}_{i+1}-{a}_{i}\right)-{\frac {3}{{h}_{i-1}}}\left({a}_{i}-{a}_{i-1}\right)}
  .
Create new arrays c, l, μ, and z each of size 
  
    
      
        n
        +
        1
        
      
    
    {\displaystyle n+1\,}
  .
Set 
  
    
      
        
          l
          
            0
          
        
        =
        1
        ,
        
          
            μ
          
          
            0
          
        
        =
        
          z
          
            0
          
        
        =
        0
        
      
    
    {\displaystyle l_{0}=1,{\mu }_{0}=z_{0}=0\,}
  
For 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
        −
        1
        
      
    
    {\displaystyle i=1,\ldots ,n-1\,}
  
Set  
  
    
      
        
          
            l
          
          
            i
          
        
        =
        2
        
          (
          
            
              
                x
              
              
                i
                +
                1
              
            
            −
            
              
                x
              
              
                i
                −
                1
              
            
          
          )
        
        −
        
          
            h
          
          
            i
            −
            1
          
        
        
          
            μ
          
          
            i
            −
            1
          
        
      
    
    {\displaystyle {l}_{i}=2\left({x}_{i+1}-{x}_{i-1}\right)-{h}_{i-1}{\mu }_{i-1}}
  .
Set 
  
    
      
        
          
            μ
          
          
            i
          
        
        =
        
          
            
              
                h
              
              
                i
              
            
            
              
                l
              
              
                i
              
            
          
        
      
    
    {\displaystyle {\mu }_{i}={\frac {{h}_{i}}{{l}_{i}}}}
  .
Set 
  
    
      
        
          
            z
          
          
            i
          
        
        =
        
          
            
              
                
                  α
                
                
                  i
                
              
              −
              
                
                  h
                
                
                  i
                  −
                  1
                
              
              
                
                  z
                
                
                  i
                  −
                  1
                
              
            
            
              
                l
              
              
                i
              
            
          
        
      
    
    {\displaystyle {z}_{i}={\frac {{\alpha }_{i}-{h}_{i-1}{z}_{i-1}}{{l}_{i}}}}
  .
Set 
  
    
      
        
          l
          
            n
          
        
        =
        1
        ;
        
          z
          
            n
          
        
        =
        
          c
          
            n
          
        
        =
        0.
        
      
    
    {\displaystyle l_{n}=1;z_{n}=c_{n}=0.\,}
  
For 
  
    
      
        j
        =
        n
        −
        1
        ,
        n
        −
        2
        ,
        …
        ,
        0
      
    
    {\displaystyle j=n-1,n-2,\ldots ,0}
  
Set 
  
    
      
        
          c
          
            j
          
        
        =
        
          z
          
            j
          
        
        −
        
          
            μ
          
          
            j
          
        
        
          c
          
            j
            +
            1
          
        
        
      
    
    {\displaystyle c_{j}=z_{j}-{\mu }_{j}c_{j+1}\,}
  
Set 
  
    
      
        
          b
          
            j
          
        
        =
        
          
            
              
                
                  a
                
                
                  j
                  +
                  1
                
              
              −
              
                
                  a
                
                
                  j
                
              
            
            
              
                h
              
              
                j
              
            
          
        
        −
        
          
            
              
                
                  h
                
                
                  j
                
              
              
                (
                
                  
                    
                      c
                    
                    
                      j
                      +
                      1
                    
                  
                  +
                  2
                  
                    
                      c
                    
                    
                      j
                    
                  
                
                )
              
            
            3
          
        
      
    
    {\displaystyle b_{j}={\frac {{a}_{j+1}-{a}_{j}}{{h}_{j}}}-{\frac {{h}_{j}\left({c}_{j+1}+2{c}_{j}\right)}{3}}}
  
Set 
  
    
      
        
          d
          
            j
          
        
        =
        
          
            
              
                
                  c
                
                
                  j
                  +
                  1
                
              
              −
              
                
                  c
                
                
                  j
                
              
            
            
              3
              
                
                  h
                
                
                  j
                
              
            
          
        
      
    
    {\displaystyle d_{j}={\frac {{c}_{j+1}-{c}_{j}}{3{h}_{j}}}}
  
Create new set Splines and call it output_set.  Populate it with n splines S.
For 
  
    
      
        i
        =
        0
        ,
        …
        ,
        n
        −
        1
      
    
    {\displaystyle i=0,\ldots ,n-1}
  
Set Si,a = ai
Set Si,b = bi
Set Si,c = ci
Set Si,d = di
Set Si,x = xi
Output output_set


== Notes ==
It might be asked what meaning more than n multiple knots in a knot vector have, since this would lead to continuities like

  
    
      
        S
        (
        t
        )
        ∈
        
          C
          
            −
            m
          
        
        
          
             , 
          
        
        m
        >
        0
      
    
    {\displaystyle S(t)\in C^{-m}{\mbox{ , }}m>0}
  at the location of this high multiplicity. By convention, any such situation indicates a simple discontinuity between the two adjacent polynomial pieces. This means that if a knot ti appears more than n + 1 times in an extended knot vector, all instances of it in excess of the (n + 1)th can be removed without changing the character of the spline, since all multiplicities n + 1, n + 2, n + 3, etc. have the same meaning. It is commonly assumed that any knot vector defining any type of spline has been culled in this fashion.
The classical spline type of degree n used in numerical analysis has continuity

  
    
      
        S
        (
        t
        )
        ∈
        
          
            C
          
          
            n
            −
            1
          
        
        [
        a
        ,
        b
        ]
        ,
        
      
    
    {\displaystyle S(t)\in \mathrm {C} ^{n-1}[a,b],\,}
  which means that every two adjacent polynomial pieces meet in their value and first n - 1 derivatives at each knot. The mathematical spline that most closely models the flat spline is a cubic (n = 3), twice continuously differentiable (C2), natural spline, which is a spline of this classical type with additional conditions imposed at endpoints a and b.
Another type of spline that is much used in graphics, for example in drawing programs such as Adobe Illustrator from Adobe Systems, has pieces that are cubic but has continuity only at most

  
    
      
        S
        (
        t
        )
        ∈
        
          
            C
          
          
            1
          
        
        [
        a
        ,
        b
        ]
        .
      
    
    {\displaystyle S(t)\in \mathrm {C} ^{1}[a,b].}
  This spline type is also used in PostScript as well as in the definition of some computer typographic fonts.
Many computer-aided design systems that are designed for high-end graphics and animation use extended knot vectors,
for example Autodesk Maya.
Computer-aided design systems often use an extended concept of a spline known as a Nonuniform rational B-spline (NURBS).
If sampled data from a function or a physical object is available, spline interpolation is an approach to creating a spline that approximates that data.


== General Expression For a C2 Interpolating Cubic Spline ==
The general expression for the ith C2 interpolating cubic spline at a point x with the natural condition can be found using the formula

  
    
      
        
          S
          
            i
          
        
        (
        x
        )
        =
        
          
            
              
                z
                
                  i
                
              
              (
              x
              −
              
                t
                
                  i
                  −
                  1
                
              
              
                )
                
                  3
                
              
            
            
              6
              
                h
                
                  i
                
              
            
          
        
        +
        
          
            
              
                z
                
                  i
                  −
                  1
                
              
              (
              
                t
                
                  i
                
              
              −
              x
              
                )
                
                  3
                
              
            
            
              6
              
                h
                
                  i
                
              
            
          
        
        +
        
          [
          
            
              
                
                  f
                  (
                  
                    t
                    
                      i
                    
                  
                  )
                
                
                  h
                  
                    i
                  
                
              
            
            −
            
              
                
                  
                    z
                    
                      i
                    
                  
                  
                    h
                    
                      i
                    
                  
                
                6
              
            
          
          ]
        
        (
        x
        −
        
          t
          
            i
            −
            1
          
        
        )
        +
        
          [
          
            
              
                
                  f
                  (
                  
                    t
                    
                      i
                      −
                      1
                    
                  
                  )
                
                
                  h
                  
                    i
                  
                
              
            
            −
            
              
                
                  
                    z
                    
                      i
                      −
                      1
                    
                  
                  
                    h
                    
                      i
                    
                  
                
                6
              
            
          
          ]
        
        (
        
          t
          
            i
          
        
        −
        x
        )
      
    
    {\displaystyle S_{i}(x)={\frac {z_{i}(x-t_{i-1})^{3}}{6h_{i}}}+{\frac {z_{i-1}(t_{i}-x)^{3}}{6h_{i}}}+\left[{\frac {f(t_{i})}{h_{i}}}-{\frac {z_{i}h_{i}}{6}}\right](x-t_{i-1})+\left[{\frac {f(t_{i-1})}{h_{i}}}-{\frac {z_{i-1}h_{i}}{6}}\right](t_{i}-x)}
  where 

  
    
      
        
          z
          
            i
          
        
        =
        
          f
          
            ′
            ′
          
        
        (
        
          t
          
            i
          
        
        )
      
    
    {\displaystyle z_{i}=f^{\prime \prime }(t_{i})}
   are the values of the second derivative at the ith knot.

  
    
      
        
          h
          
            i
          
          

          
        
        =
        
          t
          
            i
          
        
        −
        
          t
          
            i
            −
            1
          
        
      
    
    {\displaystyle h_{i}^{}=t_{i}-t_{i-1}}
  

  
    
      
        f
        (
        
          t
          
            i
          
          

          
        
        )
      
    
    {\displaystyle f(t_{i}^{})}
   are the values of the function at the ith knot.


== Representations and Names ==
For a given interval [a,b] and a given extended knot vector on that interval, the splines of degree n form a vector space. Briefly this means that adding any two splines of a given type produces spline of that given type, and multiplying a spline of a given type by any constant produces a spline of that given type. The dimension of
the space containing all splines of a certain type can be counted from the extended knot vector:

  
    
      
        a
        =
        
          t
          
            0
          
        
        <
        
          
            
              
                
                  t
                  
                    1
                  
                
                =
                ⋯
                =
                
                  t
                  
                    1
                  
                
              
              ⏟
            
          
          
            
              j
              
                1
              
            
          
        
        <
        ⋯
        <
        
          
            
              
                
                  t
                  
                    k
                    −
                    2
                  
                
                =
                ⋯
                =
                
                  t
                  
                    k
                    −
                    2
                  
                
              
              ⏟
            
          
          
            
              j
              
                k
                −
                2
              
            
          
        
        <
        
          t
          
            k
            −
            1
          
        
        =
        b
      
    
    {\displaystyle a=t_{0}<\underbrace {t_{1}=\cdots =t_{1}} _{j_{1}}<\cdots <\underbrace {t_{k-2}=\cdots =t_{k-2}} _{j_{k-2}}<t_{k-1}=b}
  

  
    
      
        
          j
          
            i
          
        
        ≤
        n
        +
        1
         
        ,
         
         
        i
        =
        1
        ,
        …
        ,
        k
        −
        2.
      
    
    {\displaystyle j_{i}\leq n+1~,~~i=1,\ldots ,k-2.}
  The dimension is equal to the sum of the degree plus the multiplicities

  
    
      
        d
        =
        n
        +
        
          ∑
          
            i
            =
            1
          
          
            k
            −
            2
          
        
        
          j
          
            i
          
        
        .
      
    
    {\displaystyle d=n+\sum _{i=1}^{k-2}j_{i}.}
  If a type of spline has additional linear conditions imposed upon it, then the resulting spline will lie in a subspace. The space of all natural cubic splines, for instance, is a subspace of the space of all cubic C2 splines.
The literature of splines is replete with names for special types of splines.
These names have been associated with:

The choices made for representing the spline, for example:
using basis functions for the entire spline (giving us the name B-splines)
using Bernstein polynomials as employed by Pierre Bézier to represent each polynomial piece (giving us the name Bézier splines)
The choices made in forming the extended knot vector, for example:
using single knots for Cn-1 continuity and spacing these knots evenly on [a,b] (giving us uniform splines)
using knots with no restriction on spacing (giving us nonuniform splines)
Any special conditions imposed on the spline, for example:
enforcing zero second derivatives at a and b (giving us natural splines)
requiring that given data values be on the spline (giving us interpolating splines)Often a special name was chosen for a type of spline satisfying two or more of the main items above. For example, the Hermite spline is a spline that is expressed using Hermite polynomials to represent each of the individual polynomial pieces. These are most often used with n = 3; that is, as Cubic Hermite splines. In this degree they may additionally be chosen to be only tangent-continuous (C1); which implies that all interior knots are double. Several methods have been invented to fit such splines to given data points; that is, to make them into interpolating splines, and to do so by estimating plausible tangent values where each two polynomial pieces meet (giving us Cardinal splines, Catmull-Rom splines, and Kochanek-Bartels splines, depending on the method used).
For each of the representations, some means of evaluation must be found so that values of the spline can be produced on demand. For those representations that express each individual polynomial piece Pi(t) in terms of
some basis for the degree n polynomials, this is conceptually straightforward:

For a given value of the argument t, find the interval in which it lies 
  
    
      
        t
        ∈
        [
        
          t
          
            i
          
        
        ,
        
          t
          
            i
            +
            1
          
        
        ]
      
    
    {\displaystyle t\in [t_{i},t_{i+1}]}
  
Look up the polynomial basis chosen for that interval 
  
    
      
        
          P
          
            0
          
        
        ,
        …
        ,
        
          P
          
            k
            −
            2
          
        
      
    
    {\displaystyle P_{0},\ldots ,P_{k-2}}
  
Find the value of each basis polynomial at t: 
  
    
      
        
          P
          
            0
          
        
        (
        t
        )
        ,
        …
        ,
        
          P
          
            k
            −
            2
          
        
        (
        t
        )
      
    
    {\displaystyle P_{0}(t),\ldots ,P_{k-2}(t)}
  
Look up the coefficients of the linear combination of those basis polynomials that give the spline on that interval c0, ..., ck-2
Add up that linear combination of basis polynomial values to get the value of the spline at t:
  
    
      
        
          ∑
          
            j
            =
            0
          
          
            k
            −
            2
          
        
        
          c
          
            j
          
        
        
          P
          
            j
          
        
        (
        t
        )
        .
      
    
    {\displaystyle \sum _{j=0}^{k-2}c_{j}P_{j}(t).}
  However, the evaluation and summation steps are often combined in clever ways. For example, Bernstein polynomials are a basis for polynomials that can be evaluated in linear combinations efficiently using special recurrence relations. This is the essence of De Casteljau's algorithm, which features in Bézier curves and  Bézier splines).
For a representation that defines a spline as a linear combination of basis splines, however, something more sophisticated is needed. The de Boor algorithm is an efficient method for evaluating B-splines.


== History ==
Before computers were used, numerical calculations were done by hand. Although piecewise-defined functions like the sign function or step function were used, polynomials were generally preferred because they were easier to work with. Through the advent of computers, splines have gained importance. They were first used as a replacement for polynomials in interpolation, then as a tool to construct smooth and flexible shapes in computer graphics.
It is commonly accepted that the first mathematical reference to splines is the 1946 paper by Schoenberg, which is probably the first place that the word ""spline"" is used in connection with smooth, piecewise polynomial approximation. However, the ideas have their roots in the aircraft and shipbuilding industries. In the foreword to (Bartels et al., 1987), Robin Forrest describes ""lofting"", a technique used in the British aircraft industry during World War II to construct templates for airplanes by passing thin wooden strips (called ""splines"") through points laid out on the floor of a large design loft, a technique borrowed from ship-hull design. For years the practice of ship design had employed models to design in the small. The successful design was then plotted on graph paper and the key points of the plot were re-plotted on larger graph paper to full size. The thin wooden strips provided an interpolation of the key points into smooth curves. The strips would be held in place at discrete points (called ""ducks"" by Forrest; Schoenberg used ""dogs"" or ""rats"") and between these points would assume shapes of minimum strain energy. According to Forrest, one possible impetus for a mathematical model for this process was the potential loss of the critical design components for an entire aircraft should the loft be hit by an enemy bomb. This gave rise to ""conic lofting"", which used conic sections to model the position of the curve between the ducks. Conic lofting was replaced by what we would call splines in the early 1960s based on work by J. C. Ferguson at Boeing and (somewhat later) by M.A. Sabin at British Aircraft Corporation. 
The word ""spline"" was originally an East Anglian dialect word.
The use of splines for modeling automobile bodies seems to have several independent beginnings. Credit is claimed on behalf of de Casteljau at Citroën, Pierre Bézier at Renault, and Birkhoff, Garabedian, and de Boor at General Motors (see Birkhoff and de Boor, 1965), all for work occurring in the very early 1960s or late 1950s. At least one of de Casteljau's papers was published, but not widely, in 1959. De Boor's work at General Motors resulted in a number of papers being published in the early 1960s, including some of the fundamental work on B-splines. 
Work was also being done at Pratt & Whitney Aircraft, where two of the authors of (Ahlberg et al., 1967) — the first book-length treatment of splines — were employed, and the David Taylor Model Basin, by Feodor Theilheimer. The work at General Motors is detailed nicely in (Birkhoff, 1990) and (Young, 1997). Davis (1997) summarizes some of this material.


== References ==
Ferguson, James C, Multi-variable curve interpolation,  J. ACM, vol. 11, no. 2, pp. 221-228, Apr. 1964.
Ahlberg, Nielson, and Walsh, The Theory of Splines and Their Applications, 1967.
Birkhoff, Fluid dynamics, reactor computations, and surface representation, in: Steve Nash (ed.), A History of Scientific Computation, 1990.
Bartels, Beatty, and Barsky, An Introduction to Splines for Use in Computer Graphics and Geometric Modeling, 1987.
Birkhoff and de Boor, Piecewise polynomial interpolation and approximation, in: H. L. Garabedian (ed.), Proc. General Motors Symposium of 1964, pp. 164–190. Elsevier, New York and Amsterdam, 1965.
Davis, B-splines and Geometric design, SIAM News, vol. 29, no. 5, 1997.
Epperson, History of Splines, NA Digest, vol. 98, no. 26, 1998.
Stoer & Bulirsch, Introduction to Numerical Analysis. Springer-Verlag. p. 93-106. ISBN 0387904204
Schoenberg, Contributions to the problem of approximation of equidistant data by analytic functions, Quart. Appl. Math., vol. 4, pp. 45–99 and 112–141, 1946.
Young, Garrett Birkhoff and applied mathematics, Notices of the AMS, vol. 44, no. 11, pp. 1446–1449, 1997.
Chapra, Canale, ""Numerical Methods for Engineers"" 5th edition.


== External links ==
Theory

An Interactive Introduction to Splines, ibiblio.orgExcel Function

XLL Excel Addin Function Implementation of cubic splineOnline utilities

Online Cubic Spline Interpolation Utility
Learning by Simulations Interactive simulation of various cubic splines
Symmetrical Spline Curves, an animation by Theodore Gray, The Wolfram Demonstrations Project, 2007.Computer Code

Notes, PPT, Mathcad, Maple, Mathematica, Matlab, Holistic Numerical Methods Institute
various routines, NTCC
Sisl: Opensource C-library for NURBS, SINTEF
VBA Spline Interpolation, vbnumericalmethods.com"
e2fcdcf442,Field (mathematics),"In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are, in general, algebraically unsolvable.
Fields serve as foundational notions in several mathematical domains. This includes different branches of mathematical analysis, which are based on fields with additional structure. Basic theorems in analysis hinge on the structural properties of the field of real numbers. Most importantly for algebraic purposes, any field may be used as the scalars for a vector space, which is the standard general context for linear algebra. Number fields, the siblings of the field of rational numbers, are studied in depth in number theory. Function fields can help describe properties of geometric objects.


== Definition ==
Informally, a field is a set, along with two operations defined on that set: an addition operation written as a + b, and a multiplication operation written as a ⋅ b, both of which behave similarly as they behave for rational numbers and real numbers, including the existence of an additive inverse −a for all elements a, and of a multiplicative inverse b−1 for every nonzero element b. This allows one to also consider the so-called inverse operations of subtraction, a − b, and division, a / b, by defining:

a − b := a + (−b),
a / b := a ⋅ b−1.


=== Classic definition ===
Formally, a field is a set F together with two binary operations on F called addition and multiplication. A binary operation on F is a mapping F × F → F, that is, a correspondence that associates with each ordered pair of elements of F a uniquely determined element of F. The result of the addition of a and b is called the sum of a and b, and is denoted a + b. Similarly, the result of the multiplication of a and b is called the product of a and b, and is denoted ab or a ⋅ b. These operations are required to satisfy the following properties, referred to as field axioms (in these axioms, a, b, and c are arbitrary elements of the field F):

Associativity of addition and multiplication: a + (b + c) = (a + b) + c, and a ⋅ (b ⋅ c) = (a ⋅ b) ⋅ c.
Commutativity of addition and multiplication: a + b = b + a, and a ⋅ b = b ⋅ a.
Additive and multiplicative identity: there exist two different elements 0 and 1 in F such that a + 0 = a and a ⋅ 1 = a.
Additive inverses: for every a in F, there exists an element in F, denoted −a, called the additive inverse of a, such that a + (−a) = 0.
Multiplicative inverses: for every a ≠ 0 in F, there exists an element in F, denoted by a−1 or 1/a, called the multiplicative inverse of a, such that a ⋅ a−1 = 1.
Distributivity of multiplication over addition: a ⋅ (b + c) = (a ⋅ b) + (a ⋅ c).This may be summarized by saying: a field has two operations, called addition and multiplication; it is an abelian group under addition with 0 as the additive identity; the nonzero elements are an abelian group under multiplication with 1 as the multiplicative identity; and multiplication distributes over addition.
Even more summarized: a field is a commutative ring where 
  
    
      
        0
        ≠
        1
      
    
    {\displaystyle 0\neq 1}
   and all nonzero elements are invertible under multiplication.


=== Alternative definition ===
Fields can also be defined in different, but equivalent ways. One can alternatively define a field by four binary operations (addition, subtraction, multiplication, and division) and their required properties. Division by zero is, by definition, excluded. In order to avoid existential quantifiers, fields can be defined by two binary operations (addition and multiplication), two unary operations (yielding the additive and multiplicative inverses respectively), and two nullary operations (the constants 0 and 1). These operations are then subject to the conditions above. Avoiding existential quantifiers is important in constructive mathematics and computing. One may equivalently define a field by the same two binary operations, one unary operation (the multiplicative inverse), and two (not necessarily distinct) constants 1 and −1, since 0 = 1 + (−1) and −a = (−1)a.


== Examples ==


=== Rational numbers ===

Rational numbers have been widely used a long time before the elaboration of the concept of field.
They are numbers that can be written as fractions
a/b, where a and b are integers, and b ≠ 0. The additive inverse of such a fraction is −a/b, and the multiplicative inverse (provided that a ≠ 0) is b/a, which can be seen as follows:

  
    
      
        
          
            b
            a
          
        
        ⋅
        
          
            a
            b
          
        
        =
        
          
            
              b
              a
            
            
              a
              b
            
          
        
        =
        1.
      
    
    {\displaystyle {\frac {b}{a}}\cdot {\frac {a}{b}}={\frac {ba}{ab}}=1.}
  The abstractly required field axioms reduce to standard properties of rational numbers. For example, the law of distributivity can be proven as follows:

  
    
      
        
          
            
              
              
                
                  
                    a
                    b
                  
                
                ⋅
                
                  (
                  
                    
                      
                        c
                        d
                      
                    
                    +
                    
                      
                        e
                        f
                      
                    
                  
                  )
                
              
            
            
              
                =
                

                
              
              
                
                  
                    a
                    b
                  
                
                ⋅
                
                  (
                  
                    
                      
                        c
                        d
                      
                    
                    ⋅
                    
                      
                        f
                        f
                      
                    
                    +
                    
                      
                        e
                        f
                      
                    
                    ⋅
                    
                      
                        d
                        d
                      
                    
                  
                  )
                
              
            
            
              
                =
                

                
              
              
                
                  
                    a
                    b
                  
                
                ⋅
                
                  (
                  
                    
                      
                        
                          c
                          f
                        
                        
                          d
                          f
                        
                      
                    
                    +
                    
                      
                        
                          e
                          d
                        
                        
                          f
                          d
                        
                      
                    
                  
                  )
                
                =
                
                  
                    a
                    b
                  
                
                ⋅
                
                  
                    
                      c
                      f
                      +
                      e
                      d
                    
                    
                      d
                      f
                    
                  
                
              
            
            
              
                =
                

                
              
              
                
                  
                    
                      a
                      (
                      c
                      f
                      +
                      e
                      d
                      )
                    
                    
                      b
                      d
                      f
                    
                  
                
                =
                
                  
                    
                      a
                      c
                      f
                    
                    
                      b
                      d
                      f
                    
                  
                
                +
                
                  
                    
                      a
                      e
                      d
                    
                    
                      b
                      d
                      f
                    
                  
                
                =
                
                  
                    
                      a
                      c
                    
                    
                      b
                      d
                    
                  
                
                +
                
                  
                    
                      a
                      e
                    
                    
                      b
                      f
                    
                  
                
              
            
            
              
                =
                

                
              
              
                
                  
                    a
                    b
                  
                
                ⋅
                
                  
                    c
                    d
                  
                
                +
                
                  
                    a
                    b
                  
                
                ⋅
                
                  
                    e
                    f
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\frac {a}{b}}\cdot \left({\frac {c}{d}}+{\frac {e}{f}}\right)\\[6pt]={}&{\frac {a}{b}}\cdot \left({\frac {c}{d}}\cdot {\frac {f}{f}}+{\frac {e}{f}}\cdot {\frac {d}{d}}\right)\\[6pt]={}&{\frac {a}{b}}\cdot \left({\frac {cf}{df}}+{\frac {ed}{fd}}\right)={\frac {a}{b}}\cdot {\frac {cf+ed}{df}}\\[6pt]={}&{\frac {a(cf+ed)}{bdf}}={\frac {acf}{bdf}}+{\frac {aed}{bdf}}={\frac {ac}{bd}}+{\frac {ae}{bf}}\\[6pt]={}&{\frac {a}{b}}\cdot {\frac {c}{d}}+{\frac {a}{b}}\cdot {\frac {e}{f}}.\end{aligned}}}
  


=== Real and complex numbers ===

The real numbers R, with the usual operations of addition and multiplication, also form a field. The complex numbers C consist of expressions

a + bi, with a, b real,where i is the imaginary unit, i.e., a (non-real) number satisfying i2 = −1.
Addition and multiplication of real numbers are defined in such a way that expressions of this type satisfy all field axioms and thus hold for C. For example, the distributive law enforces

(a + bi)(c + di) = ac + bci + adi + bdi2 = ac−bd + (bc + ad)i.It is immediate that this is again an expression of the above type, and so the complex numbers form a field. Complex numbers can be geometrically represented as points in the plane, with Cartesian coordinates given by the real numbers of their describing expression, or as the arrows from the origin to these points, specified by their length and an angle enclosed with some distinct direction. Addition then corresponds to combining the arrows to the intuitive parallelogram (adding the Cartesian coordinates), and the multiplication is – less intuitively – combining rotating and scaling of the arrows (adding the angles and multiplying the lengths). The fields of real and complex numbers are used throughout mathematics, physics, engineering, statistics, and many other scientific disciplines.


=== Constructible numbers ===

In antiquity, several geometric problems concerned the (in)feasibility of constructing certain numbers with compass and straightedge. For example, it was unknown to the Greeks that it is, in general, impossible to trisect a given angle in this way. These problems can be settled using the field of constructible numbers. Real constructible numbers are, by definition, lengths of line segments that can be constructed from the points 0 and 1 in finitely many steps using only compass and straightedge. These numbers, endowed with the field operations of real numbers, restricted to the constructible numbers, form a field, which properly includes the field Q of rational numbers. The illustration shows the construction of square roots of constructible numbers, not necessarily contained within Q. Using the labeling in the illustration, construct the segments AB, BD, and a semicircle over AD (center at the midpoint C), which intersects the perpendicular line through B in a point F, at a distance of exactly 
  
    
      
        h
        =
        
          
            p
          
        
      
    
    {\displaystyle h={\sqrt {p}}}
   from B when BD has length one.
Not all real numbers are constructible. It can be shown that 
  
    
      
        
          
            2
            
              3
            
          
        
      
    
    {\displaystyle {\sqrt[{3}]{2}}}
   is not a constructible number, which implies that it is impossible to construct with compass and straightedge the length of the side of a cube with volume 2, another problem posed by the ancient Greeks.


=== A field with four elements ===

In addition to familiar number systems such as the rationals, there are other, less immediate examples of fields. The following example is a field consisting of four elements called O, I, A, and B. The notation is chosen such that O plays the role of the additive identity element (denoted 0 in the axioms above), and I is the multiplicative identity (denoted 1 in the axioms above). The field axioms can be verified by using some more field theory, or by direct computation. For example,

A ⋅ (B + A) = A ⋅ I = A, which equals A ⋅ B + A ⋅ A = I + B = A, as required by the distributivity.This field is called a finite field with four elements, and is denoted F4 or GF(4). The subset consisting of O and I (highlighted in red in the tables at the right) is also a field, known as the binary field F2 or GF(2). In the context of computer science and Boolean algebra, O and I are often denoted respectively by false and true, and the addition is then denoted XOR (exclusive or). In other words, the structure of the binary field is the basic structure that allows computing with bits.


== Elementary notions ==
In this section, F denotes an arbitrary field and a and b are arbitrary elements of F.


=== Consequences of the definition ===
One has a ⋅ 0 = 0 and −a = (−1) ⋅ a. In particular, one may deduce the additive inverse of every element as soon as one knows −1.If ab = 0 then a or b must be 0, since, if a ≠ 0, then
b = (a−1a)b = a−1(ab)  = a−1 ⋅ 0 = 0. This means that every field is an integral domain.
In addition, the following properties are true for any elements a and b:

−0 = 0
1−1 = 1
(−(−a)) = a
(−a) ⋅ b = a ⋅ (−b) = −(a ⋅ b)
(a−1)−1 = a if a ≠ 0


=== The additive and the multiplicative group of a field ===
The axioms of a field F imply that it is an abelian group under addition. This group is called the additive group of the field, and is sometimes denoted by (F, +) when denoting it simply as F could be confusing.
Similarly, the nonzero elements of F form an abelian group under multiplication, called the multiplicative group, and denoted by (F \ {0}, ⋅) or just F \ {0}  or F*.
A field may thus be defined as set F equipped with two operations denoted as an addition and a multiplication such that F is an abelian group under addition, F \ {0}  is an abelian group under multiplication (where 0 is the identity element of the addition), and multiplication is distributive over addition. Some elementary statements about fields can therefore be obtained by applying general facts of groups. For example, the additive and multiplicative inverses −a and a−1 are uniquely determined by a.
The requirement 1 ≠ 0 follows, because 1 is the identity element of a group that does not contain 0. Thus, the trivial ring, consisting of a single element, is not a field.
Every finite subgroup of the multiplicative group of a field is cyclic (see Root of unity § Cyclic groups).


=== Characteristic ===
In addition to the multiplication of two elements of F, it is possible to define the product n ⋅ a of an arbitrary element a of F by a positive integer n to be the n-fold sum

a + a + ⋯ + a (which is an element of F.)If there is no positive integer such that

n ⋅ 1 = 0,then F is said to have characteristic 0. For example, the field of rational numbers Q has characteristic 0 since no positive integer n is zero. Otherwise, if there is a positive integer n satisfying this equation, the smallest such positive integer can be shown to be a prime number. It is usually denoted by p and the field is said to have characteristic p then.
For example, the field F4 has characteristic 2 since (in the notation of the above addition table) I + I = O.
If F has characteristic p, then p ⋅ a = 0 for all a in F. This implies that

(a + b)p = ap + bp,since all other binomial coefficients appearing in the binomial formula are divisible by p. Here, ap := a ⋅ a ⋅ ⋯ ⋅ a (p factors) is the p-th power, i.e., the p-fold product of the element a. Therefore, the Frobenius map

Fr: F → F, x ⟼ xpis compatible with the addition in F (and also with the multiplication), and is therefore a field homomorphism. The existence of this homomorphism makes fields in characteristic p quite different from fields of characteristic 0.


=== Subfields and prime fields ===
A subfield E of a field F is a subset of F that is a field with respect to the field operations of F. Equivalently E is a subset of F that contains 1, and is closed under addition, multiplication, additive inverse and multiplicative inverse of a nonzero element. This means that 1 ∊ E, that for all a, b ∊ E both a + b and a ⋅ b are in E, and that for all a ≠ 0 in E, both −a and 1/a are in E.
Field homomorphisms are maps φ: E → F between two fields such that φ(e1 + e2) = φ(e1) + φ(e2), φ(e1e2) = φ(e1) φ(e2), and φ(1E) = 1F, where e1 and e2 are arbitrary elements of E. All field homomorphisms are injective. If φ is also surjective, it is called an isomorphism (or the fields E and F are called isomorphic).
A field is called a prime field if it has no proper (i.e., strictly smaller) subfields. Any field F contains a prime field. If the characteristic of F is p (a prime number), the prime field is isomorphic to the finite field Fp introduced below. Otherwise the prime field is isomorphic to Q.


== Finite fields ==

Finite fields (also called Galois fields) are fields with finitely many elements, whose number is also referred to as the order of the field. The above introductory example F4 is a field with four elements. Its subfield F2 is the smallest field, because by definition a field has at least two distinct elements 1 ≠ 0.

The simplest finite fields, with prime order, are most directly accessible using modular arithmetic. For a fixed positive integer n, arithmetic ""modulo n"" means to work with the numbers

Z/nZ = {0, 1, ..., n − 1}.The addition and multiplication on this set are done by performing the operation in question in the set Z of integers, dividing by n and taking the remainder as result. This construction yields a field precisely if n is a prime number. For example, taking the prime n = 2 results in the above-mentioned field F2. For n = 4 and more generally, for any composite number (i.e., any number n which can be expressed as a product n = r⋅s of two strictly smaller natural numbers), Z/nZ is not a field: the product of two non-zero elements is zero since r⋅s = 0 in Z/nZ, which, as was explained above, prevents Z/nZ from being a field. The field Z/pZ with p elements (p being prime) constructed in this way is usually denoted by Fp.
Every finite field F has q = pn elements, where p is prime and n ≥ 1. This statement holds since F may be viewed as a vector space over its prime field. The dimension of this vector space is necessarily finite, say n, which implies the asserted statement.A field with q = pn elements can be constructed as the splitting field of the polynomial

f (x) = xq − x.Such a splitting field is an extension of Fp in which the polynomial f has q zeros. This means f has as many zeros as possible since the degree of f is q. For q = 22 = 4, it can be checked case by case using the above multiplication table that all four elements of F4 satisfy the equation x4 = x, so they are zeros of f. By contrast, in F2, f has only two zeros (namely 0 and 1), so f does not split into linear factors in this smaller field. Elaborating further on basic field-theoretic notions, it can be shown that two finite fields with the same order are isomorphic. It is thus customary to speak of the finite field with q elements, denoted by Fq or GF(q).


== History ==
Historically, three algebraic disciplines led to the concept of a field: the question of solving polynomial equations, algebraic number theory, and algebraic geometry. A first step towards the notion of a field was made in 1770 by Joseph-Louis Lagrange, who observed that permuting the zeros x1, x2, x3 of a cubic polynomial in the expression

(x1 + ωx2 + ω2x3)3(with ω being a third root of unity) only yields two values. This way, Lagrange conceptually explained the classical solution method of Scipione del Ferro and François Viète, which proceeds by reducing a cubic equation for an unknown x to a quadratic equation for x3. Together with a similar observation for equations of degree 4, Lagrange thus linked what eventually became the concept of fields and the concept of groups. Vandermonde, also in 1770, and to a fuller extent, Carl Friedrich Gauss, in his Disquisitiones Arithmeticae (1801), studied the equation

x p = 1for a prime p and, again using modern language, the resulting cyclic Galois group. Gauss deduced that a regular p-gon can be constructed if p = 22k + 1. Building on Lagrange's work, Paolo Ruffini claimed (1799) that quintic equations (polynomial equations of degree 5) cannot be solved algebraically; however, his arguments were flawed. These gaps were filled by Niels Henrik Abel in 1824. Évariste Galois, in 1832, devised necessary and sufficient criteria for a polynomial equation to be algebraically solvable, thus establishing in effect what is known as Galois theory today. Both Abel and Galois worked with what is today called an algebraic number field, but conceived neither an explicit notion of a field, nor of a group.
In 1871 Richard Dedekind introduced, for a set of real or complex numbers that is closed under the four arithmetic operations, the German word Körper, which means ""body"" or ""corpus"" (to suggest an organically closed entity). The English term ""field"" was introduced by Moore (1893).
By a field we will mean every infinite system of real or complex numbers so closed in itself and perfect that addition, subtraction, multiplication, and division of any two of these numbers again yields a number of the system.
In 1881 Leopold Kronecker defined what he called a domain of rationality, which is a field of rational fractions in modern terms. Kronecker's notion did not cover the field of all algebraic numbers (which is a field in Dedekind's sense), but on the other hand was more abstract than Dedekind's in that it made no specific assumption on the nature of the elements of a field. Kronecker interpreted a field such as Q(π) abstractly as the rational function field Q(X). Prior to this, examples of transcendental numbers were known since Joseph Liouville's work in 1844, until Charles Hermite (1873) and Ferdinand von Lindemann (1882) proved the transcendence of e and π, respectively.The first clear definition of an abstract field is due to Weber (1893). In particular, Heinrich Martin Weber's notion included the field Fp. Giuseppe Veronese (1891) studied the field of formal power series, which led Hensel (1904) to introduce the field of p-adic numbers. Steinitz (1910) synthesized the knowledge of abstract field theory accumulated so far. He axiomatically studied the properties of fields and defined many important field-theoretic concepts. The majority of the theorems mentioned in the sections Galois theory, Constructing fields and Elementary notions can be found in Steinitz's work. Artin & Schreier (1927) linked the notion of orderings in a field, and thus the area of analysis, to purely algebraic properties. Emil Artin redeveloped Galois theory from 1928 through 1942, eliminating the dependency on the primitive element theorem.


== Constructing fields ==


=== Constructing fields from rings ===
A commutative ring is a set, equipped with an addition and multiplication operation, satisfying all the axioms of a field, except for the existence of multiplicative inverses a−1. For example, the integers Z form a commutative ring, but not a field: the reciprocal of an integer n is not itself an integer, unless n = ±1.
In the hierarchy of algebraic structures fields can be characterized as the commutative rings R in which every nonzero element is a unit (which means every element is invertible). Similarly, fields are the commutative rings with precisely two distinct ideals, (0) and R. Fields are also precisely the commutative rings in which (0) is the only prime ideal.
Given a commutative ring R, there are two ways to construct a field related to R, i.e., two ways of modifying R such that all nonzero elements become invertible: forming the field of fractions, and forming residue fields. The field of fractions of Z is Q, the rationals, while the residue fields of Z are the finite fields Fp.


==== Field of fractions ====
Given an integral domain R, its field of fractions Q(R) is built with the fractions of two elements of R exactly as Q is constructed from the integers. More precisely, the elements of Q(R) are the fractions a/b where a and b are in R, and b ≠ 0. Two fractions a/b and c/d are equal if and only if ad = bc. The operation on the fractions work exactly as for rational numbers. For example,

  
    
      
        
          
            a
            b
          
        
        +
        
          
            c
            d
          
        
        =
        
          
            
              a
              d
              +
              b
              c
            
            
              b
              d
            
          
        
        .
      
    
    {\displaystyle {\frac {a}{b}}+{\frac {c}{d}}={\frac {ad+bc}{bd}}.}
  It is straightforward to show that, if the ring is an integral domain, the set of the fractions form a field.The field F(x) of the rational fractions over a field (or an integral domain) F is the field of fractions of the polynomial ring F[x]. The field F((x)) of Laurent series

  
    
      
        
          ∑
          
            i
            =
            k
          
          
            ∞
          
        
        
          a
          
            i
          
        
        
          x
          
            i
          
        
         
        (
        k
        ∈
        
          Z
        
        ,
        
          a
          
            i
          
        
        ∈
        F
        )
      
    
    {\displaystyle \sum _{i=k}^{\infty }a_{i}x^{i}\ (k\in \mathbb {Z} ,a_{i}\in F)}
  over a field F is the field of fractions of the ring F[[x]] of formal power series (in which k ≥ 0). Since any Laurent series is a fraction of a power series divided by a power of x (as opposed to an arbitrary power series), the representation of fractions is less important in this situation, though.


==== Residue fields ====
In addition to the field of fractions, which embeds R injectively into a field, a field can be obtained from a commutative ring R by means of a surjective map onto a field F. Any field obtained in this way is a quotient R / m, where m is a maximal ideal of R. If R has only one maximal ideal m, this field is called the residue field of R.The ideal generated by a single polynomial f in the polynomial ring R = E[X] (over a field E) is maximal if and only if f is irreducible in E, i.e., if f cannot be expressed as the product of two polynomials in E[X] of smaller degree. This yields a field

F = E[X] / ( f (X)).This field F contains an element x (namely the residue class of X) which satisfies the equation

f (x) = 0.For example, C is obtained from R by adjoining the imaginary unit symbol i, which satisfies f (i) = 0, where f (X) = X2 + 1. Moreover, f is irreducible over R, which implies that the map that sends a polynomial f (X) ∊ R[X] to f (i ) yields an isomorphism

  
    
      
        
          R
        
        [
        X
        ]
        
          /
        
        
          (
          
            
              X
              
                2
              
            
            +
            1
          
          )
        
         
        
          
            
              
                ⟶
              
              
                ≅
              
            
          
        
         
        
          C
        
        .
      
    
    {\displaystyle \mathbf {R} [X]/\left(X^{2}+1\right)\ {\stackrel {\cong }{\longrightarrow }}\ \mathbf {C} .}
  


=== Constructing fields within a bigger field ===
Fields can be constructed inside a given bigger container field. Suppose given a field E, and a field F containing E as a subfield. For any element x of F, there is a smallest subfield of F containing E and x, called the subfield of F generated by x and denoted E(x). The passage from E to E(x) is referred to by adjoining an element to E. More generally, for a subset S ⊂ F, there is a minimal subfield of F containing E and S, denoted by E(S).
The compositum of two subfields E and E'  of some field F is the smallest subfield of F containing both E and E'. The compositum can be used to construct the biggest subfield of F satisfying a certain property, for example the biggest subfield of F, which is, in the language introduced below, algebraic over E.


=== Field extensions ===

The notion of a subfield E ⊂ F can also be regarded from the opposite point of view, by referring to F being a field extension (or just extension) of E, denoted by

F / E,and read ""F over E"".
A basic datum of a field extension is its degree [F : E], i.e., the dimension of F as an E-vector space. It satisfies the formula
[G : E] = [G : F] [F : E].Extensions whose degree is finite are referred to as finite extensions. The extensions C / R and F4 / F2 are of degree 2, whereas R / Q is an infinite extension.


==== Algebraic extensions ====
A pivotal notion in the study of field extensions F / E are algebraic elements. An element 
  
    
      
        x
        ∈
        F
      
    
    {\displaystyle x\in F}
   is algebraic over E if it is a root of a polynomial with coefficients in E, that is, if it satisfies a polynomial equation

en xn + en−1xn−1 + ⋯ + e1x + e0 = 0,with en, ..., e0 in E, and en ≠ 0.
For example, the imaginary unit i in C is algebraic over R, and even over Q, since it satisfies the equation

i2 + 1 = 0.A field extension in which every element of F is algebraic over E is called an algebraic extension. Any finite extension is necessarily algebraic, as can be deduced from the above multiplicativity formula.The subfield E(x) generated by an element x, as above, is an algebraic extension of E if and only if x is an algebraic element. That is to say, if x is algebraic, all other elements of E(x) are necessarily algebraic as well. Moreover, the degree of the extension E(x) / E, i.e., the dimension of E(x) as an E-vector space, equals the minimal degree n such that there is a polynomial equation involving x, as above. If this degree is n, then the elements of E(x) have the form

  
    
      
        
          ∑
          
            k
            =
            0
          
          
            n
            −
            1
          
        
        
          a
          
            k
          
        
        
          x
          
            k
          
        
        ,
         
         
        
          a
          
            k
          
        
        ∈
        E
        .
      
    
    {\displaystyle \sum _{k=0}^{n-1}a_{k}x^{k},\ \ a_{k}\in E.}
  For example, the field Q(i) of Gaussian rationals is the subfield of C consisting of all numbers of the form a + bi where both a and b are rational numbers: summands of the form i2 (and similarly for higher exponents) do not have to be considered here, since a + bi + ci2 can be simplified to a − c + bi.


==== Transcendence bases ====
The above-mentioned field of rational fractions E(X), where X is an indeterminate, is not an algebraic extension of E since there is no polynomial equation with coefficients in E whose zero is X. Elements, such as X, which are not algebraic are called transcendental. Informally speaking, the indeterminate X and its powers do not interact with elements of E. A similar construction can be carried out with a set of indeterminates, instead of just one.
Once again, the field extension E(x) / E discussed above is a key example: if x is not algebraic (i.e., x is not a root of a polynomial with coefficients in E), then E(x) is isomorphic to E(X). This isomorphism is obtained by substituting x to X in rational fractions.
A subset S of a field F is a transcendence basis if it is algebraically independent (do not satisfy any polynomial relations) over E and if F is an algebraic extension of E(S). Any field extension F / E has a transcendence basis. Thus, field extensions can be split into ones of the form E(S) / E (purely transcendental extensions) and algebraic extensions.


=== Closure operations ===
A field is algebraically closed if it does not have any strictly bigger algebraic extensions or, equivalently, if any polynomial equation

fn xn + fn−1xn−1 + ⋯ + f1x + f0 = 0, with coefficients fn, ..., f0 ∈ F, n > 0,has a solution x ∊ F. By the fundamental theorem of algebra, C is algebraically closed, i.e., any polynomial equation with complex coefficients has a complex solution. The rational and the real numbers are not algebraically closed since the equation

x2 + 1 = 0does not have any rational or real solution. A field containing F is called an algebraic closure of F if it is algebraic over F (roughly speaking, not too big compared to F) and is algebraically closed (big enough to contain solutions of all polynomial equations).
By the above, C is an algebraic closure of R. The situation that the algebraic closure is a finite extension of the field F is quite special: by the Artin-Schreier theorem, the degree of this extension is necessarily 2, and F is elementarily equivalent to R. Such fields are also known as real closed fields.
Any field F has an algebraic closure, which is moreover unique up to (non-unique) isomorphism. It is commonly referred to as the algebraic closure and denoted F. For example, the algebraic closure Q of Q is called the field of algebraic numbers. The field F is usually rather implicit since its construction requires the ultrafilter lemma, a set-theoretic axiom that is weaker than the axiom of choice. In this regard, the algebraic closure of Fq, is exceptionally simple. It is the union of the finite fields containing Fq (the ones of order qn). For any algebraically closed field F of characteristic 0, the algebraic closure of the field F((t)) of Laurent series is the field of Puiseux series, obtained by adjoining roots of t.


== Fields with additional structure ==
Since fields are ubiquitous in mathematics and beyond, several refinements of the concept have been adapted to the needs of particular mathematical areas.


=== Ordered fields ===

A field F is called an ordered field if any two elements can be compared, so that x + y ≥ 0 and xy ≥ 0 whenever x ≥ 0 and y ≥ 0. For example, the real numbers form an ordered field, with the usual ordering ≥. The Artin-Schreier theorem states that a field can be ordered if and only if it is a formally real field, which means that any quadratic equation

  
    
      
        
          x
          
            1
          
          
            2
          
        
        +
        
          x
          
            2
          
          
            2
          
        
        +
        ⋯
        +
        
          x
          
            n
          
          
            2
          
        
        =
        0
      
    
    {\displaystyle x_{1}^{2}+x_{2}^{2}+\dots +x_{n}^{2}=0}
  only has the solution x1 = x2 = ⋯ = xn = 0. The set of all possible orders on a fixed field F is isomorphic to the set of ring homomorphisms from the Witt ring W(F) of quadratic forms over F, to Z.An Archimedean field is an ordered field such that for each element there exists a finite expression

1 + 1 + ⋯ + 1whose value is greater than that element, that is, there are no infinite elements. Equivalently, the field contains no infinitesimals (elements smaller than all rational numbers); or, yet equivalent, the field is isomorphic to a subfield of R.

An ordered field is Dedekind-complete if all upper bounds, lower bounds (see Dedekind cut) and limits, which should exist, do exist. More formally, each bounded subset of F is required to have a least upper bound. Any complete field is necessarily Archimedean, since in any non-Archimedean field there is neither a greatest infinitesimal nor a least positive rational, whence the sequence 1/2, 1/3, 1/4, ..., every element of which is greater than every infinitesimal, has no limit.
Since every proper subfield of the reals also contains such gaps, R is the unique complete ordered field, up to isomorphism. Several foundational results in calculus follow directly from this characterization of the reals.
The hyperreals R* form an ordered field that is not Archimedean. It is an extension of the reals obtained by including infinite and infinitesimal numbers. These are larger, respectively smaller than any real number. The hyperreals form the foundational basis of non-standard analysis.


=== Topological fields ===
Another refinement of the notion of a field is a topological field, in which the set F is a topological space, such that all operations of the field (addition, multiplication, the maps a ↦ −a and a ↦ a−1) are continuous maps with respect to the topology of the space.
The topology of all the fields discussed below is induced from a metric, i.e., a function

d : F × F → R,that measures a distance between any two elements of F.
The completion of F is another field in which, informally speaking, the ""gaps"" in the original field F are filled, if there are any. For example, any irrational number x, such as x = √2, is a ""gap"" in the rationals Q in the sense that it is a real number that can be approximated arbitrarily closely by rational numbers p/q, in the sense that distance of x and p/q given by the absolute value | x − p/q | is as small as desired.
The following table lists some examples of this construction. The fourth column shows an example of a zero sequence, i.e., a sequence whose limit (for n → ∞) is zero.

The field Qp is used in number theory and p-adic analysis. The algebraic closure Qp carries a unique norm extending the one on Qp, but is not complete. The completion of this algebraic closure, however, is algebraically closed. Because of its rough analogy to the complex numbers, it is sometimes called the field of complex p-adic numbers and is denoted by Cp.


==== Local fields ====
The following topological fields are called local fields:
finite extensions of Qp (local fields of characteristic zero)
finite extensions of Fp((t)), the field of Laurent series over Fp (local fields of characteristic p).These two types of local fields share some fundamental similarities. In this relation, the elements p ∈ Qp and t ∈ Fp((t)) (referred to as uniformizer) correspond to each other. The first manifestation of this is at an elementary level: the elements of both fields can be expressed as power series in the uniformizer, with coefficients in Fp. (However, since the addition in Qp is done using carrying, which is not the case in Fp((t)), these fields are not isomorphic.) The following facts show that this superficial similarity goes much deeper:

Any first-order statement that is true for almost all Qp is also true for almost all Fp((t)). An application of this is the Ax-Kochen theorem describing zeros of homogeneous polynomials in Qp.
Tamely ramified extensions of both fields are in bijection to one another.
Adjoining arbitrary p-power roots of p (in Qp), respectively of t (in Fp((t))), yields (infinite) extensions of these fields known as perfectoid fields. Strikingly, the Galois groups of these two fields are isomorphic, which is the first glimpse of a remarkable parallel between these two fields: 


=== Differential fields ===
Differential fields are fields equipped with a derivation, i.e., allow to take derivatives of elements in the field. For example, the field R(X), together with the standard derivative of polynomials forms a differential field. These fields are central to differential Galois theory, a variant of Galois theory dealing with linear differential equations.


== Galois theory ==

Galois theory studies algebraic extensions of a field by studying the symmetry in the arithmetic operations of addition and multiplication. An important notion in this area is that of finite Galois extensions F / E, which are, by definition, those that are separable and normal. The primitive element theorem shows that finite separable extensions are necessarily simple, i.e., of the form

F = E[X] / f (X),where f is an irreducible polynomial (as above). For such an extension, being normal and separable means that all zeros of f are contained in F and that f has only simple zeros. The latter condition is always satisfied if E has characteristic 0.
For a finite Galois extension, the Galois group Gal(F/E) is the group of field automorphisms of F that are trivial on E (i.e., the bijections σ : F → F that preserve addition and multiplication and that send elements of E to themselves). The importance of this group stems from the fundamental theorem of Galois theory, which constructs an explicit one-to-one correspondence between the set of subgroups of Gal(F/E) and the set of intermediate extensions of the extension F/E. By means of this correspondence, group-theoretic properties translate into facts about fields. For example, if the Galois group of a Galois extension as above is not solvable (cannot be built from abelian groups), then the zeros of f cannot be expressed in terms of addition, multiplication, and radicals, i.e., expressions involving 
  
    
      
        
          
             
            
              n
            
          
        
      
    
    {\displaystyle {\sqrt[{n}]{\ }}}
  . For example, the symmetric groups Sn is not solvable for n ≥ 5. Consequently, as can be shown, the zeros of the following polynomials are not expressible by sums, products, and radicals. For the latter polynomial, this fact is known as the Abel–Ruffini theorem:

f(X) = X5 − 4X + 2 (and E = Q),
f(X) = X n + an−1X n−1 + ⋯ + a0 (where f is regarded as a polynomial in E(a0, ..., an−1), for some indeterminates ai, E is any field, and n ≥ 5).The tensor product of fields is not usually a field. For example, a finite extension F / E of degree n is a Galois extension if and only if there is an isomorphism of F-algebras

F ⊗E F ≅ Fn.This fact is the beginning of Grothendieck's Galois theory, a far-reaching extension of Galois theory applicable to algebro-geometric objects.


== Invariants of fields ==
Basic invariants of a field F include the characteristic and the transcendence degree of F over its prime field. The latter is defined as the maximal number of elements in F that are algebraically independent over the prime field. Two algebraically closed fields E and F are isomorphic precisely if these two data agree. This implies that any two uncountable algebraically closed fields of the same cardinality and the same characteristic are isomorphic. For example, Qp, Cp and C are isomorphic (but not isomorphic as topological fields).


=== Model theory of fields ===
In model theory, a branch of mathematical logic, two fields E and F are called elementarily equivalent if every mathematical statement that is true for E is also true for F and conversely. The mathematical statements in question are required to be first-order sentences (involving 0, 1, the addition and multiplication). A typical example, for n > 0, n an integer, is

φ(E) = ""any polynomial of degree n in E has a zero in E""The set of such formulas for all n expresses that  E is algebraically closed.
The Lefschetz principle states that C is elementarily equivalent to any algebraically closed field F of characteristic zero. Moreover, any fixed statement φ holds in C if and only if it holds in any algebraically closed field of sufficiently high characteristic.If U is an ultrafilter on a set I, and Fi is a field for every i in I, the ultraproduct of the Fi with respect to U is a field. It is denoted by

ulimi→∞ Fi,since it behaves in several ways as a limit of the fields Fi: Łoś's theorem states that any first order statement that holds for all but finitely many Fi, also holds for the ultraproduct. Applied to the above sentence φ, this shows that there is an isomorphism

  
    
      
        
          ulim
          
            p
            →
            ∞
          
        
        ⁡
        
          
            
              
                F
              
              ¯
            
          
          
            p
          
        
        ≅
        
          C
        
        .
      
    
    {\displaystyle \operatorname {ulim} _{p\to \infty }{\overline {\mathbf {F} }}_{p}\cong \mathbf {C} .}
  The Ax–Kochen theorem mentioned above also follows from this and an isomorphism of the ultraproducts (in both cases over all primes p)

ulimp Qp ≅ ulimp Fp((t)).In addition, model theory also studies the logical properties of various other types of fields, such as real closed fields or exponential fields (which are equipped with an exponential function exp : F → Fx).


=== The absolute Galois group ===
For fields that are not algebraically closed (or not separably closed), the absolute Galois group Gal(F) is fundamentally important: extending the case of finite Galois extensions outlined above, this group governs all finite separable extensions of F. By elementary means, the group Gal(Fq) can be shown to be the Prüfer group, the profinite completion of Z. This statement subsumes the fact that the only algebraic extensions of Gal(Fq) are the fields Gal(Fqn) for n > 0, and that the Galois groups of these finite extensions are given by

Gal(Fqn / Fq) = Z/nZ.A description in terms of generators and relations is also known for the Galois groups of p-adic number fields (finite extensions of Qp).Representations of Galois groups and of related groups such as the Weil group are fundamental in many branches of arithmetic, such as the Langlands program. The cohomological study of such representations is done using Galois cohomology. For example, the Brauer group, which is classically defined as the group of central simple F-algebras, can be reinterpreted as a Galois cohomology group, namely

Br(F) = H2(F, Gm).


=== K-theory ===
Milnor K-theory is defined as

  
    
      
        
          K
          
            n
          
          
            M
          
        
        (
        F
        )
        =
        
          F
          
            ×
          
        
        ⊗
        ⋯
        ⊗
        
          F
          
            ×
          
        
        
          /
        
        
          ⟨
          
            x
            ⊗
            (
            1
            −
            x
            )
            ∣
            x
            ∈
            F
            ∖
            {
            0
            ,
            1
            }
          
          ⟩
        
        .
      
    
    {\displaystyle K_{n}^{M}(F)=F^{\times }\otimes \cdots \otimes F^{\times }/\left\langle x\otimes (1-x)\mid x\in F\setminus \{0,1\}\right\rangle .}
  The norm residue isomorphism theorem, proved around 2000 by Vladimir Voevodsky, relates this to Galois cohomology by means of an isomorphism

  
    
      
        
          K
          
            n
          
          
            M
          
        
        (
        F
        )
        
          /
        
        p
        =
        
          H
          
            n
          
        
        (
        F
        ,
        
          μ
          
            l
          
          
            ⊗
            n
          
        
        )
        .
      
    
    {\displaystyle K_{n}^{M}(F)/p=H^{n}(F,\mu _{l}^{\otimes n}).}
  Algebraic K-theory is related to the group of invertible matrices with coefficients the given field. For example, the process of taking the determinant of an invertible matrix leads to an isomorphism K1(F) = F×. Matsumoto's theorem shows that K2(F) agrees with K2M(F). In higher degrees, K-theory diverges from Milnor K-theory and remains hard to compute in general.


== Applications ==


=== Linear algebra and commutative algebra ===
If a ≠ 0, then the equation

ax = bhas a unique solution x in a field F, namely 
  
    
      
        x
        =
        
          a
          
            −
            1
          
        
        b
        .
      
    
    {\displaystyle x=a^{-1}b.}
   This immediate consequence of the definition of a field is fundamental in linear algebra. For example, it is an essential ingredient of Gaussian elimination and of the proof that any vector space has a basis.The theory of modules (the analogue of vector spaces over rings instead of fields) is much more complicated, because the above equation may have several or no solutions. In particular systems of linear equations over a ring are much more difficult to solve than in the case of fields, even in the specially simple case of the ring 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   of the integers.


=== Finite fields: cryptography and coding theory ===

A widely applied cryptographic routine uses the fact that discrete exponentiation, i.e., computing

an = a ⋅ a ⋅ ⋯ ⋅ a (n factors, for an integer n ≥ 1)in a (large) finite field Fq can be performed much more efficiently than the discrete logarithm, which is the inverse operation, i.e., determining the solution n to an equation

an = b.In elliptic curve cryptography, the multiplication in a finite field is replaced by the operation of adding points on an elliptic curve, i.e., the solutions of an equation of the form

y2 = x3 + ax + b.Finite fields are also used in coding theory and combinatorics.


=== Geometry: field of functions ===

Functions on a suitable topological space X into a field k can be added and multiplied pointwise, e.g., the product of two functions is defined by the product of their values within the domain:

(f ⋅ g)(x) = f(x) ⋅ g(x).This makes these functions a k-commutative algebra.
For having a field of functions, one must consider algebras of functions that are integral domains. In this case the ratios of two functions, i.e., expressions of the form

  
    
      
        
          
            
              f
              (
              x
              )
            
            
              g
              (
              x
              )
            
          
        
        ,
      
    
    {\displaystyle {\frac {f(x)}{g(x)}},}
  form a field, called field of functions.
This occurs in two main cases. When X is a complex manifold X. In this case, one considers the algebra of holomorphic functions, i.e., complex differentiable functions. Their ratios form the field of meromorphic functions on X.
The function field of an algebraic variety X (a geometric object defined as the common zeros of polynomial equations) consists of ratios of regular functions, i.e., ratios of polynomial functions on the variety. The function field of the n-dimensional space over a field k is k(x1, ..., xn), i.e., the field consisting of ratios of polynomials in n indeterminates. The function field of X is the same as the one of any open dense subvariety. In other words, the function field is insensitive to replacing X by a (slightly) smaller subvariety.
The function field is invariant under isomorphism and birational equivalence of varieties. It is therefore an important tool for the study of abstract algebraic varieties and for the classification of algebraic varieties. For example, the dimension, which equals the transcendence degree of k(X), is invariant under birational equivalence. For curves (i.e., the dimension is one), the function field k(X) is very close to X: if X is smooth and proper (the analogue of being compact), X can be reconstructed, up to isomorphism, from its field of functions. In higher dimension the function field remembers less, but still decisive information about X. The study of function fields and their geometric meaning in higher dimensions is referred to as birational geometry. The minimal model program attempts to identify the simplest (in a certain precise sense) algebraic varieties with a prescribed function field.


=== Number theory: global fields ===
Global fields are in the limelight in algebraic number theory and arithmetic geometry.
They are, by definition, number fields (finite extensions of Q) or function fields over Fq (finite extensions of Fq(t)). As for local fields, these two types of fields share several similar features, even though they are of characteristic 0 and positive characteristic, respectively. This function field analogy can help to shape mathematical expectations, often first by understanding questions about function fields, and later treating the number field case. The latter is often more difficult. For example, the Riemann hypothesis concerning the zeros of the Riemann zeta function (open as of 2017) can be regarded as being parallel to the Weil conjectures (proven in 1974 by Pierre Deligne).

Cyclotomic fields are among the most intensely studied number fields. They are of the form Q(ζn), where ζn is a primitive n-th root of unity, i.e., a complex number satisfying ζn = 1 and ζm ≠ 1 for all m < n. For n being a regular prime, Kummer used cyclotomic fields to prove Fermat's Last Theorem, which asserts the non-existence of rational nonzero solutions to the equation

xn + yn = zn.Local fields are completions of global fields. Ostrowski's theorem asserts that the only completions of Q, a global field, are the local fields Qp and R. Studying arithmetic questions in global fields may sometimes be done by looking at the corresponding questions locally. This technique is called the local-global principle. For example, the Hasse–Minkowski theorem reduces the problem of finding rational solutions of quadratic equations to solving these equations in R and Qp, whose solutions can easily be described.Unlike for local fields, the Galois groups of global fields are not known. Inverse Galois theory studies the (unsolved) problem whether any finite group is the Galois group Gal(F/Q) for some number field F. Class field theory describes the abelian extensions, i.e., ones with abelian Galois group, or equivalently the abelianized Galois groups of global fields. A classical statement, the Kronecker–Weber theorem, describes the maximal abelian Qab extension of Q: it is the field

Q(ζn, n ≥ 2)obtained by adjoining all primitive n-th roots of unity. Kronecker's Jugendtraum asks for a similarly explicit description of Fab of general number fields F. For imaginary quadratic fields, 
  
    
      
        F
        =
        
          Q
        
        (
        
          
            −
            d
          
        
        )
      
    
    {\displaystyle F=\mathbf {Q} ({\sqrt {-d}})}
  , d > 0, the theory of complex multiplication describes Fab using elliptic curves. For general number fields, no such explicit description is known.


== Related notions ==
In addition to the additional structure that fields may enjoy, fields admit various other related notions. Since in any field 0 ≠ 1, any field has at least two elements. Nonetheless, there is a concept of field with one element, which is suggested to be a limit of the finite fields Fp, as p tends to 1. In addition to division rings, there are various other weaker algebraic structures related to fields such as quasifields, near-fields and semifields.
There are also proper classes with field structure, which are sometimes called Fields, with a capital F. The surreal numbers form a Field containing the reals, and would be a field except for the fact that they are a proper class, not a set. The nimbers, a concept from game theory, form such a Field as well.


=== Division rings ===

Dropping one or several axioms in the definition of a field leads to other algebraic structures. As was mentioned above, commutative rings satisfy all field axioms except for the existence of multiplicative inverses. Dropping instead commutativity of multiplication leads to the concept of a division ring or skew field; sometimes associativity is weakened as well.  The only division rings that are finite-dimensional R-vector spaces are R itself, C (which is a field), and the quaternions H (in which multiplication is non-commutative). This result is known as the Frobenius theorem. The octonions O, for which multiplication is neither commutative nor associative, is a normed alternative division algebra, but is not a division ring. This fact was proved using methods of algebraic topology in 1958 by Michel Kervaire, Raoul Bott, and John Milnor. The non-existence of an odd-dimensional division algebra is more classical. It can be deduced from the hairy ball theorem illustrated at the right.


== Notes ==


== References ==


== External links =="
fd96656195,Set (mathematics),"A set is the mathematical model for a collection of different things; a set contains elements or members, which can be mathematical objects of any kind: numbers, symbols, points in space, lines, other geometrical shapes, variables, or even other sets.  The set with no element is the empty set; a set with a single element is a singleton.  A set may have a finite number of elements or be an infinite set.  Two sets are equal if they have precisely the same elements.Sets are ubiquitous in modern mathematics. Indeed, set theory, more specifically Zermelo–Fraenkel set theory, has been the standard way to provide rigorous foundations for all branches of mathematics since the first half of the 20th century.


== History ==

The concept of a set emerged in mathematics at the end of the 19th century. The German word for set, Menge, was coined by Bernard Bolzano in his work Paradoxes of the Infinite. Georg Cantor, one of the founders of set theory,   gave the following definition at the beginning of his Beiträge zur Begründung der transfiniten Mengenlehre:A set is a gathering together into a whole of definite, distinct objects of our perception or our thought—which are called elements of the set.
Bertrand Russell called a set a class:
When mathematicians deal with what they call a manifold, aggregate, Menge, ensemble, or some equivalent name, it is  common, especially where the number of terms involved is finite, to regard the object in question (which is in fact a class) as defined by the enumeration of its terms, and as consisting possibly of a single term, which in that case is the class.


=== Naive set theory ===

The foremost property of a set is that it can have elements, also called members. Two sets are equal when they have the same elements. More precisely, sets A and B are equal if every element of A is an element of B, and every element of B is an element of A; this property is called the extensionality of sets.The simple concept of a set has proved enormously useful in mathematics, but paradoxes arise if no restrictions are placed on how sets can be constructed:

Russell's paradox shows that the ""set of all sets that do not contain themselves"", i.e., {x | x is a set and x ∉ x}, cannot exist.
Cantor's paradox shows that ""the set of all sets"" cannot exist.Naïve set theory defines a set as any well-defined collection of distinct elements, but problems arise from the vagueness of the term well-defined.


=== Axiomatic set theory ===
In subsequent efforts to resolve these paradoxes since the time of the original formulation of naïve set theory, the properties of sets have been defined by axioms. Axiomatic set theory takes the concept of a set as a primitive notion. The purpose of the axioms is to provide a basic framework from which to deduce the truth or falsity of particular mathematical propositions (statements) about sets, using first-order logic. According to Gödel's incompleteness theorems however, it is not possible to use first-order logic to prove any such particular axiomatic set theory is free from paradox.


== How sets are defined and set notation ==
Mathematical texts commonly denote sets by capital letters in italic, such as A, B, C. A set may also be called a collection or family, especially when its elements are themselves sets.


=== Roster notation ===
Roster or enumeration notation defines a set by listing its elements between curly brackets, separated by commas:

In a set, all that matters is whether each element is in it or not, so the ordering of the elements in roster notation is irrelevant (in contrast, in a sequence, a tuple, or a permutation of a set, the ordering of the terms matters). For example, {2, 4, 6} and {4, 6, 4, 2} represent the same set.For sets with many elements, especially those following an implicit pattern, the list of members can be abbreviated using an ellipsis '...'. For instance, the set of the first thousand positive integers may be specified in roster notation as


==== Infinite sets in roster notation ====
An infinite set is a set with an endless list of elements. To describe an infinite set in roster notation, an ellipsis is placed at the end of the list, or at both ends, to indicate that the list continues forever. For example, the set of nonnegative integers is

and the set of all integers is


=== Semantic definition ===
Another way to define a set is to use a rule to determine what the elements are:

Such a definition is called a semantic description.


=== Set-builder notation ===

Set-builder notation specifies a set as a selection from a larger set, determined by a condition on the elements. For example, a set F can be defined as follows:

In this notation, the vertical bar ""|"" means ""such that"", and the description can be interpreted as ""F is the set of all numbers n such that n is an integer in the range from 0 to 19 inclusive"". Some authors use a colon "":"" instead of the vertical bar.


=== Classifying methods of definition ===
Philosophy uses specific terms to classify types of definitions:

An intensional definition uses a rule to determine membership. Semantic definitions and definitions using set-builder notation are examples.
An extensional definition describes a set by listing all its elements. Such definitions are also called enumerative.
An ostensive definition is one that describes a set by giving examples of elements; a roster involving an ellipsis would be an example.


== Membership ==

If B is a set and x is an element of B, this is written in shorthand as x ∈ B, which can also be read as ""x belongs to B"", or ""x is in B"". The statement ""y is not an element of B"" is written as y ∉ B, which can also be read as ""y is not in B"".For example, with respect to the sets A = {1, 2, 3, 4}, B = {blue, white, red}, and F = {n | n is an integer, and 0 ≤ n ≤ 19},


== The empty set ==

The empty set (or null set) is the unique set that has no members. It is denoted ∅ or 
  
    
      
        ∅
      
    
    {\displaystyle \emptyset }
   or { } or ϕ (or ϕ).


== Singleton sets ==

A singleton set is a set with exactly one element; such a set may also be called a unit set. Any such set can be written as {x}, where x is the element.
The set {x} and the element x mean different things; Halmos draws the analogy that a box containing a hat is not the same as the hat.


== Subsets ==

If every element of set A is also in B, then A is described as being a subset of B, or contained in B, written A ⊆ B, or B ⊇ A. The latter notation may be read B contains A, B includes A, or B is a superset of A. The relationship between sets established by ⊆ is called inclusion or containment. Two sets are equal if they contain each other: A ⊆ B and B ⊆ A is equivalent to A = B.If A is a subset of B, but A is not equal to B, then A is called a proper subset of B. This can be written A ⊊ B. Likewise, B ⊋ A means B is a proper superset of A, i.e. B contains A, and is not equal to A.
A third pair of operators ⊂ and ⊃ are used differently by different authors: some authors use A ⊂ B and B ⊃ A to mean A is any subset of B (and not necessarily a proper subset), while others reserve A ⊂ B and B ⊃ A for cases where A is a proper subset of B.Examples:

The set of all humans is a proper subset of the set of all mammals.
{1, 3} ⊂ {1, 2, 3, 4}.
{1, 2, 3, 4} ⊆ {1, 2, 3, 4}.The empty set is a subset of every set, and every set is a subset of itself:
∅ ⊆ A.
A ⊆ A.


== Euler and Venn diagrams ==

An Euler diagram is a graphical representation of a collection of sets; each set is depicted as a planar region enclosed by a loop, with its elements inside. If A is a subset of B, then the region representing A is completely inside the region representing B. If two sets have no elements in common, the regions do not overlap. 
A Venn diagram, in contrast, is a graphical representation of n sets in which the n loops divide the plane into 2n zones such that for each way of selecting some of the n sets (possibly all or none), there is a zone for the elements that belong to all the selected sets and none of the others. For example, if the sets are A, B, and C, there should be a zone for the elements that are inside A and C and outside B (even if such elements do not exist).


== Special sets of numbers in mathematics ==

There are sets of such mathematical importance, to which mathematicians refer so frequently, that they have acquired special names and notational conventions to identify them. 
Many of these important sets are represented in mathematical texts using bold (e.g. 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
  ) or blackboard bold (e.g. 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
  ) typeface. These include

  
    
      
        
          N
        
      
    
    {\displaystyle \mathbf {N} }
   or 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
  , the set of all natural numbers: 
  
    
      
        
          N
        
        =
        {
        0
        ,
        1
        ,
        2
        ,
        3
        ,
        .
        .
        .
        }
      
    
    {\displaystyle \mathbf {N} =\{0,1,2,3,...\}}
   (often, authors exclude 0);

  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   or 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
  , the set of all integers (whether positive, negative or zero): 
  
    
      
        
          Z
        
        =
        {
        .
        .
        .
        ,
        −
        2
        ,
        −
        1
        ,
        0
        ,
        1
        ,
        2
        ,
        3
        ,
        .
        .
        .
        }
      
    
    {\displaystyle \mathbf {Z} =\{...,-2,-1,0,1,2,3,...\}}
  ;

  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbf {Q} }
   or 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  , the set of all rational numbers (that is, the set of all proper and improper fractions): 
  
    
      
        
          Q
        
        =
        
          {
          
            
              
                a
                b
              
            
            ∣
            a
            ,
            b
            ∈
            
              Z
            
            ,
            b
            ≠
            0
          
          }
        
      
    
    {\displaystyle \mathbf {Q} =\left\{{\frac {a}{b}}\mid a,b\in \mathbf {Z} ,b\neq 0\right\}}
  . For example, −7/4 ∈ Q and 5 = 5/1 ∈ Q;

  
    
      
        
          R
        
      
    
    {\displaystyle \mathbf {R} }
   or 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , the set of all real numbers, including all rational numbers and all irrational numbers (which include algebraic numbers such as 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   that cannot be rewritten as fractions, as well as transcendental numbers such as π and e);

  
    
      
        
          C
        
      
    
    {\displaystyle \mathbf {C} }
   or 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
  , the set of all complex numbers: C = {a + bi | a, b ∈ R}, for example, 1 + 2i ∈ C.Each of the above sets of numbers has an infinite number of elements. Each is a subset of the sets listed below it. 
Sets of positive or negative numbers are sometimes denoted by superscript plus and minus signs, respectively. For example, 
  
    
      
        
          
            Q
          
          
            +
          
        
      
    
    {\displaystyle \mathbf {Q} ^{+}}
   represents the set of positive rational numbers.


== Functions ==
A function (or mapping) from a set A to a set B is a rule that assigns to each ""input"" element of A an ""output"" that is an element of B; more formally, a function is a special kind of relation, one that relates each element of A to exactly one element of B. A function is called

injective (or one-to-one) if it maps any two different elements of A to different elements of B,
surjective (or onto) if for every element of B, there is at least one element of A that maps to it, and
bijective (or a one-to-one correspondence) if the function is both injective and surjective — in this case, each element of A is paired with a unique element of B, and each element of B is paired with a unique element of A, so that there are no unpaired elements.An injective function is called an injection, a surjective function is called a surjection, and a bijective function is called a bijection or one-to-one correspondence.


== Cardinality ==

The cardinality of a set S, denoted |S|, is the number of members of S. For example, if B = {blue, white, red}, then |B| = 3. Repeated members in roster notation are not counted, so |{blue, white, red, blue, white}| = 3, too.
More formally, two sets share the same cardinality if there exists a one-to-one correspondence between them.
The cardinality of the empty set is zero.


=== Infinite sets and infinite cardinality ===
The list of elements of some sets is endless, or infinite. For example, the set 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   of natural numbers is infinite. In fact, all the special sets of numbers mentioned in the section above are infinite. Infinite sets have infinite cardinality. 
Some infinite cardinalities are greater than others. Arguably one of the most significant results from set theory is that the set of real numbers has greater cardinality than the set of natural numbers. Sets with cardinality less than or equal to that of 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   are called countable sets; these are either finite sets or countably infinite sets (sets of the same cardinality as 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
  ); some authors use ""countable"" to mean ""countably infinite"". Sets with cardinality strictly greater than that of 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   are called uncountable sets.
However, it can be shown that the cardinality of a straight line (i.e., the number of points on a line) is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.


=== The continuum hypothesis ===

The continuum hypothesis, formulated by Georg Cantor in 1878, is the statement that there is no set with cardinality strictly between the cardinality of the natural numbers and the cardinality of a straight line. In 1963, Paul Cohen proved that the continuum hypothesis is independent of the axiom system ZFC consisting of Zermelo–Fraenkel set theory with the axiom of choice. (ZFC is the most widely-studied version of axiomatic set theory.)


== Power sets ==

The power set of a set S is the set of all subsets of S. The empty set and S itself are elements of the power set of S, because these are both subsets of S. For example, the power set of {1, 2, 3} is {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}. The power set of a set S is commonly written as P(S) or 2S.If S has n elements, then P(S) has 2n elements. For example, {1, 2, 3} has three elements, and its power set has 23 = 8 elements, as shown above.
If S is infinite (whether countable or uncountable), then P(S) is uncountable. Moreover, the power set is always strictly ""bigger"" than the original set, in the sense that any attempt to pair up the elements of S with the elements of P(S) will leave some elements of P(S) unpaired. (There is never a bijection from S onto P(S).)


== Partitions ==

A partition of a set S is a set of nonempty subsets of S, such that every element x in S is in exactly one of these subsets. That is, the subsets are pairwise disjoint (meaning any two sets of the partition contain no element in common), and the union of all the subsets of the partition is S.


== Basic operations ==

Suppose that a universal set U (a set containing all elements being discussed) has been fixed, and that A is a subset of U.

The complement of A is the set of all elements (of U) that do not belong to A.  It may be denoted Ac or A′.  In set-builder notation, 
  
    
      
        
          A
          
            c
          
        
        =
        {
        a
        ∈
        U
        :
        a
        ∉
        A
        }
      
    
    {\displaystyle A^{\text{c}}=\{a\in U:a\notin A\}}
  .  The complement may also be called the absolute complement to distinguish it from the relative complement below.  Example: If the universal set is taken to be the set of integers, then the complement of the set of even integers is the set of odd integers.

Given any two sets A and B, 

their union A ∪ B is the set of all things that are members of A or B or both.
their intersection A ∩ B is the set of all things that are members of both A and B.  If A ∩ B = ∅, then A and B are said to be disjoint.
the set difference A \ B (also written A − B) is the set of all things that belong to A but not B.  Especially when B is a subset of A, it is also called the relative complement of B in A.
their symmetric difference A Δ B is the set of all things that belong to A or B but not both.  One has 
  
    
      
        A
        
        Δ
        
        B
        =
        (
        A
        ∖
        B
        )
        ∪
        (
        B
        ∖
        A
        )
      
    
    {\displaystyle A\,\Delta \,B=(A\setminus B)\cup (B\setminus A)}
  .
their cartesian product A × B is the set of all ordered pairs (a,b) such that a is an element of A and b is an element of B.Examples:

{1, 2, 3} ∪ {3, 4, 5} = {1, 2, 3, 4, 5}.
{1, 2, 3} ∩ {3, 4, 5} = {3}.
{1, 2, 3} − {3, 4, 5} = {1, 2}.
{1, 2, 3} Δ {3, 4, 5} = {1, 2, 4, 5}.
{a, b} × {1, 2, 3} = {(a,1), (a,2), (a,3), (b,1), (b,2), (b,3)}.The operations above satisfy many identities.  For example, one of De Morgan's laws states that (A ∪ B)′ = A′ ∩ B′ (that is, the elements outside the union of A and B are the elements that are outside A and outside B).
The cardinality of A × B is the product of the cardinalities of A and B.
(This is an elementary fact when A and B are finite.  When one or both are infinite, multiplication of cardinal numbers is defined to make this true.)
The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring and intersection as the multiplication of the ring.


== Applications ==
Sets are ubiquitous in modern mathematics. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.
One of the main applications of naive set theory is in the construction of relations. A relation from a domain A to a codomain B is a subset of the Cartesian product A × B. For example, considering the set S = {rock, paper, scissors} of shapes in the game of the same name, the relation ""beats"" from S to S is the set B = {(scissors,paper), (paper,rock), (rock,scissors)}; thus x beats y in the game if the pair (x,y) is a member of B. Another example is the set F of all pairs (x, x2), where x is real. This relation is a subset of R × R, because the set of all squares is subset of the set of all real numbers. Since for every x in R, one and only one pair (x,...) is found in F, it is called a function. In functional notation, this relation can be written as F(x) = x2.


== Principle of inclusion and exclusion ==

The inclusion–exclusion principle is a technique for counting the elements in a union of two finite sets in terms of the sizes of the two sets and their intersection. It can be expressed symbolically as

A more general form of the principle gives the cardinality of any finite union of finite sets:


== See also ==


== Notes ==


== References ==
Dauben, Joseph W. (1979). Georg Cantor: His Mathematics and Philosophy of the Infinite. Boston: Harvard University Press. ISBN 0-691-02447-2.
Halmos, Paul R. (1960). Naive Set Theory. Princeton, N.J.: Van Nostrand. ISBN 0-387-90092-6.
Stoll, Robert R. (1979). Set Theory and Logic. Mineola, N.Y.: Dover Publications. ISBN 0-486-63829-4.
Velleman, Daniel (2006). How To Prove It: A Structured Approach. Cambridge University Press. ISBN 0-521-67599-5.


== External links ==
 The dictionary definition of set at Wiktionary
Cantor's ""Beiträge zur Begründung der transfiniten Mengenlehre"" (in German)"
0ec5d16e94,Engineering mathematics,"Engineering mathematics is a branch of applied mathematics concerning mathematical methods and techniques that are typically used in engineering and industry. Along with fields like engineering physics and engineering geology, both of which may belong in the wider category engineering science, engineering mathematics is an interdisciplinary subject motivated by engineers' needs both for practical, theoretical and other considerations outwith their specialization, and to deal with constraints to be effective in their work.


== Description ==
Historically, engineering mathematics consisted mostly of applied analysis, most notably: differential equations; real and complex analysis (including vector and tensor analysis); approximation theory (broadly construed, to include asymptotic, variational, and perturbative methods, representations, numerical analysis); Fourier analysis; potential theory; as well as linear algebra and applied probability, outside of analysis.  These areas of mathematics were intimately tied to the development of Newtonian physics, and the mathematical physics of that period.  This history also left a legacy: until the early 20th century subjects such as classical mechanics were often taught in applied mathematics departments at American universities, and fluid mechanics may still be taught in (applied) mathematics as well as engineering departments.The success of modern numerical computer methods and software has led to the emergence of computational mathematics, computational science, and computational engineering (the last two are sometimes lumped together and abbreviated as CS&E), which occasionally use high-performance computing for the simulation of phenomena and the solution of problems in the sciences and engineering. These are often considered interdisciplinary fields, but are also of interest to engineering mathematics.Specialized branches include engineering optimization and engineering statistics.
Engineering mathematics in tertiary education typically consists of mathematical methods and models courses.


== See also ==
Industrial mathematics
Control theory, a mathematical discipline concerned with engineering
Further mathematics and additional mathematics, A-level mathematics courses with similar content
Mathematical methods in electronics, signal processing and radio engineering


== References =="
b66194a802,Mathematical object,"A mathematical object is an abstract concept arising in mathematics.
In the usual language of mathematics, an object is anything that has been (or could be) formally defined, and with which one may do deductive reasoning and mathematical proofs. Typically, a mathematical object can be a value that can be assigned to a variable, and therefore can be involved in formulas. Commonly encountered mathematical objects include numbers, sets, functions, expressions, geometric objects, transformations of other mathematical objects, and spaces. Mathematical objects can be very complex; for example, theorems, proofs, and even theories are considered as mathematical objects in proof theory.
The ontological status of mathematical objects has been the subject of much investigation and debate by philosophers of mathematics.


== List of mathematical objects by branch ==
Number theory
numbers, operations
Combinatorics
permutations, derangements, combinations
Set theory
sets, set partitions
functions, and relations
Geometry
points, lines, line segments,
polygons (triangles, squares, pentagons, hexagons, ...), circles, ellipses, parabolas, hyperbolas,
polyhedra (tetrahedrons, cubes, octahedrons, dodecahedrons, icosahedrons), spheres, ellipsoids, paraboloids, hyperboloids, cylinders, cones.
Graph theory
graphs, trees, nodes, edges
Topology
topological spaces and manifolds.
Linear algebra
scalars, vectors, matrices, tensors.
Abstract algebra
groups,
rings, modules,
fields, vector spaces,
group-theoretic lattices, and order-theoretic lattices.Categories are simultaneously homes to mathematical objects and mathematical objects in their own right. In proof theory, proofs and theorems are also mathematical objects.


== See also ==
Abstract object
Mathematical structure


== References ==
Azzouni, J., 1994. Metaphysical Myths, Mathematical Practice. Cambridge University Press.
Burgess, John, and Rosen, Gideon, 1997. A Subject with No Object. Oxford Univ. Press.
Davis, Philip and Reuben Hersh, 1999 [1981]. The Mathematical Experience. Mariner Books: 156–62.
Gold, Bonnie, and Simons, Roger A., 2011. Proof and Other Dilemmas: Mathematics and Philosophy. Mathematical Association of America.
Hersh, Reuben, 1997. What is Mathematics, Really?  Oxford University Press.
Sfard, A., 2000, ""Symbolizing mathematical reality into being,  Or how mathematical discourse and mathematical objects create each other,"" in Cobb, P., et al., Symbolizing and communicating in mathematics classrooms:  Perspectives on discourse, tools and instructional design. Lawrence Erlbaum.
Stewart Shapiro, 2000. Thinking about mathematics: The philosophy of mathematics.  Oxford University Press.


== External links ==
Stanford Encyclopedia of Philosophy: ""Abstract Objects""—by Gideon Rosen.
Wells, Charles, ""Mathematical Objects.""
AMOF: The Amazing Mathematical Object Factory
Mathematical Object Exhibit"
6c6420de89,Sheaf (mathematics),"In mathematics, a sheaf is a tool for systematically tracking data (such as sets, abelian groups, rings) attached to the open sets of a topological space and defined locally with regard to them. For example, for each open set, the data could be the ring of continuous functions defined on that open set. Such data is well behaved in that it can be restricted to smaller open sets, and also the data assigned to an open set is equivalent to all collections of compatible data assigned to collections of smaller open sets covering the original open set (intuitively, every piece of data is the sum of its parts).
The field of mathematics that studies sheaves is called sheaf theory.
Sheaves are understood conceptually as general and abstract objects. Their correct definition is rather technical. They are specifically defined as sheaves of sets or as sheaves of rings, for example, depending on the type of data assigned to the open sets.
There are also maps (or morphisms) from one sheaf to another; sheaves (of a specific type, such as sheaves of abelian groups) with their morphisms on a fixed topological space form a category. On the other hand, to each continuous map there is associated both a direct image functor, taking sheaves and their morphisms on the domain to sheaves and morphisms on the codomain, and an inverse image functor operating in the opposite direction. These functors, and certain variants of them, are essential parts of sheaf theory.
Due to their general nature and versatility, sheaves have several applications in topology and especially in algebraic and differential geometry. First, geometric structures such as that of a differentiable manifold or a scheme can be expressed in terms of a sheaf of rings on the space. In such contexts, several geometric constructions such as vector bundles or divisors are naturally specified in terms of sheaves. Second, sheaves provide the framework for a very general cohomology theory, which encompasses also the ""usual"" topological cohomology theories such as singular cohomology. Especially in algebraic geometry and the theory of complex manifolds, sheaf cohomology provides a powerful link between topological and geometric properties of spaces. Sheaves also provide the basis for the theory of D-modules, which provide applications to the theory of differential equations. In addition, generalisations of sheaves to more general settings than topological spaces, such as Grothendieck topology, have provided applications to mathematical logic and to number theory.


== Definitions and examples ==
In many mathematical branches, several structures defined on a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   (e.g., a differentiable manifold) can be naturally localised or restricted to open subsets 
  
    
      
        U
        ⊂
        X
      
    
    {\displaystyle U\subset X}
  : typical examples include continuous real-valued or complex-valued functions, 
  
    
      
        n
      
    
    {\displaystyle n}
  -times differentiable (real-valued or complex-valued) functions, bounded real-valued functions, vector fields, and sections of any vector bundle on the space. The ability to restrict data to smaller open subsets gives rise to the concept of presheaves. Roughly speaking, sheaves are then those presheaves, where local data can be glued to global data.


=== Presheaves ===

Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a topological space. A presheaf of sets 
  
    
      
        F
      
    
    {\displaystyle F}
   on 
  
    
      
        X
      
    
    {\displaystyle X}
   consists of the following data:

For each open set 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
  , a set 
  
    
      
        F
        (
        U
        )
      
    
    {\displaystyle F(U)}
  . This set is also denoted 
  
    
      
        Γ
        (
        U
        ,
        F
        )
      
    
    {\displaystyle \Gamma (U,F)}
  . The elements in this set are called the sections of 
  
    
      
        F
      
    
    {\displaystyle F}
   over 
  
    
      
        U
      
    
    {\displaystyle U}
  . The sections of 
  
    
      
        F
      
    
    {\displaystyle F}
   over 
  
    
      
        X
      
    
    {\displaystyle X}
   are called the global sections of 
  
    
      
        F
      
    
    {\displaystyle F}
  .
For each inclusion of open sets 
  
    
      
        V
        ⊆
        U
      
    
    {\displaystyle V\subseteq U}
  , a function 
  
    
      
        
          res
          
            V
            ,
            U
          
        
        :
        F
        (
        U
        )
        →
        F
        (
        V
        )
      
    
    {\displaystyle \operatorname {res} _{V,U}\colon F(U)\rightarrow F(V)}
  . In view of many of the examples below, the morphisms 
  
    
      
        
          
            res
          
          
            V
            ,
            U
          
        
      
    
    {\displaystyle {\text{res}}_{V,U}}
   are called restriction morphisms. If 
  
    
      
        s
        ∈
        F
        (
        U
        )
      
    
    {\displaystyle s\in F(U)}
  , then its restriction 
  
    
      
        
          
            res
          
          
            V
            ,
            U
          
        
        (
        s
        )
      
    
    {\displaystyle {\text{res}}_{V,U}(s)}
   is often denoted 
  
    
      
        s
        
          
            |
          
          
            V
          
        
      
    
    {\displaystyle s|_{V}}
   by analogy with restriction of functions.The restriction morphisms are required to satisfy two additional (functorial) properties:

For every open set 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
  , the restriction morphism 
  
    
      
        
          res
          
            U
            ,
            U
          
        
        :
        F
        (
        U
        )
        →
        F
        (
        U
        )
      
    
    {\displaystyle \operatorname {res} _{U,U}\colon F(U)\rightarrow F(U)}
   is the identity morphism on 
  
    
      
        F
        (
        U
        )
      
    
    {\displaystyle F(U)}
  .
If we have three open sets 
  
    
      
        W
        ⊆
        V
        ⊆
        U
      
    
    {\displaystyle W\subseteq V\subseteq U}
  , then the composite 
  
    
      
        
          
            res
          
          
            W
            ,
            V
          
        
        ∘
        
          
            res
          
          
            V
            ,
            U
          
        
        =
        
          
            res
          
          
            W
            ,
            U
          
        
      
    
    {\displaystyle {\text{res}}_{W,V}\circ {\text{res}}_{V,U}={\text{res}}_{W,U}}
  Informally, the second axiom says it doesn't matter whether we restrict to W in one step or restrict first to V, then to W. A concise functorial reformulation of this definition is given further below.
Many examples of presheaves come from different classes of functions: to any 
  
    
      
        U
      
    
    {\displaystyle U}
  , one can assign the set 
  
    
      
        
          C
          
            0
          
        
        (
        U
        )
      
    
    {\displaystyle C^{0}(U)}
   of continuous real-valued functions on 
  
    
      
        U
      
    
    {\displaystyle U}
  . The restriction maps are then just given by restricting a continuous function on 
  
    
      
        U
      
    
    {\displaystyle U}
   to a smaller open subset 
  
    
      
        V
      
    
    {\displaystyle V}
  , which again is a continuous function. The two presheaf axioms are immediately checked, thereby giving an example of a presheaf. This can be extended to a sheaf of holomorphic functions 
  
    
      
        
          
            H
          
        
        (
        −
        )
      
    
    {\displaystyle {\mathcal {H}}(-)}
   and a sheaf of smooth functions 
  
    
      
        
          C
          
            ∞
          
        
        (
        −
        )
      
    
    {\displaystyle C^{\infty }(-)}
  .
Another common class of examples is assigning to 
  
    
      
        U
      
    
    {\displaystyle U}
   the set of constant real-valued functions on 
  
    
      
        U
      
    
    {\displaystyle U}
  . This presheaf is called the constant presheaf associated to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   and is denoted 
  
    
      
        
          
            
              
                R
              
              _
            
          
          
            p
            s
            h
          
        
      
    
    {\displaystyle {\underline {\mathbb {R} }}^{psh}}
  .


=== Sheaves ===
Given a presheaf, a natural question to ask is to what extent its sections over an open set 
  
    
      
        U
      
    
    {\displaystyle U}
   are specified by their restrictions to smaller open sets 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   of an open cover 
  
    
      
        
          
            U
          
        
        =
        {
        
          U
          
            i
          
        
        
          }
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle {\mathcal {U}}=\{U_{i}\}_{i\in I}}
   of 
  
    
      
        U
      
    
    {\displaystyle U}
  . A sheaf is a presheaf that satisfies both of the following two additional axioms:

(Locality) Suppose 
  
    
      
        U
      
    
    {\displaystyle U}
   is an open set, 
  
    
      
        {
        
          U
          
            i
          
        
        
          }
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \{U_{i}\}_{i\in I}}
   is an open cover of 
  
    
      
        U
      
    
    {\displaystyle U}
  , and 
  
    
      
        s
        ,
        t
        ∈
        F
        (
        U
        )
      
    
    {\displaystyle s,t\in F(U)}
   are sections. If 
  
    
      
        s
        
          
            |
          
          
            
              U
              
                i
              
            
          
        
        =
        t
        
          
            |
          
          
            
              U
              
                i
              
            
          
        
      
    
    {\displaystyle s|_{U_{i}}=t|_{U_{i}}}
   for all 
  
    
      
        i
        ∈
        I
      
    
    {\displaystyle i\in I}
  , then 
  
    
      
        s
        =
        t
      
    
    {\displaystyle s=t}
  .
(Gluing) Suppose 
  
    
      
        U
      
    
    {\displaystyle U}
   is an open set, 
  
    
      
        {
        
          U
          
            i
          
        
        
          }
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \{U_{i}\}_{i\in I}}
   is an open cover of 
  
    
      
        U
      
    
    {\displaystyle U}
  , and 
  
    
      
        {
        
          s
          
            i
          
        
        ∈
        F
        (
        
          U
          
            i
          
        
        )
        
          }
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \{s_{i}\in F(U_{i})\}_{i\in I}}
   is a family of sections. If all pairs of sections agree on the overlap of their domains, that is, if 
  
    
      
        
          s
          
            i
          
        
        
          
            |
          
          
            
              U
              
                i
              
            
            ∩
            
              U
              
                j
              
            
          
        
        =
        
          s
          
            j
          
        
        
          
            |
          
          
            
              U
              
                i
              
            
            ∩
            
              U
              
                j
              
            
          
        
      
    
    {\displaystyle s_{i}|_{U_{i}\cap U_{j}}=s_{j}|_{U_{i}\cap U_{j}}}
   for all 
  
    
      
        i
        ,
        j
        ∈
        I
      
    
    {\displaystyle i,j\in I}
  , then there exists a section 
  
    
      
        s
        ∈
        F
        (
        U
        )
      
    
    {\displaystyle s\in F(U)}
   such that 
  
    
      
        s
        
          
            |
          
          
            
              U
              
                i
              
            
          
        
        =
        
          s
          
            i
          
        
      
    
    {\displaystyle s|_{U_{i}}=s_{i}}
   for all 
  
    
      
        i
        ∈
        I
      
    
    {\displaystyle i\in I}
  .The section 
  
    
      
        s
      
    
    {\displaystyle s}
   whose existence is guaranteed by axiom 2 is called the gluing, concatenation, or collation of the sections si. By axiom 1 it is unique. Sections 
  
    
      
        
          s
          
            i
          
        
      
    
    {\displaystyle s_{i}}
   and 
  
    
      
        
          s
          
            j
          
        
      
    
    {\displaystyle s_{j}}
   satisfying the agreement precondition of axiom 2 are often called compatible; thus axioms 1 and 2 together state that any collection of pairwise compatible sections can be uniquely glued together. A separated presheaf, or monopresheaf, is a presheaf satisfying axiom 1.The presheaf consisting of continuous functions mentioned above is a sheaf. This assertion reduces to checking that, given continuous functions 
  
    
      
        
          f
          
            i
          
        
        :
        
          U
          
            i
          
        
        →
        
          R
        
      
    
    {\displaystyle f_{i}:U_{i}\to \mathbb {R} }
   which agree on the intersections 
  
    
      
        
          U
          
            i
          
        
        ∩
        
          U
          
            j
          
        
      
    
    {\displaystyle U_{i}\cap U_{j}}
  , there is a unique continuous function 
  
    
      
        f
        :
        U
        →
        
          R
        
      
    
    {\displaystyle f:U\to \mathbb {R} }
   whose restriction equals the 
  
    
      
        
          f
          
            i
          
        
      
    
    {\displaystyle f_{i}}
  . By contrast, the constant presheaf is usually not a sheaf as it fails to satisfy the locality axiom on the empty set (this is explained in more detail at constant sheaf).
Presheaves and sheaves are typically denoted by capital letters, 
  
    
      
        F
      
    
    {\displaystyle F}
   being particularly common, presumably for the French word for sheaf, faisceau. Use of calligraphic letters such as 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is also common.
It can be shown that to specify a sheaf, it is enough to specify its restriction to the open sets of a basis for the topology of the underlying space. Moreover, it can also be shown that it is enough to verify the sheaf axioms above relative to the open sets of a covering. This observation is used to construct another example which is crucial in algebraic geometry, namely quasi-coherent sheaves. Here the topological space in question is the spectrum of a commutative ring 
  
    
      
        R
      
    
    {\displaystyle R}
  , whose points are the prime ideals 
  
    
      
        p
      
    
    {\displaystyle p}
   in 
  
    
      
        R
      
    
    {\displaystyle R}
  . The open sets 
  
    
      
        
          D
          
            f
          
        
        :=
        {
        p
        ⊂
        R
        ,
        f
        ∉
        p
        }
      
    
    {\displaystyle D_{f}:=\{p\subset R,f\notin p\}}
   form a basis for the Zariski topology on this space. Given an 
  
    
      
        R
      
    
    {\displaystyle R}
  -module 
  
    
      
        M
      
    
    {\displaystyle M}
  , there is a sheaf, denoted by 
  
    
      
        
          
            
              M
              ~
            
          
        
      
    
    {\displaystyle {\tilde {M}}}
   on the Spec 
  
    
      
        R
      
    
    {\displaystyle R}
  , that satisfies

  
    
      
        
          
            
              M
              ~
            
          
        
        (
        
          D
          
            f
          
        
        )
        :=
        M
        [
        1
        
          /
        
        f
        ]
        ,
      
    
    {\displaystyle {\tilde {M}}(D_{f}):=M[1/f],}
   the localization of 
  
    
      
        M
      
    
    {\displaystyle M}
   at 
  
    
      
        f
      
    
    {\displaystyle f}
  .There is another characterization of sheaves that is equivalent to the previously discussed.
A presheaf 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is a sheaf if and only if for any open 
  
    
      
        U
      
    
    {\displaystyle U}
   and any open cover 
  
    
      
        (
        
          U
          
            a
          
        
        )
      
    
    {\displaystyle (U_{a})}
   of 
  
    
      
        U
      
    
    {\displaystyle U}
  , 
  
    
      
        
          
            F
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {F}}(U)}
   is the fibre product 
  
    
      
        
          
            F
          
        
        (
        U
        )
        ≅
        
          
            F
          
        
        (
        
          U
          
            a
          
        
        )
        
          ×
          
            
              
                F
              
            
            (
            
              U
              
                a
              
            
            ∩
            
              U
              
                b
              
            
            )
          
        
        
          
            F
          
        
        (
        
          U
          
            b
          
        
        )
      
    
    {\displaystyle {\mathcal {F}}(U)\cong {\mathcal {F}}(U_{a})\times _{{\mathcal {F}}(U_{a}\cap U_{b})}{\mathcal {F}}(U_{b})}
  . This characterization is useful in construction of sheaves, for example, if 
  
    
      
        
          
            F
          
        
        ,
        
          
            G
          
        
      
    
    {\displaystyle {\mathcal {F}},{\mathcal {G}}}
   are abelian sheaves, then the kernel of sheaves morphism 
  
    
      
        
          
            F
          
        
        →
        
          
            G
          
        
      
    
    {\displaystyle {\mathcal {F}}\to {\mathcal {G}}}
   is a sheaf, since projective limits commutes with projective limits. On the other hand, without consider any examples, the cokernel is not always a sheaf because inductive limit not necessarily commutes with projective limits. One of the way to fix this is to consider Noetherian topological spaces; every open sets are compact so that the cokernel is a sheaf, since finite projective limits commutes with inductive limits.


=== Further examples ===


==== Sheaf of sections of a continuous map ====
Any continuous map 
  
    
      
        f
        :
        Y
        →
        X
      
    
    {\displaystyle f:Y\to X}
   of topological spaces determines a sheaf 
  
    
      
        Γ
        (
        Y
        
          /
        
        X
        )
      
    
    {\displaystyle \Gamma (Y/X)}
   on 
  
    
      
        X
      
    
    {\displaystyle X}
   by setting

  
    
      
        Γ
        (
        Y
        
          /
        
        X
        )
        (
        U
        )
        =
        {
        s
        :
        U
        →
        Y
        ,
        f
        ∘
        s
        =
        
          id
          
            U
          
        
        }
        .
      
    
    {\displaystyle \Gamma (Y/X)(U)=\{s:U\to Y,f\circ s=\operatorname {id} _{U}\}.}
  Any such 
  
    
      
        s
      
    
    {\displaystyle s}
   is commonly called a section of 
  
    
      
        f
      
    
    {\displaystyle f}
  , and this example is the reason why the elements in 
  
    
      
        F
        (
        U
        )
      
    
    {\displaystyle F(U)}
   are generally called sections. This construction is especially important when 
  
    
      
        f
      
    
    {\displaystyle f}
   is the projection of a fiber bundle onto its base space. For example, the sheaves of smooth functions are the sheaves of sections of the trivial bundle. Another example: the sheaf of sections of

  
    
      
        
          C
        
        
          
            
              
                →
              
              
                exp
              
            
          
        
        
          C
        
        ∖
        {
        0
        }
      
    
    {\displaystyle \mathbb {C} {\stackrel {\exp }{\to }}\mathbb {C} \setminus \{0\}}
  is the sheaf which assigns to any 
  
    
      
        U
      
    
    {\displaystyle U}
   the set of branches of the complex logarithm on 
  
    
      
        U
      
    
    {\displaystyle U}
  .
Given a point 
  
    
      
        x
      
    
    {\displaystyle x}
   and an abelian group 
  
    
      
        S
      
    
    {\displaystyle S}
  , the skyscraper sheaf 
  
    
      
        
          S
          
            x
          
        
      
    
    {\displaystyle S_{x}}
   is defined as follows: if 
  
    
      
        U
      
    
    {\displaystyle U}
   is an open set containing 
  
    
      
        x
      
    
    {\displaystyle x}
  , then 
  
    
      
        
          S
          
            x
          
        
        (
        U
        )
        =
        S
      
    
    {\displaystyle S_{x}(U)=S}
  . If 
  
    
      
        U
      
    
    {\displaystyle U}
   does not contain 
  
    
      
        x
      
    
    {\displaystyle x}
  , then 
  
    
      
        
          S
          
            x
          
        
        (
        U
        )
        =
        0
      
    
    {\displaystyle S_{x}(U)=0}
  , the trivial group. The restriction maps are either the identity on 
  
    
      
        S
      
    
    {\displaystyle S}
  , if both open sets contain 
  
    
      
        x
      
    
    {\displaystyle x}
  , or the zero map otherwise.


==== Sheaves on manifolds ====
On an 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C^{k}}
  -manifold 
  
    
      
        M
      
    
    {\displaystyle M}
  , there are a number of important sheaves, such as the sheaf of 
  
    
      
        j
      
    
    {\displaystyle j}
  -times continuously differentiable functions 
  
    
      
        
          
            
              O
            
          
          
            M
          
          
            j
          
        
      
    
    {\displaystyle {\mathcal {O}}_{M}^{j}}
   (with 
  
    
      
        j
        ≤
        k
      
    
    {\displaystyle j\leq k}
  ). Its sections on some open 
  
    
      
        U
      
    
    {\displaystyle U}
   are the 
  
    
      
        
          C
          
            j
          
        
      
    
    {\displaystyle C^{j}}
  -functions 
  
    
      
        U
        →
        
          R
        
      
    
    {\displaystyle U\to \mathbb {R} }
  . For 
  
    
      
        j
        =
        k
      
    
    {\displaystyle j=k}
  , this sheaf is called the structure sheaf and is denoted 
  
    
      
        
          
            
              O
            
          
          
            M
          
        
      
    
    {\displaystyle {\mathcal {O}}_{M}}
  . The nonzero 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C^{k}}
   functions also form a sheaf, denoted 
  
    
      
        
          
            
              O
            
          
          
            X
          
          
            ×
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}^{\times }}
  . Differential forms (of degree 
  
    
      
        p
      
    
    {\displaystyle p}
  ) also form a sheaf 
  
    
      
        
          Ω
          
            M
          
          
            p
          
        
      
    
    {\displaystyle \Omega _{M}^{p}}
  . In all these examples, the restriction morphisms are given by restricting functions or forms.
The assignment sending 
  
    
      
        U
      
    
    {\displaystyle U}
   to the compactly supported functions on 
  
    
      
        U
      
    
    {\displaystyle U}
   is not a sheaf, since there is, in general, no way to preserve this property by passing to a smaller open subset. Instead, this forms a cosheaf, a dual concept where the restriction maps go in the opposite direction than with sheaves. However, taking the dual of these vector spaces does give a sheaf, the sheaf of distributions.


==== Presheaves that are not sheaves ====
In addition to the constant presheaf mentioned above, which is usually not a sheaf, there are further examples of presheaves that are not sheaves:

Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be the two-point topological space 
  
    
      
        {
        x
        ,
        y
        }
      
    
    {\displaystyle \{x,y\}}
   with the discrete topology. Define a presheaf 
  
    
      
        F
      
    
    {\displaystyle F}
   as follows: The restriction map 
  
    
      
        F
        (
        {
        x
        ,
        y
        }
        )
        →
        F
        (
        {
        x
        }
        )
      
    
    {\displaystyle F(\{x,y\})\to F(\{x\})}
   is the projection of 
  
    
      
        
          R
        
        ×
        
          R
        
        ×
        
          R
        
      
    
    {\displaystyle \mathbb {R} \times \mathbb {R} \times \mathbb {R} }
   onto its first coordinate, and the restriction map 
  
    
      
        F
        (
        {
        x
        ,
        y
        }
        )
        →
        F
        (
        {
        y
        }
        )
      
    
    {\displaystyle F(\{x,y\})\to F(\{y\})}
   is the projection of 
  
    
      
        
          R
        
        ×
        
          R
        
        ×
        
          R
        
      
    
    {\displaystyle \mathbb {R} \times \mathbb {R} \times \mathbb {R} }
   onto its second coordinate. 
  
    
      
        F
      
    
    {\displaystyle F}
   is a presheaf that is not separated: a global section is determined by three numbers, but the values of that section over 
  
    
      
        {
        x
        }
      
    
    {\displaystyle \{x\}}
   and 
  
    
      
        {
        y
        }
      
    
    {\displaystyle \{y\}}
   determine only two of those numbers. So while we can glue any two sections over 
  
    
      
        {
        x
        }
      
    
    {\displaystyle \{x\}}
   and 
  
    
      
        {
        y
        }
      
    
    {\displaystyle \{y\}}
  , we cannot glue them uniquely.
Let 
  
    
      
        X
        =
        
          R
        
      
    
    {\displaystyle X=\mathbb {R} }
   be the real line, and let 
  
    
      
        F
        (
        U
        )
      
    
    {\displaystyle F(U)}
   be the set of bounded continuous functions on 
  
    
      
        U
      
    
    {\displaystyle U}
  . This is not a sheaf because it is not always possible to glue. For example, let 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   be the set of all 
  
    
      
        x
      
    
    {\displaystyle x}
   such that 
  
    
      
        
          |
        
        x
        
          |
        
        <
        i
      
    
    {\displaystyle |x|<i}
  . The identity function 
  
    
      
        f
        (
        x
        )
        =
        x
      
    
    {\displaystyle f(x)=x}
   is bounded on each 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  . Consequently, we get a section 
  
    
      
        
          s
          
            i
          
        
      
    
    {\displaystyle s_{i}}
   on 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  . However, these sections do not glue, because the function 
  
    
      
        f
      
    
    {\displaystyle f}
   is not bounded on the real line. Consequently 
  
    
      
        F
      
    
    {\displaystyle F}
   is a presheaf, but not a sheaf. In fact, 
  
    
      
        F
      
    
    {\displaystyle F}
   is separated because it is a sub-presheaf of the sheaf of continuous functions.


=== Motivating sheaves from complex analytic spaces and algebraic geometry ===
One of the historical motivations for sheaves have come from studying complex manifolds, complex analytic geometry, and scheme theory from algebraic geometry. This is because in all of the previous cases, we consider a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   together with a structure sheaf 
  
    
      
        
          
            O
          
        
      
    
    {\displaystyle {\mathcal {O}}}
   giving it the structure of a complex manifold, complex analytic space, or scheme. This perspective of equipping a topological space with a sheaf is essential to the theory of locally ringed spaces (see below).


==== Technical challenges with complex manifolds ====
One of the main historical motivations for introducing sheaves was constructing a device which keeps track of holomorphic functions on complex manifolds. For example, on a compact complex manifold 
  
    
      
        X
      
    
    {\displaystyle X}
   (like complex projective space or the vanishing locus of a homogeneous polynomial), the only holomorphic functions
  
    
      
        f
        :
        X
        →
        
          C
        
      
    
    {\displaystyle f:X\to \mathbb {C} }
  are the constant functions. This means there could exist two compact complex manifolds 
  
    
      
        X
        ,
        
          X
          ′
        
      
    
    {\displaystyle X,X'}
   which are not isomorphic, but nevertheless their ring of global holomorphic functions, denoted 
  
    
      
        
          
            H
          
        
        (
        X
        )
        ,
        
          
            H
          
        
        (
        
          X
          ′
        
        )
      
    
    {\displaystyle {\mathcal {H}}(X),{\mathcal {H}}(X')}
  , are isomorphic. Contrast this with smooth manifolds where every manifold 
  
    
      
        M
      
    
    {\displaystyle M}
   can be embedded inside some 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  , hence its ring of smooth functions 
  
    
      
        
          C
          
            ∞
          
        
        (
        M
        )
      
    
    {\displaystyle C^{\infty }(M)}
   comes from restricting the smooth functions from 
  
    
      
        
          C
          
            ∞
          
        
        (
        
          
            R
          
          
            n
          
        
        )
      
    
    {\displaystyle C^{\infty }(\mathbb {R} ^{n})}
  . Another complexity when considering the ring of holomorphic functions on a complex manifold 
  
    
      
        X
      
    
    {\displaystyle X}
   is given a small enough open set 
  
    
      
        U
        ⊂
        X
      
    
    {\displaystyle U\subset X}
  , the holomorphic functions will be isomorphic to 
  
    
      
        
          
            H
          
        
        (
        U
        )
        ≅
        
          
            H
          
        
        (
        
          
            C
          
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {H}}(U)\cong {\mathcal {H}}(\mathbb {C} ^{n})}
  . Sheaves are a direct tool for dealing with this complexity since they make it possible to keep track of the holomorphic structure on the underlying topological space of 
  
    
      
        X
      
    
    {\displaystyle X}
   on arbitrary open subsets 
  
    
      
        U
        ⊂
        X
      
    
    {\displaystyle U\subset X}
  . This means as 
  
    
      
        U
      
    
    {\displaystyle U}
   becomes more complex topologically, the ring 
  
    
      
        
          
            H
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {H}}(U)}
   can be expressed from gluing the 
  
    
      
        
          
            H
          
        
        (
        
          U
          
            i
          
        
        )
      
    
    {\displaystyle {\mathcal {H}}(U_{i})}
  . Note that sometimes this sheaf is denoted 
  
    
      
        
          
            O
          
        
        (
        −
        )
      
    
    {\displaystyle {\mathcal {O}}(-)}
   or just 
  
    
      
        
          
            O
          
        
      
    
    {\displaystyle {\mathcal {O}}}
  , or even 
  
    
      
        
          
            
              O
            
          
          
            X
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}}
   when we want to emphasize the space the structure sheaf is associated to.


==== Tracking submanifolds with sheaves ====
Another common example of sheaves can be constructed by considering a complex submanifold 
  
    
      
        Y
        ↪
        X
      
    
    {\displaystyle Y\hookrightarrow X}
  . There is an associated sheaf 
  
    
      
        
          
            
              O
            
          
          
            Y
          
        
      
    
    {\displaystyle {\mathcal {O}}_{Y}}
   which takes an open subset 
  
    
      
        U
        ⊂
        X
      
    
    {\displaystyle U\subset X}
   and gives the ring of holomorphic functions on 
  
    
      
        U
        ∩
        Y
      
    
    {\displaystyle U\cap Y}
  . This kind of formalism was found to be extremely powerful and motivates a lot of homological algebra such as sheaf cohomology since an intersection theory can be built using these kinds of sheaves from the Serre intersection formula.


== Operations with sheaves ==


=== Morphisms ===
Morphisms of sheaves are, roughly speaking, analogous to functions between them. In contrast to a function between sets, which have no additional structure, morphisms of sheaves are those functions which preserve the structure inherent in the sheaves. This idea is made precise in the following definition.
Let 
  
    
      
        F
      
    
    {\displaystyle F}
   and 
  
    
      
        G
      
    
    {\displaystyle G}
   be two sheaves on 
  
    
      
        X
      
    
    {\displaystyle X}
  . A morphism 
  
    
      
        φ
        :
        G
        →
        F
      
    
    {\displaystyle \varphi :G\to F}
   consists of a morphism 
  
    
      
        
          φ
          
            U
          
        
        :
        G
        (
        U
        )
        →
        F
        (
        U
        )
      
    
    {\displaystyle \varphi _{U}:G(U)\to F(U)}
   for each open set 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
  , subject to the condition that this morphism is compatible with restrictions. In other words, for every open subset 
  
    
      
        V
      
    
    {\displaystyle V}
   of an open set 
  
    
      
        U
      
    
    {\displaystyle U}
  , the following diagram is commutative.

  
    
      
        
          
            
              
                G
                (
                U
                )
              
              
                
                  →
                  
                    
                    
                      φ
                      
                        U
                      
                    
                    
                  
                
              
              
                F
                (
                U
                )
              
            
            
              
                
                  r
                  
                    V
                    ,
                    U
                  
                
                
                  
                    ↓
                  
                
              
              
              
                
                  
                    ↓
                  
                
                
                  r
                  
                    V
                    ,
                    U
                  
                  ′
                
              
            
            
              
                G
                (
                V
                )
              
              
                
                  
                    →
                    
                      
                        
                        
                          φ
                          
                            V
                          
                        
                        
                      
                    
                    
                  
                
              
              
                F
                (
                V
                )
              
            
          
        
      
    
    {\displaystyle {\begin{array}{rcl}G(U)&\xrightarrow {\quad \varphi _{U}\quad } &F(U)\\r_{V,U}{\Biggl \downarrow }&&{\Biggl \downarrow }r'_{V,U}\\G(V)&{\xrightarrow[{\quad \varphi _{V}\quad }]{}}&F(V)\end{array}}}
  For example, taking the derivative gives a morphism of sheaves on 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  :

  
    
      
        
          
            
              O
            
          
          
            
              R
            
          
          
            n
          
        
        →
        
          
            
              O
            
          
          
            
              R
            
          
          
            n
            −
            1
          
        
        .
      
    
    {\displaystyle {\mathcal {O}}_{\mathbb {R} }^{n}\to {\mathcal {O}}_{\mathbb {R} }^{n-1}.}
  
Indeed, given an (
  
    
      
        n
      
    
    {\displaystyle n}
  -times continuously differentiable) function 
  
    
      
        f
        :
        U
        →
        
          R
        
      
    
    {\displaystyle f:U\to \mathbb {R} }
   (with 
  
    
      
        U
      
    
    {\displaystyle U}
   in 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   open), the restriction (to a smaller open subset 
  
    
      
        V
      
    
    {\displaystyle V}
  ) of its derivative equals the derivative of 
  
    
      
        f
        
          
            |
          
          
            V
          
        
      
    
    {\displaystyle f|_{V}}
  .
With this notion of morphism, sheaves on a fixed topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   form a category. The general categorical notions of mono-, epi- and isomorphisms can therefore be applied to sheaves. A sheaf morphism 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   is an isomorphism (resp. monomorphism) if and only if each 
  
    
      
        
          φ
          
            U
          
        
      
    
    {\displaystyle \varphi _{U}}
   is a bijection (resp. injective map). Moreover, a morphism of sheaves 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   is an isomorphism if and only if there exists an open cover 
  
    
      
        {
        
          U
          
            α
          
        
        }
      
    
    {\displaystyle \{U_{\alpha }\}}
   such that 
  
    
      
        φ
        
          
            |
          
          
            
              U
              
                α
              
            
          
        
      
    
    {\displaystyle \varphi |_{U_{\alpha }}}
   are isomorphisms of sheaves for all 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  . This statement, which also holds for monomorphisms, but does not hold for presheaves, is another instance of the idea that sheaves are of a local nature.
The corresponding statements do not hold for epimorphisms (of sheaves), and their failure is measured by sheaf cohomology.


=== Stalks of a sheaf ===

The stalk 
  
    
      
        
          
            
              F
            
          
          
            x
          
        
      
    
    {\displaystyle {\mathcal {F}}_{x}}
   of a sheaf 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   captures the properties of a sheaf ""around"" a point 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
  , generalizing the germs of functions.
Here, ""around"" means that, conceptually speaking, one looks at smaller and smaller neighborhoods of the point. Of course, no single neighborhood will be small enough, which requires considering a limit of some sort. More precisely, the stalk is defined by

  
    
      
        
          
            
              F
            
          
          
            x
          
        
        =
        
          
            
              lim
              →
            
          
          
            U
            ∋
            x
          
        
        ⁡
        
          
            F
          
        
        (
        U
        )
        ,
      
    
    {\displaystyle {\mathcal {F}}_{x}=\varinjlim _{U\ni x}{\mathcal {F}}(U),}
  the direct limit being over all open subsets of 
  
    
      
        X
      
    
    {\displaystyle X}
   containing the given point 
  
    
      
        x
      
    
    {\displaystyle x}
  . In other words, an element of the stalk is given by a section over some open neighborhood of 
  
    
      
        x
      
    
    {\displaystyle x}
  , and two such sections are considered equivalent if their restrictions agree on a smaller neighborhood.
The natural morphism 
  
    
      
        F
        (
        U
        )
        →
        
          F
          
            x
          
        
      
    
    {\displaystyle F(U)\to F_{x}}
   takes a section 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        F
        (
        U
        )
      
    
    {\displaystyle F(U)}
   to its germ at 
  
    
      
        x
      
    
    {\displaystyle x}
  . This generalises the usual definition of a germ.
In many situations, knowing the stalks of a sheaf is enough to control the sheaf itself. For example, whether or not a morphism of sheaves is a monomorphism, epimorphism, or isomorphism can be tested on the stalks. In this sense, a sheaf is determined by its stalks, which are a local data. By contrast, the global information present in a sheaf, i.e., the global sections, i.e., the sections 
  
    
      
        
          
            F
          
        
        (
        X
        )
      
    
    {\displaystyle {\mathcal {F}}(X)}
   on the whole space 
  
    
      
        X
      
    
    {\displaystyle X}
  , typically carry less information. For example, for a compact complex manifold 
  
    
      
        X
      
    
    {\displaystyle X}
  , the global sections of the sheaf of holomorphic functions are just 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
  , since any holomorphic function

  
    
      
        X
        →
        
          C
        
      
    
    {\displaystyle X\to \mathbb {C} }
  is constant by Liouville's theorem.


=== Turning a presheaf into a sheaf ===
It is frequently useful to take the data contained in a presheaf and to express it as a sheaf. It turns out that there is a best possible way to do this. It takes a presheaf 
  
    
      
        F
      
    
    {\displaystyle F}
   and produces a new sheaf 
  
    
      
        a
        F
      
    
    {\displaystyle aF}
   called the sheafification or sheaf associated to the presheaf 
  
    
      
        F
      
    
    {\displaystyle F}
  . For example, the sheafification of the constant presheaf (see above) is called the constant sheaf. Despite its name, its sections are locally constant functions.
The sheaf 
  
    
      
        a
        F
      
    
    {\displaystyle aF}
   can be constructed using the étalé space of 
  
    
      
        F
      
    
    {\displaystyle F}
  , namely as the sheaf of sections of the map

  
    
      
        
          S
          p
          e
        
        (
        F
        )
        →
        X
        .
      
    
    {\displaystyle \mathrm {Spe} (F)\to X.}
  Another construction of the sheaf 
  
    
      
        a
        F
      
    
    {\displaystyle aF}
   proceeds by means of a functor 
  
    
      
        L
      
    
    {\displaystyle L}
   from presheaves to presheaves that gradually improves the properties of a presheaf: for any presheaf 
  
    
      
        F
      
    
    {\displaystyle F}
  , 
  
    
      
        L
        F
      
    
    {\displaystyle LF}
   is a separated presheaf, and for any separated presheaf 
  
    
      
        F
      
    
    {\displaystyle F}
  , 
  
    
      
        L
        F
      
    
    {\displaystyle LF}
   is a sheaf. The associated sheaf 
  
    
      
        a
        F
      
    
    {\displaystyle aF}
   is given by 
  
    
      
        L
        L
        F
      
    
    {\displaystyle LLF}
  .The idea that the sheaf 
  
    
      
        a
        F
      
    
    {\displaystyle aF}
   is the best possible approximation to 
  
    
      
        F
      
    
    {\displaystyle F}
   by a sheaf is made precise using the following universal property: there is a natural morphism of presheaves 
  
    
      
        i
        :
        F
        →
        a
        F
      
    
    {\displaystyle i\colon F\to aF}
   so that for any sheaf 
  
    
      
        G
      
    
    {\displaystyle G}
   and any morphism of presheaves 
  
    
      
        f
        :
        F
        →
        G
      
    
    {\displaystyle f\colon F\to G}
  , there is a unique morphism of sheaves 
  
    
      
        
          
            
              f
              ~
            
          
        
        :
        a
        F
        →
        G
      
    
    {\displaystyle {\tilde {f}}\colon aF\rightarrow G}
   such that 
  
    
      
        f
        =
        
          
            
              f
              ~
            
          
        
        i
      
    
    {\displaystyle f={\tilde {f}}i}
  . In fact 
  
    
      
        a
      
    
    {\displaystyle a}
   is the left adjoint functor to the inclusion functor (or forgetful functor) from the category of sheaves to the category of presheaves, and 
  
    
      
        i
      
    
    {\displaystyle i}
   is the unit of the adjunction. In this way, the category of sheaves turns into a Giraud subcategory of presheaves. This categorical situation is the reason why the sheafification functor appears in constructing cokernels of sheaf morphisms or tensor products of sheaves, but not for kernels, say.


=== Subsheaves, quotient sheaves ===
If 
  
    
      
        K
      
    
    {\displaystyle K}
   is a subsheaf of a sheaf 
  
    
      
        F
      
    
    {\displaystyle F}
   of abelian groups, then the quotient sheaf 
  
    
      
        Q
      
    
    {\displaystyle Q}
   is the sheaf associated to the presheaf 
  
    
      
        U
        ↦
        F
        (
        U
        )
        
          /
        
        K
        (
        U
        )
      
    
    {\displaystyle U\mapsto F(U)/K(U)}
  ; in other words, the quotient sheaf fits into an exact sequence of sheaves of abelian groups;

  
    
      
        0
        →
        K
        →
        F
        →
        Q
        →
        0.
      
    
    {\displaystyle 0\to K\to F\to Q\to 0.}
  (this is also called a sheaf extension.)
Let 
  
    
      
        F
        ,
        G
      
    
    {\displaystyle F,G}
   be sheaves of abelian groups. The set 
  
    
      
        Hom
        ⁡
        (
        F
        ,
        G
        )
      
    
    {\displaystyle \operatorname {Hom} (F,G)}
   of morphisms of sheaves from 
  
    
      
        F
      
    
    {\displaystyle F}
   to 
  
    
      
        G
      
    
    {\displaystyle G}
   forms an abelian group (by the abelian group structure of 
  
    
      
        G
      
    
    {\displaystyle G}
  ). The sheaf hom of 
  
    
      
        F
      
    
    {\displaystyle F}
   and 
  
    
      
        G
      
    
    {\displaystyle G}
  , denoted by,

  
    
      
        
          
            H
            o
            m
          
        
        (
        F
        ,
        G
        )
      
    
    {\displaystyle {\mathcal {Hom}}(F,G)}
  is the sheaf of abelian groups 
  
    
      
        U
        ↦
        Hom
        ⁡
        (
        F
        
          
            |
          
          
            U
          
        
        ,
        G
        
          
            |
          
          
            U
          
        
        )
      
    
    {\displaystyle U\mapsto \operatorname {Hom} (F|_{U},G|_{U})}
   where 
  
    
      
        F
        
          
            |
          
          
            U
          
        
      
    
    {\displaystyle F|_{U}}
   is the sheaf on 
  
    
      
        U
      
    
    {\displaystyle U}
   given by 
  
    
      
        (
        F
        
          
            |
          
          
            U
          
        
        )
        (
        V
        )
        =
        F
        (
        V
        )
      
    
    {\displaystyle (F|_{U})(V)=F(V)}
   (note sheafification is not needed here). The direct sum of 
  
    
      
        F
      
    
    {\displaystyle F}
   and 
  
    
      
        G
      
    
    {\displaystyle G}
   is the sheaf given by 
  
    
      
        U
        ↦
        F
        (
        U
        )
        ⊕
        G
        (
        U
        )
      
    
    {\displaystyle U\mapsto F(U)\oplus G(U)}
  , and the tensor product of 
  
    
      
        F
      
    
    {\displaystyle F}
   and 
  
    
      
        G
      
    
    {\displaystyle G}
   is the sheaf associated to the presheaf 
  
    
      
        U
        ↦
        F
        (
        U
        )
        ⊗
        G
        (
        U
        )
      
    
    {\displaystyle U\mapsto F(U)\otimes G(U)}
  .
All of these operations extend to sheaves of modules over a sheaf of rings 
  
    
      
        A
      
    
    {\displaystyle A}
  ; the above is the special case when 
  
    
      
        A
      
    
    {\displaystyle A}
   is the constant sheaf 
  
    
      
        
          
            
              Z
            
            _
          
        
      
    
    {\displaystyle {\underline {\mathbf {Z} }}}
  .


=== Basic functoriality ===

Since the data of a (pre-)sheaf depends on the open subsets of the base space, sheaves on different topological spaces are unrelated to each other in the sense that there are no morphisms between them. However, given a continuous map 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   between two topological spaces, pushforward and pullback relate sheaves on 
  
    
      
        X
      
    
    {\displaystyle X}
   to those on 
  
    
      
        Y
      
    
    {\displaystyle Y}
   and vice versa.


==== Direct image ====
The pushforward (also known as direct image) of a sheaf 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   on 
  
    
      
        X
      
    
    {\displaystyle X}
   is the sheaf defined by

  
    
      
        (
        
          f
          
            ∗
          
        
        
          
            F
          
        
        )
        (
        V
        )
        =
        
          
            F
          
        
        (
        
          f
          
            −
            1
          
        
        (
        V
        )
        )
        .
      
    
    {\displaystyle (f_{*}{\mathcal {F}})(V)={\mathcal {F}}(f^{-1}(V)).}
  Here 
  
    
      
        V
      
    
    {\displaystyle V}
   is an open subset of 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , so that its preimage is open in 
  
    
      
        X
      
    
    {\displaystyle X}
   by the continuity of 
  
    
      
        f
      
    
    {\displaystyle f}
  . 
This construction recovers the skyscraper sheaf 
  
    
      
        
          S
          
            x
          
        
      
    
    {\displaystyle S_{x}}
   mentioned above:

  
    
      
        
          S
          
            x
          
        
        =
        
          i
          
            ∗
          
        
        (
        S
        )
      
    
    {\displaystyle S_{x}=i_{*}(S)}
  where 
  
    
      
        i
        :
        {
        x
        }
        →
        X
      
    
    {\displaystyle i:\{x\}\to X}
   is the inclusion, and 
  
    
      
        S
      
    
    {\displaystyle S}
   is regarded as a sheaf on the singleton (by 
  
    
      
        S
        (
        {
        ∗
        }
        )
        =
        S
        ,
        S
        (
        ∅
        )
        =
        ∅
      
    
    {\displaystyle S(\{*\})=S,S(\emptyset )=\emptyset }
  .
For a map between locally compact spaces, the direct image with compact support is a subsheaf of the direct image. By definition, 
  
    
      
        (
        
          f
          
            !
          
        
        
          
            F
          
        
        )
        (
        V
        )
      
    
    {\displaystyle (f_{!}{\mathcal {F}})(V)}
   consists of those 
  
    
      
        f
        ∈
        
          
            F
          
        
        (
        
          f
          
            −
            1
          
        
        (
        V
        )
        )
      
    
    {\displaystyle f\in {\mathcal {F}}(f^{-1}(V))}
   whose support is proper map over 
  
    
      
        V
      
    
    {\displaystyle V}
  . If 
  
    
      
        f
      
    
    {\displaystyle f}
   is proper itself, then 
  
    
      
        
          f
          
            !
          
        
        
          
            F
          
        
        =
        
          f
          
            ∗
          
        
        
          
            F
          
        
      
    
    {\displaystyle f_{!}{\mathcal {F}}=f_{*}{\mathcal {F}}}
  , but in general they disagree.


==== Inverse image ====
The pullback or inverse image goes the other way: it produces a sheaf on 
  
    
      
        X
      
    
    {\displaystyle X}
  , denoted 
  
    
      
        
          f
          
            −
            1
          
        
        
          
            G
          
        
      
    
    {\displaystyle f^{-1}{\mathcal {G}}}
   out of a sheaf 
  
    
      
        
          
            G
          
        
      
    
    {\displaystyle {\mathcal {G}}}
   on 
  
    
      
        Y
      
    
    {\displaystyle Y}
  . If 
  
    
      
        f
      
    
    {\displaystyle f}
   is the inclusion of an open subset, then the inverse image is just a restriction, i.e., it is given by 
  
    
      
        (
        
          f
          
            −
            1
          
        
        
          
            G
          
        
        )
        (
        U
        )
        =
        
          
            G
          
        
        (
        U
        )
      
    
    {\displaystyle (f^{-1}{\mathcal {G}})(U)={\mathcal {G}}(U)}
   for an open 
  
    
      
        U
      
    
    {\displaystyle U}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
  . A sheaf 
  
    
      
        F
      
    
    {\displaystyle F}
   (on some space 
  
    
      
        X
      
    
    {\displaystyle X}
  ) is called locally constant if 
  
    
      
        X
        =
        
          ⋃
          
            i
            ∈
            I
          
        
        
          U
          
            i
          
        
      
    
    {\displaystyle X=\bigcup _{i\in I}U_{i}}
   by some open subsets 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   such that the restriction of 
  
    
      
        F
      
    
    {\displaystyle F}
   to all these open subsets is constant. One a wide range of topological spaces 
  
    
      
        X
      
    
    {\displaystyle X}
  , such sheaves are equivalent to representations of the fundamental group 
  
    
      
        
          π
          
            1
          
        
        (
        X
        )
      
    
    {\displaystyle \pi _{1}(X)}
  .
For general maps 
  
    
      
        f
      
    
    {\displaystyle f}
  , the definition of 
  
    
      
        
          f
          
            −
            1
          
        
        
          
            G
          
        
      
    
    {\displaystyle f^{-1}{\mathcal {G}}}
   is more involved; it is detailed at inverse image functor. The stalk is an essential special case of the pullback in view of a natural identification, where 
  
    
      
        i
      
    
    {\displaystyle i}
   is as above:

  
    
      
        
          
            
              G
            
          
          
            x
          
        
        =
        
          i
          
            −
            1
          
        
        
          
            G
          
        
        (
        {
        x
        }
        )
        .
      
    
    {\displaystyle {\mathcal {G}}_{x}=i^{-1}{\mathcal {G}}(\{x\}).}
  More generally, stalks satisfy 
  
    
      
        (
        
          f
          
            −
            1
          
        
        
          
            G
          
        
        
          )
          
            x
          
        
        =
        
          
            
              G
            
          
          
            f
            (
            x
            )
          
        
      
    
    {\displaystyle (f^{-1}{\mathcal {G}})_{x}={\mathcal {G}}_{f(x)}}
  .


==== Extension by zero ====
For the inclusion 
  
    
      
        j
        :
        U
        →
        X
      
    
    {\displaystyle j:U\to X}
   of an open subset, the extension by zero of a sheaf of abelian groups on 
  
    
      
        U
      
    
    {\displaystyle U}
   is defined as 

  
    
      
        (
        
          j
          
            !
          
        
        
          
            F
          
        
        )
        (
        V
        )
        =
        
          
            F
          
        
        (
        V
        )
      
    
    {\displaystyle (j_{!}{\mathcal {F}})(V)={\mathcal {F}}(V)}
   if 
  
    
      
        V
        ⊂
        U
      
    
    {\displaystyle V\subset U}
   and 
  
    
      
        (
        
          j
          
            !
          
        
        
          
            F
          
        
        )
        (
        V
        )
        =
        0
      
    
    {\displaystyle (j_{!}{\mathcal {F}})(V)=0}
   otherwise.For a sheaf 
  
    
      
        
          
            G
          
        
      
    
    {\displaystyle {\mathcal {G}}}
   on 
  
    
      
        X
      
    
    {\displaystyle X}
  , this construction is in a sense complementary to 
  
    
      
        
          i
          
            ∗
          
        
      
    
    {\displaystyle i_{*}}
  , where 
  
    
      
        i
      
    
    {\displaystyle i}
   is the inclusion of the complement of 
  
    
      
        U
      
    
    {\displaystyle U}
  :

  
    
      
        (
        
          j
          
            !
          
        
        
          j
          
            ∗
          
        
        
          
            G
          
        
        
          )
          
            x
          
        
        =
        
          
            
              G
            
          
          
            x
          
        
      
    
    {\displaystyle (j_{!}j^{*}{\mathcal {G}})_{x}={\mathcal {G}}_{x}}
   for 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        U
      
    
    {\displaystyle U}
  , and the stalk is zero otherwise, while

  
    
      
        (
        
          i
          
            ∗
          
        
        
          i
          
            ∗
          
        
        
          
            G
          
        
        
          )
          
            x
          
        
        =
        0
      
    
    {\displaystyle (i_{*}i^{*}{\mathcal {G}})_{x}=0}
   for 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        U
      
    
    {\displaystyle U}
  , and equals 
  
    
      
        
          
            
              G
            
          
          
            x
          
        
      
    
    {\displaystyle {\mathcal {G}}_{x}}
   otherwise.These functors are therefore useful in reducing sheaf-theoretic questions on 
  
    
      
        X
      
    
    {\displaystyle X}
   to ones on the strata of a stratification, i.e., a decomposition of 
  
    
      
        X
      
    
    {\displaystyle X}
   into smaller, locally closed subsets.


== Complements ==


=== Sheaves in more general categories ===
In addition to (pre-)sheaves as introduced above, where 
  
    
      
        
          
            F
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {F}}(U)}
   is merely a set, it is in many cases important to keep track of additional structure on these sections. For example, the sections of the sheaf of continuous functions naturally form a real vector space, and restriction is a linear map between these vector spaces.
Presheaves with values in an arbitrary category 
  
    
      
        C
      
    
    {\displaystyle C}
   are defined by first considering the category of open sets on 
  
    
      
        X
      
    
    {\displaystyle X}
   to be the posetal category 
  
    
      
        O
        (
        X
        )
      
    
    {\displaystyle O(X)}
   whose objects are the open sets of 
  
    
      
        X
      
    
    {\displaystyle X}
   and whose morphisms are inclusions. Then a 
  
    
      
        C
      
    
    {\displaystyle C}
  -valued presheaf on 
  
    
      
        X
      
    
    {\displaystyle X}
   is the same as a contravariant functor from 
  
    
      
        O
        (
        X
        )
      
    
    {\displaystyle O(X)}
   to 
  
    
      
        C
      
    
    {\displaystyle C}
  . Morphisms in this category of functors, also known as natural transformations, are the same as the morphisms defined above, as can be seen by unraveling the definitions.
If the target category 
  
    
      
        C
      
    
    {\displaystyle C}
   admits all limits, a 
  
    
      
        C
      
    
    {\displaystyle C}
  -valued presheaf is a sheaf if the following diagram is an equalizer for every open cover

  
    
      
        
          
            U
          
        
        =
        {
        
          U
          
            i
          
        
        
          }
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle {\mathcal {U}}=\{U_{i}\}_{i\in I}}
   of any open set 
  
    
      
        U
      
    
    {\displaystyle U}
  :

  
    
      
        F
        (
        U
        )
        →
        
          ∏
          
            i
          
        
        F
        (
        
          U
          
            i
          
        
        )
        
          
            
              
                

                
                ⟶
              
            
            
              
                ⟶
                

                
              
            
          
        
        
          ∏
          
            i
            ,
            j
          
        
        F
        (
        
          U
          
            i
          
        
        ∩
        
          U
          
            j
          
        
        )
        .
      
    
    {\displaystyle F(U)\rightarrow \prod _{i}F(U_{i}){{{} \atop \longrightarrow } \atop {\longrightarrow  \atop {}}}\prod _{i,j}F(U_{i}\cap U_{j}).}
  Here the first map is the product of the restriction maps

  
    
      
        
          res
          
            
              U
              
                i
              
            
            ,
            U
          
        
        :
        F
        (
        U
        )
        →
        F
        (
        
          U
          
            i
          
        
        )
      
    
    {\displaystyle \operatorname {res} _{U_{i},U}\colon F(U)\rightarrow F(U_{i})}
  and the pair of arrows the products of the two sets of restrictions

  
    
      
        
          res
          
            
              U
              
                i
              
            
            ∩
            
              U
              
                j
              
            
            ,
            
              U
              
                i
              
            
          
        
        :
        F
        (
        
          U
          
            i
          
        
        )
        →
        F
        (
        
          U
          
            i
          
        
        ∩
        
          U
          
            j
          
        
        )
      
    
    {\displaystyle \operatorname {res} _{U_{i}\cap U_{j},U_{i}}\colon F(U_{i})\rightarrow F(U_{i}\cap U_{j})}
  and

  
    
      
        
          res
          
            
              U
              
                i
              
            
            ∩
            
              U
              
                j
              
            
            ,
            
              U
              
                j
              
            
          
        
        :
        F
        (
        
          U
          
            j
          
        
        )
        →
        F
        (
        
          U
          
            i
          
        
        ∩
        
          U
          
            j
          
        
        )
        .
      
    
    {\displaystyle \operatorname {res} _{U_{i}\cap U_{j},U_{j}}\colon F(U_{j})\rightarrow F(U_{i}\cap U_{j}).}
  If 
  
    
      
        C
      
    
    {\displaystyle C}
   is an abelian category, this condition can also be rephrased by requiring that there is an exact sequence

  
    
      
        0
        →
        F
        (
        U
        )
        →
        
          ∏
          
            i
          
        
        F
        (
        
          U
          
            i
          
        
        )
        
          →
          
            
              res
              
                
                  U
                  
                    i
                  
                
                ∩
                
                  U
                  
                    j
                  
                
                ,
                
                  U
                  
                    i
                  
                
              
            
            −
            
              res
              
                
                  U
                  
                    i
                  
                
                ∩
                
                  U
                  
                    j
                  
                
                ,
                
                  U
                  
                    j
                  
                
              
            
          
        
        
          ∏
          
            i
            ,
            j
          
        
        F
        (
        
          U
          
            i
          
        
        ∩
        
          U
          
            j
          
        
        )
        .
      
    
    {\displaystyle 0\to F(U)\to \prod _{i}F(U_{i})\xrightarrow {\operatorname {res} _{U_{i}\cap U_{j},U_{i}}-\operatorname {res} _{U_{i}\cap U_{j},U_{j}}} \prod _{i,j}F(U_{i}\cap U_{j}).}
  A particular case of this sheaf condition occurs for 
  
    
      
        U
      
    
    {\displaystyle U}
   being the empty set, and the index set 
  
    
      
        I
      
    
    {\displaystyle I}
   also being empty. In this case, the sheaf condition requires 
  
    
      
        
          
            F
          
        
        (
        ∅
        )
      
    
    {\displaystyle {\mathcal {F}}(\emptyset )}
   to be the terminal object in 
  
    
      
        C
      
    
    {\displaystyle C}
  .


=== Ringed spaces and sheaves of modules ===

In several geometrical disciplines, including algebraic geometry and differential geometry, the spaces come along with a natural sheaf of rings, often called the structure sheaf and denoted by 
  
    
      
        
          
            
              O
            
          
          
            X
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}}
  . Such a pair 
  
    
      
        (
        X
        ,
        
          
            
              O
            
          
          
            X
          
        
        )
      
    
    {\displaystyle (X,{\mathcal {O}}_{X})}
   is called a ringed space. Many types of spaces can be defined as certain types of ringed spaces. Commonly, all the stalks 
  
    
      
        
          
            
              O
            
          
          
            X
            ,
            x
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X,x}}
   of the structure sheaf are local rings, in which case the pair is called a locally ringed space.
For example, an 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C^{k}}
   manifold 
  
    
      
        M
      
    
    {\displaystyle M}
   is a locally ringed space whose structure sheaf consists of 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C^{k}}
  -functions on the open subsets of 
  
    
      
        M
      
    
    {\displaystyle M}
  . The property of being a locally ringed space translates into the fact that such a function, which is nonzero at a point 
  
    
      
        x
      
    
    {\displaystyle x}
  , is also non-zero on a sufficiently small open neighborhood of 
  
    
      
        x
      
    
    {\displaystyle x}
  . Some authors actually define real (or complex) manifolds to be locally ringed spaces that are locally isomorphic to the pair consisting of an open subset of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   (resp. 
  
    
      
        
          
            C
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {C} ^{n}}
  ) together with the sheaf of 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C^{k}}
   (resp. holomorphic) functions. Similarly, schemes, the foundational notion of spaces in algebraic geometry, are locally ringed spaces that are locally isomorphic to the spectrum of a ring.
Given a ringed space, a sheaf of modules is a sheaf 
  
    
      
        
          
            M
          
        
      
    
    {\displaystyle {\mathcal {M}}}
   such that on every open set 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
  , 
  
    
      
        
          
            M
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {M}}(U)}
   is an 
  
    
      
        
          
            
              O
            
          
          
            X
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {O}}_{X}(U)}
  -module and for every inclusion of open sets 
  
    
      
        V
        ⊆
        U
      
    
    {\displaystyle V\subseteq U}
  , the restriction map 
  
    
      
        
          
            M
          
        
        (
        U
        )
        →
        
          
            M
          
        
        (
        V
        )
      
    
    {\displaystyle {\mathcal {M}}(U)\to {\mathcal {M}}(V)}
   is compatible with the restriction map 
  
    
      
        
          
            O
          
        
        (
        U
        )
        →
        
          
            O
          
        
        (
        V
        )
      
    
    {\displaystyle {\mathcal {O}}(U)\to {\mathcal {O}}(V)}
  : the restriction of fs is the restriction of 
  
    
      
        f
      
    
    {\displaystyle f}
   times that of 
  
    
      
        s
      
    
    {\displaystyle s}
   for any 
  
    
      
        f
      
    
    {\displaystyle f}
   in 
  
    
      
        
          
            O
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {O}}(U)}
   and 
  
    
      
        s
      
    
    {\displaystyle s}
   in 
  
    
      
        
          
            M
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {M}}(U)}
  .
Most important geometric objects are sheaves of modules. For example, there is a one-to-one correspondence between vector bundles and locally free sheaves of 
  
    
      
        
          
            
              O
            
          
          
            X
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}}
  -modules. This paradigm applies to real vector bundles, complex vector bundles, or vector bundles in algebraic geometry (where 
  
    
      
        
          
            O
          
        
      
    
    {\displaystyle {\mathcal {O}}}
   consists of smooth functions, holomorphic functions, or regular functions, respectively). Sheaves of solutions to differential equations are 
  
    
      
        D
      
    
    {\displaystyle D}
  -modules, that is, modules over the sheaf of differential operators. On any topological space, modules over the constant sheaf 
  
    
      
        
          
            
              Z
            
            _
          
        
      
    
    {\displaystyle {\underline {\mathbf {Z} }}}
   are the same as sheaves of abelian groups in the sense above.
There is a different inverse image functor for sheaves of modules over sheaves of rings. This functor is usually denoted 
  
    
      
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f^{*}}
   and it is distinct from 
  
    
      
        
          f
          
            −
            1
          
        
      
    
    {\displaystyle f^{-1}}
  . See inverse image functor.


==== Finiteness conditions for sheaves of modules ====
Finiteness conditions for module over commutative rings give rise to similar finiteness conditions for sheaves of modules: 
  
    
      
        
          
            M
          
        
      
    
    {\displaystyle {\mathcal {M}}}
   is called finitely generated (resp. finitely presented) if, for every point 
  
    
      
        x
      
    
    {\displaystyle x}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
  , there exists an open neighborhood 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        x
      
    
    {\displaystyle x}
  , a natural number 
  
    
      
        n
      
    
    {\displaystyle n}
   (possibly depending on 
  
    
      
        U
      
    
    {\displaystyle U}
  ), and a surjective morphism of sheaves 
  
    
      
        
          
            
              O
            
          
          
            X
          
          
            n
          
        
        
          
            |
          
          
            U
          
        
        →
        
          
            M
          
        
        
          
            |
          
          
            U
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}^{n}|_{U}\to {\mathcal {M}}|_{U}}
   (respectively, in addition a natural number 
  
    
      
        m
      
    
    {\displaystyle m}
  , and an exact sequence 
  
    
      
        
          
            
              O
            
          
          
            X
          
          
            m
          
        
        
          
            |
          
          
            U
          
        
        →
        
          
            
              O
            
          
          
            X
          
          
            n
          
        
        
          
            |
          
          
            U
          
        
        →
        
          
            M
          
        
        
          
            |
          
          
            U
          
        
        →
        0
      
    
    {\displaystyle {\mathcal {O}}_{X}^{m}|_{U}\to {\mathcal {O}}_{X}^{n}|_{U}\to {\mathcal {M}}|_{U}\to 0}
  .) Paralleling the notion of a coherent module, 
  
    
      
        
          
            M
          
        
      
    
    {\displaystyle {\mathcal {M}}}
   is called a coherent sheaf if it is of finite type and if, for every open set 
  
    
      
        U
      
    
    {\displaystyle U}
   and every morphism of sheaves 
  
    
      
        ϕ
        :
        
          
            
              O
            
          
          
            X
          
          
            n
          
        
        →
        
          
            M
          
        
      
    
    {\displaystyle \phi :{\mathcal {O}}_{X}^{n}\to {\mathcal {M}}}
   (not necessarily surjective), the kernel of 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   is of finite type. 
  
    
      
        
          
            
              O
            
          
          
            X
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}}
   is coherent if it is coherent as a module over itself. Like for modules, coherence is in general a strictly stronger condition than finite presentation. The Oka coherence theorem states that the sheaf of holomorphic functions on a complex manifold is coherent.


=== The étalé space of a sheaf ===

In the examples above it was noted that some sheaves occur naturally as sheaves of sections. In fact, all sheaves of sets can be represented as sheaves of sections of a topological space called the étalé space, from the French word étalé [etale], meaning roughly ""spread out"". If 
  
    
      
        F
        ∈
        
          Sh
        
        (
        X
        )
      
    
    {\displaystyle F\in {\text{Sh}}(X)}
   is a sheaf over 
  
    
      
        X
      
    
    {\displaystyle X}
  , then the étalé space (sometimes called the étale space) of 
  
    
      
        F
      
    
    {\displaystyle F}
   is a topological space 
  
    
      
        E
      
    
    {\displaystyle E}
   together with a local homeomorphism 
  
    
      
        π
        :
        E
        →
        X
      
    
    {\displaystyle \pi :E\to X}
   such that the sheaf of sections 
  
    
      
        Γ
        (
        π
        ,
        −
        )
      
    
    {\displaystyle \Gamma (\pi ,-)}
   of 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is 
  
    
      
        F
      
    
    {\displaystyle F}
  . The space 
  
    
      
        E
      
    
    {\displaystyle E}
   is usually very strange, and even if the sheaf 
  
    
      
        F
      
    
    {\displaystyle F}
   arises from a natural topological situation, 
  
    
      
        E
      
    
    {\displaystyle E}
   may not have any clear topological interpretation. For example, if 
  
    
      
        F
      
    
    {\displaystyle F}
   is the sheaf of sections of a continuous function 
  
    
      
        f
        :
        Y
        →
        X
      
    
    {\displaystyle f:Y\to X}
  , then 
  
    
      
        E
        =
        Y
      
    
    {\displaystyle E=Y}
   if and only if 
  
    
      
        f
      
    
    {\displaystyle f}
   is a local homeomorphism.
The étalé space 
  
    
      
        E
      
    
    {\displaystyle E}
   is constructed from the stalks of 
  
    
      
        F
      
    
    {\displaystyle F}
   over 
  
    
      
        X
      
    
    {\displaystyle X}
  . As a set, it is their disjoint union and 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is the obvious map that takes the value 
  
    
      
        x
      
    
    {\displaystyle x}
   on the stalk of 
  
    
      
        F
      
    
    {\displaystyle F}
   over 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
  . The topology of 
  
    
      
        E
      
    
    {\displaystyle E}
   is defined as follows. For each element 
  
    
      
        s
        ∈
        F
        (
        U
        )
      
    
    {\displaystyle s\in F(U)}
   and each 
  
    
      
        x
        ∈
        U
      
    
    {\displaystyle x\in U}
  , we get a germ of 
  
    
      
        s
      
    
    {\displaystyle s}
   at 
  
    
      
        x
      
    
    {\displaystyle x}
  , denoted 
  
    
      
        [
        s
        
          ]
          
            x
          
        
      
    
    {\displaystyle [s]_{x}}
   or 
  
    
      
        
          s
          
            x
          
        
      
    
    {\displaystyle s_{x}}
  . These germs determine points of 
  
    
      
        E
      
    
    {\displaystyle E}
  . For any 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        s
        ∈
        F
        (
        U
        )
      
    
    {\displaystyle s\in F(U)}
  , the union of these points (for all 
  
    
      
        x
        ∈
        U
      
    
    {\displaystyle x\in U}
  ) is declared to be open in 
  
    
      
        E
      
    
    {\displaystyle E}
  . Notice that each stalk has the discrete topology as subspace topology. Two morphisms between sheaves determine a continuous map of the corresponding étalé spaces that is compatible with the projection maps (in the sense that every germ is mapped to a germ over the same point). This makes the construction into a functor.
The construction above determines an equivalence of categories between the category of sheaves of sets on 
  
    
      
        X
      
    
    {\displaystyle X}
   and the category of étalé spaces over 
  
    
      
        X
      
    
    {\displaystyle X}
  . The construction of an étalé space can also be applied to a presheaf, in which case the sheaf of sections of the étalé space recovers the sheaf associated to the given presheaf.

This construction makes all sheaves into representable functors on certain categories of topological spaces. As above, let 
  
    
      
        F
      
    
    {\displaystyle F}
   be a sheaf on 
  
    
      
        X
      
    
    {\displaystyle X}
  , let 
  
    
      
        E
      
    
    {\displaystyle E}
   be its étalé space, and let 
  
    
      
        π
        :
        E
        →
        X
      
    
    {\displaystyle \pi :E\to X}
   be the natural projection. Consider the overcategory 
  
    
      
        
          Top
        
        
          /
        
        X
      
    
    {\displaystyle {\text{Top}}/X}
   of topological spaces over 
  
    
      
        X
      
    
    {\displaystyle X}
  , that is, the category of topological spaces together with fixed continuous maps to 
  
    
      
        X
      
    
    {\displaystyle X}
  . Every object of this category is a continuous map 
  
    
      
        f
        :
        Y
        →
        X
      
    
    {\displaystyle f:Y\to X}
  , and a morphism from 
  
    
      
        Y
        →
        X
      
    
    {\displaystyle Y\to X}
   to 
  
    
      
        Z
        →
        X
      
    
    {\displaystyle Z\to X}
   is a continuous map 
  
    
      
        Y
        →
        Z
      
    
    {\displaystyle Y\to Z}
   that commutes with the two maps to 
  
    
      
        X
      
    
    {\displaystyle X}
  . There is a functor
  
    
      
        Γ
        :
        
          Top
        
        
          /
        
        X
        →
        
          Sets
        
      
    
    {\displaystyle \Gamma :{\text{Top}}/X\to {\text{Sets}}}
  sending an object 
  
    
      
        f
        :
        Y
        →
        X
      
    
    {\displaystyle f:Y\to X}
   to 
  
    
      
        
          f
          
            −
            1
          
        
        F
        (
        Y
        )
      
    
    {\displaystyle f^{-1}F(Y)}
  . For example, if 
  
    
      
        i
        :
        U
        ↪
        X
      
    
    {\displaystyle i:U\hookrightarrow X}
   is the inclusion of an open subset, then
  
    
      
        Γ
        (
        i
        )
        =
        
          f
          
            −
            1
          
        
        F
        (
        U
        )
        =
        F
        (
        U
        )
        =
        Γ
        (
        F
        ,
        U
        )
      
    
    {\displaystyle \Gamma (i)=f^{-1}F(U)=F(U)=\Gamma (F,U)}
  and for the inclusion of a point 
  
    
      
        i
        :
        {
        x
        }
        ↪
        X
      
    
    {\displaystyle i:\{x\}\hookrightarrow X}
  , then
  
    
      
        Γ
        (
        i
        )
        =
        
          f
          
            −
            1
          
        
        F
        (
        {
        x
        }
        )
        =
        F
        
          
            |
          
          
            x
          
        
      
    
    {\displaystyle \Gamma (i)=f^{-1}F(\{x\})=F|_{x}}
  is the stalk of 
  
    
      
        F
      
    
    {\displaystyle F}
   at 
  
    
      
        x
      
    
    {\displaystyle x}
  . There is a natural isomorphism
  
    
      
        (
        
          f
          
            −
            1
          
        
        F
        )
        (
        Y
        )
        ≅
        
          Hom
          
            
              T
              o
              p
            
            
              /
            
            X
          
        
        ⁡
        (
        f
        ,
        π
        )
      
    
    {\displaystyle (f^{-1}F)(Y)\cong \operatorname {Hom} _{\mathbf {Top} /X}(f,\pi )}
  ,which shows that 
  
    
      
        π
        :
        E
        →
        X
      
    
    {\displaystyle \pi :E\to X}
   (for the étalé space) represents the functor 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  .

  
    
      
        E
      
    
    {\displaystyle E}
   is constructed so that the projection map 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is a covering map. In algebraic geometry, the natural analog of a covering map is called an étale morphism. Despite its similarity to ""étalé"", the word étale [etal] has a different meaning in French. It is possible to turn 
  
    
      
        E
      
    
    {\displaystyle E}
   into a scheme and 
  
    
      
        π
      
    
    {\displaystyle \pi }
   into a morphism of schemes in such a way that 
  
    
      
        π
      
    
    {\displaystyle \pi }
   retains the same universal property, but 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is not in general an étale morphism because it is not quasi-finite. It is, however, formally étale.
The definition of sheaves by étalé spaces is older than the definition given earlier in the article. It is still common in some areas of mathematics such as mathematical analysis.


== Sheaf cohomology ==

In contexts where the open set 
  
    
      
        U
      
    
    {\displaystyle U}
   is fixed, and the sheaf is regarded as a variable, the set 
  
    
      
        F
        (
        U
        )
      
    
    {\displaystyle F(U)}
   is also often denoted 
  
    
      
        Γ
        (
        U
        ,
        F
        )
        .
      
    
    {\displaystyle \Gamma (U,F).}
  

As was noted above, this functor does not preserve epimorphisms. Instead, an epimorphism of sheaves 
  
    
      
        
          
            F
          
        
        →
        
          
            G
          
        
      
    
    {\displaystyle {\mathcal {F}}\to {\mathcal {G}}}
   is a map with the following property: for any section 
  
    
      
        g
        ∈
        
          
            G
          
        
        (
        U
        )
      
    
    {\displaystyle g\in {\mathcal {G}}(U)}
   there is a covering 
  
    
      
        
          
            U
          
        
        =
        {
        
          U
          
            i
          
        
        
          }
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle {\mathcal {U}}=\{U_{i}\}_{i\in I}}
   where 
  
    
      
        U
        =
        
          ⋃
          
            i
            ∈
            I
          
        
        
          U
          
            i
          
        
      
    
    {\displaystyle U=\bigcup _{i\in I}U_{i}}
   of open subsets, such that the restriction 
  
    
      
        g
        
          
            |
          
          
            
              U
              
                i
              
            
          
        
      
    
    {\displaystyle g|_{U_{i}}}
   are in the image of 
  
    
      
        
          
            F
          
        
        (
        
          U
          
            i
          
        
        )
      
    
    {\displaystyle {\mathcal {F}}(U_{i})}
  . However, 
  
    
      
        g
      
    
    {\displaystyle g}
   itself need not be in the image of 
  
    
      
        
          
            F
          
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {F}}(U)}
  . A concrete example of this phenomenon is the exponential map 

  
    
      
        
          
            O
          
        
        
          
            
              
                →
              
              
                exp
              
            
          
        
        
          
            
              O
            
          
          
            ×
          
        
      
    
    {\displaystyle {\mathcal {O}}{\stackrel {\exp }{\to }}{\mathcal {O}}^{\times }}
  between the sheaf of holomorphic functions and non-zero holomorphic functions. This map is an epimorphism, which amounts to saying that any non-zero holomorphic function 
  
    
      
        g
      
    
    {\displaystyle g}
   (on some open subset in 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
  , say), admits a complex logarithm locally, i.e., after restricting 
  
    
      
        g
      
    
    {\displaystyle g}
   to appropriate open subsets. However, 
  
    
      
        g
      
    
    {\displaystyle g}
   need not have a logarithm globally.
Sheaf cohomology captures this phenomenon. More precisely, for an exact sequence of sheaves of abelian groups

  
    
      
        0
        →
        
          
            
              F
            
          
          
            1
          
        
        →
        
          
            
              F
            
          
          
            2
          
        
        →
        
          
            
              F
            
          
          
            3
          
        
        →
        0
        ,
      
    
    {\displaystyle 0\to {\mathcal {F}}_{1}\to {\mathcal {F}}_{2}\to {\mathcal {F}}_{3}\to 0,}
  (i.e., an epimorphism 
  
    
      
        
          
            
              F
            
          
          
            2
          
        
        →
        
          
            
              F
            
          
          
            3
          
        
      
    
    {\displaystyle {\mathcal {F}}_{2}\to {\mathcal {F}}_{3}}
   whose kernel is 
  
    
      
        
          
            
              F
            
          
          
            1
          
        
      
    
    {\displaystyle {\mathcal {F}}_{1}}
  ), there is a long exact sequenceBy means of this sequence, the first cohomology group 
  
    
      
        
          H
          
            1
          
        
        (
        U
        ,
        
          
            
              F
            
          
          
            1
          
        
        )
      
    
    {\displaystyle H^{1}(U,{\mathcal {F}}_{1})}
   is a measure for the non-surjectivity of the map between sections of 
  
    
      
        
          
            
              F
            
          
          
            2
          
        
      
    
    {\displaystyle {\mathcal {F}}_{2}}
   and 
  
    
      
        
          
            
              F
            
          
          
            3
          
        
      
    
    {\displaystyle {\mathcal {F}}_{3}}
  .
There are several different ways of constructing sheaf cohomology. Grothendieck (1957) introduced them by defining sheaf cohomology as the derived functor of 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  . This method is theoretically satisfactory, but, being based on injective resolutions, of little use in concrete computations. Godement resolutions are another general, but practically inaccessible approach.


=== Computing sheaf cohomology ===
Especially in the context of sheaves on manifolds, sheaf cohomology can often be computed using resolutions by soft sheaves, fine sheaves, and flabby sheaves (also known as flasque sheaves from the French flasque meaning flabby). For example, a partition of unity argument shows that the sheaf of smooth functions on a manifold is soft. The higher cohomology groups 
  
    
      
        
          H
          
            i
          
        
        (
        U
        ,
        
          
            F
          
        
        )
      
    
    {\displaystyle H^{i}(U,{\mathcal {F}})}
   for 
  
    
      
        i
        >
        0
      
    
    {\displaystyle i>0}
   vanish for soft sheaves, which gives a way of computing cohomology of other sheaves. For example, the de Rham complex is a resolution of the constant sheaf 
  
    
      
        
          
            
              R
            
            _
          
        
      
    
    {\displaystyle {\underline {\mathbf {R} }}}
   on any smooth manifold, so the sheaf cohomology of 
  
    
      
        
          
            
              R
            
            _
          
        
      
    
    {\displaystyle {\underline {\mathbf {R} }}}
   is equal to its de Rham cohomology.
A different approach is by Čech cohomology. Čech cohomology was the first cohomology theory developed for sheaves and it is well-suited to concrete calculations, such as computing the coherent sheaf cohomology of complex projective space 
  
    
      
        
          
            P
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {P} ^{n}}
  . It relates sections on open subsets of the space to cohomology classes on the space. In most cases, Čech cohomology computes the same cohomology groups as the derived functor cohomology. However, for some pathological spaces, Čech cohomology will give the correct 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H^{1}}
   but incorrect higher cohomology groups. To get around this, Jean-Louis Verdier developed hypercoverings. Hypercoverings not only give the correct higher cohomology groups but also allow the open subsets mentioned above to be replaced by certain morphisms from another space. This flexibility is necessary in some applications, such as the construction of Pierre Deligne's mixed Hodge structures.

Many other coherent sheaf cohomology groups are found using an embedding 
  
    
      
        i
        :
        X
        ↪
        Y
      
    
    {\displaystyle i:X\hookrightarrow Y}
   of a space 
  
    
      
        X
      
    
    {\displaystyle X}
   into a space with known cohomology, such as 
  
    
      
        
          
            P
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {P} ^{n}}
  , or some weighted projective space. In this way, the known sheaf cohomology groups on these ambient spaces can be related to the sheaves 
  
    
      
        
          i
          
            ∗
          
        
        
          
            F
          
        
      
    
    {\displaystyle i_{*}{\mathcal {F}}}
  , giving 
  
    
      
        
          H
          
            i
          
        
        (
        Y
        ,
        
          i
          
            ∗
          
        
        
          
            F
          
        
        )
        ≅
        
          H
          
            i
          
        
        (
        X
        ,
        
          
            F
          
        
        )
      
    
    {\displaystyle H^{i}(Y,i_{*}{\mathcal {F}})\cong H^{i}(X,{\mathcal {F}})}
  . For example, computing the coherent sheaf cohomology of projective plane curves is easily found. One big theorem in this space is the Hodge decomposition found using a spectral sequence associated to sheaf cohomology groups, proved by Deligne. Essentially, the 
  
    
      
        
          E
          
            1
          
        
      
    
    {\displaystyle E_{1}}
  -page with terms
  
    
      
        
          E
          
            1
          
          
            p
            ,
            q
          
        
        =
        
          H
          
            p
          
        
        (
        X
        ,
        
          Ω
          
            X
          
          
            q
          
        
        )
      
    
    {\displaystyle E_{1}^{p,q}=H^{p}(X,\Omega _{X}^{q})}
  the sheaf cohomology of a smooth projective variety 
  
    
      
        X
      
    
    {\displaystyle X}
  , degenerates, meaning 
  
    
      
        
          E
          
            1
          
        
        =
        
          E
          
            ∞
          
        
      
    
    {\displaystyle E_{1}=E_{\infty }}
  . This gives the canonical Hodge structure on the cohomology groups 
  
    
      
        
          H
          
            k
          
        
        (
        X
        ,
        
          C
        
        )
      
    
    {\displaystyle H^{k}(X,\mathbb {C} )}
  . It was later found these cohomology groups can be easily explicitly computed using Griffiths residues. See Jacobian ideal. These kinds of theorems lead to one of the deepest theorems about the cohomology of algebraic varieties, the decomposition theorem, paving the path for Mixed Hodge modules.
Another clean approach to the computation of some cohomology groups is the Borel–Bott–Weil theorem, which identifies the cohomology groups of some line bundles on flag manifolds with irreducible representations of Lie groups. This theorem can be used, for example, to easily compute the cohomology groups of all line bundles on projective space and grassmann manifolds.
In many cases there is a duality theory for sheaves that generalizes Poincaré duality. See Grothendieck duality and Verdier duality.


=== Derived categories of sheaves ===
The derived category of the category of sheaves of, say, abelian groups on some space X, denoted here as 
  
    
      
        D
        (
        X
        )
      
    
    {\displaystyle D(X)}
  , is the conceptual haven for sheaf cohomology, by virtue of the following relation: 

  
    
      
        
          H
          
            n
          
        
        (
        X
        ,
        
          
            F
          
        
        )
        =
        
          Hom
          
            D
            (
            X
            )
          
        
        ⁡
        (
        
          Z
        
        ,
        
          
            F
          
        
        [
        n
        ]
        )
        .
      
    
    {\displaystyle H^{n}(X,{\mathcal {F}})=\operatorname {Hom} _{D(X)}(\mathbf {Z} ,{\mathcal {F}}[n]).}
  The adjunction between 
  
    
      
        
          f
          
            −
            1
          
        
      
    
    {\displaystyle f^{-1}}
  , which is the left adjoint of 
  
    
      
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f_{*}}
   (already on the level of sheaves of abelian groups) gives rise to an adjunction 

  
    
      
        
          f
          
            −
            1
          
        
        :
        D
        (
        Y
        )
        ⇄
        D
        (
        X
        )
        :
        R
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f^{-1}:D(Y)\rightleftarrows D(X):Rf_{*}}
   (for 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
  ),where 
  
    
      
        R
        
          f
          
            ∗
          
        
      
    
    {\displaystyle Rf_{*}}
   is the derived functor. This latter functor encompasses the notion of sheaf cohomology since 
  
    
      
        
          H
          
            n
          
        
        (
        X
        ,
        
          
            F
          
        
        )
        =
        
          R
          
            n
          
        
        
          f
          
            ∗
          
        
        
          
            F
          
        
      
    
    {\displaystyle H^{n}(X,{\mathcal {F}})=R^{n}f_{*}{\mathcal {F}}}
   for 
  
    
      
        f
        :
        X
        →
        {
        ∗
        }
      
    
    {\displaystyle f:X\to \{*\}}
  .

Like 
  
    
      
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f_{*}}
  , the direct image with compact support 
  
    
      
        
          f
          
            !
          
        
      
    
    {\displaystyle f_{!}}
   can also be derived. By virtue of the following isomorphism 
  
    
      
        R
        
          f
          
            !
          
        
        F
      
    
    {\displaystyle Rf_{!}F}
   parametrizes the cohomology with compact support of the fibers of 
  
    
      
        f
      
    
    {\displaystyle f}
  :

  
    
      
        (
        
          R
          
            i
          
        
        
          f
          
            !
          
        
        F
        
          )
          
            y
          
        
        =
        
          H
          
            c
          
          
            i
          
        
        (
        
          f
          
            −
            1
          
        
        (
        y
        )
        ,
        F
        )
        .
      
    
    {\displaystyle (R^{i}f_{!}F)_{y}=H_{c}^{i}(f^{-1}(y),F).}
  This isomorphism is an example of a base change theorem. There is another adjunction

  
    
      
        R
        
          f
          
            !
          
        
        :
        D
        (
        X
        )
        ⇄
        D
        (
        Y
        )
        :
        
          f
          
            !
          
        
        .
      
    
    {\displaystyle Rf_{!}:D(X)\rightleftarrows D(Y):f^{!}.}
  Unlike all the functors considered above, the twisted (or exceptional) inverse image functor 
  
    
      
        
          f
          
            !
          
        
      
    
    {\displaystyle f^{!}}
   is in general only defined on the level of derived categories, i.e., the functor is not obtained as the derived functor of some functor between abelian categories. If 
  
    
      
        f
        :
        X
        →
        {
        ∗
        }
      
    
    {\displaystyle f:X\to \{*\}}
   and X is a smooth orientable manifold of dimension n, then

  
    
      
        
          f
          
            !
          
        
        
          
            
              R
            
            _
          
        
        ≅
        
          
            
              R
            
            _
          
        
        [
        n
        ]
        .
      
    
    {\displaystyle f^{!}{\underline {\mathbf {R} }}\cong {\underline {\mathbf {R} }}[n].}
  This computation, and the compatibility of the functors with duality (see Verdier duality) can be used to obtain a high-brow explanation of Poincaré duality. In the context of quasi-coherent sheaves on schemes, there is a similar duality known as coherent duality.
Perverse sheaves are certain objects in 
  
    
      
        D
        (
        X
        )
      
    
    {\displaystyle D(X)}
  , i.e., complexes of sheaves (but not in general sheaves proper). They are an important tool to study the geometry of singularities.


==== Derived categories of coherent sheaves and the Grothendieck group ====
Another important application of derived categories of sheaves is with the derived category of coherent sheaves on a scheme 
  
    
      
        X
      
    
    {\displaystyle X}
   denoted 
  
    
      
        
          D
          
            C
            o
            h
          
        
        (
        X
        )
      
    
    {\displaystyle D_{Coh}(X)}
  . This was used by Grothendieck in his development of intersection theory using derived categories and K-theory, that the intersection product of subschemes 
  
    
      
        
          Y
          
            1
          
        
        ,
        
          Y
          
            2
          
        
      
    
    {\displaystyle Y_{1},Y_{2}}
   is represented in K-theory as
  
    
      
        [
        
          Y
          
            1
          
        
        ]
        ⋅
        [
        
          Y
          
            2
          
        
        ]
        =
        [
        
          
            
              O
            
          
          
            
              Y
              
                1
              
            
          
        
        
          ⊗
          
            
              
                
                  O
                
              
              
                X
              
            
          
          
            
              L
            
          
        
        
          
            
              O
            
          
          
            
              Y
              
                2
              
            
          
        
        ]
        ∈
        K
        (
        
          Coh(X)
        
        )
      
    
    {\displaystyle [Y_{1}]\cdot [Y_{2}]=[{\mathcal {O}}_{Y_{1}}\otimes _{{\mathcal {O}}_{X}}^{\mathbf {L} }{\mathcal {O}}_{Y_{2}}]\in K({\text{Coh(X)}})}
  where 
  
    
      
        
          
            
              O
            
          
          
            
              Y
              
                i
              
            
          
        
      
    
    {\displaystyle {\mathcal {O}}_{Y_{i}}}
   are coherent sheaves defined by the 
  
    
      
        
          
            
              O
            
          
          
            X
          
        
      
    
    {\displaystyle {\mathcal {O}}_{X}}
  -modules given by their structure sheaves.


== Sites and topoi ==

André Weil's Weil conjectures stated that there was a cohomology theory for algebraic varieties over finite fields that would give an analogue of the Riemann hypothesis. The cohomology of a complex manifold can be defined as the sheaf cohomology of the locally constant sheaf 
  
    
      
        
          
            
              C
            
            _
          
        
      
    
    {\displaystyle {\underline {\mathbf {C} }}}
   in the Euclidean topology, which suggests defining a Weil cohomology theory in positive characteristic as the sheaf cohomology of a constant sheaf. But the only classical topology on such a variety is the Zariski topology, and the Zariski topology has very few open sets, so few that the cohomology of any Zariski-constant sheaf on an irreducible variety vanishes (except in degree zero). Alexandre Grothendieck solved this problem by introducing Grothendieck topologies, which axiomatize the notion of covering. Grothendieck's insight was that the definition of a sheaf depends only on the open sets of a topological space, not on the individual points. Once he had axiomatized the notion of covering, open sets could be replaced by other objects. A presheaf takes each one of these objects to data, just as before, and a sheaf is a presheaf that satisfies the gluing axiom with respect to our new notion of covering. This allowed Grothendieck to define étale cohomology and ℓ-adic cohomology, which eventually were used to prove the Weil conjectures.
A category with a Grothendieck topology is called a site. A category of sheaves on a site is called a topos or a Grothendieck topos. The notion of a topos was later abstracted by William Lawvere and Miles Tierney to define an elementary topos, which has connections to mathematical logic.


== History ==
The first origins of sheaf theory are hard to pin down – they may be co-extensive with the idea of analytic continuation. It took about 15 years for a recognisable, free-standing theory of sheaves to emerge from the foundational work on cohomology.

1936 Eduard Čech introduces the nerve construction, for associating a simplicial complex to an open covering.
1938 Hassler Whitney gives a 'modern' definition of cohomology, summarizing the work since J. W. Alexander and Kolmogorov first defined cochains.
1943 Norman Steenrod publishes on homology with local coefficients.
1945 Jean Leray publishes work carried out as a prisoner of war, motivated by proving fixed-point theorems for application to PDE theory; it is the start of sheaf theory and spectral sequences.
1947 Henri Cartan reproves the de Rham theorem by sheaf methods, in correspondence with André Weil (see De Rham–Weil theorem). Leray gives a sheaf definition in his courses via closed sets (the later carapaces).
1948 The Cartan seminar writes up sheaf theory for the first time.
1950 The ""second edition"" sheaf theory from the Cartan seminar: the sheaf space (espace étalé) definition is used, with stalkwise structure. Supports are introduced, and cohomology with supports. Continuous mappings give rise to spectral sequences. At the same time Kiyoshi Oka introduces an idea (adjacent to that) of a sheaf of ideals, in several complex variables.
1951 The Cartan seminar proves theorems A and B, based on Oka's work.
1953 The finiteness theorem for coherent sheaves in the analytic theory is proved by Cartan and Jean-Pierre Serre, as is Serre duality.
1954 Serre's paper Faisceaux algébriques cohérents (published in 1955) introduces sheaves into algebraic geometry. These ideas are immediately exploited by Friedrich Hirzebruch, who writes a major 1956 book on topological methods.
1955 Alexander Grothendieck in lectures in Kansas defines abelian category and presheaf, and by using injective resolutions allows direct use of sheaf cohomology on all topological spaces, as derived functors.
1956 Oscar Zariski's report Algebraic sheaf theory
1957 Grothendieck's Tohoku paper rewrites homological algebra; he proves Grothendieck duality (i.e., Serre duality for possibly singular algebraic varieties).
1957 onwards: Grothendieck extends sheaf theory in line with the needs of algebraic geometry, introducing: schemes and general sheaves on them, local cohomology, derived categories (with Verdier), and Grothendieck topologies. There emerges also his influential schematic idea of 'six operations' in homological algebra.
1958 Roger Godement's book on sheaf theory is published. At around this time Mikio Sato proposes his hyperfunctions, which will turn out to have sheaf-theoretic nature.At this point sheaves had become a mainstream part of mathematics, with use by no means restricted to algebraic topology. It was later discovered that the logic in categories of sheaves is intuitionistic logic (this observation is now often referred to as Kripke–Joyal semantics, but probably should be attributed to a number of authors).


== See also ==
Coherent sheaf
Gerbe
Stack (mathematics)
Sheaf of spectra
Perverse sheaf
Presheaf of spaces
Constructible sheaf
De Rham's theorem


== Notes ==


== References ==
Bredon, Glen E. (1997), Sheaf theory, Graduate Texts in Mathematics, vol. 170 (2nd ed.), Springer-Verlag, ISBN 978-0-387-94905-5, MR 1481706 (oriented towards conventional topological applications)
de Cataldo, Andrea Mark; Migliorini, Luca (2010). ""What is a perverse sheaf?"" (PDF). Notices of the American Mathematical Society. 57 (5): 632–4. arXiv:1004.2983. Bibcode:2010arXiv1004.2983D. MR 2664042.
Godement, Roger (2006) [1973], Topologie algébrique et théorie des faisceaux, Paris: Hermann, ISBN 2705612521, MR 0345092
Grothendieck, Alexander (1957), ""Sur quelques points d'algèbre homologique"", The Tohoku Mathematical Journal, Second Series, 9 (2): 119–221, doi:10.2748/tmj/1178244839, ISSN 0040-8735, MR 0102537
Hirzebruch, Friedrich (1995), Topological methods in algebraic geometry, Classics in Mathematics, Springer-Verlag, ISBN 978-3-540-58663-0, MR 1335917 (updated edition of a classic using enough sheaf theory to show its power)
Iversen, Birger (1986), Cohomology of sheaves, Universitext, Springer, doi:10.1007/978-3-642-82783-9, ISBN 3-540-16389-1, MR 0842190
Kashiwara, Masaki; Schapira, Pierre (1994), Sheaves on manifolds, Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], vol. 292, Springer-Verlag, ISBN 978-3-540-51861-7, MR 1299726 (advanced techniques such as the derived category and vanishing cycles on the most reasonable spaces)
Mac Lane, Saunders; Moerdijk, Ieke (1994), Sheaves in Geometry and Logic: A First Introduction to Topos Theory, Universitext, Springer-Verlag, ISBN 978-0-387-97710-2, MR 1300636 (category theory and toposes emphasised)
Martin, William T.; Chern, Shiing-Shen; Zariski, Oscar (1956), ""Scientific report on the Second Summer Institute, several complex variables"", Bulletin of the American Mathematical Society, 62 (2): 79–141, doi:10.1090/S0002-9904-1956-10013-X, ISSN 0002-9904, MR 0077995
Ramanan, S. (2005), Global calculus, Graduate Studies in Mathematics, vol. 65, American Mathematical Society, doi:10.1090/gsm/065, ISBN 0-8218-3702-8, MR 2104612
Seebach, J. Arthur; Seebach, Linda A.; Steen, Lynn A. (1970), ""What is a Sheaf"", American Mathematical Monthly, 77 (7): 681–703, doi:10.1080/00029890.1970.11992563, MR 0263073, S2CID 203043621
Serre, Jean-Pierre (1955), ""Faisceaux algébriques cohérents"" (PDF), Annals of Mathematics, Second Series, 61 (2): 197–278, doi:10.2307/1969915, ISSN 0003-486X, JSTOR 1969915, MR 0068874
Swan, Richard G. (1964), The Theory of Sheaves, Chicago lectures in mathematics (3 ed.), University of Chicago Press, ISBN 9780226783291 (concise lecture notes)
Tennison, Barry R. (1975), Sheaf theory, London Mathematical Society Lecture Note Series, vol. 20, Cambridge University Press, ISBN 978-0-521-20784-3, MR 0404390 (pedagogic treatment)
Rosiak, Daniel (2022). Sheaf theory through examples. Cambridge, Massachusetts. doi:10.7551/mitpress/12581.001.0001. ISBN 978-0-262-37042-4. OCLC 1333708310. S2CID 253133215. (introductory book with open access)"
fde96e75b7,Parity (mathematics),"In mathematics, parity is the property of an integer of whether it is even or odd. An integer is even if it is a multiple of two, and odd if it is not. For example, −4, 0, 82 are even because

By contrast, −3, 5, 7, 21 are odd numbers. The above definition of parity applies only to integer numbers, hence it cannot be applied to numbers like 1/2 or 4.201. See the section ""Higher mathematics"" below for some extensions of the notion of parity to a larger class of ""numbers"" or in other more general settings.
Even and odd numbers have opposite parities, e.g., 22 (even number) and 13 (odd number) have opposite parities. In particular, the parity of zero is even. Any two consecutive integers have opposite parity. A number (i.e., integer) expressed in the decimal numeral system is even or odd according to whether its last digit is even or odd. That is, if the last digit is 1, 3, 5, 7, or 9, then it is odd; otherwise it is even—as the last digit of any even number is 0, 2, 4, 6, or 8. The same idea will work using any even base. In particular, a number expressed in the binary numeral system is odd if its last digit is 1; and it is even if its last digit is 0. In an odd base, the number is even according to the sum of its digits—it is even if and only if the sum of its digits is even.


== Definition ==
An even number is an integer of the form

where k is an integer; an odd number is an integer of the form

An equivalent definition is that an even number is divisible by 2:

and an odd number is not:

The sets of even and odd numbers can be defined as following:

The set of even numbers is a normal subgroup of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   and create the factor group 
  
    
      
        
          Z
        
        
          /
        
        2
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /2\mathbb {Z} }
  . Parity can then be defined as a homomorphism from 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   to 
  
    
      
        
          Z
        
        
          /
        
        2
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /2\mathbb {Z} }
   where odd numbers are 1 and even numbers are 0. The consequences of this homomorphism are covered below.


== Properties ==
The following laws can be verified using the properties of divisibility. They are a special case of rules in modular arithmetic, and are commonly used to check if an equality is likely to be correct by testing the parity of each side. As with ordinary arithmetic, multiplication and addition are commutative and associative in modulo 2 arithmetic, and multiplication is distributive over addition. However, subtraction in modulo 2 is identical to addition, so subtraction also possesses these properties, which is not true for normal integer arithmetic.


=== Addition and subtraction ===
even ± even = even;
even ± odd = odd;
odd ± odd = even;


=== Multiplication ===
even × even = even;
even × odd = even;
odd × odd = odd;The structure ({even, odd}, +, ×) is in fact a field with two elements.


=== Division ===
The division of two whole numbers does not necessarily result in a whole number. For example, 1 divided by 4 equals 1/4, which is neither even nor odd, since the concepts of even and odd apply only to integers. But when the quotient is an integer, it will be even if and only if the dividend has more factors of two than the divisor.


== History ==
The ancient Greeks considered 1, the monad, to be neither fully odd nor fully even. Some of this sentiment survived into the 19th century: Friedrich Wilhelm August Fröbel's 1826 The Education of Man instructs the teacher to drill students with the claim that 1 is neither even nor odd, to which Fröbel attaches the philosophical afterthought,

It is well to direct the pupil's attention here at once to a great far-reaching law of nature and of thought. It is this, that between two relatively different things or ideas there stands always a third, in a sort of balance, seeming to unite the two. Thus, there is here between odd and even numbers one number (one) which is neither of the two. Similarly, in form, the right angle stands between the acute and obtuse angles; and in language, the semi-vowels or aspirants between the mutes and vowels. A thoughtful teacher and a pupil taught to think for himself can scarcely help noticing this and other important laws.


== Higher mathematics ==


=== Higher dimensions and more general classes of numbers ===

Integer coordinates of points in Euclidean spaces of two or more dimensions also have a parity, usually defined as the parity of the sum of the coordinates. For instance, the face-centered cubic lattice and its higher-dimensional that is generalizations, the Dn lattices, consist of all of the integer points whose sum of coordinates is even. This feature manifests itself in chess, where the parity of a square is indicated by its color: bishops are constrained to moving between squares of the same parity, whereas knights alternate parity between moves. This form of parity was famously used to solve the mutilated chessboard problem: if two opposite corner squares are removed from a chessboard, then the remaining board cannot be covered by dominoes, because each domino covers one square of each parity and there are two more squares of one parity than of the other.The parity of an ordinal number may be defined to be even if the number is a limit ordinal, or a limit ordinal plus a finite even number, and odd otherwise.Let R be a commutative ring and let I be an ideal of R whose index is 2. Elements of the coset 
  
    
      
        0
        +
        I
      
    
    {\displaystyle 0+I}
   may be called even, while elements of the coset 
  
    
      
        1
        +
        I
      
    
    {\displaystyle 1+I}
   may be called odd.
As an example, let R = Z(2) be the localization of Z at the prime ideal (2). Then an element of R is even or odd if and only if its numerator is so in Z.


=== Number theory ===
The even numbers form an ideal in the ring of integers, but the odd numbers do not—this is clear from the fact that the identity element for addition, zero, is an element of the even numbers only. An integer is even if it is congruent to 0 modulo this ideal, in other words if it is congruent to 0 modulo 2, and odd if it is congruent to 1 modulo 2.
All prime numbers are odd, with one exception: the prime number 2. All known perfect numbers are even; it is unknown whether any odd perfect numbers exist.Goldbach's conjecture states that every even integer greater than 2 can be represented as a sum of two prime numbers. Modern computer calculations have shown this conjecture to be true for integers up to at least 4 × 1018, but still no general proof has been found.


=== Group theory ===

The parity of a permutation (as defined in abstract algebra) is the parity of the number of transpositions into which the permutation can be decomposed. For example (ABC) to (BCA) is even because it can be done by swapping A and B then C and A (two transpositions). It can be shown that no permutation can be decomposed both in an even and in an odd number of transpositions. Hence the above is a suitable definition. In Rubik's Cube, Megaminx, and other twisting puzzles, the moves of the puzzle allow only even permutations of the puzzle pieces, so parity is important in understanding the configuration space of these puzzles.The Feit–Thompson theorem states that a finite group is always solvable if its order is an odd number.  This is an example of odd numbers playing a role in an advanced mathematical theorem where the method of application of the simple hypothesis of ""odd order"" is far from obvious.


=== Analysis ===
The parity of a function describes how its values change when its arguments are exchanged with their negations. An even function, such as an even power of a variable, gives the same result for any argument as for its negation. An odd function, such as an odd power of a variable, gives for any argument the negation of its result when given the negation of that argument. It is possible for a function to be neither odd nor even, and for the case f(x) = 0, to be both odd and even. The Taylor series of an even function contains only terms whose exponent is an even number, and the Taylor series of an odd function contains only terms whose exponent is an odd number.


=== Combinatorial game theory ===
In combinatorial game theory, an evil number is a number that has an even number of 1's in its binary representation, and an odious number is a number that has an odd number of 1's in its binary representation; these numbers play an important role in the strategy for the game Kayles. The parity function maps a number to the number of 1's in its binary representation, modulo 2, so its value is zero for evil numbers and one for odious numbers.  The Thue–Morse sequence, an infinite sequence of 0's and 1's, has a 0 in position i when i is evil, and a 1 in that position when i is odious.


== Additional applications ==
In information theory, a parity bit appended to a binary number provides the simplest form of error detecting code. If a single bit in the resulting value is changed, then it will no longer have the correct parity: changing a bit in the original number gives it a different parity than the recorded one, and changing the parity bit while not changing the number it was derived from again produces an incorrect result. In this way, all single-bit transmission errors may be reliably detected. Some more sophisticated error detecting codes are also based on the use of multiple parity bits for subsets of the bits of the original encoded value.In wind instruments with a cylindrical bore and in effect closed at one end, such as the clarinet at the mouthpiece, the harmonics produced are odd multiples of the fundamental frequency. (With cylindrical pipes open at both ends, used for example in some organ stops such as the open diapason, the harmonics are even multiples of the same frequency for the given bore length, but this has the effect of the fundamental frequency being doubled and all multiples of this fundamental frequency being produced.) See harmonic series (music).In some countries, house numberings are chosen so that the houses on one side of a street have even numbers and the houses on the other side have odd numbers. Similarly, among United States numbered highways, even numbers primarily indicate east–west highways while odd numbers primarily indicate north–south highways. Among airline flight numbers, even numbers typically identify eastbound or northbound flights, and odd numbers typically identify westbound or southbound flights.


== See also ==
Divisor
Half-integer


== References =="
f7073e5ab4,Product (mathematics),"In mathematics, a product is the result of multiplication, or an expression that identifies objects (numbers or variables) to be multiplied, called factors. For example, 30 is the product of 6 and 5 (the result of multiplication), and 
  
    
      
        x
        ⋅
        (
        2
        +
        x
        )
      
    
    {\displaystyle x\cdot (2+x)}
   is the product of 
  
    
      
        x
      
    
    {\displaystyle x}
   and 
  
    
      
        (
        2
        +
        x
        )
      
    
    {\displaystyle (2+x)}
   (indicating that the two factors should be multiplied together).
The order in which real or complex numbers are multiplied has no bearing on the product; this is known as the commutative law of multiplication. When matrices or members of various other associative algebras are multiplied, the product usually depends on the order of the factors. Matrix multiplication, for example, is non-commutative, and so is multiplication in other algebras in general as well.
There are many different kinds of products in mathematics: besides being able to multiply just numbers, polynomials or matrices, one can also define products on many different algebraic structures.


== Product of two numbers ==


== Product of a sequence ==

The product operator for the product of a sequence is denoted by the capital Greek letter pi Π (in analogy to the use of the capital Sigma Σ as summation symbol). For example, the expression 
  
    
      
        
          
            ∏
            
              i
              =
              1
            
            
              6
            
          
          
            i
            
              2
            
          
        
      
    
    {\displaystyle \textstyle \prod _{i=1}^{6}i^{2}}
  is another way of writing 
  
    
      
        1
        ⋅
        4
        ⋅
        9
        ⋅
        16
        ⋅
        25
        ⋅
        36
      
    
    {\displaystyle 1\cdot 4\cdot 9\cdot 16\cdot 25\cdot 36}
  .The product of a sequence consisting of only one number is just that number itself; the product of no factors at all is known as the empty product, and is equal to 1.


== Commutative rings ==
Commutative rings have a product operation.


=== Residue classes of integers ===

Residue classes in the rings 
  
    
      
        
          Z
        
        
          /
        
        N
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /N\mathbb {Z} }
   can be added:

  
    
      
        (
        a
        +
        N
        
          Z
        
        )
        +
        (
        b
        +
        N
        
          Z
        
        )
        =
        a
        +
        b
        +
        N
        
          Z
        
      
    
    {\displaystyle (a+N\mathbb {Z} )+(b+N\mathbb {Z} )=a+b+N\mathbb {Z} }
  and multiplied:

  
    
      
        (
        a
        +
        N
        
          Z
        
        )
        ⋅
        (
        b
        +
        N
        
          Z
        
        )
        =
        a
        ⋅
        b
        +
        N
        
          Z
        
      
    
    {\displaystyle (a+N\mathbb {Z} )\cdot (b+N\mathbb {Z} )=a\cdot b+N\mathbb {Z} }
  


=== Convolution ===

Two functions from the reals to itself can be multiplied in another way, called the convolution.
If

  
    
      
        
          ∫
          
            −
            ∞
          
          
            ∞
          
        
        
          |
        
        f
        (
        t
        )
        
          |
        
        
        
          d
        
        t
        <
        ∞
        
        
          
            and
          
        
        
        
          ∫
          
            −
            ∞
          
          
            ∞
          
        
        
          |
        
        g
        (
        t
        )
        
          |
        
        
        
          d
        
        t
        <
        ∞
        ,
      
    
    {\displaystyle \int \limits _{-\infty }^{\infty }|f(t)|\,\mathrm {d} t<\infty \qquad {\mbox{and}}\qquad \int \limits _{-\infty }^{\infty }|g(t)|\,\mathrm {d} t<\infty ,}
  then the integral

  
    
      
        (
        f
        ∗
        g
        )
        (
        t
        )
        
        :=
        
          ∫
          
            −
            ∞
          
          
            ∞
          
        
        f
        (
        τ
        )
        ⋅
        g
        (
        t
        −
        τ
        )
        
        
          d
        
        τ
      
    
    {\displaystyle (f*g)(t)\;:=\int \limits _{-\infty }^{\infty }f(\tau )\cdot g(t-\tau )\,\mathrm {d} \tau }
  is well defined and is called the convolution.
Under the Fourier transform, convolution becomes point-wise function multiplication.


=== Polynomial rings ===

The product of two polynomials is given by the following:

  
    
      
        
          (
          
            
              ∑
              
                i
                =
                0
              
              
                n
              
            
            
              a
              
                i
              
            
            
              X
              
                i
              
            
          
          )
        
        ⋅
        
          (
          
            
              ∑
              
                j
                =
                0
              
              
                m
              
            
            
              b
              
                j
              
            
            
              X
              
                j
              
            
          
          )
        
        =
        
          ∑
          
            k
            =
            0
          
          
            n
            +
            m
          
        
        
          c
          
            k
          
        
        
          X
          
            k
          
        
      
    
    {\displaystyle \left(\sum _{i=0}^{n}a_{i}X^{i}\right)\cdot \left(\sum _{j=0}^{m}b_{j}X^{j}\right)=\sum _{k=0}^{n+m}c_{k}X^{k}}
  with

  
    
      
        
          c
          
            k
          
        
        =
        
          ∑
          
            i
            +
            j
            =
            k
          
        
        
          a
          
            i
          
        
        ⋅
        
          b
          
            j
          
        
      
    
    {\displaystyle c_{k}=\sum _{i+j=k}a_{i}\cdot b_{j}}
  


== Products in linear algebra ==
There are many different kinds of products in linear algebra. Some of these have confusingly similar names (outer product, exterior product) with very different meanings, while others have very different names (outer product, tensor product, Kronecker product) and yet convey essentially the same idea. A brief overview of these is given in the following sections.


=== Scalar multiplication ===

By the very definition of a vector space, one can form the product of any scalar with any vector, giving a map 
  
    
      
        
          R
        
        ×
        V
        →
        V
      
    
    {\displaystyle \mathbb {R} \times V\rightarrow V}
  .


=== Scalar product ===

A scalar product is a bi-linear map:

  
    
      
        ⋅
        :
        V
        ×
        V
        →
        
          R
        
      
    
    {\displaystyle \cdot :V\times V\rightarrow \mathbb {R} }
  with the following conditions, that 
  
    
      
        v
        ⋅
        v
        >
        0
      
    
    {\displaystyle v\cdot v>0}
   for all 
  
    
      
        0
        ≠
        v
        ∈
        V
      
    
    {\displaystyle 0\not =v\in V}
  .
From the scalar product, one can define a norm by letting 
  
    
      
        ‖
        v
        ‖
        :=
        
          
            v
            ⋅
            v
          
        
      
    
    {\displaystyle \|v\|:={\sqrt {v\cdot v}}}
  .
The scalar product also allows one to define an angle between two vectors:

  
    
      
        cos
        ⁡
        ∠
        (
        v
        ,
        w
        )
        =
        
          
            
              v
              ⋅
              w
            
            
              ‖
              v
              ‖
              ⋅
              ‖
              w
              ‖
            
          
        
      
    
    {\displaystyle \cos \angle (v,w)={\frac {v\cdot w}{\|v\|\cdot \|w\|}}}
  In 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space, the standard scalar product (called the dot product) is given by:

  
    
      
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              α
              
                i
              
            
            
              e
              
                i
              
            
          
          )
        
        ⋅
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              β
              
                i
              
            
            
              e
              
                i
              
            
          
          )
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          α
          
            i
          
        
        
        
          β
          
            i
          
        
      
    
    {\displaystyle \left(\sum _{i=1}^{n}\alpha _{i}e_{i}\right)\cdot \left(\sum _{i=1}^{n}\beta _{i}e_{i}\right)=\sum _{i=1}^{n}\alpha _{i}\,\beta _{i}}
  


=== Cross product in 3-dimensional space ===

The cross product of two vectors in 3-dimensions is a vector perpendicular to the two factors, with length equal to the area of the parallelogram spanned by the two factors.
The cross product can also be expressed as the formal determinant:

  
    
      
        
          u
          ×
          v
        
        =
        
          
            |
            
              
                
                  
                    i
                  
                
                
                  
                    j
                  
                
                
                  
                    k
                  
                
              
              
                
                  
                    u
                    
                      1
                    
                  
                
                
                  
                    u
                    
                      2
                    
                  
                
                
                  
                    u
                    
                      3
                    
                  
                
              
              
                
                  
                    v
                    
                      1
                    
                  
                
                
                  
                    v
                    
                      2
                    
                  
                
                
                  
                    v
                    
                      3
                    
                  
                
              
            
            |
          
        
      
    
    {\displaystyle \mathbf {u\times v} ={\begin{vmatrix}\mathbf {i} &\mathbf {j} &\mathbf {k} \\u_{1}&u_{2}&u_{3}\\v_{1}&v_{2}&v_{3}\\\end{vmatrix}}}
  


=== Composition of linear mappings ===

A linear mapping can be defined as a function f between two vector spaces V and W with underlying field F, satisfying

  
    
      
        f
        (
        
          t
          
            1
          
        
        
          x
          
            1
          
        
        +
        
          t
          
            2
          
        
        
          x
          
            2
          
        
        )
        =
        
          t
          
            1
          
        
        f
        (
        
          x
          
            1
          
        
        )
        +
        
          t
          
            2
          
        
        f
        (
        
          x
          
            2
          
        
        )
        ,
        ∀
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ∈
        V
        ,
        ∀
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        ∈
        
          F
        
        .
      
    
    {\displaystyle f(t_{1}x_{1}+t_{2}x_{2})=t_{1}f(x_{1})+t_{2}f(x_{2}),\forall x_{1},x_{2}\in V,\forall t_{1},t_{2}\in \mathbb {F} .}
  If one only considers finite dimensional vector spaces, then

  
    
      
        f
        (
        
          v
        
        )
        =
        f
        
          (
          
            
              v
              
                i
              
            
            
              
                
                  b
                  
                    V
                  
                
              
              
                i
              
            
          
          )
        
        =
        
          v
          
            i
          
        
        f
        
          (
          
            
              
                b
                
                  V
                
              
            
            
              i
            
          
          )
        
        =
        
          
            
              f
              
                i
              
            
          
          
            j
          
        
        
          v
          
            i
          
        
        
          
            
              b
              
                W
              
            
          
          
            j
          
        
        ,
      
    
    {\displaystyle f(\mathbf {v} )=f\left(v_{i}\mathbf {b_{V}} ^{i}\right)=v_{i}f\left(\mathbf {b_{V}} ^{i}\right)={f^{i}}_{j}v_{i}\mathbf {b_{W}} ^{j},}
  in which bV and bW denote the bases of V and W, and vi denotes the component of v on bVi, and Einstein summation convention is applied.
Now we consider the composition of two linear mappings between finite dimensional vector spaces. Let the linear mapping f map V to W, and let the linear mapping g map W to U. Then one can get

  
    
      
        g
        ∘
        f
        (
        
          v
        
        )
        =
        g
        
          (
          
            
              
                
                  f
                  
                    i
                  
                
              
              
                j
              
            
            
              v
              
                i
              
            
            
              
                
                  b
                  
                    W
                  
                
              
              
                j
              
            
          
          )
        
        =
        
          
            
              g
              
                j
              
            
          
          
            k
          
        
        
          
            
              f
              
                i
              
            
          
          
            j
          
        
        
          v
          
            i
          
        
        
          
            
              b
              
                U
              
            
          
          
            k
          
        
        .
      
    
    {\displaystyle g\circ f(\mathbf {v} )=g\left({f^{i}}_{j}v_{i}\mathbf {b_{W}} ^{j}\right)={g^{j}}_{k}{f^{i}}_{j}v_{i}\mathbf {b_{U}} ^{k}.}
  Or in matrix form:

  
    
      
        g
        ∘
        f
        (
        
          v
        
        )
        =
        
          G
        
        
          F
        
        
          v
        
        ,
      
    
    {\displaystyle g\circ f(\mathbf {v} )=\mathbf {G} \mathbf {F} \mathbf {v} ,}
  in which the i-row, j-column element of F, denoted by Fij, is fji, and Gij=gji.
The composition of more than two linear mappings can be similarly represented by a chain of matrix multiplication.


=== Product of two matrices ===

Given two matrices

  
    
      
        A
        =
        (
        
          a
          
            i
            ,
            j
          
        
        
          )
          
            i
            =
            1
            …
            s
            ;
            j
            =
            1
            …
            r
          
        
        ∈
        
          
            R
          
          
            s
            ×
            r
          
        
      
    
    {\displaystyle A=(a_{i,j})_{i=1\ldots s;j=1\ldots r}\in \mathbb {R} ^{s\times r}}
   and 
  
    
      
        B
        =
        (
        
          b
          
            j
            ,
            k
          
        
        
          )
          
            j
            =
            1
            …
            r
            ;
            k
            =
            1
            …
            t
          
        
        ∈
        
          
            R
          
          
            r
            ×
            t
          
        
      
    
    {\displaystyle B=(b_{j,k})_{j=1\ldots r;k=1\ldots t}\in \mathbb {R} ^{r\times t}}
  their product is given by

  
    
      
        B
        ⋅
        A
        =
        
          
            (
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  r
                
              
              
                a
                
                  i
                  ,
                  j
                
              
              ⋅
              
                b
                
                  j
                  ,
                  k
                
              
            
            )
          
          
            i
            =
            1
            …
            s
            ;
            k
            =
            1
            …
            t
          
        
        
        ∈
        
          
            R
          
          
            s
            ×
            t
          
        
      
    
    {\displaystyle B\cdot A=\left(\sum _{j=1}^{r}a_{i,j}\cdot b_{j,k}\right)_{i=1\ldots s;k=1\ldots t}\;\in \mathbb {R} ^{s\times t}}
  


=== Composition of linear functions as matrix product ===
There is a relationship between the composition of linear functions and the product of two matrices. To see this, let r = dim(U), s = dim(V) and t = dim(W) be the (finite) dimensions of vector spaces U, V and W. Let 

  
    
      
        
          
            U
          
        
        =
        {
        
          u
          
            1
          
        
        ,
        …
        ,
        
          u
          
            r
          
        
        }
      
    
    {\displaystyle {\mathcal {U}}=\{u_{1},\ldots ,u_{r}\}}
   be a basis of U, 

  
    
      
        
          
            V
          
        
        =
        {
        
          v
          
            1
          
        
        ,
        …
        ,
        
          v
          
            s
          
        
        }
      
    
    {\displaystyle {\mathcal {V}}=\{v_{1},\ldots ,v_{s}\}}
   be a basis of V and 

  
    
      
        
          
            W
          
        
        =
        {
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            t
          
        
        }
      
    
    {\displaystyle {\mathcal {W}}=\{w_{1},\ldots ,w_{t}\}}
   be a basis of W. In terms of this basis, let

  
    
      
        A
        =
        
          M
          
            
              V
            
          
          
            
              U
            
          
        
        (
        f
        )
        ∈
        
          
            R
          
          
            s
            ×
            r
          
        
      
    
    {\displaystyle A=M_{\mathcal {V}}^{\mathcal {U}}(f)\in \mathbb {R} ^{s\times r}}
  
be the matrix representing f : U → V and 

  
    
      
        B
        =
        
          M
          
            
              W
            
          
          
            
              V
            
          
        
        (
        g
        )
        ∈
        
          
            R
          
          
            r
            ×
            t
          
        
      
    
    {\displaystyle B=M_{\mathcal {W}}^{\mathcal {V}}(g)\in \mathbb {R} ^{r\times t}}
   
be the matrix representing g : V → W. Then

  
    
      
        B
        ⋅
        A
        =
        
          M
          
            
              W
            
          
          
            
              U
            
          
        
        (
        g
        ∘
        f
        )
        ∈
        
          
            R
          
          
            s
            ×
            t
          
        
      
    
    {\displaystyle B\cdot A=M_{\mathcal {W}}^{\mathcal {U}}(g\circ f)\in \mathbb {R} ^{s\times t}}
  is the matrix representing 
  
    
      
        g
        ∘
        f
        :
        U
        →
        W
      
    
    {\displaystyle g\circ f:U\rightarrow W}
  .
In other words: the matrix product is the description in coordinates of the composition of linear functions.


=== Tensor product of vector spaces ===

Given two finite dimensional vector spaces V and W, the tensor product of them can be defined as a (2,0)-tensor satisfying:

  
    
      
        V
        ⊗
        W
        (
        v
        ,
        m
        )
        =
        V
        (
        v
        )
        W
        (
        w
        )
        ,
        ∀
        v
        ∈
        
          V
          
            ∗
          
        
        ,
        ∀
        w
        ∈
        
          W
          
            ∗
          
        
        ,
      
    
    {\displaystyle V\otimes W(v,m)=V(v)W(w),\forall v\in V^{*},\forall w\in W^{*},}
  where V* and W* denote the dual spaces of V and W.For infinite-dimensional vector spaces, one also has the:

Tensor product of Hilbert spaces
Topological tensor product.The tensor product, outer product and Kronecker product all convey the same general idea.  The differences between these are that the Kronecker product is just a tensor product of matrices, with respect to a previously-fixed basis, whereas the tensor product is usually given in its intrinsic definition. The outer product is simply the Kronecker product, limited to vectors (instead of matrices).


=== The class of all objects with a tensor product ===
In general, whenever one has two mathematical objects that can be combined in a way that behaves like a linear algebra tensor product, then this can be most generally understood as the internal product of a monoidal category.  That is, the monoidal category captures precisely the meaning of a tensor product; it captures exactly the notion of why it is that tensor products behave the way they do.  More precisely, a monoidal category is the class of all things (of a given type) that have a tensor product.


=== Other products in linear algebra ===
Other kinds of products in linear algebra include:

Hadamard product
Kronecker product
The product of tensors:
Wedge product or exterior product
Interior product
Outer product
Tensor product


== Cartesian product ==
In set theory, a Cartesian product is a mathematical operation which returns a set (or product set) from multiple sets. That is, for sets A and B, the Cartesian product A × B is the set of all ordered pairs (a, b)—where a ∈ A and b ∈ B.The class of all things (of a given type) that have Cartesian products is called a Cartesian category. Many of these are Cartesian closed categories. Sets are an example of such objects.


== Empty product ==
The empty product on numbers and most algebraic structures has the value of 1 (the identity element of multiplication), just like the empty sum has the value of 0 (the identity element of addition). However, the concept of the empty product is more general, and requires special treatment in logic, set theory, computer programming and category theory.


== Products over other algebraic structures ==
Products over other kinds of algebraic structures include:

the Cartesian product of sets
the direct product of groups, and also the semidirect product, knit product and wreath product
the free product of groups
the product of rings
the product of ideals
the product of topological spaces
the Wick product of random variables
the cap, cup, Massey and slant product in algebraic topology
the smash product and wedge sum (sometimes called the wedge product) in homotopyA few of the above products are examples of the general notion of an internal product in a monoidal category; the rest are describable by the general notion of a product in category theory.


== Products in category theory ==
All of the previous examples are special cases or examples of the general notion of a product. For the general treatment of the concept of a product, see product (category theory), which describes how to combine two objects of some kind to create an object, possibly of a different kind.  But also, in category theory, one has:

the fiber product or pullback,
the product category, a category that is the product of categories.
the ultraproduct, in model theory.
the internal product of a monoidal category, which captures the essence of a tensor product.


== Other products ==
A function's product integral (as a continuous equivalent to the product of a sequence or as the multiplicative version of the normal/standard/additive integral. The product integral is also known as ""continuous product"" or ""multiplical"".
Complex multiplication, a theory of elliptic curves.


== See also ==
Deligne tensor product of abelian categories – Belgian mathematicianPages displaying short descriptions of redirect targets
Indefinite product
Infinite product
Iterated binary operation – Repeated application of an operation to a sequence
Multiplication – Arithmetical operation


== Notes ==


== References ==


== Bibliography ==
Jarchow, Hans (1981). Locally convex spaces. Stuttgart: B.G. Teubner. ISBN 978-3-519-02224-4. OCLC 8210342."
9770dde9ee,Interval (mathematics),"In mathematics, a (real) interval is a set of real numbers that contains all real numbers lying between any two numbers of the set. For example, the set of numbers x satisfying 0 ≤ x ≤ 1 is an interval which contains 0, 1, and all numbers in between. Other examples of intervals are the set of numbers such that 0 < x < 1, the set of all real numbers 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , the set of nonnegative real numbers, the set of positive real numbers, the empty set, and any singleton (set of one element).
Real intervals play an important role in the theory of integration, because they are the simplest sets whose ""length"" (or ""measure"" or ""size"") is easy to define. The concept of measure can then be extended to more complicated sets of real numbers, leading to the Borel measure and eventually to the Lebesgue measure.
Intervals are central to interval arithmetic, a general numerical computing technique that automatically provides guaranteed enclosures for arbitrary formulas, even in the presence of uncertainties, mathematical approximations, and arithmetic roundoff.
Intervals are likewise defined on an arbitrary totally ordered set, such as integers or rational numbers. The notation of integer intervals is considered in the special section below.


== Terminology ==
An open interval does not include its endpoints, and is indicated with parentheses. For example, (0, 1) means greater than 0 and less than 1. This means (0, 1) = {x | 0 < x < 1}.
This interval can also be denoted by ]0, 1[, see below. 
A closed interval is an interval which includes all its limit points, and is denoted with square brackets. For example, [0, 1] means greater than or equal to 0 and less than or equal to 1. 
A half-open interval includes only one of its endpoints, and is denoted by mixing the notations for open and closed intervals. For example, (0, 1] means greater than 0 and less than or equal to 1, while [0, 1) means greater than or equal to 0 and less than 1.
A degenerate interval is any set consisting of a single real number (i.e., an interval of the form [a, a]). Some authors include the empty set in this definition. A real interval that is neither empty nor degenerate is said to be proper, and has infinitely many elements.
An interval is said to be left-bounded or right-bounded, if there is some real number that is, respectively, smaller than or larger than all its elements. An interval is said to be bounded, if it is both left- and right-bounded; and is said to be unbounded otherwise. Intervals that are bounded at only one end are said to be half-bounded. The empty set is bounded, and the set of all reals is the only interval that is unbounded at both ends. Bounded intervals are also commonly known as finite intervals.
Bounded intervals are bounded sets, in the sense that their diameter (which is equal to the absolute difference between the endpoints) is finite.  The diameter may be called the length, width, measure, range, or size of the interval. The size of unbounded intervals is usually defined as +∞, and the size of the empty interval may be defined as 0 (or left undefined).
The centre (midpoint) of a bounded interval with endpoints a and b is (a + b)/2, and its radius is the half-length |a − b|/2. These concepts are undefined for empty or unbounded intervals.
An interval is said to be left-open if and only if it contains no minimum (an element that is smaller than all other elements); right-open if it contains no maximum; and open if it contains neither. The interval [0, 1) = {x | 0 ≤ x < 1}, for example, is left-closed and right-open. The empty set and the set of all reals are both open and closed intervals, while the set of non-negative reals, is a closed interval that is right-open but not left-open. The open intervals are open sets of the real line in its standard topology, and form a base of the open sets.
An interval is said to be left-closed if it has a minimum element or is left-unbounded, right-closed if it has a maximum or is right unbounded; it is simply closed if it is both left-closed and right closed. So, the closed intervals coincide with the closed sets in that topology.
The interior of an interval I is the largest open interval that is contained in I; it is also the set of points in I which are not endpoints of I. The closure of I is the smallest closed interval that contains I; which is also the set I augmented with its finite endpoints.
For any set X of real numbers, the interval enclosure or interval span of X is the unique interval that contains X, and does not properly contain any other interval that also contains X.
An interval I is subinterval of interval J if I is a subset of J. An interval I is a proper subinterval of J if I is a proper subset of J.


=== Note on conflicting terminology ===
The terms segment and interval have been employed in the literature in two essentially opposite ways, resulting in ambiguity when these terms are used. The Encyclopedia of Mathematics defines interval (without a qualifier) to exclude both endpoints (i.e., open interval) and segment to include both endpoints (i.e., closed interval), while Rudin's Principles of Mathematical Analysis calls sets of the form [a, b] intervals and sets of the form (a, b) segments throughout. These terms tend to appear in older works; modern texts increasingly favor the term interval (qualified by open, closed, or half-open), regardless of whether endpoints are included.


== Notations for intervals ==
The interval of numbers between a and b, including a and b, is often denoted [a, b]. The two numbers are called the endpoints of the interval. In countries where numbers are written with a decimal comma, a semicolon may be used as a separator to avoid ambiguity.


=== Including or excluding endpoints ===
To indicate that one of the endpoints is to be excluded from the set, the corresponding square bracket can be either replaced with a parenthesis, or reversed. Both notations are described in International standard ISO 31-11. Thus, in set builder notation,

  
    
      
        
          
            
              
                
                  
                    (
                  
                
                a
                ,
                b
                
                  
                    )
                  
                
                =
                
                  
                    
                      ]
                    
                  
                
                a
                ,
                b
                
                  
                    
                      [
                    
                  
                
              
              
                
                =
                {
                x
                ∈
                
                  R
                
                ∣
                a
                
                  
                    

                    
                    <
                    

                    
                  
                
                x
                
                  
                    

                    
                    <
                    

                    
                  
                
                b
                }
                ,
              
            
            
              
                

                
                
                  
                    [
                  
                
                a
                ,
                b
                
                  
                    )
                  
                
                =
                
                  
                    
                      [
                    
                  
                
                a
                ,
                b
                
                  
                    
                      [
                    
                  
                
              
              
                
                =
                {
                x
                ∈
                
                  R
                
                ∣
                a
                
                  
                    

                    
                    ≤
                    

                    
                  
                
                x
                
                  
                    

                    
                    <
                    

                    
                  
                
                b
                }
                ,
              
            
            
              
                

                
                
                  
                    (
                  
                
                a
                ,
                b
                
                  
                    ]
                  
                
                =
                
                  
                    
                      ]
                    
                  
                
                a
                ,
                b
                
                  
                    
                      ]
                    
                  
                
              
              
                
                =
                {
                x
                ∈
                
                  R
                
                ∣
                a
                
                  
                    

                    
                    <
                    

                    
                  
                
                x
                
                  
                    

                    
                    ≤
                    

                    
                  
                
                b
                }
                ,
              
            
            
              
                

                
                
                  
                    [
                  
                
                a
                ,
                b
                
                  
                    ]
                  
                
                =
                
                  
                    
                      [
                    
                  
                
                a
                ,
                b
                
                  
                    
                      ]
                    
                  
                
              
              
                
                =
                {
                x
                ∈
                
                  R
                
                ∣
                a
                
                  
                    

                    
                    ≤
                    

                    
                  
                
                x
                
                  
                    

                    
                    ≤
                    

                    
                  
                
                b
                }
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\color {Maroon}(}a,b{\color {Maroon})}={\mathopen {\color {Maroon}]}}a,b{\mathclose {\color {Maroon}[}}&=\{x\in \mathbb {R} \mid a{\color {Maroon}{}<{}}x{\color {Maroon}{}<{}}b\},\\{}{\color {DarkGreen}[}a,b{\color {Maroon})}={\mathopen {\color {DarkGreen}[}}a,b{\mathclose {\color {Maroon}[}}&=\{x\in \mathbb {R} \mid a{\color {DarkGreen}{}\leq {}}x{\color {Maroon}{}<{}}b\},\\{}{\color {Maroon}(}a,b{\color {DarkGreen}]}={\mathopen {\color {Maroon}]}}a,b{\mathclose {\color {DarkGreen}]}}&=\{x\in \mathbb {R} \mid a{\color {Maroon}{}<{}}x{\color {DarkGreen}{}\leq {}}b\},\\{}{\color {DarkGreen}[}a,b{\color {DarkGreen}]}={\mathopen {\color {DarkGreen}[}}a,b{\mathclose {\color {DarkGreen}]}}&=\{x\in \mathbb {R} \mid a{\color {DarkGreen}{}\leq {}}x{\color {DarkGreen}{}\leq {}}b\}.\end{aligned}}}
  Each interval (a, a), [a, a), and (a, a] represents the empty set, whereas [a, a] denotes the singleton set {a}.  When a > b, all four notations are usually taken to represent the empty set.
Both notations may overlap with other uses of parentheses and brackets in mathematics. For instance, the notation (a, b) is often used to denote an ordered pair in set theory, the coordinates of a point or vector in analytic geometry and linear algebra, or (sometimes) a complex number in algebra. That is why Bourbaki introduced the notation ]a, b[ to denote the open interval. The notation [a, b] too is occasionally used for ordered pairs, especially in computer science.
Some authors such as Yves Tillé use ]a, b[ to denote the complement of the interval (a, b); namely, the set of all real numbers that are either less than or equal to a, or greater than or equal to b.


=== Infinite endpoints ===
In some contexts, an interval may be defined as a subset of the extended real numbers, the set of all real numbers augmented with −∞ and +∞.
In this interpretation, the notations [−∞, b] , (−∞, b] , [a, +∞] , and [a, +∞) are all meaningful and distinct. In particular, (−∞, +∞) denotes the set of all ordinary real numbers, while [−∞, +∞] denotes the extended reals. 
Even in the context of the ordinary reals, one may use an infinite endpoint to indicate that there is no bound in that direction. For example, (0, +∞) is the set of positive real numbers, also written as 
  
    
      
        
          
            R
          
          
            +
          
        
      
    
    {\displaystyle \mathbb {R} _{+}}
  . The context affects some of the above definitions and terminology. For instance, the interval (−∞, +∞) = 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is closed in the realm of ordinary reals, but not in the realm of the extended reals.


=== Integer intervals ===
When a and b are integers, the notation ⟦a, b⟧, or [a .. b] or {a .. b} or just a .. b, is sometimes used to indicate the interval of all integers between a and b included. The notation [a .. b] is used in some programming languages; in Pascal, for example, it is used to formally define a subrange type, most frequently used to specify lower and upper bounds of valid indices of an array.
An integer interval that has a finite lower or upper endpoint always includes that endpoint. Therefore, the exclusion of endpoints can be explicitly denoted by writing a .. b − 1 ,  a + 1 .. b , or  a + 1 .. b − 1.  Alternate-bracket notations like [a .. b) or [a .. b[ are rarely used for integer intervals.


== Classification of intervals ==
The intervals of real numbers can be classified into the eleven different types listed below, where a and b are real numbers, and 
  
    
      
        a
        <
        b
      
    
    {\displaystyle a<b}
  :

Empty: 
  
    
      
        [
        b
        ,
        a
        ]
        =
        (
        b
        ,
        a
        )
        =
        [
        b
        ,
        a
        )
        =
        (
        b
        ,
        a
        ]
        =
        (
        a
        ,
        a
        )
        =
        [
        a
        ,
        a
        )
        =
        (
        a
        ,
        a
        ]
        =
        {
        }
        =
        ∅
      
    
    {\displaystyle [b,a]=(b,a)=[b,a)=(b,a]=(a,a)=[a,a)=(a,a]=\{\}=\varnothing }
  
Degenerate: 
  
    
      
        [
        a
        ,
        a
        ]
        =
        {
        a
        }
      
    
    {\displaystyle [a,a]=\{a\}}
  
Proper and bounded:
Open: 
  
    
      
        (
        a
        ,
        b
        )
        =
        {
        x
        ∣
        a
        <
        x
        <
        b
        }
      
    
    {\displaystyle (a,b)=\{x\mid a<x<b\}}
  
Closed: 
  
    
      
        [
        a
        ,
        b
        ]
        =
        {
        x
        ∣
        a
        ≤
        x
        ≤
        b
        }
      
    
    {\displaystyle [a,b]=\{x\mid a\leq x\leq b\}}
  
Left-closed, right-open: 
  
    
      
        [
        a
        ,
        b
        )
        =
        {
        x
        ∣
        a
        ≤
        x
        <
        b
        }
      
    
    {\displaystyle [a,b)=\{x\mid a\leq x<b\}}
  
Left-open, right-closed: 
  
    
      
        (
        a
        ,
        b
        ]
        =
        {
        x
        ∣
        a
        <
        x
        ≤
        b
        }
      
    
    {\displaystyle (a,b]=\{x\mid a<x\leq b\}}
  
Left-bounded and right-unbounded:
Left-open: 
  
    
      
        (
        a
        ,
        +
        ∞
        )
        =
        {
        x
        ∣
        x
        >
        a
        }
      
    
    {\displaystyle (a,+\infty )=\{x\mid x>a\}}
  
Left-closed: 
  
    
      
        [
        a
        ,
        +
        ∞
        )
        =
        {
        x
        ∣
        x
        ≥
        a
        }
      
    
    {\displaystyle [a,+\infty )=\{x\mid x\geq a\}}
  
Left-unbounded and right-bounded:
Right-open: 
  
    
      
        (
        −
        ∞
        ,
        b
        )
        =
        {
        x
        ∣
        x
        <
        b
        }
      
    
    {\displaystyle (-\infty ,b)=\{x\mid x<b\}}
  
Right-closed: 
  
    
      
        (
        −
        ∞
        ,
        b
        ]
        =
        {
        x
        ∣
        x
        ≤
        b
        }
      
    
    {\displaystyle (-\infty ,b]=\{x\mid x\leq b\}}
  
Unbounded at both ends (simultaneously open and closed):  
  
    
      
        (
        −
        ∞
        ,
        +
        ∞
        )
        =
        
          R
        
      
    
    {\displaystyle (-\infty ,+\infty )=\mathbb {R} }
  :


== Properties of intervals ==
The intervals are precisely the connected subsets of 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  . It follows that the image of an interval by any continuous function is also an interval. This is one formulation of the intermediate value theorem.
The intervals are also the convex subsets of 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  . The interval enclosure of a subset 
  
    
      
        X
        ⊆
        
          R
        
      
    
    {\displaystyle X\subseteq \mathbb {R} }
   is also the convex hull of 
  
    
      
        X
      
    
    {\displaystyle X}
  .
The intersection of any collection of intervals is always an interval. The union of two intervals is an interval if and only if they have a non-empty intersection or an open end-point of one interval is a closed end-point of the other – e.g., 
  
    
      
        (
        a
        ,
        b
        )
        ∪
        [
        b
        ,
        c
        ]
        =
        (
        a
        ,
        c
        ]
      
    
    {\displaystyle (a,b)\cup [b,c]=(a,c]}
  .
If 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is viewed as a metric space, its open balls are the open bounded sets (c + r, c − r), and its closed balls are the closed bounded sets [c + r, c − r].
Any element x of an interval I defines a partition of I into three disjoint intervals I1, I2, I3: respectively, the elements of I that are less than x, the singleton 
  
    
      
        [
        x
        ,
        x
        ]
        =
        {
        x
        }
      
    
    {\displaystyle [x,x]=\{x\}}
  , and the elements that are greater than x. The parts I1 and I3 are both non-empty (and have non-empty interiors), if and only if x is in the interior of I.  This is an interval version of the trichotomy principle.


== Dyadic intervals ==
A dyadic interval is a bounded real interval whose endpoints are 
  
    
      
        
          
            j
            
              2
              
                n
              
            
          
        
      
    
    {\textstyle {\frac {j}{2^{n}}}}
   and 
  
    
      
        
          
            
              j
              +
              1
            
            
              2
              
                n
              
            
          
        
      
    
    {\textstyle {\frac {j+1}{2^{n}}}}
  , where 
  
    
      
        j
      
    
    {\textstyle j}
   and 
  
    
      
        n
      
    
    {\textstyle n}
   are integers. Depending on the context, either endpoint may or may not be included in the interval.
Dyadic intervals have the following properties:

The length of a dyadic interval is always an integer power of two.
Each dyadic interval is contained in exactly one dyadic interval of twice the length.
Each dyadic interval is spanned by two dyadic intervals of half the length.
If two open dyadic intervals overlap, then one of them is a subset of the other.The dyadic intervals consequently have a structure that reflects that of an infinite binary tree.
Dyadic intervals are relevant to several areas of numerical analysis, including adaptive mesh refinement, multigrid methods and wavelet analysis. Another way to represent such a structure is p-adic analysis (for p = 2).


== Generalizations ==


=== Multi-dimensional intervals ===

In many contexts, an 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional interval is defined as a subset of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   that is the Cartesian product of 
  
    
      
        n
      
    
    {\displaystyle n}
   intervals, 
  
    
      
        I
        =
        
          I
          
            1
          
        
        ×
        
          I
          
            2
          
        
        ×
        ⋯
        ×
        
          I
          
            n
          
        
      
    
    {\displaystyle I=I_{1}\times I_{2}\times \cdots \times I_{n}}
  , one on each coordinate axis.
For 
  
    
      
        n
        =
        2
      
    
    {\displaystyle n=2}
  , this can be thought of as region bounded by a square or rectangle, whose sides are parallel to the coordinate axes, depending on whether the width of the intervals are the same or not; likewise, for 
  
    
      
        n
        =
        3
      
    
    {\displaystyle n=3}
  , this can be thought of as a region bounded by an axis-aligned cube or a rectangular cuboid.
In higher dimensions, the Cartesian product of 
  
    
      
        n
      
    
    {\displaystyle n}
   intervals is bounded by an n-dimensional hypercube or hyperrectangle.
A facet of such an interval 
  
    
      
        I
      
    
    {\displaystyle I}
   is the result of replacing any non-degenerate interval factor 
  
    
      
        
          I
          
            k
          
        
      
    
    {\displaystyle I_{k}}
   by a degenerate interval consisting of a finite endpoint of 
  
    
      
        
          I
          
            k
          
        
      
    
    {\displaystyle I_{k}}
  . The faces of 
  
    
      
        I
      
    
    {\displaystyle I}
   comprise 
  
    
      
        I
      
    
    {\displaystyle I}
   itself and all faces of its facets. The corners of 
  
    
      
        I
      
    
    {\displaystyle I}
   are the faces that consist of a single point of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  .


=== Complex intervals ===
Intervals of complex numbers can be defined as regions of the complex plane, either rectangular or circular.


== Topological algebra ==
Intervals can be associated with points of the plane, and hence regions of intervals can be associated with regions of the plane. Generally, an interval in mathematics corresponds to an ordered pair (x,y) taken from the direct product R × R of real numbers with itself, where it is often assumed that y > x. For purposes of mathematical structure, this restriction is discarded, and ""reversed intervals"" where y − x < 0 are allowed. Then, the collection of all intervals [x,y] can be identified with the topological ring formed by the direct sum of R with itself, where addition and multiplication are defined component-wise.
The direct sum algebra 
  
    
      
        (
        R
        ⊕
        R
        ,
        +
        ,
        ×
        )
      
    
    {\displaystyle (R\oplus R,+,\times )}
   has two ideals, { [x,0] : x ∈ R } and { [0,y] : y ∈ R }. The identity element of this algebra is the condensed interval [1,1]. If interval [x,y] is not in one of the ideals, then it has multiplicative inverse [1/x, 1/y]. Endowed with the usual topology, the algebra of intervals forms a topological ring. The group of units of this ring consists of four quadrants determined by the axes, or ideals in this case. The identity component of this group is quadrant I.
Every interval can be considered a symmetric interval around its midpoint.  In a reconfiguration published in 1956 by M Warmus, the axis of ""balanced intervals"" [x, −x] is used along with the axis of intervals [x,x] that reduce to a point. Instead of the direct sum 
  
    
      
        R
        ⊕
        R
      
    
    {\displaystyle R\oplus R}
  , the ring of intervals has been identified with the split-complex number plane by M. Warmus and D. H. Lehmer through the identification

z = (x + y)/2 + j (x − y)/2.This linear mapping of the plane, which amounts of a ring isomorphism, provides the plane with a multiplicative structure having some analogies to ordinary complex arithmetic, such as polar decomposition.


== See also ==
Arc (geometry)
Inequality
Interval graph
Interval finite element
Interval (statistics)
Line segment
Partition of an interval
Unit interval


== References ==


== Bibliography ==
T. Sunaga, ""Theory of interval algebra and its application to numerical analysis"" Archived 2012-03-09 at the Wayback Machine, In: Research Association of Applied Geometry (RAAG) Memoirs, Ggujutsu Bunken Fukuy-kai. Tokyo, Japan, 1958, Vol. 2, pp. 29–46 (547-564); reprinted in Japan Journal on Industrial and Applied Mathematics, 2009, Vol. 26, No. 2-3, pp. 126–143.


== External links ==
A Lucid Interval by Brian Hayes: An American Scientist article provides an introduction.
Interval computations website Archived 2006-03-02 at the Wayback Machine
Interval computations research centers Archived 2007-02-03 at the Wayback Machine
Interval Notation by George Beck, Wolfram Demonstrations Project.
Weisstein, Eric W. ""Interval"". MathWorld."
40f3464134,Inequality (mathematics),"In mathematics, an inequality is a relation which makes a non-equal comparison between two numbers or other mathematical expressions. It is used most often to compare two numbers on the number line by their size. There are several different notations used to represent different kinds of inequalities:

The notation a < b means that a is less than b.
The notation a > b means that a is greater than b.In either case, a is not equal to b. These relations are known as strict inequalities, meaning that a is strictly less than or strictly greater than b. Equivalence is excluded.
In contrast to strict inequalities, there are two types of inequality relations that are not strict:

The notation a ≤ b or a ⩽ b means that a is less than or equal to b (or, equivalently, at most b, or not greater than b).
The notation a ≥ b or a ⩾ b means that a is greater than or equal to b (or, equivalently, at least b, or not less than b).The relation not greater than can also be represented by a ≯ b, the symbol for ""greater than"" bisected by a slash, ""not"". The same is true for not less than and a ≮ b.
The notation a ≠ b means that a is not equal to b; this inequation sometimes is considered a form of strict inequality. It does not say that one is greater than the other; it does not even require a and b to be member of an ordered set.
In engineering sciences, less formal use of the notation is to state that one quantity is ""much greater"" than another, normally by several orders of magnitude.  

The notation a ≪ b means that a is much less than b.
The notation a ≫ b means that a is much greater than b.This implies that the lesser value can be neglected with little effect on the accuracy of an approximation (such as the case of ultrarelativistic limit in physics).
In all of the cases above, any two symbols mirroring each other are symmetrical; a < b and b > a are equivalent, etc.


== Properties on the number line ==
Inequalities are governed by the following properties. All of these properties also hold if all of the non-strict inequalities (≤ and ≥) are replaced by their corresponding strict inequalities (< and >) and — in the case of applying a function — monotonic functions are limited to strictly monotonic functions.


=== Converse ===
The relations ≤ and ≥ are each other's converse, meaning that for any real numbers a and b:


=== Transitivity ===
The transitive property of inequality states that for any real numbers a, b, c:

If either of the premises is a strict inequality, then the conclusion is a strict inequality:


=== Addition and subtraction ===

A common constant c may be added to or subtracted from both sides of an inequality. So, for any real numbers a, b, c:

In other words, the inequality relation is preserved under addition (or subtraction) and the real numbers are an ordered group under addition.


=== Multiplication and division ===

The properties that deal with multiplication and division state that for any real numbers, a, b and non-zero c:

In other words, the inequality relation is preserved under multiplication and division with positive constant, but is reversed when a negative constant is involved. More generally, this applies for an ordered field. For more information, see § Ordered fields.


=== Additive inverse ===
The property for the additive inverse states that for any real numbers a and b:


=== Multiplicative inverse ===
If both numbers are positive, then the inequality relation between the multiplicative inverses is opposite of that between the original numbers. More specifically, for any non-zero real numbers a and b that are both positive (or both negative):

All of the cases for the signs of a and b can also be written in chained notation, as follows:


=== Applying a function to both sides ===

Any monotonically increasing function, by its definition, may be applied to both sides of an inequality without breaking the inequality relation (provided that both expressions are in the domain of that function). However, applying a monotonically decreasing function to both sides of an inequality means the inequality relation would be reversed. The rules for the additive inverse, and the multiplicative inverse for positive numbers, are both examples of applying a monotonically decreasing function.
If the inequality is strict (a < b, a > b) and the function is strictly monotonic, then the inequality remains strict. If only one of these conditions is strict, then the resultant inequality is non-strict. In fact, the rules for additive and multiplicative inverses are both examples of applying a strictly monotonically decreasing function.
A few examples of this rule are: 

Raising both sides of an inequality to a power n > 0 (equiv., −n < 0), when a and b are positive real numbers:  
Taking the natural logarithm on both sides of an inequality, when a and b are positive real numbers:      (this is true because the natural logarithm is a strictly increasing function.)


== Formal definitions and generalizations ==
A (non-strict) partial order is a binary relation ≤ over a set P which is reflexive, antisymmetric, and transitive. That is, for all a, b, and c in P, it must satisfy the three following clauses:

a ≤ a (reflexivity)
if a ≤ b and b ≤ a, then a = b (antisymmetry)
if a ≤ b and b ≤ c, then a ≤ c (transitivity)A set with a partial order is called a partially ordered set. Those are the very basic axioms that every kind of order has to satisfy. Other axioms that exist for other definitions of orders on a set P include:

For every a and b in P, a ≤ b or b ≤ a (total order).
For all a and b in P for which a < b, there is a c in P such that a < c < b (dense order).
Every non-empty subset of P with an upper bound has a least upper bound (supremum) in P (least-upper-bound property).


=== Ordered fields ===

If (F, +, ×) is a field and ≤ is a total order on F, then (F, +, ×, ≤) is called an ordered field if and only if:

a ≤ b implies a + c ≤ b + c;
0 ≤ a and 0 ≤ b implies 0 ≤ a × b.Both (Q, +, ×, ≤) and (R, +, ×, ≤) are ordered fields, but ≤ cannot be defined in order to make (C, +, ×, ≤) an ordered field, because −1 is the square of i and would therefore be positive.
Besides from being an ordered field, R also has the Least-upper-bound property. In fact, R can be defined as the only ordered field with that quality.


== Chained notation ==
The notation a < b < c stands for ""a < b and b < c"", from which, by the transitivity property above, it also follows that a < c. By the above laws, one can add or subtract the same number to all three terms, or multiply or divide all three terms by same nonzero number and reverse all inequalities if that number is negative. Hence, for example, a < b + e < c is equivalent to a − e < b < c − e.
This notation can be generalized to any number of terms: for instance, a1 ≤ a2 ≤ ... ≤ an means that ai ≤ ai+1 for i = 1, 2, ..., n − 1.  By transitivity, this condition is equivalent to ai ≤ aj for any 1 ≤ i ≤ j ≤ n.
When solving inequalities using chained notation, it is possible and sometimes necessary to evaluate the terms independently. For instance, to solve the inequality 4x < 2x + 1 ≤ 3x + 2, it is not possible to isolate x in any one part of the inequality through addition or subtraction. Instead, the inequalities must be solved independently, yielding x < 1/2 and x ≥ −1 respectively, which can be combined into the final solution −1 ≤ x < 1/2.
Occasionally, chained notation is used with inequalities in different directions, in which case the meaning is the logical conjunction of the inequalities between adjacent terms.  For example, the defining condition of a zigzag poset is written as a1 < a2 > a3 < a4 > a5 < a6 > ... .   Mixed chained notation is used more often with compatible relations, like <, =, ≤.  For instance, a < b = c ≤ d means that a < b, b = c, and c ≤ d.  This notation exists in a few programming languages such as Python.  In contrast, in programming languages that provide an ordering on the type of comparison results, such as C, even homogeneous chains may have a completely different meaning.


== Sharp inequalities ==
An inequality is said to be sharp if it cannot be relaxed and still be valid in general. Formally, a universally quantified inequality φ is called sharp if, for every valid universally quantified inequality ψ, if ψ ⇒ φ holds, then ψ ⇔ φ also holds. For instance, the inequality ∀a ∈ R. a2 ≥ 0 is sharp, whereas the inequality ∀a ∈ R. a2 ≥ −1 is not sharp.


== Inequalities between means ==

There are many inequalities between means. For example, for any positive numbers a1, a2, ..., an we have H ≤ G ≤ A ≤ Q, where they represent the following means of the sequence:

Harmonic mean

  
    
      
        H
        =
        
          
            n
            
              
                
                  1
                  
                    a
                    
                      1
                    
                  
                
              
              +
              
                
                  1
                  
                    a
                    
                      2
                    
                  
                
              
              +
              ⋯
              +
              
                
                  1
                  
                    a
                    
                      n
                    
                  
                
              
            
          
        
      
    
    {\displaystyle H={\frac {n}{{\frac {1}{a_{1}}}+{\frac {1}{a_{2}}}+\cdots +{\frac {1}{a_{n}}}}}}
  
Geometric mean

  
    
      
        G
        =
        
          
            
              
                a
                
                  1
                
              
              ⋅
              
                a
                
                  2
                
              
              ⋯
              
                a
                
                  n
                
              
            
            
              n
            
          
        
      
    
    {\displaystyle G={\sqrt[{n}]{a_{1}\cdot a_{2}\cdots a_{n}}}}
  
Arithmetic mean

  
    
      
        A
        =
        
          
            
              
                a
                
                  1
                
              
              +
              
                a
                
                  2
                
              
              +
              ⋯
              +
              
                a
                
                  n
                
              
            
            n
          
        
      
    
    {\displaystyle A={\frac {a_{1}+a_{2}+\cdots +a_{n}}{n}}}
  
quadratic mean

  
    
      
        Q
        =
        
          
            
              
                
                  a
                  
                    1
                  
                  
                    2
                  
                
                +
                
                  a
                  
                    2
                  
                  
                    2
                  
                
                +
                ⋯
                +
                
                  a
                  
                    n
                  
                  
                    2
                  
                
              
              n
            
          
        
      
    
    {\displaystyle Q={\sqrt {\frac {a_{1}^{2}+a_{2}^{2}+\cdots +a_{n}^{2}}{n}}}}
  


== Cauchy–Schwarz inequality ==

The Cauchy–Schwarz inequality states that for all vectors u and v of an inner product space it is true that

where 
  
    
      
        ⟨
        ⋅
        ,
        ⋅
        ⟩
      
    
    {\displaystyle \langle \cdot ,\cdot \rangle }
   is the inner product. Examples of inner products include the real and complex dot product; In Euclidean space Rn with the standard inner product, the Cauchy–Schwarz inequality is


== Power inequalities ==
A ""power inequality"" is an inequality containing terms of the form ab, where a and b are real positive numbers or variable expressions. They often appear in mathematical olympiads exercises.


=== Examples ===
For any real  x, 
If x > 0 and p > 0, then  In the limit of p → 0, the upper and lower bounds converge to ln(x).
If x > 0, then 
If x > 0, then 
If x, y, z > 0, then 
For any real distinct numbers a and b, 
If x, y > 0 and 0 < p < 1, then 
If x, y, z > 0, then 
If a, b > 0, then 
If a, b > 0, then 
If a, b, c > 0, then 
If a, b > 0, then 


== Well-known inequalities ==

Mathematicians often use inequalities to bound quantities for which exact formulas cannot be computed easily.  Some inequalities are used so often that they have names:


== Complex numbers and inequalities ==
The set of complex numbers ℂ with its operations of addition and multiplication is a field, but it is impossible to define any relation ≤ so that (C, +, ×, ≤) becomes an ordered field. To make (ℂ, +, ×, ≤) an ordered field, it would have to satisfy the following two properties:

if a ≤ b, then a + c ≤ b + c;
if 0 ≤ a and 0 ≤ b, then 0 ≤ ab.Because ≤ is a total order, for any number a, either 0 ≤ a or a ≤ 0 (in which case the first property above implies that 0 ≤ −a). In either case 0 ≤ a2; this means that i2 > 0 and 12 > 0; so −1 > 0 and 1 > 0, which means (−1 + 1) > 0; contradiction.
However, an operation ≤ can be defined so as to satisfy only the first property (namely, ""if a ≤ b, then a + c ≤ b + c""). Sometimes the lexicographical order definition is used:

a ≤ b, if
Re(a) < Re(b), or
Re(a) = Re(b) and Im(a) ≤ Im(b)It can easily be proven that for this definition a ≤ b implies a + c ≤ b + c.


== Vector inequalities ==
Inequality relationships similar to those defined above can also be defined for column vectors.  If we let the vectors 
  
    
      
        x
        ,
        y
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x,y\in \mathbb {R} ^{n}}
   (meaning that 
  
    
      
        x
        =
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        
          )
          
            
              T
            
          
        
      
    
    {\displaystyle x=(x_{1},x_{2},\ldots ,x_{n})^{\mathsf {T}}}
   and 
  
    
      
        y
        =
        (
        
          y
          
            1
          
        
        ,
        
          y
          
            2
          
        
        ,
        …
        ,
        
          y
          
            n
          
        
        
          )
          
            
              T
            
          
        
      
    
    {\displaystyle y=(y_{1},y_{2},\ldots ,y_{n})^{\mathsf {T}}}
  , where 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   and 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   are real numbers for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  ), we can define the following relationships:

  
    
      
        x
        =
        y
      
    
    {\displaystyle x=y}
  , if 
  
    
      
        
          x
          
            i
          
        
        =
        
          y
          
            i
          
        
      
    
    {\displaystyle x_{i}=y_{i}}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  .

  
    
      
        x
        <
        y
      
    
    {\displaystyle x<y}
  , if 
  
    
      
        
          x
          
            i
          
        
        <
        
          y
          
            i
          
        
      
    
    {\displaystyle x_{i}<y_{i}}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  .

  
    
      
        x
        ≤
        y
      
    
    {\displaystyle x\leq y}
  , if 
  
    
      
        
          x
          
            i
          
        
        ≤
        
          y
          
            i
          
        
      
    
    {\displaystyle x_{i}\leq y_{i}}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
   and 
  
    
      
        x
        ≠
        y
      
    
    {\displaystyle x\neq y}
  .

  
    
      
        x
        ≦
        y
      
    
    {\displaystyle x\leqq y}
  , if 
  
    
      
        
          x
          
            i
          
        
        ≤
        
          y
          
            i
          
        
      
    
    {\displaystyle x_{i}\leq y_{i}}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  .Similarly, we can define relationships for 
  
    
      
        x
        >
        y
      
    
    {\displaystyle x>y}
  , 
  
    
      
        x
        ≥
        y
      
    
    {\displaystyle x\geq y}
  , and 
  
    
      
        x
        ≧
        y
      
    
    {\displaystyle x\geqq y}
  . This notation is consistent with that used by Matthias Ehrgott in Multicriteria Optimization (see References).
The trichotomy property (as stated above) is not valid for vector relationships.  For example, when 
  
    
      
        x
        =
        (
        2
        ,
        5
        
          )
          
            
              T
            
          
        
      
    
    {\displaystyle x=(2,5)^{\mathsf {T}}}
   and 
  
    
      
        y
        =
        (
        3
        ,
        4
        
          )
          
            
              T
            
          
        
      
    
    {\displaystyle y=(3,4)^{\mathsf {T}}}
  , there exists no valid inequality relationship between these two vectors.  However, for the rest of the aforementioned properties, a parallel property for vector inequalities exists.


== Systems of inequalities ==
Systems of linear inequalities can be simplified by Fourier–Motzkin elimination.The cylindrical algebraic decomposition is an algorithm that allows testing whether a system of polynomial equations and inequalities has solutions, and, if solutions exist, describing them. The complexity of this algorithm is doubly exponential in the number of variables. It is an active research domain to design algorithms that are more efficient in specific cases.


== See also ==
Binary relation
Bracket (mathematics), for the use of similar ‹ and › signs as brackets
Inclusion (set theory)
Inequation
Interval (mathematics)
List of inequalities
List of triangle inequalities
Partially ordered set
Relational operators, used in programming languages to denote inequality


== References ==


== Sources ==
Hardy, G., Littlewood J. E., Pólya, G. (1999). Inequalities. Cambridge Mathematical Library, Cambridge University Press. ISBN 0-521-05206-8.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Beckenbach, E. F., Bellman, R. (1975). An Introduction to Inequalities. Random House Inc. ISBN 0-394-01559-2.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Drachman, Byron C., Cloud, Michael J. (1998). Inequalities: With Applications to Engineering. Springer-Verlag. ISBN 0-387-98404-6.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Grinshpan, A. Z. (2005), ""General inequalities, consequences, and applications"", Advances in Applied Mathematics, 34 (1): 71–100, doi:10.1016/j.aam.2004.05.001
Murray S. Klamkin. ""'Quickie' inequalities"" (PDF). Math Strategies. Archived (PDF) from the original on 2022-10-09.
Arthur Lohwater (1982). ""Introduction to Inequalities"". Online e-book in PDF format.
Harold Shapiro (2005). ""Mathematical Problem Solving"". The Old Problem Seminar. Kungliga Tekniska högskolan.
""3rd USAMO"". Archived from the original on 2008-02-03.
Pachpatte, B. G. (2005). Mathematical Inequalities. North-Holland Mathematical Library. Vol. 67 (first ed.). Amsterdam, The Netherlands: Elsevier. ISBN 0-444-51795-2. ISSN 0924-6509. MR 2147066. Zbl 1091.26008.
Ehrgott, Matthias (2005). Multicriteria Optimization. Springer-Berlin. ISBN 3-540-21398-8.
Steele, J. Michael (2004). The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities. Cambridge University Press. ISBN 978-0-521-54677-5.


== External links ==

""Inequality"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Graph of Inequalities by Ed Pegg, Jr.
AoPS Wiki entry about Inequalities"
f061f9cade,Norm (mathematics),"In mathematics, a norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways like the distance from the origin: it commutes with scaling, obeys a form of the triangle inequality, and is zero only at the origin. In particular, the Euclidean distance in a Euclidean space is defined by a norm on the associated Euclidean vector space, called the Euclidean norm, the 2-norm, or, sometimes, the magnitude of the vector. This norm can be defined as the square root of the inner product of a vector with itself.
A seminorm satisfies the first two properties of a norm, but may be zero for vectors other than the origin. A vector space with a specified norm is called a normed vector space. In a similar manner, a vector space with a seminorm is called a seminormed vector space.
The term pseudonorm has been used for several related meanings. It may be a synonym of ""seminorm"". 
A pseudonorm may satisfy the same axioms as a norm, with the equality replaced by an inequality ""
  
    
      
        
        ≤
        
      
    
    {\displaystyle \,\leq \,}
  "" in the homogeneity axiom.
It can also refer to a norm that can take infinite values, or to certain functions parametrised by a directed set.


== Definition ==
Given a vector space 
  
    
      
        X
      
    
    {\displaystyle X}
   over a subfield 
  
    
      
        F
      
    
    {\displaystyle F}
   of the  complex numbers 
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   a norm on 
  
    
      
        X
      
    
    {\displaystyle X}
   is a real-valued function 
  
    
      
        p
        :
        X
        →
        
          R
        
      
    
    {\displaystyle p:X\to \mathbb {R} }
   with the following properties, where 
  
    
      
        
          |
        
        s
        
          |
        
      
    
    {\displaystyle |s|}
   denotes the usual absolute value of a scalar 
  
    
      
        s
      
    
    {\displaystyle s}
  :
Subadditivity/Triangle inequality: 
  
    
      
        p
        (
        x
        +
        y
        )
        ≤
        p
        (
        x
        )
        +
        p
        (
        y
        )
      
    
    {\displaystyle p(x+y)\leq p(x)+p(y)}
   for all 
  
    
      
        x
        ,
        y
        ∈
        X
        .
      
    
    {\displaystyle x,y\in X.}
  
Absolute homogeneity: 
  
    
      
        p
        (
        s
        x
        )
        =
        
          |
        
        s
        
          |
        
        p
        (
        x
        )
      
    
    {\displaystyle p(sx)=|s|p(x)}
   for all 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   and all scalars 
  
    
      
        s
        .
      
    
    {\displaystyle s.}
  
Positive definiteness/positiveness/Point-separating: for all 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
   if 
  
    
      
        p
        (
        x
        )
        =
        0
      
    
    {\displaystyle p(x)=0}
   then 
  
    
      
        x
        =
        0.
      
    
    {\displaystyle x=0.}
  
Because property (2.) implies 
  
    
      
        p
        (
        0
        )
        =
        0
        ,
      
    
    {\displaystyle p(0)=0,}
   some authors replace property (3.) with the equivalent condition: for every 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
   
  
    
      
        p
        (
        x
        )
        =
        0
      
    
    {\displaystyle p(x)=0}
   if and only if 
  
    
      
        x
        =
        0.
      
    
    {\displaystyle x=0.}
  A seminorm on 
  
    
      
        X
      
    
    {\displaystyle X}
   is a function 
  
    
      
        p
        :
        X
        →
        
          R
        
      
    
    {\displaystyle p:X\to \mathbb {R} }
   that has properties (1.) and (2.) so that in particular, every norm is also a seminorm (and thus also a sublinear functional). However, there exist seminorms that are not norms. Properties (1.) and (2.) imply that if 
  
    
      
        p
      
    
    {\displaystyle p}
   is a norm (or more generally, a seminorm) then 
  
    
      
        p
        (
        0
        )
        =
        0
      
    
    {\displaystyle p(0)=0}
   and that 
  
    
      
        p
      
    
    {\displaystyle p}
   also has the following property:

Non-negativity: 
  
    
      
        p
        (
        x
        )
        ≥
        0
      
    
    {\displaystyle p(x)\geq 0}
   for all 
  
    
      
        x
        ∈
        X
        .
      
    
    {\displaystyle x\in X.}
  Some authors include non-negativity as part of the definition of ""norm"", although this is not necessary. 
Although this article defined ""positive"" to be a synonym of ""positive definite"", some authors instead define ""positive"" to be a synonym of ""non-negative""; these definitions are not equivalent.


=== Equivalent norms ===
Suppose that 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
   are two norms (or seminorms) on a vector space 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   Then 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
   are called equivalent, if there exist two positive real constants 
  
    
      
        c
      
    
    {\displaystyle c}
   and 
  
    
      
        C
      
    
    {\displaystyle C}
   with 
  
    
      
        c
        >
        0
      
    
    {\displaystyle c>0}
   such that for every vector 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
  

The relation ""
  
    
      
        p
      
    
    {\displaystyle p}
   is equivalent to 
  
    
      
        q
      
    
    {\displaystyle q}
  "" is reflexive, symmetric (
  
    
      
        c
        q
        ≤
        p
        ≤
        C
        q
      
    
    {\displaystyle cq\leq p\leq Cq}
   implies 
  
    
      
        
          
            
              1
              C
            
          
        
        p
        ≤
        q
        ≤
        
          
            
              1
              c
            
          
        
        p
      
    
    {\displaystyle {\tfrac {1}{C}}p\leq q\leq {\tfrac {1}{c}}p}
  ), and transitive and thus defines an equivalence relation on the set of all norms on 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   
The norms 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
   are equivalent if and only if they induce the same topology on 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   Any two norms on a finite-dimensional space are equivalent but this does not extend to infinite-dimensional spaces.


=== Notation ===
If a norm 
  
    
      
        p
        :
        X
        →
        
          R
        
      
    
    {\displaystyle p:X\to \mathbb {R} }
   is given on a vector space 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then the norm of a vector 
  
    
      
        z
        ∈
        X
      
    
    {\displaystyle z\in X}
   is usually denoted by enclosing it within double vertical lines: 
  
    
      
        ‖
        z
        ‖
        =
        p
        (
        z
        )
        .
      
    
    {\displaystyle \|z\|=p(z).}
    Such notation is also sometimes used if 
  
    
      
        p
      
    
    {\displaystyle p}
   is only a seminorm.  For the length of a vector in Euclidean space (which is an example of a norm, as explained below), the notation 
  
    
      
        
          |
        
        x
        
          |
        
      
    
    {\displaystyle |x|}
   with single vertical lines is also widespread.


== Examples ==
Every (real or complex) vector space admits a norm: If 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{i}\right)_{i\in I}}
   is a Hamel basis for a vector space 
  
    
      
        X
      
    
    {\displaystyle X}
   then the real-valued map that sends 
  
    
      
        x
        =
        
          ∑
          
            i
            ∈
            I
          
        
        
          s
          
            i
          
        
        
          x
          
            i
          
        
        ∈
        X
      
    
    {\displaystyle x=\sum _{i\in I}s_{i}x_{i}\in X}
   (where all but finitely many of the scalars 
  
    
      
        
          s
          
            i
          
        
      
    
    {\displaystyle s_{i}}
   are 
  
    
      
        0
      
    
    {\displaystyle 0}
  ) to 
  
    
      
        
          ∑
          
            i
            ∈
            I
          
        
        
          |
          
            s
            
              i
            
          
          |
        
      
    
    {\displaystyle \sum _{i\in I}\left|s_{i}\right|}
   is a norm on 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
    There are also a large number of norms that exhibit additional properties that make them useful for specific problems.


=== Absolute-value norm ===
The absolute value

is a norm on the one-dimensional vector spaces formed by the real or complex numbers.
Any norm 
  
    
      
        p
      
    
    {\displaystyle p}
   on a one-dimensional vector space 
  
    
      
        X
      
    
    {\displaystyle X}
   is equivalent (up to scaling) to the absolute value norm, meaning that there is a norm-preserving isomorphism of vector spaces 
  
    
      
        f
        :
        
          F
        
        →
        X
        ,
      
    
    {\displaystyle f:\mathbb {F} \to X,}
   where 
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
   is either 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   or 
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   and norm-preserving means that 
  
    
      
        
          |
        
        x
        
          |
        
        =
        p
        (
        f
        (
        x
        )
        )
        .
      
    
    {\displaystyle |x|=p(f(x)).}
  
This isomorphism is given by sending 
  
    
      
        1
        ∈
        
          F
        
      
    
    {\displaystyle 1\in \mathbb {F} }
   to a vector of norm 
  
    
      
        1
        ,
      
    
    {\displaystyle 1,}
   which exists since such a vector is obtained by multiplying any non-zero vector by the inverse of its norm.


=== Euclidean norm ===

On the 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space 
  
    
      
        
          
            R
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{n},}
   the intuitive notion of length of the vector 
  
    
      
        
          x
        
        =
        
          (
          
            
              x
              
                1
              
            
            ,
            
              x
              
                2
              
            
            ,
            …
            ,
            
              x
              
                n
              
            
          
          )
        
      
    
    {\displaystyle {\boldsymbol {x}}=\left(x_{1},x_{2},\ldots ,x_{n}\right)}
   is captured by the formula
This is the Euclidean norm, which gives the ordinary distance from the origin to the point X—a consequence of the Pythagorean theorem. 
This operation may also be referred to as ""SRSS"", which is an acronym for the square root of the sum of squares.The Euclidean norm is by far the most commonly used norm on 
  
    
      
        
          
            R
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{n},}
   but there are other norms on this vector space as will be shown below. 
However, all these norms are equivalent in the sense that they all define the same topology.
The inner product of two vectors of a Euclidean vector space is the dot product of their coordinate vectors over an orthonormal basis. 
Hence, the Euclidean norm can be written in a coordinate-free way as

The Euclidean norm is also called the 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L^{2}}
   norm, 
  
    
      
        
          ℓ
          
            2
          
        
      
    
    {\displaystyle \ell ^{2}}
   norm, 2-norm, or square norm; see 
  
    
      
        
          L
          
            p
          
        
      
    
    {\displaystyle L^{p}}
   space. 
It defines a distance function called the Euclidean length, 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L^{2}}
   distance, or 
  
    
      
        
          ℓ
          
            2
          
        
      
    
    {\displaystyle \ell ^{2}}
   distance.
The set of vectors in 
  
    
      
        
          
            R
          
          
            n
            +
            1
          
        
      
    
    {\displaystyle \mathbb {R} ^{n+1}}
   whose Euclidean norm is a given positive constant forms an 
  
    
      
        n
      
    
    {\displaystyle n}
  -sphere.


==== Euclidean norm of complex numbers ====

The Euclidean norm of a complex number is the absolute value (also called the modulus) of it, if the complex plane is identified with the Euclidean plane 
  
    
      
        
          
            R
          
          
            2
          
        
        .
      
    
    {\displaystyle \mathbb {R} ^{2}.}
   This identification of the complex number 
  
    
      
        x
        +
        i
        y
      
    
    {\displaystyle x+iy}
   as a vector in the Euclidean plane, makes the quantity 
  
    
      
        
          
            
              x
              
                2
              
            
            +
            
              y
              
                2
              
            
          
        
      
    
    {\textstyle {\sqrt {x^{2}+y^{2}}}}
   (as first suggested by Euler) the Euclidean norm associated with the complex number.


=== Quaternions and octonions ===

There are exactly four Euclidean Hurwitz algebras over the real numbers. These are the real numbers 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   the complex numbers 
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   the quaternions 
  
    
      
        
          H
        
        ,
      
    
    {\displaystyle \mathbb {H} ,}
   and lastly the octonions 
  
    
      
        
          O
        
        ,
      
    
    {\displaystyle \mathbb {O} ,}
   where the dimensions of these spaces over the real numbers are 
  
    
      
        1
        ,
        2
        ,
        4
        ,
        
           and 
        
        8
        ,
      
    
    {\displaystyle 1,2,4,{\text{ and }}8,}
   respectively. 
The canonical norms on 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   and 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
   are their absolute value functions, as discussed previously.
The canonical norm on 
  
    
      
        
          H
        
      
    
    {\displaystyle \mathbb {H} }
   of quaternions is defined by

for every quaternion 
  
    
      
        q
        =
        a
        +
        b
        
        
          i
        
        +
        c
        
        
          j
        
        +
        d
        
        
          k
        
      
    
    {\displaystyle q=a+b\,\mathbf {i} +c\,\mathbf {j} +d\,\mathbf {k} }
   in 
  
    
      
        
          H
        
        .
      
    
    {\displaystyle \mathbb {H} .}
   This is the same as the Euclidean norm on 
  
    
      
        
          H
        
      
    
    {\displaystyle \mathbb {H} }
   considered as the vector space 
  
    
      
        
          
            R
          
          
            4
          
        
        .
      
    
    {\displaystyle \mathbb {R} ^{4}.}
   Similarly, the canonical norm on the octonions is just the Euclidean norm on 
  
    
      
        
          
            R
          
          
            8
          
        
        .
      
    
    {\displaystyle \mathbb {R} ^{8}.}
  


=== Finite-dimensional complex normed spaces ===
On an 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional complex space 
  
    
      
        
          
            C
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {C} ^{n},}
   the most common norm is

In this case, the norm can be expressed as the square root of the inner product of the vector and itself:

where 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
   is represented as a column vector 
  
    
      
        
          
            
              [
              
                
                  
                    
                      x
                      
                        1
                      
                    
                    
                    
                      x
                      
                        2
                      
                    
                    
                    …
                    
                    
                      x
                      
                        n
                      
                    
                  
                
              
              ]
            
          
          
            
              T
            
          
        
      
    
    {\displaystyle {\begin{bmatrix}x_{1}\;x_{2}\;\dots \;x_{n}\end{bmatrix}}^{\rm {T}}}
   and 
  
    
      
        
          
            x
          
          
            H
          
        
      
    
    {\displaystyle {\boldsymbol {x}}^{H}}
   denotes its conjugate transpose.
This formula is valid for any inner product space, including Euclidean and complex spaces.  For complex spaces, the inner product is equivalent to the complex dot product. Hence the formula in this case can also be written using the following notation:


=== Taxicab norm or Manhattan norm ===

The name relates to the distance a taxi has to drive in a rectangular street grid (like that of the New York borough of Manhattan) to get from the origin to the point 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
  
The set of vectors whose 1-norm is a given constant forms the surface of a cross polytope of dimension equivalent to that of the norm minus 1. 
The Taxicab norm is also called the 
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell ^{1}}
   norm. The distance derived from this norm is called the Manhattan distance or 
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell _{1}}
   distance.
The 1-norm is simply the sum of the absolute values of the columns.
In contrast,

is not a norm because it may yield negative results.


=== p-norm ===

Let 
  
    
      
        p
        ≥
        1
      
    
    {\displaystyle p\geq 1}
   be a real number. 
The 
  
    
      
        p
      
    
    {\displaystyle p}
  -norm (also called 
  
    
      
        
          ℓ
          
            p
          
        
      
    
    {\displaystyle \ell _{p}}
  -norm) of vector 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{n})}
   is
For 
  
    
      
        p
        =
        1
        ,
      
    
    {\displaystyle p=1,}
   we get the taxicab norm, for 
  
    
      
        p
        =
        2
      
    
    {\displaystyle p=2}
   we get the Euclidean norm, and as 
  
    
      
        p
      
    
    {\displaystyle p}
   approaches 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
   the 
  
    
      
        p
      
    
    {\displaystyle p}
  -norm approaches the infinity norm or maximum norm:

The 
  
    
      
        p
      
    
    {\displaystyle p}
  -norm is related to the generalized mean or power mean.
For 
  
    
      
        p
        =
        2
        ,
      
    
    {\displaystyle p=2,}
   the 
  
    
      
        ‖
        
        ⋅
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \|\,\cdot \,\|_{2}}
  -norm is even induced by a canonical inner product 
  
    
      
        ⟨
        
        ⋅
        ,
        
        ⋅
        ⟩
        ,
      
    
    {\displaystyle \langle \,\cdot ,\,\cdot \rangle ,}
   meaning that 
  
    
      
        ‖
        
          x
        
        
          ‖
          
            2
          
        
        =
        
          
            ⟨
            
              x
            
            ,
            
              x
            
            ⟩
          
        
      
    
    {\displaystyle \|\mathbf {x} \|_{2}={\sqrt {\langle \mathbf {x} ,\mathbf {x} \rangle }}}
   for all vectors 
  
    
      
        
          x
        
        .
      
    
    {\displaystyle \mathbf {x} .}
   This inner product can be expressed in terms of the norm by using the polarization identity. 
On 
  
    
      
        
          ℓ
          
            2
          
        
        ,
      
    
    {\displaystyle \ell ^{2},}
   this inner product is the Euclidean inner product defined by

while for the space 
  
    
      
        
          L
          
            2
          
        
        (
        X
        ,
        μ
        )
      
    
    {\displaystyle L^{2}(X,\mu )}
   associated with a measure space 
  
    
      
        (
        X
        ,
        Σ
        ,
        μ
        )
        ,
      
    
    {\displaystyle (X,\Sigma ,\mu ),}
   which consists of all square-integrable functions, this inner product is 

This definition is still of some interest for 
  
    
      
        0
        <
        p
        <
        1
        ,
      
    
    {\displaystyle 0<p<1,}
   but the resulting function does not define a norm, because it violates the triangle inequality. 
What is true for this case of 
  
    
      
        0
        <
        p
        <
        1
        ,
      
    
    {\displaystyle 0<p<1,}
   even in the measurable analog, is that the corresponding 
  
    
      
        
          L
          
            p
          
        
      
    
    {\displaystyle L^{p}}
   class is a vector space, and it is also true that the function

(without 
  
    
      
        p
      
    
    {\displaystyle p}
  th root) defines a distance that makes 
  
    
      
        
          L
          
            p
          
        
        (
        X
        )
      
    
    {\displaystyle L^{p}(X)}
   into a complete metric topological vector space. These spaces are of great interest in functional analysis, probability theory and harmonic analysis.
However, aside from trivial cases, this topological vector space is not locally convex, and has no continuous non-zero linear forms. Thus the topological dual space contains only the zero functional.
The partial derivative of the 
  
    
      
        p
      
    
    {\displaystyle p}
  -norm is given by

The derivative with respect to 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   therefore, is

where 
  
    
      
        ∘
      
    
    {\displaystyle \circ }
   denotes Hadamard product and 
  
    
      
        
          |
        
        ⋅
        
          |
        
      
    
    {\displaystyle |\cdot |}
   is used for absolute value of each component of the vector.
For the special case of 
  
    
      
        p
        =
        2
        ,
      
    
    {\displaystyle p=2,}
   this becomes

or


=== Maximum norm (special case of: infinity norm, uniform norm, or supremum norm) ===

If 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   is some vector such that 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        ,
      
    
    {\displaystyle \mathbf {x} =(x_{1},x_{2},\ldots ,x_{n}),}
   then:

The set of vectors whose infinity norm is a given constant, 
  
    
      
        c
        ,
      
    
    {\displaystyle c,}
   forms the surface of a hypercube with edge length 
  
    
      
        2
        c
        .
      
    
    {\displaystyle 2c.}
  


=== Zero norm ===
In probability and functional analysis, the zero norm induces a complete metric topology for the space of measurable functions and for the F-space of sequences with F–norm 
  
    
      
        (
        
          x
          
            n
          
        
        )
        ↦
        
          ∑
          
            n
          
        
        
          
            2
            
              −
              n
            
          
          
            x
            
              n
            
          
          
            /
          
          (
          1
          +
          
            x
            
              n
            
          
          )
        
        .
      
    
    {\textstyle (x_{n})\mapsto \sum _{n}{2^{-n}x_{n}/(1+x_{n})}.}
   
Here we mean by F-norm some real-valued function 
  
    
      
        ‖
        ⋅
        ‖
      
    
    {\displaystyle \lVert \cdot \rVert }
   on an F-space with distance 
  
    
      
        d
        ,
      
    
    {\displaystyle d,}
   such that 
  
    
      
        ‖
        x
        ‖
        =
        d
        (
        x
        ,
        0
        )
        .
      
    
    {\displaystyle \lVert x\rVert =d(x,0).}
   The F-norm described above is not a norm in the usual sense because it lacks the required homogeneity property.


==== Hamming distance of a vector from zero ====

In metric geometry, the discrete metric takes the value one for distinct points and zero otherwise. When applied coordinate-wise to the elements of a vector space, the discrete distance defines the Hamming distance, which is important in coding and information theory. 
In the field of real or complex numbers, the distance of the discrete metric from zero is not homogeneous in the non-zero point; indeed, the distance from zero remains one as its non-zero argument approaches zero. 
However, the discrete distance of a number from zero does satisfy the other properties of a norm, namely the triangle inequality and positive definiteness. 
When applied component-wise to vectors, the discrete distance from zero behaves like a non-homogeneous ""norm"", which counts the number of non-zero components in its vector argument; again, this non-homogeneous ""norm"" is discontinuous.
In signal processing and statistics, David Donoho referred to the zero ""norm"" with quotation marks. 
Following Donoho's notation, the zero ""norm"" of 
  
    
      
        x
      
    
    {\displaystyle x}
   is simply the number of non-zero coordinates of 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   or the Hamming distance of the vector from zero. 
When this ""norm"" is localized to a bounded set, it is the limit of 
  
    
      
        p
      
    
    {\displaystyle p}
  -norms as 
  
    
      
        p
      
    
    {\displaystyle p}
   approaches 0. 
Of course, the zero ""norm"" is not truly a norm, because it is not positive homogeneous. 
Indeed, it is not even an F-norm in the sense described above, since it is discontinuous, jointly and severally, with respect to the scalar argument in scalar–vector multiplication and with respect to its vector argument. 
Abusing terminology, some engineers omit Donoho's quotation marks and inappropriately call the number-of-non-zeros function the 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L^{0}}
   norm, echoing the notation for the Lebesgue space of measurable functions.


=== Infinite dimensions ===
The generalization of the above norms to an infinite number of components leads to 
  
    
      
        
          ℓ
          
            p
          
        
      
    
    {\displaystyle \ell ^{p}}
   and 
  
    
      
        
          L
          
            p
          
        
      
    
    {\displaystyle L^{p}}
   spaces, with norms

for complex-valued sequences and functions on 
  
    
      
        X
        ⊆
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle X\subseteq \mathbb {R} ^{n}}
   respectively, which can be further generalized (see Haar measure).
Any inner product induces in a natural way the norm 
  
    
      
        ‖
        x
        ‖
        :=
        
          
            ⟨
            x
            ,
            x
            ⟩
          
        
        .
      
    
    {\textstyle \|x\|:={\sqrt {\langle x,x\rangle }}.}
  
Other examples of infinite-dimensional normed vector spaces can be found in the Banach space article.


=== Composite norms ===
Other norms on 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   can be constructed by combining the above; for example

is a norm on 
  
    
      
        
          
            R
          
          
            4
          
        
        .
      
    
    {\displaystyle \mathbb {R} ^{4}.}
  
For any norm and any injective linear transformation 
  
    
      
        A
      
    
    {\displaystyle A}
   we can define a new norm of 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   equal to

In 2D, with 
  
    
      
        A
      
    
    {\displaystyle A}
   a rotation by 45° and a suitable scaling, this changes the taxicab norm into the maximum norm. Each 
  
    
      
        A
      
    
    {\displaystyle A}
   applied to the taxicab norm, up to inversion and interchanging of axes, gives a different unit ball: a parallelogram of a particular shape, size, and orientation.
In 3D, this is similar but different for the 1-norm (octahedrons) and the maximum norm (prisms with parallelogram base).
There are examples of norms that are not defined by ""entrywise"" formulas.  For instance, the Minkowski functional of a centrally-symmetric convex body in 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   (centered at zero) defines a norm on 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   (see § Classification of seminorms: absolutely convex absorbing sets below).
All the above formulas also yield norms on 
  
    
      
        
          
            C
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {C} ^{n}}
   without modification.
There are also norms on spaces of matrices (with real or complex entries), the so-called matrix norms.


=== In abstract algebra ===

Let 
  
    
      
        E
      
    
    {\displaystyle E}
   be a finite extension of a field 
  
    
      
        k
      
    
    {\displaystyle k}
   of inseparable degree 
  
    
      
        
          p
          
            μ
          
        
        ,
      
    
    {\displaystyle p^{\mu },}
   and let 
  
    
      
        k
      
    
    {\displaystyle k}
   have algebraic closure 
  
    
      
        K
        .
      
    
    {\displaystyle K.}
    If the distinct embeddings of 
  
    
      
        E
      
    
    {\displaystyle E}
   are 
  
    
      
        
          
            {
            
              σ
              
                j
              
            
            }
          
          
            j
          
        
        ,
      
    
    {\displaystyle \left\{\sigma _{j}\right\}_{j},}
   then the Galois-theoretic norm of an element 
  
    
      
        α
        ∈
        E
      
    
    {\displaystyle \alpha \in E}
   is the value 
  
    
      
        
          
            (
            
              
                ∏
                
                  j
                
              
              
                
                  σ
                  
                    k
                  
                
                (
                α
                )
              
            
            )
          
          
            
              p
              
                μ
              
            
          
        
        .
      
    
    {\textstyle \left(\prod _{j}{\sigma _{k}(\alpha )}\right)^{p^{\mu }}.}
    As that function is homogeneous of degree 
  
    
      
        [
        E
        :
        k
        ]
      
    
    {\displaystyle [E:k]}
  , the Galois-theoretic norm is not a norm in the sense of this article.  However, the 
  
    
      
        [
        E
        :
        k
        ]
      
    
    {\displaystyle [E:k]}
  -th root of the norm (assuming that concept makes sense) is a norm.


==== Composition algebras ====
The concept of norm 
  
    
      
        N
        (
        z
        )
      
    
    {\displaystyle N(z)}
   in composition algebras does not share the usual properties of a norm as it may be negative or zero for 
  
    
      
        z
        ≠
        0.
      
    
    {\displaystyle z\neq 0.}
   A composition algebra 
  
    
      
        (
        A
        ,
        
          

          
          
            ∗
          
        
        ,
        N
        )
      
    
    {\displaystyle (A,{}^{*},N)}
   consists of an algebra over a field 
  
    
      
        A
        ,
      
    
    {\displaystyle A,}
   an involution 
  
    
      
        
          

          
          
            ∗
          
        
        ,
      
    
    {\displaystyle {}^{*},}
   and a quadratic form 
  
    
      
        N
        (
        z
        )
        =
        z
        
          z
          
            ∗
          
        
      
    
    {\displaystyle N(z)=zz^{*}}
   called the ""norm"".
The characteristic feature of composition algebras is the homomorphism property of 
  
    
      
        N
      
    
    {\displaystyle N}
  : for the product 
  
    
      
        w
        z
      
    
    {\displaystyle wz}
   of two elements 
  
    
      
        w
      
    
    {\displaystyle w}
   and 
  
    
      
        z
      
    
    {\displaystyle z}
   of the composition algebra, its norm satisfies 
  
    
      
        N
        (
        w
        z
        )
        =
        N
        (
        w
        )
        N
        (
        z
        )
        .
      
    
    {\displaystyle N(wz)=N(w)N(z).}
   For 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   
  
    
      
        
          H
        
        ,
      
    
    {\displaystyle \mathbb {H} ,}
   and O the composition algebra norm is the square of the norm discussed above. In those cases the norm is a definite quadratic form. In other composition algebras the norm is an isotropic quadratic form.


== Properties ==
For any norm 
  
    
      
        p
        :
        X
        →
        
          R
        
      
    
    {\displaystyle p:X\to \mathbb {R} }
   on a vector space 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   the reverse triangle inequality holds: 

If 
  
    
      
        u
        :
        X
        →
        Y
      
    
    {\displaystyle u:X\to Y}
   is a continuous linear map between normed spaces, then the norm of 
  
    
      
        u
      
    
    {\displaystyle u}
   and the norm of the transpose of 
  
    
      
        u
      
    
    {\displaystyle u}
   are equal.For the 
  
    
      
        
          L
          
            p
          
        
      
    
    {\displaystyle L^{p}}
    norms, we have Hölder's inequality
A special case of this is the Cauchy–Schwarz inequality:

Every norm is a seminorm and thus satisfies all properties of the latter. In turn, every seminorm is a sublinear function and thus satisfies all properties of the latter. In particular, every norm is a convex function.


=== Equivalence ===
The concept of unit circle (the set of all vectors of norm 1) is different in different norms: for the 1-norm, the unit circle is a square, for the 2-norm (Euclidean norm), it is the well-known unit circle, while for the infinity norm, it is a different square. For any 
  
    
      
        p
      
    
    {\displaystyle p}
  -norm, it is a superellipse with congruent axes (see the accompanying illustration). Due to the definition of the norm, the unit circle must be convex and centrally symmetric (therefore, for example, the unit ball may be a rectangle but cannot be a triangle, and 
  
    
      
        p
        ≥
        1
      
    
    {\displaystyle p\geq 1}
   for a 
  
    
      
        p
      
    
    {\displaystyle p}
  -norm).
In terms of the vector space, the seminorm defines a topology on the space, and this is a Hausdorff topology precisely when the seminorm can distinguish between distinct vectors, which is again equivalent to the seminorm being a norm. The topology thus defined (by either a norm or a seminorm) can be understood either in terms of sequences or open sets. A sequence of vectors 
  
    
      
        {
        
          v
          
            n
          
        
        }
      
    
    {\displaystyle \{v_{n}\}}
   is said to converge in norm to 
  
    
      
        v
        ,
      
    
    {\displaystyle v,}
   if 
  
    
      
        
          ‖
          
            
              v
              
                n
              
            
            −
            v
          
          ‖
        
        →
        0
      
    
    {\displaystyle \left\|v_{n}-v\right\|\to 0}
   as 
  
    
      
        n
        →
        ∞
        .
      
    
    {\displaystyle n\to \infty .}
   Equivalently, the topology consists of all sets that can be represented as a union of open balls. If 
  
    
      
        (
        X
        ,
        ‖
        ⋅
        ‖
        )
      
    
    {\displaystyle (X,\|\cdot \|)}
   is a normed space then
  
    
      
        ‖
        x
        −
        y
        ‖
        =
        ‖
        x
        −
        z
        ‖
        +
        ‖
        z
        −
        y
        ‖
        
           for all 
        
        x
        ,
        y
        ∈
        X
        
           and 
        
        z
        ∈
        [
        x
        ,
        y
        ]
        .
      
    
    {\displaystyle \|x-y\|=\|x-z\|+\|z-y\|{\text{ for all }}x,y\in X{\text{ and }}z\in [x,y].}
  
Two norms 
  
    
      
        ‖
        ⋅
        
          ‖
          
            α
          
        
      
    
    {\displaystyle \|\cdot \|_{\alpha }}
   and 
  
    
      
        ‖
        ⋅
        
          ‖
          
            β
          
        
      
    
    {\displaystyle \|\cdot \|_{\beta }}
   on a vector space 
  
    
      
        X
      
    
    {\displaystyle X}
   are called equivalent if they induce the same topology, which happens if and only if there exist positive real numbers 
  
    
      
        C
      
    
    {\displaystyle C}
   and 
  
    
      
        D
      
    
    {\displaystyle D}
   such that for all 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
  

For instance, if 
  
    
      
        p
        >
        r
        ≥
        1
      
    
    {\displaystyle p>r\geq 1}
   on 
  
    
      
        
          
            C
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {C} ^{n},}
   then
In particular,

That is, 

If the vector space is a finite-dimensional real or complex one, all norms are equivalent. On the other hand, in the case of infinite-dimensional vector spaces, not all norms are equivalent.
Equivalent norms define the same notions of continuity and convergence and for many purposes do not need to be distinguished. To be more precise the uniform structure defined by equivalent norms on the vector space is uniformly isomorphic.


== Classification of seminorms: absolutely convex absorbing sets ==

All seminorms on a vector space 
  
    
      
        X
      
    
    {\displaystyle X}
   can be classified in terms of absolutely convex absorbing subsets 
  
    
      
        A
      
    
    {\displaystyle A}
   of 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   To each such subset corresponds a seminorm 
  
    
      
        
          p
          
            A
          
        
      
    
    {\displaystyle p_{A}}
   called the gauge of 
  
    
      
        A
        ,
      
    
    {\displaystyle A,}
   defined as

where 
  
    
      
        
          inf
          

          
        
      
    
    {\displaystyle \inf _{}}
   is the infimum, with the property that

Conversely:
Any locally convex topological vector space has a local basis consisting of absolutely convex sets. A common method to construct such a basis is to use a family 
  
    
      
        (
        p
        )
      
    
    {\displaystyle (p)}
   of seminorms 
  
    
      
        p
      
    
    {\displaystyle p}
   that separates points: the collection of all finite intersections of sets 
  
    
      
        {
        p
        <
        1
        
          /
        
        n
        }
      
    
    {\displaystyle \{p<1/n\}}
   turns the space into a locally convex topological vector space so that every p is continuous.
Such a method is used to design weak and weak* topologies.
norm case:

Suppose now that 
  
    
      
        (
        p
        )
      
    
    {\displaystyle (p)}
   contains a single 
  
    
      
        p
        :
      
    
    {\displaystyle p:}
   since 
  
    
      
        (
        p
        )
      
    
    {\displaystyle (p)}
   is separating, 
  
    
      
        p
      
    
    {\displaystyle p}
   is a norm, and 
  
    
      
        A
        =
        {
        p
        <
        1
        }
      
    
    {\displaystyle A=\{p<1\}}
   is its open unit ball. Then 
  
    
      
        A
      
    
    {\displaystyle A}
   is an absolutely convex bounded neighbourhood of 0, and 
  
    
      
        p
        =
        
          p
          
            A
          
        
      
    
    {\displaystyle p=p_{A}}
   is continuous.The converse is due to Andrey Kolmogorov: any locally convex and locally bounded topological vector space is normable. Precisely:
If 
  
    
      
        X
      
    
    {\displaystyle X}
   is an absolutely convex bounded neighbourhood of 0, the gauge 
  
    
      
        
          g
          
            X
          
        
      
    
    {\displaystyle g_{X}}
   (so that 
  
    
      
        X
        =
        {
        
          g
          
            X
          
        
        <
        1
        }
      
    
    {\displaystyle X=\{g_{X}<1\}}
   is a norm.


== See also ==
Asymmetric norm – Generalization of the concept of a norm
F-seminorm – A topological vector space whose topology can be defined by a metricPages displaying short descriptions of redirect targets
Gowers norm
Kadec norm – All infinite-dimensional, separable Banach spaces are homeomorphicPages displaying short descriptions of redirect targets
Least-squares spectral analysis – Periodicity computation method
Mahalanobis distance – Statistical distance measure
Magnitude (mathematics) – mathematical concept related to comparison and orderingPages displaying wikidata descriptions as a fallback
Matrix norm – Norm on a vector space of matrices
Minkowski distance – distance between vectors or points computed as the pth root of the sum of pth powers of coordinate differencesPages displaying wikidata descriptions as a fallback
Minkowski functional
Operator norm – Measure of the ""size"" of linear operators
Paranorm – A topological vector space whose topology can be defined by a metricPages displaying short descriptions of redirect targets
Relation of norms and metrics – Mathematical space with a notion of distancePages displaying short descriptions of redirect targets
Seminorm – nonnegative-real-valued function on a real or complex vector space that satisfies the triangle inequality and is absolutely homogenousPages displaying wikidata descriptions as a fallback
Sublinear function


== References ==


== Bibliography ==
Bourbaki, Nicolas (1987) [1981]. Topological Vector Spaces: Chapters 1–5. Éléments de mathématique. Translated by Eggleston, H.G.; Madan, S. Berlin New York: Springer-Verlag. ISBN 3-540-13627-4. OCLC 17499190.
Khaleelulla, S. M. (1982). Counterexamples in Topological Vector Spaces. Lecture Notes in Mathematics. Vol. 936. Berlin, Heidelberg, New York: Springer-Verlag. ISBN 978-3-540-11565-6. OCLC 8588370.
Kubrusly, Carlos S. (2011). The Elements of Operator Theory (Second ed.). Boston: Birkhäuser Basel. ISBN 978-0-8176-4998-2. OCLC 710154895.
Narici, Lawrence; Beckenstein, Edward (2011). Topological Vector Spaces. Pure and applied mathematics (Second ed.). Boca Raton, FL: CRC Press. ISBN 978-1584888666. OCLC 144216834.
Schaefer, Helmut H.; Wolff, Manfred P. (1999). Topological Vector Spaces. GTM. Vol. 8 (Second ed.). New York, NY: Springer New York Imprint Springer. ISBN 978-1-4612-7155-0. OCLC 840278135.
Trèves, François (2006) [1967]. Topological Vector Spaces, Distributions and Kernels. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-45352-1. OCLC 853623322.
Wilansky, Albert (2013). Modern Methods in Topological Vector Spaces. Mineola, New York: Dover Publications, Inc. ISBN 978-0-486-49353-4. OCLC 849801114."
5969c6c4d3,Mathematics (disambiguation),
aa85338d7d,Mathematical analysis,"Analysis is the branch of mathematics dealing with continuous functions, limits, and related theories, such as differentiation, integration, measure, infinite sequences, series, and analytic functions.These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).


== History ==


=== Ancient ===
Mathematical analysis formally developed in the 17th century during the Scientific Revolution, but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy. (Strictly speaking, the point of the paradox is to deny that the infinite sum exists.) Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids. The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century. In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century AD to find the area of a circle. From Jain literature, it appears that Hindus were in possession of the formulae for the sum of the arithmetic and geometric series as early as the 4th century B.C.Ācārya Bhadrabāhu uses the sum of a geometric series in his Kalpasūtra in 433 B.C. In Indian mathematics, particular instances of arithmetic series have been found to implicitly occur in Vedic Literature as early as 2000 B.C.


=== Medieval ===
Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century. In the 12th century, the Indian mathematician Bhāskara II gave examples of derivatives and used what is now known as Rolle's theorem.In the 14th century, Madhava of Sangamagrama developed infinite series expansions, now called Taylor series, of functions such as sine, cosine, tangent and arctangent. Alongside his development of Taylor series of trigonometric functions, he also estimated the magnitude of the error terms resulting of truncating these series, and gave a rational approximation of some infinite series.  His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.


=== Modern ===


==== Foundations ====
The modern foundations of mathematical analysis were established in 17th century Europe. This began when Fermat and Descartes developed analytic geometry, which is the precursor to modern calculus. Fermat's method of adequality allowed him to determine the maxima and minima of functions and the tangents of curves. Descartes's publication of La Géométrie in 1637, which introduced the Cartesian coordinate system, is considered to be the establishment of mathematical analysis. It would be a few decades later that Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18th century, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.


==== Modernization ====
In the 18th century, Euler introduced the notion of a mathematical function. Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816, but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler.  Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals.  Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y.  He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis.  The contributions of these mathematicians and others, such as Weierstrass, developed the (ε, δ)-definition of limit approach, thus founding the modern field of mathematical analysis. Around the same time, Riemann introduced his theory of integration, and made significant advances in complex analysis.
Towards the end of the 19th century, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the ""gaps"" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the ""size"" of the set of discontinuities of real functions.
Also, various pathological objects, (such as nowhere continuous functions, continuous but nowhere differentiable functions, and space-filling curves), commonly known as ""monsters"", began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue greatly improved measure theory, and introduced his own theory of integration, now known as Lebesgue integration, which proved to be a big improvement over Riemann's. Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.


== Important concepts ==


=== Metric spaces ===

In mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.
Much of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).
Formally, a metric space is an ordered pair 
  
    
      
        (
        M
        ,
        d
        )
      
    
    {\displaystyle (M,d)}
   where 
  
    
      
        M
      
    
    {\displaystyle M}
   is a set and 
  
    
      
        d
      
    
    {\displaystyle d}
   is a metric on 
  
    
      
        M
      
    
    {\displaystyle M}
  , i.e., a function

  
    
      
        d
        :
        M
        ×
        M
        →
        
          R
        
      
    
    {\displaystyle d\colon M\times M\rightarrow \mathbb {R} }
  such that for any 
  
    
      
        x
        ,
        y
        ,
        z
        ∈
        M
      
    
    {\displaystyle x,y,z\in M}
  , the following holds:

  
    
      
        d
        (
        x
        ,
        y
        )
        ≥
        0
      
    
    {\displaystyle d(x,y)\geq 0}
  , with equality if and only if 
  
    
      
        x
        =
        y
      
    
    {\displaystyle x=y}
      (identity of indiscernibles),

  
    
      
        d
        (
        x
        ,
        y
        )
        =
        d
        (
        y
        ,
        x
        )
      
    
    {\displaystyle d(x,y)=d(y,x)}
      (symmetry), and

  
    
      
        d
        (
        x
        ,
        z
        )
        ≤
        d
        (
        x
        ,
        y
        )
        +
        d
        (
        y
        ,
        z
        )
      
    
    {\displaystyle d(x,z)\leq d(x,y)+d(y,z)}
      (triangle inequality).By taking the third property and letting 
  
    
      
        z
        =
        x
      
    
    {\displaystyle z=x}
  , it can be shown that 
  
    
      
        d
        (
        x
        ,
        y
        )
        ≥
        0
      
    
    {\displaystyle d(x,y)\geq 0}
       (non-negative).


=== Sequences and limits ===

A sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.
One of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n → ∞, denoted

  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        =
        x
        .
      
    
    {\displaystyle \lim _{n\to \infty }a_{n}=x.}
  


== Main branches ==


=== Real analysis ===

Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable. In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.


=== Complex analysis ===

Complex analysis (traditionally known as the theory of functions of a complex variable) is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.
Complex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.


=== Functional analysis ===

Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces.  This point of view turned out to be particularly useful for the study of differential and integral equations.


=== Harmonic analysis ===

Harmonic analysis is a branch of mathematical analysis concerned with the representation of functions and signals as the superposition of basic waves. This includes the study of the notions of Fourier series and Fourier transforms (Fourier analysis), and of their generalizations. Harmonic analysis has applications in areas as diverse as music theory, number theory, representation theory, signal processing, quantum mechanics, tidal analysis, and neuroscience.


=== Differential equations ===

A differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders. Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.
Differential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly.


=== Measure theory ===

A measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size. In this sense, a measure is a generalization of the concepts of length, area, and volume. A particularly important example is the Lebesgue measure on a Euclidean space, which assigns the conventional length, area, and volume of Euclidean geometry to suitable subsets of the 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  . For instance, the Lebesgue measure of the interval 
  
    
      
        
          [
          
            0
            ,
            1
          
          ]
        
      
    
    {\displaystyle \left[0,1\right]}
   in the real numbers is its length in the everyday sense of the word – specifically, 1.
Technically, a measure is a function that assigns a non-negative real number or +∞ to (certain) subsets of a set 
  
    
      
        X
      
    
    {\displaystyle X}
  . It must assign 0 to the empty set and be (countably) additive: the measure of a 'large' subset that can be decomposed into a finite (or countable) number of 'smaller' disjoint subsets, is the sum of the measures of the ""smaller"" subsets. In general, if one wants to associate a consistent size to each subset of a given set while satisfying the other axioms of a measure, one only finds trivial examples like the counting measure. This problem was resolved by defining measure only on a sub-collection of all subsets; the so-called measurable subsets, which are required to form a 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  -algebra. This means that countable unions, countable intersections and complements of measurable subsets are measurable. Non-measurable sets in a Euclidean space, on which the Lebesgue measure cannot be defined consistently, are necessarily complicated in the sense of being badly mixed up with their complement. Indeed, their existence is a non-trivial consequence of the axiom of choice.


=== Numerical analysis ===

Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).Modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.
Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.


=== Vector analysis ===

Vector analysis is a branch of mathematical analysis dealing with values which have both magnitude and direction. Some examples of vectors include velocity, force, and displacement. Vectors are commonly associated with scalars, values which describe magnitude.


=== Scalar analysis ===

Scalar analysis is a branch of mathematical analysis dealing with values related to scale as opposed to direction. Values such as temperature are scalar because they describe the magnitude of a value without regard to direction, force, or displacement that value may or may not have.


=== Tensor analysis ===


== Other topics ==
Calculus of variations deals with extremizing functionals, as opposed to ordinary calculus which deals with functions.
Harmonic analysis deals with the representation of functions or signals as the superposition of basic waves.
Geometric analysis involves the use of geometrical methods in the study of partial differential equations and the application of the theory of partial differential equations to geometry.
Clifford analysis, the study of Clifford valued functions that are annihilated by Dirac or Dirac-like operators,  termed in general as monogenic or Clifford analytic functions.
p-adic analysis, the study of analysis within the context of p-adic numbers, which differs in some interesting and surprising ways from its real and complex counterparts.
Non-standard analysis, which investigates the hyperreal numbers and their functions and gives a rigorous treatment of infinitesimals and infinitely large numbers.
Computable analysis, the study of which parts of analysis can be carried out in a computable manner.
Stochastic calculus – analytical notions developed for stochastic processes.
Set-valued analysis – applies ideas from analysis and topology to set-valued functions.
Convex analysis, the study of convex sets and functions.
Idempotent analysis – analysis in the context of an idempotent semiring, where the lack of an additive inverse is compensated somewhat by the idempotent rule A + A = A.
Tropical analysis  – analysis of the idempotent semiring called the tropical semiring (or max-plus algebra/min-plus algebra).
Constructive analysis, which is built upon a foundation of constructive, rather than classical, logic and set theory.
Intuitionistic analysis, which is developed from constructive logic like constructive analysis but also incorporates choice sequences.
Paraconsistent analysis, which is built upon a foundation of paraconsistent, rather than classical, logic and set theory.
Smooth infinitesimal analysis, which is developed in a smooth topos.


== Applications ==
Techniques from analysis are also found in other areas such as:


=== Physical sciences ===
The vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schrödinger equation, and the Einstein field equations.
Functional analysis is also a major factor in quantum mechanics.


=== Signal processing ===
When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal.  A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.


=== Other areas of mathematics ===
Techniques from analysis are used in many areas of mathematics, including:

Analytic number theory
Analytic combinatorics
Continuous probability
Differential entropy in information theory
Differential games
Differential geometry, the application of calculus to specific mathematical spaces known as manifolds that possess a complicated internal structure but behave in a simple manner locally.
Differentiable manifolds
Differential topology
Partial differential equations


== Famous Textbooks ==
Foundation of Analysis: The Arithmetic of Whole Rational, Irrational and Complex Numbers, by Edmund Landau
Introductory Real Analysis, by Andrey Kolmogorov, Sergei Fomin
Differential and Integral Calculus (3 volumes), by Grigorii Fichtenholz
The Fundamentals of Mathematical Analysis (2 volumes), by Grigorii Fichtenholz
A Course Of Mathematical Analysis (2 volumes), by Sergey Nikolsky
Mathematical Analysis (2 volumes), by Vladimir Zorich
A Course of Higher Mathematics (5 volumes, 6 parts), by Vladimir Smirnov
Differential And Integral Calculus, by Nikolai Piskunov
A Course of Mathematical Analysis, by Aleksandr Khinchin
Mathematical Analysis: A Special Course, by Georgiy Shilov
Theory of Functions of a Real Variable (2 volumes), by Isidor Natanson
Problems in Mathematical Analysis, by Boris Demidovich
Problems and Theorems in Analysis (2 volumes), by George Polya, Gabor Szegö
Mathematical Analysis: A Modern Approach to Advanced Calculus, by Tom Apostol
Principles of Mathematical Analysis, by Walter Rudin
Real Analysis: Measure Theory, Integration, and Hilbert Spaces, by Elias Stein
Complex Analysis, by Elias Stein
Functional Analysis: Introduction to Further Topics in Analysis, by Elias Stein
Analysis (2 volumes), by Terence Tao
Analysis (3 volumes), by Herbert Amann, Joachim Escher
Real and Functional Analysis, by Vladimir Bogachev, Oleg Smolyanov
Real and Functional Analysis, by Serge Lang


== See also ==
Constructive analysis
History of calculus
Hypercomplex analysis
Multiple rule-based problems
Multivariable calculus
Paraconsistent logic
Smooth infinitesimal analysis
Timeline of calculus and mathematical analysis


== References ==


== Further reading ==
Aleksandrov [Алекса́ндров], Aleksandr Danilovich [Алекса́ндр Дани́лович]; Lavrent'ev [Лавре́нтьев], Mikhail Alexseevich [Михаи́л Алексе́евич]; Nikol'skiĭ [Нико́льский], Sergey Mikhailovich [Серге́й Миха́йлович]; Delone [Делоне́], Boris Nikolaevich [Бори́с Никола́евич]; Petrovskiĭ [Петро́вский], Ivan Georgievich [Ива́н Гео́ргиевич]; Sobolev [Со́болев], Sergei Lvovich [Серге́й Льво́вич]; Ladyženskaja [Лады́женская], Olga Aleksandrovna [Óльга Алекса́ндровна]; Krylov [Крылоў], Vladimir Ivanovich [Уладзімір Іванавіч] [at Wikidata]; Keldyš [Ке́лдыш], Mstislav Vsevolodovich [Мстисла́в Все́володович]; Mardzanisvili [Марджанишвили], Konstantin Konstantinovich [Константин Константинович] [in Russian]; Postnikov [Постников], Aleksei Georgievich [Алексей Георгиевич]; Kolmogorov [Колмого́ров], Andrey Nikolaevich [Андре́й Никола́евич]; Lebedev [Ле́бедев], Sergey Alexeyevich [Серге́й Алексе́евич]; Kantorovič [Канторо́вич], Leonid Vitaliyevich [Леони́д Вита́льевич]; Stečkin [Сте́чкин], Sergey Borisovich [Серге́й Бори́сович]; Faddeev [Фадде́ев], Dmitry Konstantinovich [Дми́трий Константи́нович]; Aleksandrov [Алекса́ндров], Pavel Sergeyevich [Па́вел Серге́евич]; Gel'fand [Гельфа́нд], Israïl Moyseyovich [Изра́иль Моисе́евич]; Mal'cev [Ма́льцев], Anatoly Ivanovich [Анато́лий Ива́нович] (March 1969).  Aleksandrov [Алекса́ндров], Aleksandr Danilovich [Алекса́ндр Дани́лович]; Kolmogorov [Колмого́ров], Andrey Nikolaevich [Андре́й Никола́евич]; Lavrent'ev [Лавре́нтьев], Mikhail Alexseevich [Михаи́л Алексе́евич] (eds.). Mathematics: Its Content, Methods, and Meaning. Vol. 1–3. Translated by Gould, Sydney Henry [at Wikidata]; Hirsch, Kurt August; Bartha, Tamas. Translation edited by Gould (2nd ed.). Cambridge, Massachusetts, USA: The M.I.T. Press / American Mathematical Society. LCCN 64-7547. MIT 106, 107, 108.  ark:/13960/t4sj8550w. [1] (NB. 3 softcover volumes in slipcase. Original Russian title in March 1956: Математика, ее содержание, методы и значение [2][3][4]. First English edition in 6 volumes by AMS in 1962/1963, revised English edition in 3 volumes by MIT Press in August 1964: [5], 2nd printing by MIT Press in April 1965. First MIT paperback edition in March 1969. Reprinted in one volume by Dover.)
Apostol, Tom M. (1974). Mathematical Analysis (2nd ed.). Addison–Wesley. ISBN 978-0-201-00288-1.
Binmore, Kenneth George (1981) [1981]. The foundations of analysis: a straightforward introduction. Cambridge University Press.
Johnsonbaugh, Richard; Pfaffenberger, William Elmer (1981). Foundations of mathematical analysis. New York: M. Dekker.
Nikol'skiĭ [Нико́льский], Sergey Mikhailovich [Серге́й Миха́йлович] (2002). ""Mathematical analysis"".  In Hazewinkel, Michiel (ed.). Encyclopaedia of Mathematics. Springer-Verlag. ISBN 978-1-4020-0609-8.
Fusco, Nicola; Marcellini, Paolo; Sbordone, Carlo (1996). Analisi Matematica Due (in Italian). Liguori Editore. ISBN 978-88-207-2675-1.
Rombaldi, Jean-Étienne (2004). Éléments d'analyse réelle : CAPES et agrégation interne de mathématiques (in French). EDP Sciences. ISBN 978-2-86883-681-6.
Rudin, Walter (1976). Principles of Mathematical Analysis (3rd ed.). New York, USA: McGraw-Hill. ISBN 978-0-07-054235-8.
Rudin, Walter (1987). Real and Complex Analysis (3rd ed.). New York, USA: McGraw-Hill. ISBN 978-0-07-054234-1.
Whittaker, Edmund Taylor; Watson, George Neville (1927-01-02). A Course Of Modern Analysis: An Introduction to the General Theory of Infinite Processes and of Analytic Functions; with an Account of the Principal Transcendental Functions (4th ed.). Cambridge, UK: at the University Press. ISBN 0-521-06794-4. (vi+608 pages) (reprinted: 1935, 1940, 1946, 1950, 1952, 1958, 1962, 1963, 1992)
""Real Analysis - Course Notes"" (PDF). Archived (PDF) from the original on 2007-04-19.


== External links ==

Earliest Known Uses of Some of the Words of Mathematics: Calculus & Analysis
Basic Analysis: Introduction to Real Analysis by Jiri Lebl (Creative Commons BY-NC-SA)
Mathematical Analysis-Encyclopædia Britannica
Calculus and Analysis"
cca14b0f65,Computational mathematics,"Computational mathematics is an area of mathematics devoted to the interaction between mathematics and computer computation.A large part of computational mathematics consists roughly of using mathematics for allowing and improving computer computation in areas of science and engineering where mathematics are useful. This involves in particular algorithm design, computational complexity, numerical methods and computer algebra.
Computational mathematics refers also to the use of computers for mathematics itself. This includes mathematical experimentation for establishing conjectures (particularly in number theory), the use of computers for proving theorems (for example the four color theorem), and the design and use of proof assistants.


== Areas of computational mathematics ==
Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. Currently, computational mathematics can refer to or include:

Computational science, also known as scientific computation or computational engineering
Solving mathematical problems by computer simulation as opposed to analytic methods of applied mathematics
Numerical methods used in scientific computation, for example numerical linear algebra and numerical solution of partial differential equations
Stochastic methods, such as Monte Carlo methods and other representations of uncertainty in scientific computation
The mathematics of scientific computation, in particular numerical analysis, the theory of numerical methods
Computational complexity
Computer algebra and computer algebra systems
Computer-assisted research in various areas of mathematics, such as logic (automated theorem proving), discrete mathematics, combinatorics, number theory, and computational algebraic topology
Cryptography and computer security, which involve, in particular, research on primality testing, factorization, elliptic curves, and mathematics of blockchain
Computational linguistics, the use of mathematical and computer techniques in natural languages
Computational algebraic geometry
Computational group theory
Computational geometry
Computational number theory
Computational topology
Computational statistics
Algorithmic information theory
Algorithmic game theory
Mathematical economics, the use of mathematics in economics, finance and, to certain extents, of accounting.
Experimental mathematics


== See also ==
 Mathematics portal


== References ==


== Further reading ==
Cucker, F. (2003). Foundations of Computational Mathematics: Special Volume. Handbook of Numerical Analysis. North-Holland Publishing. ISBN 978-0-444-51247-5.
Harris, J. W.; Stocker, H. (1998). Handbook of Mathematics and Computational Science. Springer-Verlag. ISBN 978-0-387-94746-4.
Hartmann, A.K. (2009). Practical Guide to Computer Simulations. World Scientific. ISBN 978-981-283-415-7. Archived from the original on February 11, 2009. Retrieved May 3, 2012.
Nonweiler, T. R. (1986). Computational Mathematics: An Introduction to Numerical Approximation. John Wiley and Sons. ISBN 978-0-470-20260-9.
Gentle, J. E. (2007). Foundations of Computational Science. Springer-Verlag. ISBN 978-0-387-00450-1.
White, R. E. (2003). Computational Mathematics: Models, Methods, and Analysis with MATLAB. Chapman and Hall. ISBN 978-1584883647.
Yang, X. S. (2008). Introduction to Computational Mathematics. World Scientific. ISBN 978-9812818171.
Strang, G. (2007). Computational Science and Engineering. Wiley. ISBN 978-0961408817.


== External links ==
Foundations of Computational Mathematics, a non-profit organization
International Journal of Computer Discovered Mathematics"
c002a56aa6,Image (mathematics),"In mathematics, the image of a function is the set of all output values it may produce.
More generally, evaluating a given function 
  
    
      
        f
      
    
    {\displaystyle f}
   at each element of a given subset 
  
    
      
        A
      
    
    {\displaystyle A}
   of its domain produces a set, called the ""image of 
  
    
      
        A
      
    
    {\displaystyle A}
   under (or through) 
  
    
      
        f
      
    
    {\displaystyle f}
  "". Similarly, the inverse image (or preimage) of a given subset 
  
    
      
        B
      
    
    {\displaystyle B}
   of the codomain of 
  
    
      
        f
        ,
      
    
    {\displaystyle f,}
   is the set of all elements of the domain that map to the members of 
  
    
      
        B
        .
      
    
    {\displaystyle B.}
  
Image and inverse image may also be defined for general binary relations, not just functions.


== Definition ==
The word ""image"" is used in three related ways. In these definitions, 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   is a function from the set 
  
    
      
        X
      
    
    {\displaystyle X}
   to the set 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
  


=== Image of an element ===
If 
  
    
      
        x
      
    
    {\displaystyle x}
   is a member of 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then the image of 
  
    
      
        x
      
    
    {\displaystyle x}
   under 
  
    
      
        f
        ,
      
    
    {\displaystyle f,}
   denoted 
  
    
      
        f
        (
        x
        )
        ,
      
    
    {\displaystyle f(x),}
   is the value of 
  
    
      
        f
      
    
    {\displaystyle f}
   when applied to 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
   
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   is alternatively known as the output of 
  
    
      
        f
      
    
    {\displaystyle f}
   for argument 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
   
Given 
  
    
      
        y
        ,
      
    
    {\displaystyle y,}
   the function 
  
    
      
        f
      
    
    {\displaystyle f}
   is said to ""take the value 
  
    
      
        y
      
    
    {\displaystyle y}
  "" or ""take 
  
    
      
        y
      
    
    {\displaystyle y}
   as a value"" if there exists some 
  
    
      
        x
      
    
    {\displaystyle x}
   in the function's domain such that 
  
    
      
        f
        (
        x
        )
        =
        y
        .
      
    
    {\displaystyle f(x)=y.}
   
Similarly, given a set 
  
    
      
        S
        ,
      
    
    {\displaystyle S,}
   
  
    
      
        f
      
    
    {\displaystyle f}
   is said to ""take a value in 
  
    
      
        S
      
    
    {\displaystyle S}
  "" if there exists some 
  
    
      
        x
      
    
    {\displaystyle x}
   in the function's domain such that 
  
    
      
        f
        (
        x
        )
        ∈
        S
        .
      
    
    {\displaystyle f(x)\in S.}
   
However, ""
  
    
      
        f
      
    
    {\displaystyle f}
   takes [all] values in 
  
    
      
        S
      
    
    {\displaystyle S}
  "" and ""
  
    
      
        f
      
    
    {\displaystyle f}
   is valued in 
  
    
      
        S
      
    
    {\displaystyle S}
  "" means that 
  
    
      
        f
        (
        x
        )
        ∈
        S
      
    
    {\displaystyle f(x)\in S}
   for every point 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        f
      
    
    {\displaystyle f}
  's domain.


=== Image of a subset ===
Throughout, let 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   be a function. 
The image under 
  
    
      
        f
      
    
    {\displaystyle f}
   of a subset 
  
    
      
        A
      
    
    {\displaystyle A}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   is the set of all 
  
    
      
        f
        (
        a
        )
      
    
    {\displaystyle f(a)}
   for 
  
    
      
        a
        ∈
        A
        .
      
    
    {\displaystyle a\in A.}
   It is denoted by 
  
    
      
        f
        [
        A
        ]
        ,
      
    
    {\displaystyle f[A],}
   or by 
  
    
      
        f
        (
        A
        )
        ,
      
    
    {\displaystyle f(A),}
   when there is no risk of confusion. Using set-builder notation, this definition can be written as
This induces a function 
  
    
      
        f
        [
        
        ⋅
        
        ]
        :
        
          
            P
          
        
        (
        X
        )
        →
        
          
            P
          
        
        (
        Y
        )
        ,
      
    
    {\displaystyle f[\,\cdot \,]:{\mathcal {P}}(X)\to {\mathcal {P}}(Y),}
   where 
  
    
      
        
          
            P
          
        
        (
        S
        )
      
    
    {\displaystyle {\mathcal {P}}(S)}
   denotes the power set of a set 
  
    
      
        S
        ;
      
    
    {\displaystyle S;}
   that is the set of all subsets of 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   See § Notation below for more.


=== Image of a function ===
The image of a function is the image of its entire domain, also known as the range of the function. This last usage should be avoided because the word ""range"" is also commonly used to mean the codomain of 
  
    
      
        f
        .
      
    
    {\displaystyle f.}
  


=== Generalization to binary relations ===
If 
  
    
      
        R
      
    
    {\displaystyle R}
   is an arbitrary binary relation on 
  
    
      
        X
        ×
        Y
        ,
      
    
    {\displaystyle X\times Y,}
   then the set 
  
    
      
        {
        y
        ∈
        Y
        :
        x
        R
        y
        
           for some 
        
        x
        ∈
        X
        }
      
    
    {\displaystyle \{y\in Y:xRy{\text{ for some }}x\in X\}}
   is called the image, or the range, of 
  
    
      
        R
        .
      
    
    {\displaystyle R.}
   Dually, the set 
  
    
      
        {
        x
        ∈
        X
        :
        x
        R
        y
        
           for some 
        
        y
        ∈
        Y
        }
      
    
    {\displaystyle \{x\in X:xRy{\text{ for some }}y\in Y\}}
   is called the domain of 
  
    
      
        R
        .
      
    
    {\displaystyle R.}
  


== Inverse image ==

Let 
  
    
      
        f
      
    
    {\displaystyle f}
   be a function from 
  
    
      
        X
      
    
    {\displaystyle X}
   to 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
   The preimage or inverse image of a set 
  
    
      
        B
        ⊆
        Y
      
    
    {\displaystyle B\subseteq Y}
   under 
  
    
      
        f
        ,
      
    
    {\displaystyle f,}
   denoted by 
  
    
      
        
          f
          
            −
            1
          
        
        [
        B
        ]
        ,
      
    
    {\displaystyle f^{-1}[B],}
   is the subset of 
  
    
      
        X
      
    
    {\displaystyle X}
   defined by

Other notations include 
  
    
      
        
          f
          
            −
            1
          
        
        (
        B
        )
      
    
    {\displaystyle f^{-1}(B)}
   and 
  
    
      
        
          f
          
            −
          
        
        (
        B
        )
        .
      
    
    {\displaystyle f^{-}(B).}
   
The inverse image of a singleton set, denoted by 
  
    
      
        
          f
          
            −
            1
          
        
        [
        {
        y
        }
        ]
      
    
    {\displaystyle f^{-1}[\{y\}]}
   or by 
  
    
      
        
          f
          
            −
            1
          
        
        [
        y
        ]
        ,
      
    
    {\displaystyle f^{-1}[y],}
   is also called the fiber or fiber over 
  
    
      
        y
      
    
    {\displaystyle y}
   or the level set of 
  
    
      
        y
        .
      
    
    {\displaystyle y.}
   The set of all the fibers over the elements of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   is a family of sets indexed by 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
  
For example, for the function 
  
    
      
        f
        (
        x
        )
        =
        
          x
          
            2
          
        
        ,
      
    
    {\displaystyle f(x)=x^{2},}
   the inverse image of 
  
    
      
        {
        4
        }
      
    
    {\displaystyle \{4\}}
   would be 
  
    
      
        {
        −
        2
        ,
        2
        }
        .
      
    
    {\displaystyle \{-2,2\}.}
   Again, if there is no risk of confusion, 
  
    
      
        
          f
          
            −
            1
          
        
        [
        B
        ]
      
    
    {\displaystyle f^{-1}[B]}
   can be denoted by 
  
    
      
        
          f
          
            −
            1
          
        
        (
        B
        )
        ,
      
    
    {\displaystyle f^{-1}(B),}
   and 
  
    
      
        
          f
          
            −
            1
          
        
      
    
    {\displaystyle f^{-1}}
   can also be thought of as a function from the power set of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   to the power set of 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   The notation 
  
    
      
        
          f
          
            −
            1
          
        
      
    
    {\displaystyle f^{-1}}
   should not be confused with that for inverse function, although it coincides with the usual one for bijections in that the inverse image of 
  
    
      
        B
      
    
    {\displaystyle B}
   under 
  
    
      
        f
      
    
    {\displaystyle f}
   is the image of 
  
    
      
        B
      
    
    {\displaystyle B}
   under 
  
    
      
        
          f
          
            −
            1
          
        
        .
      
    
    {\displaystyle f^{-1}.}
  


== Notation for image and inverse image ==
The traditional notations used in the previous section do not distinguish the original function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   from the image-of-sets function 
  
    
      
        f
        :
        
          
            P
          
        
        (
        X
        )
        →
        
          
            P
          
        
        (
        Y
        )
      
    
    {\displaystyle f:{\mathcal {P}}(X)\to {\mathcal {P}}(Y)}
  ; likewise they do not distinguish the inverse function (assuming one exists) from the inverse image function (which again relates the powersets).  Given the right context, this keeps the notation light and usually does not cause confusion.  But if needed, an alternative is to give explicit names for the image and preimage as functions between power sets:


=== Arrow notation ===

  
    
      
        
          f
          
            →
          
        
        :
        
          
            P
          
        
        (
        X
        )
        →
        
          
            P
          
        
        (
        Y
        )
      
    
    {\displaystyle f^{\rightarrow }:{\mathcal {P}}(X)\to {\mathcal {P}}(Y)}
   with 
  
    
      
        
          f
          
            →
          
        
        (
        A
        )
        =
        {
        f
        (
        a
        )
        
        
          |
        
        
        a
        ∈
        A
        }
      
    
    {\displaystyle f^{\rightarrow }(A)=\{f(a)\;|\;a\in A\}}
  

  
    
      
        
          f
          
            ←
          
        
        :
        
          
            P
          
        
        (
        Y
        )
        →
        
          
            P
          
        
        (
        X
        )
      
    
    {\displaystyle f^{\leftarrow }:{\mathcal {P}}(Y)\to {\mathcal {P}}(X)}
   with 
  
    
      
        
          f
          
            ←
          
        
        (
        B
        )
        =
        {
        a
        ∈
        X
        
        
          |
        
        
        f
        (
        a
        )
        ∈
        B
        }
      
    
    {\displaystyle f^{\leftarrow }(B)=\{a\in X\;|\;f(a)\in B\}}
  


=== Star notation ===

  
    
      
        
          f
          
            ⋆
          
        
        :
        
          
            P
          
        
        (
        X
        )
        →
        
          
            P
          
        
        (
        Y
        )
      
    
    {\displaystyle f_{\star }:{\mathcal {P}}(X)\to {\mathcal {P}}(Y)}
   instead of 
  
    
      
        
          f
          
            →
          
        
      
    
    {\displaystyle f^{\rightarrow }}
  

  
    
      
        
          f
          
            ⋆
          
        
        :
        
          
            P
          
        
        (
        Y
        )
        →
        
          
            P
          
        
        (
        X
        )
      
    
    {\displaystyle f^{\star }:{\mathcal {P}}(Y)\to {\mathcal {P}}(X)}
   instead of 
  
    
      
        
          f
          
            ←
          
        
      
    
    {\displaystyle f^{\leftarrow }}
  


=== Other terminology ===
An alternative notation for 
  
    
      
        f
        [
        A
        ]
      
    
    {\displaystyle f[A]}
   used in mathematical logic and set theory is 
  
    
      
        f
        
          
          ″
        
        A
        .
      
    
    {\displaystyle f\,''A.}
  
Some texts refer to the image of 
  
    
      
        f
      
    
    {\displaystyle f}
   as the range of 
  
    
      
        f
        ,
      
    
    {\displaystyle f,}
   but this usage should be avoided because the word ""range"" is also commonly used to mean the codomain of 
  
    
      
        f
        .
      
    
    {\displaystyle f.}
  


== Examples ==

  
    
      
        f
        :
        {
        1
        ,
        2
        ,
        3
        }
        →
        {
        a
        ,
        b
        ,
        c
        ,
        d
        }
      
    
    {\displaystyle f:\{1,2,3\}\to \{a,b,c,d\}}
   defined by 
  
    
      
        
          {
          
            
              
                
                  1
                  ↦
                  a
                  ,
                
              
              
                
                  2
                  ↦
                  a
                  ,
                
              
              
                
                  3
                  ↦
                  c
                  .
                
              
            
          
          
        
      
    
    {\displaystyle \left\{{\begin{matrix}1\mapsto a,\\2\mapsto a,\\3\mapsto c.\end{matrix}}\right.}
   The image of the set 
  
    
      
        {
        2
        ,
        3
        }
      
    
    {\displaystyle \{2,3\}}
   under 
  
    
      
        f
      
    
    {\displaystyle f}
   is 
  
    
      
        f
        (
        {
        2
        ,
        3
        }
        )
        =
        {
        a
        ,
        c
        }
        .
      
    
    {\displaystyle f(\{2,3\})=\{a,c\}.}
   The image of the function 
  
    
      
        f
      
    
    {\displaystyle f}
   is 
  
    
      
        {
        a
        ,
        c
        }
        .
      
    
    {\displaystyle \{a,c\}.}
   The preimage of 
  
    
      
        a
      
    
    {\displaystyle a}
   is 
  
    
      
        
          f
          
            −
            1
          
        
        (
        {
        a
        }
        )
        =
        {
        1
        ,
        2
        }
        .
      
    
    {\displaystyle f^{-1}(\{a\})=\{1,2\}.}
   The preimage of 
  
    
      
        {
        a
        ,
        b
        }
      
    
    {\displaystyle \{a,b\}}
   is also 
  
    
      
        
          f
          
            −
            1
          
        
        (
        {
        a
        ,
        b
        }
        )
        =
        {
        1
        ,
        2
        }
        .
      
    
    {\displaystyle f^{-1}(\{a,b\})=\{1,2\}.}
   The preimage of 
  
    
      
        {
        b
        ,
        d
        }
      
    
    {\displaystyle \{b,d\}}
   under 
  
    
      
        f
      
    
    {\displaystyle f}
   is the empty set 
  
    
      
        {
         
        }
        =
        ∅
        .
      
    
    {\displaystyle \{\ \}=\emptyset .}
  

  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} }
   defined by 
  
    
      
        f
        (
        x
        )
        =
        
          x
          
            2
          
        
        .
      
    
    {\displaystyle f(x)=x^{2}.}
   The image of 
  
    
      
        {
        −
        2
        ,
        3
        }
      
    
    {\displaystyle \{-2,3\}}
   under 
  
    
      
        f
      
    
    {\displaystyle f}
   is 
  
    
      
        f
        (
        {
        −
        2
        ,
        3
        }
        )
        =
        {
        4
        ,
        9
        }
        ,
      
    
    {\displaystyle f(\{-2,3\})=\{4,9\},}
   and the image of 
  
    
      
        f
      
    
    {\displaystyle f}
   is 
  
    
      
        
          
            R
          
          
            +
          
        
      
    
    {\displaystyle \mathbb {R} ^{+}}
   (the set of all positive real numbers and zero). The preimage of 
  
    
      
        {
        4
        ,
        9
        }
      
    
    {\displaystyle \{4,9\}}
   under 
  
    
      
        f
      
    
    {\displaystyle f}
   is 
  
    
      
        
          f
          
            −
            1
          
        
        (
        {
        4
        ,
        9
        }
        )
        =
        {
        −
        3
        ,
        −
        2
        ,
        2
        ,
        3
        }
        .
      
    
    {\displaystyle f^{-1}(\{4,9\})=\{-3,-2,2,3\}.}
   The preimage of set 
  
    
      
        N
        =
        {
        n
        ∈
        
          R
        
        :
        n
        <
        0
        }
      
    
    {\displaystyle N=\{n\in \mathbb {R} :n<0\}}
   under 
  
    
      
        f
      
    
    {\displaystyle f}
   is the empty set, because the negative numbers do not have square roots in the set of reals.

  
    
      
        f
        :
        
          
            R
          
          
            2
          
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} ^{2}\to \mathbb {R} }
   defined by 
  
    
      
        f
        (
        x
        ,
        y
        )
        =
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        .
      
    
    {\displaystyle f(x,y)=x^{2}+y^{2}.}
   The fibers 
  
    
      
        
          f
          
            −
            1
          
        
        (
        {
        a
        }
        )
      
    
    {\displaystyle f^{-1}(\{a\})}
   are concentric circles about the origin, the origin itself, and the empty set (respectively), depending on whether 
  
    
      
        a
        >
        0
        ,
         
        a
        =
        0
        ,
        
           or 
        
         
        a
        <
        0
      
    
    {\displaystyle a>0,\ a=0,{\text{ or }}\ a<0}
   (respectively). (If 
  
    
      
        a
        ≥
        0
        ,
      
    
    {\displaystyle a\geq 0,}
   then the fiber 
  
    
      
        
          f
          
            −
            1
          
        
        (
        {
        a
        }
        )
      
    
    {\displaystyle f^{-1}(\{a\})}
   is the set of all 
  
    
      
        (
        x
        ,
        y
        )
        ∈
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle (x,y)\in \mathbb {R} ^{2}}
   satisfying the equation 
  
    
      
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        =
        a
        ,
      
    
    {\displaystyle x^{2}+y^{2}=a,}
   that is, the origin-centered circle with radius 
  
    
      
        
          
            a
          
        
        .
      
    
    {\displaystyle {\sqrt {a}}.}
  )
If 
  
    
      
        M
      
    
    {\displaystyle M}
   is a manifold and 
  
    
      
        π
        :
        T
        M
        →
        M
      
    
    {\displaystyle \pi :TM\to M}
   is the canonical projection from the tangent bundle 
  
    
      
        T
        M
      
    
    {\displaystyle TM}
   to 
  
    
      
        M
        ,
      
    
    {\displaystyle M,}
   then the fibers of 
  
    
      
        π
      
    
    {\displaystyle \pi }
   are the tangent spaces 
  
    
      
        
          T
          
            x
          
        
        (
        M
        )
        
           for 
        
        x
        ∈
        M
        .
      
    
    {\displaystyle T_{x}(M){\text{ for }}x\in M.}
   This is also an example of a fiber bundle.
A quotient group is a homomorphic image.


== Properties ==


=== General ===
For every function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   and all subsets 
  
    
      
        A
        ⊆
        X
      
    
    {\displaystyle A\subseteq X}
   and 
  
    
      
        B
        ⊆
        Y
        ,
      
    
    {\displaystyle B\subseteq Y,}
   the following properties hold:

Also:

  
    
      
        f
        (
        A
        )
        ∩
        B
        =
        ∅
        
        
           if and only if 
        
        
        A
        ∩
        
          f
          
            −
            1
          
        
        (
        B
        )
        =
        ∅
      
    
    {\displaystyle f(A)\cap B=\varnothing \,{\text{ if and only if }}\,A\cap f^{-1}(B)=\varnothing }
  


=== Multiple functions ===
For functions 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   and 
  
    
      
        g
        :
        Y
        →
        Z
      
    
    {\displaystyle g:Y\to Z}
   with subsets 
  
    
      
        A
        ⊆
        X
      
    
    {\displaystyle A\subseteq X}
   and 
  
    
      
        C
        ⊆
        Z
        ,
      
    
    {\displaystyle C\subseteq Z,}
   the following properties hold:

  
    
      
        (
        g
        ∘
        f
        )
        (
        A
        )
        =
        g
        (
        f
        (
        A
        )
        )
      
    
    {\displaystyle (g\circ f)(A)=g(f(A))}
  

  
    
      
        (
        g
        ∘
        f
        
          )
          
            −
            1
          
        
        (
        C
        )
        =
        
          f
          
            −
            1
          
        
        (
        
          g
          
            −
            1
          
        
        (
        C
        )
        )
      
    
    {\displaystyle (g\circ f)^{-1}(C)=f^{-1}(g^{-1}(C))}
  


=== Multiple subsets of domain or codomain ===
For function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   and subsets 
  
    
      
        A
        ,
        B
        ⊆
        X
      
    
    {\displaystyle A,B\subseteq X}
   and 
  
    
      
        S
        ,
        T
        ⊆
        Y
        ,
      
    
    {\displaystyle S,T\subseteq Y,}
   the following properties hold:

The results relating images and preimages to the (Boolean) algebra of intersection and union work for any collection of subsets, not just for pairs of subsets:

  
    
      
        f
        
          (
          
            
              ⋃
              
                s
                ∈
                S
              
            
            
              A
              
                s
              
            
          
          )
        
        =
        
          ⋃
          
            s
            ∈
            S
          
        
        f
        
          (
          
            A
            
              s
            
          
          )
        
      
    
    {\displaystyle f\left(\bigcup _{s\in S}A_{s}\right)=\bigcup _{s\in S}f\left(A_{s}\right)}
  

  
    
      
        f
        
          (
          
            
              ⋂
              
                s
                ∈
                S
              
            
            
              A
              
                s
              
            
          
          )
        
        ⊆
        
          ⋂
          
            s
            ∈
            S
          
        
        f
        
          (
          
            A
            
              s
            
          
          )
        
      
    
    {\displaystyle f\left(\bigcap _{s\in S}A_{s}\right)\subseteq \bigcap _{s\in S}f\left(A_{s}\right)}
  

  
    
      
        
          f
          
            −
            1
          
        
        
          (
          
            
              ⋃
              
                s
                ∈
                S
              
            
            
              B
              
                s
              
            
          
          )
        
        =
        
          ⋃
          
            s
            ∈
            S
          
        
        
          f
          
            −
            1
          
        
        
          (
          
            B
            
              s
            
          
          )
        
      
    
    {\displaystyle f^{-1}\left(\bigcup _{s\in S}B_{s}\right)=\bigcup _{s\in S}f^{-1}\left(B_{s}\right)}
  

  
    
      
        
          f
          
            −
            1
          
        
        
          (
          
            
              ⋂
              
                s
                ∈
                S
              
            
            
              B
              
                s
              
            
          
          )
        
        =
        
          ⋂
          
            s
            ∈
            S
          
        
        
          f
          
            −
            1
          
        
        
          (
          
            B
            
              s
            
          
          )
        
      
    
    {\displaystyle f^{-1}\left(\bigcap _{s\in S}B_{s}\right)=\bigcap _{s\in S}f^{-1}\left(B_{s}\right)}
  (Here, 
  
    
      
        S
      
    
    {\displaystyle S}
   can be infinite, even uncountably infinite.)
With respect to the algebra of subsets described above, the inverse image function is a lattice homomorphism, while the image function is only a semilattice homomorphism (that is, it does not always preserve intersections).


== See also ==
Bijection, injection and surjection – Properties of mathematical functions
Fiber (mathematics) – Set of all points in a function's domain that all map to some single given point
Image (category theory) – term in category theoryPages displaying wikidata descriptions as a fallback
Kernel of a function – Equivalence relation expressing that two elements have the same image under a functionPages displaying short descriptions of redirect targets
Set inversion – Mathematical problem of finding the set mapped by a specified function to a certain range


== Notes ==


== References ==
Artin, Michael (1991). Algebra. Prentice Hall. ISBN 81-203-0871-9.
Blyth, T.S. (2005). Lattices and Ordered Algebraic Structures. Springer. ISBN 1-85233-905-5..
Dolecki, Szymon; Mynard, Frederic (2016). Convergence Foundations Of Topology. New Jersey: World Scientific Publishing Company. ISBN 978-981-4571-52-4. OCLC 945169917.
Halmos, Paul R. (1960). Naive set theory. The University Series in Undergraduate Mathematics. van Nostrand Company. ISBN 9780442030643. Zbl 0087.04403.
Kelley, John L. (1985). General Topology. Graduate Texts in Mathematics. Vol. 27 (2 ed.). Birkhäuser. ISBN 978-0-387-90125-1.
Munkres, James R. (2000). Topology (Second ed.). Upper Saddle River, NJ: Prentice Hall, Inc. ISBN 978-0-13-181629-9. OCLC 42683260.This article incorporates material from Fibre on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License."
61b3dcb6ba,History of mathematics,"The history of mathematics deals with the origin of discoveries in mathematics and the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, followed closely by Ancient Egypt and the Levantine state of Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the patterns in nature, the field of astronomy and to record time and formulate calendars.
The earliest mathematical texts available are from Mesopotamia and Egypt – Plimpton 322 (Babylonian c. 2000 – 1900 BC), the Rhind Mathematical Papyrus (Egyptian c. 1800 BC) and the Moscow Mathematical Papyrus (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples, so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.
The study of mathematics as a ""demonstrative discipline"" began in the 6th century BC with the Pythagoreans, who coined the term ""mathematics"" from the ancient Greek μάθημα (mathema), meaning ""subject of instruction"". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.
Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century.


== Prehistoric ==
The origins of mathematical thought lie in the concepts of number, patterns in nature, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the ""number"" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between ""one"", ""two"", and ""many"", but not of numbers larger than two.The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a tally of the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that ""no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10."" The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.


== Babylonian ==

Babylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.

In contrast to the sparsity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onward, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.

Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 × 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different from multiplying integers, similar to modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computational accuracy; for example, the Babylonian tablet YBC 7289 gives an approximation of √2 accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.Other topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular numbers, and their reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.


== Egyptian ==

Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars. Archaeological evidence has suggested that the Ancient Egyptian counting system had origins in Sub-Saharan Africa. Also, fractal geometry designs which are widespread among Sub-Saharan African cultures are also found in Egyptian architecture and cosmological signs.The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called word problems or story problems, which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).
Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.


== Greek ==

Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.Greek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.
Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was ""All is number"". It was the Pythagoreans who coined the term ""mathematics"", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians, Indians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the mensa Pythagorica.Plato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics, clarified some of the definitions (e.g. that of a line as ""breadthless length""), and reorganized the assumptions. The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.Eudoxus (408–c. 355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.

In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the Elements, widely considered the most successful and influential textbook of all time. The Elements introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework. The Elements was known to all educated people in the West up through the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the Elements was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.

Archimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 310/71 < π < 310/70. He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.

Apollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola (""place beside"" or ""comparison""), ""ellipse"" (""deficiency""), and ""hyperbola"" (""a throw beyond""). His work Conics is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.Around the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the ""Golden Age"" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the Almagest of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.

Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the ""Silver Age"" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as ""Diophantine analysis"". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the Arithmetica, a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The Arithmetica had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the Arithmetica (that of dividing a square into two squares). Diophantus also made significant advances in notation, the Arithmetica being the first instance of algebraic symbolism and syncopation.

Among the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His Collection is a major source of knowledge on Greek mathematics as most of it has survived. Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed. Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius. Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Hagia Sophia. Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.


== Roman ==

Although ethnic Greek mathematicians continued under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison. Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks. It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.Using calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury. Siculus Flaccus, one of the Roman gromatici (i.e. land surveyor), wrote the Categories of Fields, which aided Roman surveyors in measuring the surface areas of allotted lands and territories. Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns. Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.The creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year. In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February. This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle. This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (r. 1572–1585), virtually the same solar calendar used in modern times as the international standard calendar.At roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC – c. 15 BC). The device was used at least until the reign of emperor Commodus (r. 177 – 192 AD), but its design seems to have been lost until experiments were made during the 15th century in Western Europe. Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.


== Chinese ==

An analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the Zhoubi Suanjing (周髀算經), variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.

Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called ""rod numerals"" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for ""1"", followed by the symbol for ""100"", then the symbol for ""2"" followed by the symbol for ""10"", followed by the symbol for ""3"". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the suan pan, or Chinese abacus. The date of the invention of the suan pan is not certain, but the earliest written mention dates from AD 190, in Xu Yue's Supplementary Notes on the Art of Figures.
The oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The Mo Jing described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well. It also defined the concepts of circumference, diameter, radius, and volume.

In 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is The Nine Chapters on the Mathematical Art, the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. The treatise also provides values of π, which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724, as well as 3.162 by taking the square root of 10. Liu Hui commented on the Nine Chapters in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159). Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (between 3.1415926 and 3.1415927), which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.The high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the Precious Mirror of the Four Elements by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The Precious Mirror also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.Japanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere. Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644). For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers. Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.


== Indian ==

The earliest civilization on the Indian subcontinent is the Indus Valley civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.Pāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called mātrāmeru).The next significant mathematical documents from India after the Sulba Sutras are the Siddhantas, astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words ""sine"" and ""cosine"" derive from the Sanskrit ""jiya"" and ""kojiya"".

Around 500 AD, Aryabhata wrote the Aryabhatiya, a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the Aryabhatiya that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the Aryabhatiya as a ""mix of common pebbles and costly crystals"".In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in Brahma-sphuta-siddhanta, he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.In the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.In the 14th century, Madhava of Sangamagrama, the founder of the Kerala School of Mathematics, found the Madhava–Leibniz series and obtained from it a transformed series, whose first 21 terms he used to compute the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the Yukti-bhāṣā. It has been argued that the advances of the Kerala school, which laid the foundations of the calculus, were transmitted to Europe in the 16th century via Jesuit missionaries and traders who were active around the ancient port of Muziris at the time and, as a result, directly influenced later European developments in analysis and calculus. However, other scholars argue that the Kerala School did not formulate a systematic theory of differentiation and integration, and that there is not any direct evidence of their results being transmitted outside Kerala.


== Islamic empires ==

The Islamic Empire established across the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time.In the 9th century, the mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote an important book on the Hindu–Arabic numerals and one on methods for solving equations. His book On the Calculation with Hindu Numerals, written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word algorithm is derived from the Latinization of his name, Algoritmi, and the word algebra from the title of one of his works, Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala (The Compendious Book on Calculation by Completion and Balancing). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as al-jabr. His algebra was also no longer concerned ""with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study."" He also studied an equation for its own sake and ""in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.""In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.
Further developments in algebra were made by Al-Karaji in his treatise al-Fakhri, where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being ""the first who introduced the theory of algebraic calculus."" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.In the late 11th century, Omar Khayyam wrote Discussions of the Difficulties in Euclid, a book about what he perceived as flaws in Euclid's Elements, especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating nth roots, which was a special case of the methods given many centuries later by Ruffini and Horner.
Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.


== Maya ==

In the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics. Maya numerals used a base of twenty, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures. The Maya used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy. While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Maya developed a standard symbol for it.


== Medieval European ==

Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's Timaeus and the biblical passage (in the Book of Wisdom) that God had ordered all things in measure, and number, and weight.Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term quadrivium to describe the study of arithmetic, geometry, astronomy, and music. He wrote De institutione arithmetica, a free translation from the Greek of Nicomachus's Introduction to Arithmetic; De institutione musica, also derived from Greek sources; and a series of excerpts from Euclid's Elements. His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's The Compendious Book on Calculation by Completion and Balancing, translated into Latin by Robert of Chester, and the complete text of Euclid's Elements, translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.
Leonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote Liber Abaci in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which Fibonacci used as an unremarkable example.
The 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.
Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.

One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed ""by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant"".Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that ""a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]"".Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's Elements, Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.


== Renaissance ==

During the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as abbaco in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.
Piero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including De Prospectiva Pingendi (On Perspective for Painting), Trattato d’Abaco (Abacus Treatise), and De quinque corporibus regularibus (On the Five Regular Solids).

Luca Pacioli's Summa de Arithmetica, Geometria, Proportioni et Proportionalità (Italian: ""Review of Arithmetic, Geometry, Ratio and Proportion"") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, ""Particularis de Computis et Scripturis"" (Italian: ""Details of Calculation and Recording""). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In Summa Arithmetica, Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. Summa Arithmetica was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.
In Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book Ars Magna, together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his L'Algebra in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.
Simon Stevin's book De Thiende ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.
Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Regiomontanus's table of sines and cosines was published in 1533.During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.


== Mathematics during the Scientific Revolution ==


=== 17th century ===

The 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.
The analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.
Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, developed calculus and much of the calculus notation still in use today. He also refined the binary number system, which is the foundation of nearly all digital (electronic, solid-state, discrete logic) computers, including the Von Neumann architecture, which is the standard design paradigm, or ""computer architecture"", followed from the second half of the 20th century, and into the 21st. Leibniz has been called the ""founder of computer science"".Science and mathematics had become an international endeavor, which would soon spread over the entire world.In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.


=== 18th century ===

The most influential mathematician of the 18th century was arguably Leonhard Euler (1707–1783). His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol i, and he popularized the use of the Greek letter 
  
    
      
        π
      
    
    {\displaystyle \pi }
   to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.
Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Pierre-Simon Laplace, who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.


== Modern ==


=== 19th century ===

Throughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.

This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.
The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in electrical engineering and computer science.
Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.
Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians used this in their proofs that straight edge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.
Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.
In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.
The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.
In 1897, Kurt Hensel introduced p-adic numbers.


=== 20th century ===
The 20th century saw mathematics become a major profession. By the end of the century, thousands of new Ph.D.s in mathematics were being awarded every year, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.
In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.

Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.
Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the ""enormous theorem""), whose proof between 1955 and 2004 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym ""Nicolas Bourbaki"", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.

Differential geometry came into its own when Albert Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.
Non-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.
The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the fast Fourier transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.
At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus either addition or multiplication (but not both), was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.

One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.
Paul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the ""collaborative distance"" between a person and Erdős, as measured by joint authorship of mathematical papers.
Emmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.
As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.


=== 21st century ===

In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).
Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive toward open access publishing, first popularized by arXiv.


== Future ==

There are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is expanding exponentially.


== See also ==


== Notes ==


== References ==
Berggren, Lennart; Borwein, Jonathan M.; Borwein, Peter B. (2004), Pi: A Source Book, New York: Springer, ISBN 978-0-387-20571-7
Boyer, C.B. (1991) [1989], A History of Mathematics (2nd ed.), New York: Wiley, ISBN 978-0-471-54397-8
Cuomo, Serafina (2001), Ancient Mathematics, London: Routledge, ISBN 978-0-415-16495-5
Goodman, Michael, K.J. (2016), An introduction of the Early Development of Mathematics, Hoboken: Wiley, ISBN 978-1-119-10497-1
Gullberg, Jan (1997), Mathematics: From the Birth of Numbers, New York: W.W. Norton and Company, ISBN 978-0-393-04002-9
Joyce, Hetty (July 1979), ""Form, Function and Technique in the Pavements of Delos and Pompeii"", American Journal of Archaeology, 83 (3): 253–63, doi:10.2307/505056, JSTOR 505056, S2CID 191394716.
Katz, Victor J. (1998), A History of Mathematics: An Introduction (2nd ed.), Addison-Wesley, ISBN 978-0-321-01618-8
Katz, Victor J. (2007), The Mathematics of Egypt, Mesopotamia, China, India, and Islam: A Sourcebook, Princeton, NJ: Princeton University Press, ISBN 978-0-691-11485-9
Needham, Joseph; Wang, Ling (1995) [1959], Science and Civilization in China: Mathematics and the Sciences of the Heavens and the Earth, vol. 3, Cambridge: Cambridge University Press, ISBN 978-0-521-05801-8
Needham, Joseph; Wang, Ling (2000) [1965], Science and Civilization in China: Physics and Physical Technology: Mechanical Engineering, vol. 4 (reprint ed.), Cambridge: Cambridge University Press, ISBN 978-0-521-05803-2
Sleeswyk, Andre (October 1981), ""Vitruvius' odometer"", Scientific American, 252 (4): 188–200, Bibcode:1981SciAm.245d.188S, doi:10.1038/scientificamerican1081-188.
Straffin, Philip D. (1998), ""Liu Hui and the First Golden Age of Chinese Mathematics"", Mathematics Magazine, 71 (3): 163–81, doi:10.1080/0025570X.1998.11996627
Tang, Birgit (2005), Delos, Carthage, Ampurias: the Housing of Three Mediterranean Trading Centres, Rome: L'Erma di Bretschneider (Accademia di Danimarca), ISBN 978-88-8265-305-7.
Volkov, Alexei (2009), ""Mathematics and Mathematics Education in Traditional Vietnam"",  in Robson, Eleanor; Stedall, Jacqueline (eds.), The Oxford Handbook of the History of Mathematics, Oxford: Oxford University Press, pp. 153–76, ISBN 978-0-19-921312-2


== Further reading ==


=== General ===
Aaboe, Asger (1964). Episodes from the Early History of Mathematics. New York: Random House.
Bell, E.T. (1937). Men of Mathematics. Simon and Schuster.
Burton, David M. The History of Mathematics: An Introduction. McGraw Hill: 1997.
Grattan-Guinness, Ivor (2003). Companion Encyclopedia of the History and Philosophy of the Mathematical Sciences. The Johns Hopkins University Press. ISBN 978-0-8018-7397-3.
Kline, Morris. Mathematical Thought from Ancient to Modern Times.
Struik, D.J. (1987). A Concise History of Mathematics, fourth revised edition. Dover Publications, New York.


=== Books on a specific period ===
Gillings, Richard J. (1972). Mathematics in the Time of the Pharaohs. Cambridge, MA: MIT Press.
Heath, Sir Thomas (1981). A History of Greek Mathematics. Dover. ISBN 978-0-486-24073-2.
van der Waerden, B.L., Geometry and Algebra in Ancient Civilizations, Springer, 1983, ISBN 0-387-12159-5.


=== Books on a specific topic ===
Corry, Leo (2015), A Brief History of Numbers, Oxford University Press, ISBN 978-0198702597
Hoffman, Paul (1998). The Man Who Loved Only Numbers: The Story of Paul Erdős and the Search for Mathematical Truth. Hyperion. ISBN 0-7868-6362-5.
Menninger, Karl W. (1969). Number Words and Number Symbols: A Cultural History of Numbers. MIT Press. ISBN 978-0-262-13040-0.
Stigler, Stephen M. (1990). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press. ISBN 978-0-674-40341-3.


== External links ==


=== Documentaries ===
BBC (2008). The Story of Maths.
Renaissance Mathematics, BBC Radio 4 discussion with Robert Kaplan, Jim Bennett & Jackie Stedall (In Our Time, Jun 2, 2005)


=== Educational material ===
MacTutor History of Mathematics archive (John J. O'Connor and Edmund F. Robertson; University of St Andrews, Scotland). An award-winning website containing detailed biographies on many historical and contemporary mathematicians, as well as information on notable curves and various topics in the history of mathematics.
History of Mathematics Home Page (David E. Joyce; Clark University). Articles on various topics in the history of mathematics with an extensive bibliography.
The History of Mathematics (David R. Wilkins; Trinity College, Dublin). Collections of material on the mathematics between the 17th and 19th century.
Earliest Known Uses of Some of the Words of Mathematics (Jeff Miller). Contains information on the earliest known uses of terms used in mathematics.
Earliest Uses of Various Mathematical Symbols (Jeff Miller). Contains information on the history of mathematical notations.
Mathematical Words: Origins and Sources (John Aldrich, University of Southampton) Discusses the origins of the modern mathematical word stock.
Biographies of Women Mathematicians (Larry Riddle; Agnes Scott College).
Mathematicians of the African Diaspora (Scott W. Williams; University at Buffalo).
Notes for MAA minicourse: teaching a course in the history of mathematics. (2009) (V. Frederick Rickey & Victor J. Katz).
Ancient Rome: The Odometer Of Vitruv. Pictorial (moving) re-construction of Vitusius' Roman ododmeter.


=== Bibliographies ===
A Bibliography of Collected Works and Correspondence of Mathematicians archive dated 2007/3/17 (Steven W. Rockey; Cornell University Library).


=== Organizations ===
International Commission for the History of Mathematics


=== Journals ===
Historia Mathematica
Convergence, the Mathematical Association of America's online Math History Magazine
History of Mathematics Archived 2006-10-04 at the Wayback Machine Math Archives (University of Tennessee, Knoxville)
History/Biography The Math Forum (Drexel University)
History of Mathematics (Courtright Memorial Library).
History of Mathematics Web Sites (David Calvis; Baldwin-Wallace College)
History of mathematics at Curlie
Historia de las Matemáticas (Universidad de La La guna)
História da Matemática (Universidade de Coimbra)
Using History in Math Class
Mathematical Resources: History of Mathematics (Bruno Kevius)
History of Mathematics (Roberta Tucci)"
cc0821668b,Reflection (mathematics),"In mathematics, a reflection (also spelled reflexion) is a mapping from a Euclidean space to itself that is an isometry with a hyperplane as a set of fixed points; this set is called the axis (in dimension 2) or plane (in dimension 3) of reflection. The image of a figure by a reflection is its mirror image in the axis or plane of reflection. For example the mirror image of the small Latin letter p for a reflection with respect to a vertical axis would look like q. Its image by reflection in a horizontal axis would look like b. A reflection is an involution: when applied twice in succession, every point returns to its original location, and every geometrical object is restored to its original state.
The term reflection is sometimes used for a larger class of mappings from a Euclidean space to itself, namely the non-identity isometries that are involutions. Such isometries have a set of fixed points (the ""mirror"") that is an affine subspace, but is possibly smaller than a hyperplane. For instance a reflection through a point is an involutive isometry with just one fixed point; the image of the letter p under it
would look like a d. This operation is also known as a central inversion (Coxeter 1969, §7.2), and exhibits Euclidean space as a symmetric space. In a Euclidean vector space, the reflection in the point situated at the origin is the same as vector negation. Other examples include reflections in a line in three-dimensional space. Typically, however, unqualified use of the term ""reflection"" means reflection in a hyperplane.
Some mathematicians use ""flip"" as a synonym for ""reflection"".


== Construction ==

In a plane (or, respectively, 3-dimensional) geometry, to find the reflection of a point drop a perpendicular from the point to the line (plane) used for reflection, and extend it the same distance on the other side. To find the reflection of a figure, reflect each point in the figure.
To reflect point P through the line AB using compass and straightedge, proceed as follows (see figure):

Step 1 (red): construct a circle with center at P and some fixed radius r to create points A′ and B′ on the line AB, which will be equidistant from P.
Step 2 (green): construct circles centered at A′ and B′ having radius r. P and Q will be the points of intersection of these two circles.Point Q is then the reflection of point P through line AB.


== Properties ==

The matrix for a reflection is orthogonal with determinant −1 and eigenvalues −1, 1, 1, ..., 1. The product of two such matrices is a special orthogonal matrix that represents a rotation. Every rotation is the result of reflecting in an even number of reflections in hyperplanes through the origin, and every improper rotation is the result of reflecting in an odd number. Thus reflections generate the orthogonal group, and this result is known as the Cartan–Dieudonné theorem.
Similarly the Euclidean group, which consists of all isometries of Euclidean space, is generated by reflections in affine hyperplanes.  In general, a group generated by reflections in affine hyperplanes is known as a reflection group. The finite groups generated in this way are examples of Coxeter groups.


== Reflection across a line in the plane ==

Reflection across a line through the origin in two dimensions can be described by the following formula

  
    
      
        
          Ref
          
            l
          
        
        ⁡
        (
        v
        )
        =
        2
        
          
            
              v
              ⋅
              l
            
            
              l
              ⋅
              l
            
          
        
        l
        −
        v
        ,
      
    
    {\displaystyle \operatorname {Ref} _{l}(v)=2{\frac {v\cdot l}{l\cdot l}}l-v,}
  where 
  
    
      
        v
      
    
    {\displaystyle v}
   denotes the vector being reflected, 
  
    
      
        l
      
    
    {\displaystyle l}
   denotes any vector in the line across which the reflection is performed, and 
  
    
      
        v
        ⋅
        l
      
    
    {\displaystyle v\cdot l}
   denotes the dot product of 
  
    
      
        v
      
    
    {\displaystyle v}
   with 
  
    
      
        l
      
    
    {\displaystyle l}
  . Note the formula above can also be written as

  
    
      
        
          Ref
          
            l
          
        
        ⁡
        (
        v
        )
        =
        2
        
          Proj
          
            l
          
        
        ⁡
        (
        v
        )
        −
        v
        ,
      
    
    {\displaystyle \operatorname {Ref} _{l}(v)=2\operatorname {Proj} _{l}(v)-v,}
  saying that a reflection of 
  
    
      
        v
      
    
    {\displaystyle v}
   across 
  
    
      
        l
      
    
    {\displaystyle l}
   is equal to 2 times the projection of 
  
    
      
        v
      
    
    {\displaystyle v}
   on 
  
    
      
        l
      
    
    {\displaystyle l}
  , minus the vector 
  
    
      
        v
      
    
    {\displaystyle v}
  .  Reflections in a line have the eigenvalues of 1, and −1.


== Reflection through a hyperplane in n dimensions ==
Given a vector 
  
    
      
        v
      
    
    {\displaystyle v}
   in Euclidean space 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  , the formula for the reflection in the hyperplane through the origin, orthogonal to 
  
    
      
        a
      
    
    {\displaystyle a}
  , is given by

  
    
      
        
          Ref
          
            a
          
        
        ⁡
        (
        v
        )
        =
        v
        −
        2
        
          
            
              v
              ⋅
              a
            
            
              a
              ⋅
              a
            
          
        
        a
        ,
      
    
    {\displaystyle \operatorname {Ref} _{a}(v)=v-2{\frac {v\cdot a}{a\cdot a}}a,}
  where 
  
    
      
        v
        ⋅
        a
      
    
    {\displaystyle v\cdot a}
   denotes the dot product of 
  
    
      
        v
      
    
    {\displaystyle v}
   with 
  
    
      
        a
      
    
    {\displaystyle a}
  . Note that the second term in the above equation is just twice the vector projection of 
  
    
      
        v
      
    
    {\displaystyle v}
   onto 
  
    
      
        a
      
    
    {\displaystyle a}
  . One can easily check that

Refa(v) = −v, if 
  
    
      
        v
      
    
    {\displaystyle v}
   is parallel to 
  
    
      
        a
      
    
    {\displaystyle a}
  , and
Refa(v) =  v, if 
  
    
      
        v
      
    
    {\displaystyle v}
   is perpendicular to a.Using the geometric product, the formula is

  
    
      
        
          Ref
          
            a
          
        
        ⁡
        (
        v
        )
        =
        −
        
          
            
              a
              v
              a
            
            
              a
              
                2
              
            
          
        
        .
      
    
    {\displaystyle \operatorname {Ref} _{a}(v)=-{\frac {ava}{a^{2}}}.}
  Since these reflections are isometries of Euclidean space fixing the origin they may be represented by orthogonal matrices. The orthogonal matrix corresponding to the above reflection is the matrix

  
    
      
        R
        =
        I
        −
        2
        
          
            
              a
              
                a
                
                  T
                
              
            
            
              
                a
                
                  T
                
              
              a
            
          
        
        ,
      
    
    {\displaystyle R=I-2{\frac {aa^{T}}{a^{T}a}},}
  where 
  
    
      
        I
      
    
    {\displaystyle I}
   denotes the 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
   identity matrix and 
  
    
      
        
          a
          
            T
          
        
      
    
    {\displaystyle a^{T}}
   is the transpose of a. Its entries are

  
    
      
        
          R
          
            i
            j
          
        
        =
        
          δ
          
            i
            j
          
        
        −
        2
        
          
            
              
                a
                
                  i
                
              
              
                a
                
                  j
                
              
            
            
              
                ‖
                a
                ‖
              
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle R_{ij}=\delta _{ij}-2{\frac {a_{i}a_{j}}{\left\|a\right\|^{2}}},}
  where δij is the Kronecker delta.
The formula for the reflection in the affine hyperplane 
  
    
      
        v
        ⋅
        a
        =
        c
      
    
    {\displaystyle v\cdot a=c}
   not through the origin is

  
    
      
        
          Ref
          
            a
            ,
            c
          
        
        ⁡
        (
        v
        )
        =
        v
        −
        2
        
          
            
              v
              ⋅
              a
              −
              c
            
            
              a
              ⋅
              a
            
          
        
        a
        .
      
    
    {\displaystyle \operatorname {Ref} _{a,c}(v)=v-2{\frac {v\cdot a-c}{a\cdot a}}a.}
  


== See also ==
Coordinate rotations and reflections
Householder transformation
Inversive geometry
Plane of rotation
Reflection mapping
Reflection group


== Notes ==


== References ==
Coxeter, Harold Scott MacDonald (1969), Introduction to Geometry (2nd ed.), New York: John Wiley & Sons, ISBN 978-0-471-50458-0, MR 0123930
Popov, V.L. (2001) [1994], ""Reflection"", Encyclopedia of Mathematics, EMS Press
Weisstein, Eric W. ""Reflection"". MathWorld.


== External links ==
Reflection in Line at cut-the-knot
Understanding 2D Reflection and Understanding 3D Reflection by Roger Germundsson, The Wolfram Demonstrations Project."
1903ff87b1,Mathematics (producer),"Ronald Maurice Bean, better known professionally as Mathematics (also known as Allah Mathematics) (born October 21, 1971), is a hip hop producer and DJ for the Wu-Tang Clan and its solo and affiliate projects. He designed the Wu-Tang Clan logo.


== Biography ==
Born and raised in Jamaica, Queens, New York, Mathematics was introduced to hip hop by his brother who used to bring home recordings of the genre's pioneers like Grandmaster Flash & The Furious Five, Treacherous Three and Cold Crush Brothers. He began his career in 1987 DJing block parties and park jams in Baisley Projects, going by the name Supreme Cut Master. In 1988, he became the full-time DJ for experienced rapper Victor C, doing countless shows in clubs and colleges in New York City.
In 1990, Mathematics linked up with GZA/Genius, who would soon become one of the Wu-Tang Clan's founding members, but at the time was struggling to build a career on the Cold Chillin' label. This partnership earned Mathematics a spot on his first official tour, The Cold Chillin Blizzard Tour (with popular acts such as Biz Markie, Big Daddy Kane, Kool G. Rap & DJ Polo and Marley Marl).
GZA left Cold Chillin after his first album, Words from the Genius, did not achieve the sales target that was anticipated. He and Mathematics took to the road again, but this time with the help of GZA's cousins, RZA and Ol' Dirty Bastard. These three soon became the founding members of Wu-Tang Clan, then known as All In Together Now. The group soon dissolved, however, and the trio set their minds on creating the Wu group. During the group's inception, Mathematics used his experience as a graffiti artist to design a logo for the up-and-coming crew, as well as various other logos and designs the Wu-Tang's artists would use. In the years to come, he became a Wu-Element under the guidance of RZA.
Mathematics' first real exposure to production came late one night when he attended a session where he assisted RZA, his mentor, in constructing a beat from nothing. The track eventually developed into ""Ice Cream"" on Raekwon's Only Built 4 Cuban Linx album. RZA inspired Mathematics to follow the Wu-Tang, giving him advice on the nuances of hip hop production over the coming years. In 1996, Mathematics began record producing, in Staten Island, New York (Shaolin) and at home, in the P-Funk City, Plainfield, New Jersey, between heavily scheduled tour dates and rigorous road travelling with his father's gospel group, The Soul Seekers.
His first track, ""Fast Life"" featuring Ghostface Killah and the American football star Andre Rison, was included in the NFL Jams compilation album. Though this track faded into obscurity somewhat, it led to several more collaborations between Mathematics and Ghostface; Mathematics also began to produce for many other Wu-Tang members and affiliates, including several tracks on GZA's second album Beneath The Surface as well as Method Man's Tical 2000: Judgement Day, Inspectah Deck's Uncontrolled Substance and Method Man & Redman's Blackout!. Eventually, he produced for the Clan as a group, with ""Do You Really (Thang, Thang)"", ""Careful (Click, Click)"".
In 2003, Mathematics moved into TV work, as he produced the main theme and all original music for the short lived show Wanda At Large, which starred Wanda Sykes and was broadcast by the Fox Network. During this time and between continuous touring, Mathematics started work on his first solo full length project, Love, Hell Or Right. Completely mixed, arranged and produced by himself, Love Hell or Right was released fall 2003 on his own Quewisha Records label in conjunction with High Times Records, and it went on to sell 30,000 units.
As well as using rappers from Mathematics' home borough of Queens, Eyes-Low and Buddah Bless, Love Hell Or Right had appearances from all the Wu-Tang Clan members except GZA and the then-imprisoned Ol' Dirty Bastard. Mathematics soon signed to the popular independent hip hop label Nature Sounds (home to Wu-Tang colleague Masta Killa as well as MF DOOM) and released his second album The Problem in 2005. On this album the entire Wu-Tang Clan appeared, including a posthumous appearance from Ol' Dirty Bastard. As well as working on his solo albums, Mathematics has continued to contribute beats to many Wu-Tang releases, including the first albums by Masta Killa and Streetlife. In January 2012 he was to release a sequel to The Problem entitled The Answer, entirely produced by him with Wu-Tang members such as Raekwon, GZA, Method Man, Cappadonna, Masta Killa, Ol' Dirty Bastard, Ghostface Killah. Other artists include Redman and artists he is developing such as Ali Vegas, Eyeslow and Bad Luck.In August 2017, it was confirmed that Wu-Tang Clan will release a new album, Wu-Tang: The Saga Continues, which was entirely produced by Mathematics and was released on October 13, 2017. The first track off the album is titled ""People Say"" and features Redman.


== Discography ==


=== Albums ===


=== Production credits ===
America Is Dying Slowly - Red Hot AIDS Benefit Series, Red Hot Organization, Smoke One Productions
NFL Jams - ""Fast Life""
Wu-Syndicate - ""Pointin' Fingers"", ""Muzzle Toe""
Dirty Weaponry - ""Galactics"", ""Bastard Swordsman""
Wu-Tang Killa Bees: The Swarm - ""Cobra Clutch"", ""Punishment"", ""Fatal Sting""
The Pillage - ""Oh Donna""
Tical 2000 Judgement Day - ""Snuffed Out""
Beneath the Surface - ""Amplified Sample"", ""High Price Small Reward"", ""Publicity"", ""Feel Like An Enemy"", ""Mic Trippin’""
Uncontrolled Substance - ""Uncontrolled Substance""
Blackout! - ""Dat's Dat Shit"", ""Fire Ina Hole""
The W - ""Do You Really (Thang Thang)""
Supreme Clientele - ""Mighty Healthy"", ""Wu-Banga 101""
Iron Flag - ""Rules""
Digital Bullet - ""Must Be Bobby"", ""Cousins""
Next Friday - ""Shaolin Worldwide""
Bulletproof Wallets - ""Theodore"", ""Strawberry""
 Legend of the Liquid Sword - ""Fam (Members Only)""
Tera Iz Him - ""Roll With The Rush""
Northstar - ""Duckie"", ""We Got It""
No Said Date - ""Last Drink"", ""Do That Dance"", ""Whatever""
Street Education - ""FANZ"", ""Who Want To Rap?"", ""Sweetest Pain""
Wu-Tang Meets the Indie Culture - ""Cars On the Interstate""
4:21...The Day After - ""Dirty Mef"", ""Everything""
I - ""Two Missed Calls""
8 Diagrams - ""Stick Me for My Riches""
Pro Tools - ""Pencil""
Blackout! 2 - ""BO2 (Intro)""
Only Built 4 Cuban Linx... Pt. II - ""Mean Streets""
Wu-Massacre - ""Meth vs. Chef Part II"", ""Miranda"", ""Dangerous""
Heaven Razah - ""Raised In Hell""
Shaolin vs. Wu-Tang - ""Dart School""
Gold Cobra - ""Middle Finger"", ""Combat Jazz""
Selling My Soul - ""Intro"", ""Part 2"", ""All Natural"" & ""Wisdom""
The Saga Continues - entire album


== References ==


== External links ==
Mathematics on Myspace
Mathematics discography"
27faa6efe1,Computer mathematics,
e0ca39439a,Mathematics education,"In contemporary education, mathematics education—known in Europe as the didactics or pedagogy of mathematics—is the practice of teaching, learning, and carrying out scholarly research into the transfer of mathematical knowledge.
Although research into mathematics education is primarily concerned with the tools, methods, and approaches that facilitate practice or the study of practice, it also covers an extensive field of study encompassing a variety of different concepts, theories and methods. National and international organisations regularly hold conferences and publish literature in order to improve mathematics education.


== History ==


=== Ancient ===
Elementary mathematics were a core part of education in many ancient civilisations, including ancient Egypt, ancient Babylonia, ancient Greece, ancient Rome, and Vedic India. In most cases, formal education was only available to male children with sufficiently high status, wealth, or caste. The oldest known mathematics textbook is the Rhind papyrus, dated from circa 1650 BCE.


==== Pythagorean theorem ====
Historians of Mesopotamia have confirmed that use of the Pythagorean rule dates back to the Old Babylonian Empire (20th–16th centuries BC) and that it was being taught in scribal schools over one thousand years before the birth of Pythagoras.In Plato's division of the liberal arts into the trivium and the quadrivium, the quadrivium included the mathematical fields of arithmetic and geometry. This structure was continued in the structure of classical education that was developed in medieval Europe. The teaching of geometry was almost universally based on Euclid's Elements. Apprentices to trades such as masons, merchants, and moneylenders could expect to learn such practical mathematics as was relevant to their profession.


=== Medieval and early modern ===

In the Middle Ages, the academic status of mathematics declined, because it was strongly associated with trade and commerce, and considered somewhat un-Christian. Although it continued to be taught in European universities, it was seen as subservient to the study of natural, metaphysical, and moral philosophy. The first modern arithmetic curriculum (starting with addition, then subtraction, multiplication, and division) arose at reckoning schools in Italy in the 1300s. Spreading along trade routes, these methods were designed to be used in commerce. They contrasted with Platonic math taught at universities, which was more philosophical and concerned numbers as concepts rather than calculating methods. They also contrasted with mathematical methods learned by artisan apprentices, which were specific to the tasks and tools at hand. For example, the division of a board into thirds can be accomplished with a piece of string, instead of measuring the length and using the arithmetic operation of division.The first mathematics textbooks to be written in English and French were published by Robert Recorde, beginning with The Grounde of Artes in 1543. However, there are many different writings on mathematics and mathematics methodology that date back to 1800 BCE. These were mostly located in Mesopotamia, where the Sumerians were practicing multiplication and division. There are also artifacts demonstrating their methodology for solving equations like the quadratic equation. After the Sumerians, some of the most famous ancient works on mathematics came from Egypt in the form of the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus. The more famous Rhind Papyrus has been dated back to approximately 1650 BCE, but it is thought to be a copy of an even older scroll. This papyrus was essentially an early textbook for Egyptian students.
The social status of mathematical study was improving by the seventeenth century, with the University of Aberdeen creating a Mathematics Chair in 1613, followed by the Chair in Geometry being set up in University of Oxford in 1619 and the Lucasian Chair of Mathematics being established by the University of Cambridge in 1662.


=== Modern ===
In the 18th and 19th centuries, the Industrial Revolution led to an enormous increase in urban populations. Basic numeracy skills, such as the ability to tell the time, count money, and carry out simple arithmetic, became essential in this new urban lifestyle. Within the new public education systems, mathematics became a central part of the curriculum from an early age.
By the twentieth century, mathematics was part of the core curriculum in all developed countries.
During the twentieth century, mathematics education was established as an independent field of research. Main events in this development include the following:

In 1893, a Chair in mathematics education was created at the University of Göttingen, under the administration of Felix Klein.
The International Commission on Mathematical Instruction (ICMI) was founded in 1908, and Felix Klein became the first president of the organisation.
The professional periodical literature on mathematics education in the United States had generated more than 4,000 articles after 1920, so in 1941 William L. Schaaf published a classified index, sorting them into their various subjects.
A renewed interest in mathematics education emerged in the 1960s, and the International Commission was revitalized.
In 1968, the Shell Centre for Mathematical Education was established in Nottingham.
The first International Congress on Mathematical Education (ICME) was held in Lyon in 1969. The second congress was in Exeter in 1972, and after that, it has been held every four years.In the 20th century, the cultural impact of the ""electronic age"" (McLuhan) was also taken up by educational theory and the teaching of mathematics. While previous approach focused on ""working with specialized 'problems' in arithmetic"", the emerging structural approach to knowledge had ""small children meditating about number theory and 'sets'.""


== Objectives ==

At different times and in different cultures and countries, mathematics education has attempted to achieve a variety of different objectives. These objectives have included:

The teaching and learning of basic numeracy skills to all students
The teaching of practical mathematics (arithmetic, elementary algebra, plane and solid geometry, trigonometry, probability, statistics) to most students, to equip them to follow a trade or craft and to understand mathematics commonly used in news and Internet (such as percentages, charts, probability, and statistics)
The teaching of abstract mathematical concepts (such as set and function) at an early age
The teaching of selected areas of mathematics (such as Euclidean geometry) as an example of an axiomatic system and a model of deductive reasoning
The teaching of selected areas of mathematics (such as calculus) as an example of the intellectual achievements of the modern world
The teaching of advanced mathematics to those students who wish to follow a career in science, technology, engineering, and mathematics (STEM) fields
The teaching of heuristics and other problem-solving strategies to solve non-routine problems
The teaching of mathematics in social sciences and actuarial sciences, as well as in some selected arts under liberal arts education in liberal arts colleges or universities


== Methods ==
The method or methods used in any particular context are largely determined by the objectives that the relevant educational system is trying to achieve. Methods of teaching mathematics include the following:

Computer-based math: an approach based on the use of mathematical software as the primary tool of computation.
Computer-based mathematics education: involves the use of computers to teach mathematics. Mobile applications have also been developed to help students learn mathematics.
Classical education: the teaching of mathematics within the quadrivium, part of the classical education curriculum of the Middle Ages, which was typically based on Euclid's Elements taught as a paradigm of deductive reasoning.
Conventional approach: the gradual and systematic guiding through the hierarchy of mathematical notions, ideas and techniques. Starts with arithmetic and is followed by Euclidean geometry and elementary algebra taught concurrently. Requires the instructor to be well informed about elementary mathematics since didactic and curriculum decisions are often dictated by the logic of the subject rather than pedagogical considerations. Other methods emerge by emphasizing some aspects of this approach.
Relational approach: uses class topics to solve everyday problems and relates the topic to current events. This approach focuses on the many uses of mathematics and helps students understand why they need to know it as well as helps them to apply mathematics to real-world situations outside of the classroom.
Historical method: teaching the development of mathematics within a historical, social, and cultural context. Proponents argue it provides more human interest than the conventional approach.
Discovery math: a constructivist method of teaching (discovery learning) mathematics which centres around problem-based or inquiry-based learning, with the use of open-ended questions and manipulative tools. This type of mathematics education was implemented in various parts of Canada beginning in 2005. Discovery-based mathematics is at the forefront of the Canadian ""math wars"" debate with many criticizing it for declining math scores.
New Math: a method of teaching mathematics which focuses on abstract concepts such as set theory, functions, and bases other than ten. Adopted in the US as a response to the challenge of early Soviet technical superiority in space, it began to be challenged in the late 1960s. One of the most influential critiques of the New Math was Morris Kline's 1973 book Why Johnny Can't Add. The New Math method was the topic of one of Tom Lehrer's most popular parody songs, with his introductory remarks to the song: ""...in the new approach, as you know, the important thing is to understand what you're doing, rather than to get the right answer.""
Recreational mathematics: mathematical problems that are fun can motivate students to learn mathematics and can increase their enjoyment of mathematics.
Standards-based mathematics: a vision for pre-college mathematics education in the United States and Canada, focused on deepening student understanding of mathematical ideas and procedures, and formalized by the National Council of Teachers of Mathematics which created the Principles and Standards for School Mathematics.
Mastery: an approach in which most students are expected to achieve a high level of competence before progressing.
Problem solving: the cultivation of mathematical ingenuity, creativity, and heuristic thinking by setting students open-ended, unusual, and sometimes unsolved problems. The problems can range from simple word problems to problems from international mathematics competitions such as the International Mathematical Olympiad. Problem-solving is used as a means to build new mathematical knowledge, typically by building on students' prior understandings.
Exercises: the reinforcement of mathematical skills by completing large numbers of exercises of a similar type, such as adding simple fractions or solving quadratic equations.
Rote learning: the teaching of mathematical results, definitions and concepts by repetition and memorisation typically without meaning or supported by mathematical reasoning. A derisory term is drill and kill. In traditional education, rote learning is used to teach multiplication tables, definitions, formulas, and other aspects of mathematics.
Math walk: a walk where experience of perceived objects and scenes is translated into mathematical language.


== Content and age levels ==
Different levels of mathematics are taught at different ages and in somewhat different sequences in different countries. Sometimes a class may be taught at an earlier age than typical as a special or honors class.
Elementary mathematics in most countries is taught similarly, though there are differences. Most countries tend to cover fewer topics in greater depth than in the United States. During the primary school years, children learn about whole numbers and arithmetic, including addition, subtraction, multiplication, and division. Comparisons and measurement are taught, in both numeric and pictorial form, as well as fractions and proportionality, patterns, and various topics related to geometry.At high school level in most of the US, algebra, geometry, and analysis (pre-calculus and calculus) are taught as separate courses in different years. 
On the other hand, in most other countries (and in a few US states), mathematics is taught as an integrated subject, with topics from all branches of mathematics studied every year; 
students thus undertake a pre-defined course - entailing several topics -  rather than choosing courses à la carte as in the United States. 
Even in these cases, however, several ""mathematics"" options may be offered, selected based on the student's intended studies post high school.
(In South Africa, for example, the options are Mathematics, Mathematical Literacy and Technical Mathematics.)
Thus, a science-oriented curriculum typically overlaps the first year of university mathematics, and includes differential calculus and trigonometry at age 16–17 and integral calculus, complex numbers, analytic geometry, exponential and logarithmic functions, and infinite series in their final year of secondary school; Probability and statistics are similarly often taught. 
At college and university level, science and engineering students will be required to take multivariable calculus, differential equations, and linear algebra; at several US colleges, the minor or AS in mathematics substantively comprises these courses. Mathematics majors study additional other areas within pure mathematics—and often in applied mathematics—with the requirement of specified advanced courses in analysis and modern algebra. Applied mathematics may be taken as a major subject in its own right, while specific topics are taught within other courses: for example, civil engineers may be required to study fluid mechanics, and ""math for computer science"" might include graph theory, permutation, probability, and formal mathematical proofs. Pure and applied math degrees often include modules in probability theory or mathematical statistics, while a course in numerical methods is a common requirement for applied math. (Theoretical) physics is mathematics-intensive, often overlapping substantively with the pure or applied math degree. Business mathematics is usually limited to introductory calculus and (sometimes) matrix calculations; economics programs additionally cover optimization, often differential equations and linear algebra, and sometimes analysis.


== Standards ==
Throughout most of history, standards for mathematics education were set locally, by individual schools or teachers, depending on the levels of achievement that were relevant to, realistic for, and considered socially appropriate for their pupils.
In modern times, there has been a move towards regional or national standards, usually under the umbrella of a wider standard school curriculum. In England, for example, standards for mathematics education are set as part of the National Curriculum for England, while Scotland maintains its own educational system. Many other countries have centralized ministries which set national standards or curricula, and sometimes even textbooks.
Ma (2000) summarized the research of others who found, based on nationwide data, that students with higher scores on standardized mathematics tests had taken more mathematics courses in high school. This led some states to require three years of mathematics instead of two. But because this requirement was often met by taking another lower-level mathematics course, the additional courses had a “diluted” effect in raising achievement levels.In North America, the National Council of Teachers of Mathematics (NCTM) published the Principles and Standards for School Mathematics in 2000 for the United States and Canada, which boosted the trend towards reform mathematics. In 2006, the NCTM released Curriculum Focal Points, which recommend the most important mathematical topics for each grade level through grade 8. However, these standards were guidelines to implement as American states and Canadian provinces chose. In 2010, the National Governors Association Center for Best Practices and the Council of Chief State School Officers published the Common Core State Standards for US states, which were subsequently adopted by most states. Adoption of the Common Core State Standards in mathematics is at the discretion of each state, and is not mandated by the federal government. ""States routinely review their academic standards and may choose to change or add onto the standards to best meet the needs of their students."" The NCTM has state affiliates that have different education standards at the state level. For example, Missouri has the Missouri Council of Teachers of Mathematics (MCTM) which has its pillars and standards of education listed on its website. The MCTM also offers membership opportunities to teachers and future teachers so that they can stay up to date on the changes in math educational standards.The Programme for International Student Assessment (PISA), created by the Organisation for the Economic Co-operation and Development (OECD), is a global program studying the reading, science, and mathematics abilities of 15-year-old students. The first assessment was conducted in the year 2000 with 43 countries participating. PISA has repeated this assessment every three years to provide comparable data, helping to guide global education to better prepare youth for future economies. There have been many ramifications following the results of triennial PISA assessments due to implicit and explicit responses of stakeholders, which have led to education reform and policy change.


== Research ==
According to Hiebert and Grouws, ""Robust, useful theories of classroom teaching do not yet exist."" However, there are useful theories on how children learn mathematics, and much research has been conducted in recent decades to explore how these theories can be applied to teaching. The following results are examples of some of the current findings in the field of mathematics education.


=== Important results ===
One of the strongest results in recent research is that the most important feature of effective teaching is giving students ""the opportunity to learn"". Teachers can set expectations, times, kinds of tasks, questions, acceptable answers, and types of discussions that will influence students' opportunities to learn. This must involve both skill efficiency and conceptual understanding.


=== Conceptual understanding ===
Two of the most important features of teaching in the promotion of conceptual understanding times are attending explicitly to concepts and allowing students to struggle with important mathematics. Both of these features have been confirmed through a wide variety of studies. Explicit attention to concepts involves making connections between facts, procedures, and ideas. (This is often seen as one of the strong points in mathematics teaching in East Asian countries, where teachers typically devote about half of their time to making connections. At the other extreme is the US, where essentially no connections are made in school classrooms.) These connections can be made through explanation of the meaning of a procedure, questions comparing strategies and solutions of problems, noticing how one problem is a special case of another, reminding students of the main point, discussing how lessons connect, and so on.
Deliberate, productive struggle with mathematical ideas refers to the fact that when students exert effort with important mathematical ideas, even if this struggle initially involves confusion and errors, the result is greater learning. This is true whether the struggle is due to intentionally challenging, well-implemented teaching, or unintentionally confusing, faulty teaching.


=== Formative assessment ===
Formative assessment is both the best and cheapest way to boost student achievement, student engagement, and teacher professional satisfaction. Results surpass those of reducing class size or increasing teachers' content knowledge. Effective assessment is based on clarifying what students should know, creating appropriate activities to obtain the evidence needed, giving good feedback, encouraging students to take control of their learning and letting students be resources for one another.


=== Homework ===
Homework which leads students to practice past lessons or prepare future lessons is more effective than those going over today's lesson. Students benefit from feedback. Students with learning disabilities or low motivation may profit from rewards. For younger children, homework helps simple skills, but not broader measures of achievement.


=== Students with difficulties ===
Students with genuine difficulties (unrelated to motivation or past instruction) struggle with basic facts, answer impulsively, struggle with mental representations, have poor number sense, and have poor short-term memory. Techniques that have been found productive for helping such students include peer-assisted learning, explicit teaching with visual aids, instruction informed by formative assessment, and encouraging students to think aloud.


=== Algebraic reasoning ===
Elementary school children need to spend a long time learning to express algebraic properties without symbols before learning algebraic notation. When learning symbols, many students believe letters always represent unknowns and struggle with the concept of variable. They prefer arithmetic reasoning to algebraic equations for solving word problems. It takes time to move from arithmetic to algebraic generalizations to describe patterns. Students often have trouble with the minus sign and understand the equals sign to mean ""the answer is..."".


=== Methodology ===
As with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods to test their effects. They depend on large samples to obtain statistically significant results.
Qualitative research, such as case studies, action research, discourse analysis, and clinical interviews, depend on small but focused samples in an attempt to understand student learning and to look at how and why a given method gives the results it does. Such studies cannot conclusively establish that one method is better than another, as randomized trials can, but unless it is understood why treatment X is better than treatment Y, application of results of quantitative studies will often lead to ""lethal mutations"" of the finding in actual classrooms. Exploratory qualitative research is also useful for suggesting new hypotheses, which can eventually be tested by randomized experiments. Both qualitative and quantitative studies, therefore, are considered essential in education—just as in the other social sciences. Many studies are “mixed”, simultaneously combining aspects of both quantitative and qualitative research, as appropriate.


==== Randomized trials ====
There has been some controversy over the relative strengths of different types of research. Because randomized trials provide clear, objective evidence on “what works”, policymakers often consider only those studies. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes. In other disciplines concerned with human subjects—like biomedicine, psychology, and policy evaluation—controlled, randomized experiments remain the preferred method of evaluating treatments. Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods. On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective, or the difficulty of assuring rigid control of the independent variable in fluid, real school settings.In the United States, the National Mathematics Advisory Panel (NMAP) published a report in 2008 based on studies, some of which used randomized assignment of treatments to experimental units, such as classrooms or students. The NMAP report's preference for randomized experiments received criticism from some scholars. In 2010, the What Works Clearinghouse (essentially the research arm for the Department of Education) responded to ongoing controversy by extending its research base to include non-experimental studies, including regression discontinuity designs and single-case studies.


== Organizations ==
Advisory Committee on Mathematics Education
American Mathematical Association of Two-Year Colleges
Association of Teachers of Mathematics
Canadian Mathematical Society
C.D. Howe Institute
Mathematical Association
National Council of Teachers of Mathematics
OECD


== See also ==


== References ==


== Further reading ==


== External links ==

Math Education at Curlie
History of Mathematical Education
A quarter century of US 'math wars' and political partisanship. David Klein. California State University, Northridge, United States"
d70a617347,Mathematical Reviews,"Mathematical Reviews is a journal published by the American Mathematical Society (AMS) that contains brief synopses, and in some cases evaluations, of many articles in mathematics, statistics, and theoretical computer science. The AMS also publishes an associated online bibliographic database called MathSciNet which contains an electronic version of Mathematical Reviews and additionally contains citation information for over 3.5 million items as of 2018.


== Reviews ==
Mathematical Reviews was founded by Otto E. Neugebauer in 1940 as an alternative to the German journal Zentralblatt für Mathematik, which Neugebauer had also founded a decade earlier, but which under the Nazis had begun censoring reviews by and of Jewish mathematicians. The goal of the new journal was to give reviews of every mathematical research publication. As of November 2007, the Mathematical Reviews database contained information on over 2.2 million articles. The authors of reviews are volunteers, usually chosen by the editors because of some expertise in the area of the article. It and Zentralblatt für Mathematik are the only comprehensive resources of this type. (The Mathematics section of Referativny Zhurnal is available only in Russian and is smaller in scale and difficult to access.) Often reviews give detailed summaries of the contents of the paper, sometimes with critical comments by the reviewer and references to related work. However, reviewers are not encouraged to criticize the paper, because the author does not have an opportunity to respond. The author's summary may be quoted when it is not possible to give an independent review, or when the summary is deemed adequate by the reviewer or the editors. Only bibliographic information may be given when a work is in an unusual language, when it is a brief paper in a conference volume, or when it is outside the primary scope of the Reviews. Originally the reviews were written in several languages, but later an ""English only"" policy was introduced. Selected reviews (called ""featured reviews"") were also published as a book by the AMS, but this program has been discontinued.


== Online database ==
In 1980, all the contents of Mathematical Reviews since 1940 were integrated into an electronic searchable database. Eventually the contents became part of MathSciNet, which was officially launched in 1996. MathSciNet also has extensive citation information.


== Mathematical citation quotient ==
Mathematical Reviews computes a ""mathematical citation quotient"" (MCQ) for each journal. Like the impact factor, this is a numerical statistic that measures the frequency of citations to a journal. The MCQ is calculated by counting the total number of citations into the journal that have been indexed by Mathematical Reviews over a five-year period, and dividing this total by the total number of papers published by the journal during that five-year period.
For the period 2012 – 2014, the top five journals in Mathematical Reviews by MCQ were:
Acta Numerica — MCQ 8.14
Publications Mathématiques de l'IHÉS — MCQ 5.06
Journal of the American Mathematical Society — MCQ 4.79
Annals of Mathematics — MCQ 4.60
Forum of Mathematics, Pi — MCQ 4.54The ""All Journal MCQ"" is computed by considering all the journals indexed by Mathematical Reviews as a single meta-journal, which makes it possible to determine if a particular journal has a higher or lower MCQ than average. The 2018 All Journal MCQ is 0.41.


== Current Mathematical Publications ==
Current Mathematical Publications was a subject index in print format that published the newest and upcoming mathematical literature, chosen and indexed by Mathematical Reviews editors. It covered the period from 1965 until 2012, when it was discontinued.


== See also ==

Referativnyi Zhurnal, published in former Soviet Union and now in Russia
Zentralblatt MATH, published in Germany
INSPEC
Web of Science
IEEE Xplore
Current Index to Statistics


== References ==


== External links ==
Mathematical Reviews database with access to the online search function for the database (for subscribers), and links to information about the service, such as the following:
Mathematical Reviews editorial statement outlines the mission of Mathematical Reviews;
Mathematical Reviews guide for reviewers, intended for both reviewers and users of Mathematical Reviews.
Exceptional MathReviews collected by Kimball Martin and sorted by amusement factor."
48a1894de1,Proportionality (mathematics),"In mathematics, two sequences of numbers, often experimental data, are proportional or directly proportional if their corresponding elements have a constant ratio, which is called the coefficient of proportionality or proportionality constant. Two sequences are inversely proportional if corresponding elements have a constant product, also called the coefficient of proportionality.
This definition is commonly extended to related varying quantities, which are often called variables. This meaning of variable is not the common meaning of the term in mathematics (see variable (mathematics)); these two different concepts share the same name for historical reasons.
Two functions 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   and 
  
    
      
        g
        (
        x
        )
      
    
    {\displaystyle g(x)}
   are proportional if their ratio 
  
    
      
        
          
            
              f
              (
              x
              )
            
            
              g
              (
              x
              )
            
          
        
      
    
    {\textstyle {\frac {f(x)}{g(x)}}}
   is a constant function.
If several pairs of variables share the same direct proportionality constant, the equation expressing the equality of these ratios is called a proportion, e.g., a/b = x/y = ⋯ = k (for details see Ratio).
Proportionality is closely related to linearity.


== Direct proportionality ==

Given an independent variable x and a dependent variable y, y is directly proportional to x if there is a non-zero constant k such that

  
    
      
        y
        =
        k
        x
        .
      
    
    {\displaystyle y=kx.}
  The relation is often denoted using the symbols ""∝"" (not to be confused with the Greek letter alpha) or ""~"":

  
    
      
        y
        ∝
        x
        ,
      
    
    {\displaystyle y\propto x,}
   or 
  
    
      
        y
        ∼
        x
        .
      
    
    {\displaystyle y\sim x.}
  For 
  
    
      
        x
        ≠
        0
      
    
    {\displaystyle x\neq 0}
   the proportionality constant can be expressed as the ratio

  
    
      
        k
        =
        
          
            y
            x
          
        
        .
      
    
    {\displaystyle k={\frac {y}{x}}.}
  It is also called the constant of variation or constant of proportionality.
A direct proportionality can also be viewed as a linear equation in two variables with a y-intercept of 0 and a slope of k. This corresponds to linear growth.


=== Examples ===
If an object travels at a constant speed, then the distance traveled is directly proportional to the time spent traveling, with the speed being the constant of proportionality.
The circumference of a circle is directly proportional to its diameter, with the constant of proportionality equal to π.
On a map of a sufficiently small geographical area, drawn to scale distances, the distance between any two points on the map is directly proportional to the beeline distance between the two locations represented by those points; the constant of proportionality is the scale of the map.
The force, acting on a small object with small mass by a nearby large extended mass due to gravity, is directly proportional to the object's mass; the constant of proportionality between the force and the mass is known as gravitational acceleration.
The net force acting on an object is proportional to the acceleration of that object with respect to an inertial frame of reference. The constant of proportionality in this, Newton's second law, is the classical mass of the object.


== Inverse proportionality ==

The concept of inverse proportionality can be contrasted with direct proportionality. Consider two variables said to be ""inversely proportional"" to each other. If all other variables are held constant, the magnitude or absolute value of one inversely proportional variable decreases if the other variable increases, while their product (the constant of proportionality k) is always the same. As an example, the time taken for a journey is inversely proportional to the speed of travel.
Formally, two variables are inversely proportional (also called varying inversely, in inverse variation, in inverse proportion) if each of the variables is directly proportional to the multiplicative inverse (reciprocal) of the other, or equivalently if their product is a constant. It follows that the variable y is inversely proportional to the variable x if there exists a non-zero constant k such that

  
    
      
        y
        =
        
          
            k
            x
          
        
        ,
      
    
    {\displaystyle y={\frac {k}{x}},}
  or equivalently, 
  
    
      
        x
        y
        =
        k
        .
      
    
    {\displaystyle xy=k.}
   Hence the constant ""k"" is the product of x and y.
The graph of two variables varying inversely on the Cartesian coordinate plane is a rectangular hyperbola. The product of the x and y values of each point on the curve equals the constant of proportionality (k). Since neither x nor y can equal zero (because k is non-zero), the graph never crosses either axis.


== Hyperbolic coordinates ==

The concepts of direct and inverse proportion lead to the location of points in the Cartesian plane by hyperbolic coordinates; the two coordinates correspond to the constant of direct proportionality that specifies a point as being on a particular ray and the constant of inverse proportionality that specifies a point as being on a particular hyperbola.


== Computer encoding ==
The Unicode characters for proportionality are the following:

U+221D ∝ PROPORTIONAL TO (&prop;, &Proportional;, &propto;, &varpropto;, &vprop;)
U+007E ~ TILDE
U+2237 ∷ PROPORTION
U+223C ∼ TILDE OPERATOR (&sim;, &thicksim;, &thksim;, &Tilde;)
U+223A ∺ GEOMETRIC PROPORTION (&mDDot;)


== See also ==
Linear map
Correlation
Eudoxus of Cnidus
Golden ratio
Inverse-square law
Proportional font
Ratio
Rule of three (mathematics)
Sample size
Similarity
Basic proportionality theoremGrowthLinear growth
Hyperbolic growth


== Notes ==


== References ==
Ya. B. Zeldovich,  I. M. Yaglom: Higher math for beginners, p. 34–35.
Brian Burrell: Merriam-Webster's Guide to Everyday Math: A Home and Business Reference. Merriam-Webster, 1998, ISBN 9780877796213, p. 85–101.
Lanius, Cynthia S.; Williams Susan E.: PROPORTIONALITY: A Unifying Theme for the Middle Grades. Mathematics Teaching in the Middle School 8.8 (2003), p. 392–396.
Seeley, Cathy; Schielack Jane F.: A Look at the Development of Ratios, Rates, and Proportionality. Mathematics Teaching in the Middle School, 13.3, 2007, p. 140–142.
Van Dooren, Wim; De Bock Dirk; Evers Marleen; Verschaffel Lieven : Students' Overuse of Proportionality on Missing-Value Problems: How Numbers May Change Solutions. Journal for Research in Mathematics Education, 40.2, 2009, p. 187–211."
99fb6fedc9,Measure (mathematics),"In mathematics, the concept of a measure is a generalization and formalization of geometrical measures (length, area, volume) and other common notions, such as mass and probability of events. These seemingly distinct concepts have many similarities and can often be treated together in a single mathematical context. Measures are foundational in probability theory, integration theory, and can be generalized to assume negative values, as with electrical charge. Far-reaching  generalizations (such as spectral measures and projection-valued measures) of measure are widely used in quantum physics and physics in general.
The intuition behind this concept dates back to ancient Greece, when Archimedes tried to calculate the area of a circle. But it was not until the late 19th and early 20th centuries that measure theory became a branch of mathematics. The foundations of modern measure theory were laid in the works of Émile Borel, Henri Lebesgue, Nikolai Luzin, Johann Radon, Constantin Carathéodory, and Maurice Fréchet, among others.


== Definition ==

Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a set and 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   a 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  -algebra over 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   A set function 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   from 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   to the extended real number line is called a measure if it satisfies the following properties:

Non-negativity: For all 
  
    
      
        E
      
    
    {\displaystyle E}
   in 
  
    
      
        Σ
        ,
      
    
    {\displaystyle \Sigma ,}
   we have 
  
    
      
        μ
        (
        E
        )
        ≥
        0.
      
    
    {\displaystyle \mu (E)\geq 0.}
  
Null empty set: 
  
    
      
        μ
        (
        ∅
        )
        =
        0.
      
    
    {\displaystyle \mu (\varnothing )=0.}
  
Countable additivity (or 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  -additivity): For all countable collections 
  
    
      
        {
        
          E
          
            k
          
        
        
          }
          
            k
            =
            1
          
          
            ∞
          
        
      
    
    {\displaystyle \{E_{k}\}_{k=1}^{\infty }}
   of pairwise disjoint sets in Σ,If at least one set 
  
    
      
        E
      
    
    {\displaystyle E}
   has finite measure, then the requirement 
  
    
      
        μ
        (
        ∅
        )
        =
        0
      
    
    {\displaystyle \mu (\varnothing )=0}
   is met automatically due to countable additivity:

and therefore 
  
    
      
        μ
        (
        ∅
        )
        =
        0.
      
    
    {\displaystyle \mu (\varnothing )=0.}
  
If the condition of non-negativity is dropped, and 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   takes on at most one of the values of 
  
    
      
        ±
        ∞
        ,
      
    
    {\displaystyle \pm \infty ,}
   then 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is called a signed measure.
The pair 
  
    
      
        (
        X
        ,
        Σ
        )
      
    
    {\displaystyle (X,\Sigma )}
   is called a measurable space, and the members of 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   are called measurable sets.
A triple 
  
    
      
        (
        X
        ,
        Σ
        ,
        μ
        )
      
    
    {\displaystyle (X,\Sigma ,\mu )}
   is called a measure space. A probability measure is a measure with total measure one – that is, 
  
    
      
        μ
        (
        X
        )
        =
        1.
      
    
    {\displaystyle \mu (X)=1.}
   A probability space is a measure space with a probability measure.
For measure spaces that are also topological spaces various compatibility conditions can be placed for the measure and the topology. Most measures met in practice in analysis (and in many cases also in probability theory) are Radon measures. Radon measures have an alternative definition in terms of linear functionals on the locally convex topological vector space of continuous functions with compact support. This approach is taken by Bourbaki (2004) and a number of other sources. For more details, see the article on Radon measures.


== Instances ==

Some important measures are listed here.

The counting measure is defined by 
  
    
      
        μ
        (
        S
        )
      
    
    {\displaystyle \mu (S)}
   = number of elements in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
  
The Lebesgue measure on 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is a complete translation-invariant measure on a σ-algebra containing the intervals in 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   such that 
  
    
      
        μ
        (
        [
        0
        ,
        1
        ]
        )
        =
        1
      
    
    {\displaystyle \mu ([0,1])=1}
  ; and every other measure with these properties extends Lebesgue measure.
Circular angle measure is invariant under rotation, and hyperbolic angle measure is invariant under squeeze mapping.
The Haar measure for a locally compact topological group is a generalization of the Lebesgue measure (and also of counting measure and circular angle measure) and has similar uniqueness properties.
The Hausdorff measure is a generalization of the Lebesgue measure to sets with non-integer dimension, in particular, fractal sets.
Every probability space gives rise to a measure which takes the value 1 on the whole space (and therefore takes all its values in the unit interval [0, 1]). Such a measure is called a probability measure. See probability axioms.
The Dirac measure δa (cf. Dirac delta function) is given by δa(S) = χS(a), where χS is the indicator function of 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   The measure of a set is 1 if it contains the point 
  
    
      
        a
      
    
    {\displaystyle a}
   and 0 otherwise.Other 'named' measures used in various theories include: Borel measure, Jordan measure, ergodic measure, Gaussian measure, Baire measure, Radon measure, Young measure, and Loeb measure.
In physics an example of a measure is spatial distribution of mass (see for example, gravity potential), or another non-negative extensive property, conserved (see conservation law for a list of these) or not. Negative values lead to signed measures, see ""generalizations"" below.

Liouville measure, known also as the natural volume form on a symplectic manifold, is useful in classical statistical and Hamiltonian mechanics.
Gibbs measure is widely used in statistical mechanics, often under the name canonical ensemble.


== Basic properties ==
Let 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   be a measure.


=== Monotonicity ===
If 
  
    
      
        
          E
          
            1
          
        
      
    
    {\displaystyle E_{1}}
   and 
  
    
      
        
          E
          
            2
          
        
      
    
    {\displaystyle E_{2}}
   are measurable sets with 
  
    
      
        
          E
          
            1
          
        
        ⊆
        
          E
          
            2
          
        
      
    
    {\displaystyle E_{1}\subseteq E_{2}}
   then


=== Measure of countable unions and intersections ===


==== Countable subadditivity ====
For any countable sequence 
  
    
      
        
          E
          
            1
          
        
        ,
        
          E
          
            2
          
        
        ,
        
          E
          
            3
          
        
        ,
        …
      
    
    {\displaystyle E_{1},E_{2},E_{3},\ldots }
   of (not necessarily disjoint) measurable sets 
  
    
      
        
          E
          
            n
          
        
      
    
    {\displaystyle E_{n}}
   in 
  
    
      
        Σ
        :
      
    
    {\displaystyle \Sigma :}
  


==== Continuity from below ====
If 
  
    
      
        
          E
          
            1
          
        
        ,
        
          E
          
            2
          
        
        ,
        
          E
          
            3
          
        
        ,
        …
      
    
    {\displaystyle E_{1},E_{2},E_{3},\ldots }
   are measurable sets that are increasing (meaning that 
  
    
      
        
          E
          
            1
          
        
        ⊆
        
          E
          
            2
          
        
        ⊆
        
          E
          
            3
          
        
        ⊆
        …
      
    
    {\displaystyle E_{1}\subseteq E_{2}\subseteq E_{3}\subseteq \ldots }
  ) then the union of the sets 
  
    
      
        
          E
          
            n
          
        
      
    
    {\displaystyle E_{n}}
   is measurable and


==== Continuity from above ====
If 
  
    
      
        
          E
          
            1
          
        
        ,
        
          E
          
            2
          
        
        ,
        
          E
          
            3
          
        
        ,
        …
      
    
    {\displaystyle E_{1},E_{2},E_{3},\ldots }
   are measurable sets that are decreasing (meaning that 
  
    
      
        
          E
          
            1
          
        
        ⊇
        
          E
          
            2
          
        
        ⊇
        
          E
          
            3
          
        
        ⊇
        …
      
    
    {\displaystyle E_{1}\supseteq E_{2}\supseteq E_{3}\supseteq \ldots }
  ) then the intersection of the sets 
  
    
      
        
          E
          
            n
          
        
      
    
    {\displaystyle E_{n}}
   is measurable; furthermore, if at least one of the 
  
    
      
        
          E
          
            n
          
        
      
    
    {\displaystyle E_{n}}
   has finite measure then

This property is false without the assumption that at least one of the 
  
    
      
        
          E
          
            n
          
        
      
    
    {\displaystyle E_{n}}
   has finite measure. For instance, for each 
  
    
      
        n
        ∈
        
          N
        
        ,
      
    
    {\displaystyle n\in \mathbb {N} ,}
   let 
  
    
      
        
          E
          
            n
          
        
        =
        [
        n
        ,
        ∞
        )
        ⊆
        
          R
        
        ,
      
    
    {\displaystyle E_{n}=[n,\infty )\subseteq \mathbb {R} ,}
   which all have infinite Lebesgue measure, but the intersection is empty.


== Other properties ==


=== Completeness ===

A measurable set 
  
    
      
        X
      
    
    {\displaystyle X}
   is called a null set if 
  
    
      
        μ
        (
        X
        )
        =
        0.
      
    
    {\displaystyle \mu (X)=0.}
   A subset of a null set is called a negligible set. A negligible set need not be measurable, but every measurable negligible set is automatically a null set. A measure is called complete if every negligible set is measurable.
A measure can be extended to a complete one by considering the σ-algebra of subsets 
  
    
      
        Y
      
    
    {\displaystyle Y}
   which differ by a negligible set from a measurable set 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   that is, such that the symmetric difference of 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   is contained in a null set. One defines 
  
    
      
        μ
        (
        Y
        )
      
    
    {\displaystyle \mu (Y)}
   to equal 
  
    
      
        μ
        (
        X
        )
        .
      
    
    {\displaystyle \mu (X).}
  


=== μ{x : f(x) ≥ t} = μ{x : f(x) > t} (a.e.) ===
If 
  
    
      
        f
        :
        X
        →
        [
        0
        ,
        +
        ∞
        ]
      
    
    {\displaystyle f:X\to [0,+\infty ]}
   is 
  
    
      
        (
        Σ
        ,
        
          
            B
          
        
        (
        [
        0
        ,
        +
        ∞
        ]
        )
        )
      
    
    {\displaystyle (\Sigma ,{\cal {B}}([0,+\infty ]))}
  -measurable, then

for almost all 
  
    
      
        t
        ∈
        X
        .
      
    
    {\displaystyle t\in X.}
   This property is used in connection with Lebesgue integral.


=== Additivity ===
Measures are required to be countably additive. However, the condition can be strengthened as follows.
For any set 
  
    
      
        I
      
    
    {\displaystyle I}
   and any set of nonnegative 
  
    
      
        
          r
          
            i
          
        
        ,
        i
        ∈
        I
      
    
    {\displaystyle r_{i},i\in I}
   define:

That is, we define the sum of the 
  
    
      
        
          r
          
            i
          
        
      
    
    {\displaystyle r_{i}}
   to be the supremum of all the sums of finitely many of them.
A measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   on 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   is 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
  -additive if for any 
  
    
      
        λ
        <
        κ
      
    
    {\displaystyle \lambda <\kappa }
   and any family of disjoint sets 
  
    
      
        
          X
          
            α
          
        
        ,
        α
        <
        λ
      
    
    {\displaystyle X_{\alpha },\alpha <\lambda }
   the following hold:

Note that the second condition is equivalent to the statement that the ideal of null sets is 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
  -complete.


=== Sigma-finite measures ===

A measure space 
  
    
      
        (
        X
        ,
        Σ
        ,
        μ
        )
      
    
    {\displaystyle (X,\Sigma ,\mu )}
   is called finite if 
  
    
      
        μ
        (
        X
        )
      
    
    {\displaystyle \mu (X)}
   is a finite real number (rather than 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
  ). Nonzero finite measures are analogous to probability measures in the sense that any finite measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is proportional to the probability measure 
  
    
      
        
          
            1
            
              μ
              (
              X
              )
            
          
        
        μ
        .
      
    
    {\displaystyle {\frac {1}{\mu (X)}}\mu .}
   A measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is called σ-finite if 
  
    
      
        X
      
    
    {\displaystyle X}
   can be decomposed into a countable union of measurable sets of finite measure. Analogously, a set in a measure space is said to have a σ-finite measure if it is a countable union of sets with finite measure.
For example, the real numbers with the standard Lebesgue measure are σ-finite but not finite. Consider the closed intervals 
  
    
      
        [
        k
        ,
        k
        +
        1
        ]
      
    
    {\displaystyle [k,k+1]}
   for all integers 
  
    
      
        k
        ;
      
    
    {\displaystyle k;}
   there are countably many such intervals, each has measure 1, and their union is the entire real line. Alternatively, consider the real numbers with the counting measure, which assigns to each finite set of reals the number of points in the set. This measure space is not σ-finite, because every set with finite measure contains only finitely many points, and it would take uncountably many such sets to cover the entire real line. The σ-finite measure spaces have some very convenient properties; σ-finiteness can be compared in this respect to the Lindelöf property of topological spaces. They can be also thought of as a vague generalization of the idea that a measure space may have 'uncountable measure'.


=== Strictly localizable measures ===


=== Semifinite measures ===
Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a set, let 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\cal {A}}}
   be a sigma-algebra on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   and let 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   be a measure on 
  
    
      
        
          
            A
          
        
        .
      
    
    {\displaystyle {\cal {A}}.}
   We say 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is semifinite to mean that for all 
  
    
      
        A
        ∈
        
          μ
          
            pre
          
        
        {
        +
        ∞
        }
        ,
      
    
    {\displaystyle A\in \mu ^{\text{pre}}\{+\infty \},}
   
  
    
      
        
          
            P
          
        
        (
        A
        )
        ∩
        
          μ
          
            pre
          
        
        (
        
          
            R
          
          
            >
            0
          
        
        )
        ≠
        ∅
        .
      
    
    {\displaystyle {\cal {P}}(A)\cap \mu ^{\text{pre}}(\mathbb {R} _{>0})\neq \emptyset .}
  Semifinite measures generalize sigma-finite measures, in such a way that some big theorems of measure theory that hold for sigma-finite but not arbitrary measures can be extended with little modification to hold for semifinite measures. (To-do: add examples of such theorems; cf. the talk page.)


==== Basic examples ====
Every sigma-finite measure is semifinite.
Assume 
  
    
      
        
          
            A
          
        
        =
        
          
            P
          
        
        (
        X
        )
        ,
      
    
    {\displaystyle {\cal {A}}={\cal {P}}(X),}
   let 
  
    
      
        f
        :
        X
        →
        [
        0
        ,
        +
        ∞
        ]
        ,
      
    
    {\displaystyle f:X\to [0,+\infty ],}
   and assume 
  
    
      
        μ
        (
        A
        )
        =
        
          ∑
          
            a
            ∈
            A
          
        
        f
        (
        a
        )
      
    
    {\displaystyle \mu (A)=\sum _{a\in A}f(a)}
   for all 
  
    
      
        A
        ⊆
        X
        .
      
    
    {\displaystyle A\subseteq X.}
  
We have that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is sigma-finite if and only if 
  
    
      
        f
        (
        x
        )
        <
        +
        ∞
      
    
    {\displaystyle f(x)<+\infty }
   for all 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   and 
  
    
      
        
          f
          
            pre
          
        
        (
        
          
            R
          
          
            >
            0
          
        
        )
      
    
    {\displaystyle f^{\text{pre}}(\mathbb {R} _{>0})}
   is countable. We have that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is semifinite if and only if 
  
    
      
        f
        (
        x
        )
        <
        +
        ∞
      
    
    {\displaystyle f(x)<+\infty }
   for all 
  
    
      
        x
        ∈
        X
        .
      
    
    {\displaystyle x\in X.}
  
Taking 
  
    
      
        f
        =
        X
        ×
        {
        1
        }
      
    
    {\displaystyle f=X\times \{1\}}
   above (so that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is counting measure on 
  
    
      
        
          
            P
          
        
        (
        X
        )
      
    
    {\displaystyle {\cal {P}}(X)}
  ), we see that counting measure on 
  
    
      
        
          
            P
          
        
        (
        X
        )
      
    
    {\displaystyle {\cal {P}}(X)}
   is
sigma-finite if and only if 
  
    
      
        X
      
    
    {\displaystyle X}
   is countable; and
semifinite (without regard to whether 
  
    
      
        X
      
    
    {\displaystyle X}
   is countable). (Thus, counting measure, on the power set 
  
    
      
        
          
            P
          
        
        (
        X
        )
      
    
    {\displaystyle {\cal {P}}(X)}
   of an arbitrary uncountable set 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   gives an example of a semifinite measure that is not sigma-finite.)
Let 
  
    
      
        d
      
    
    {\displaystyle d}
   be a complete, separable metric on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   let 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\cal {B}}}
   be the Borel sigma-algebra induced by 
  
    
      
        d
        ,
      
    
    {\displaystyle d,}
   and let 
  
    
      
        s
        ∈
        
          
            R
          
          
            >
            0
          
        
        .
      
    
    {\displaystyle s\in \mathbb {R} _{>0}.}
   Then the Hausdorff measure 
  
    
      
        
          
            
              H
            
          
          
            s
          
        
        
          |
        
        
          
            B
          
        
      
    
    {\displaystyle {\cal {H}}^{s}|{\cal {B}}}
   is semifinite.
Let 
  
    
      
        d
      
    
    {\displaystyle d}
   be a complete, separable metric on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   let 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\cal {B}}}
   be the Borel sigma-algebra induced by 
  
    
      
        d
        ,
      
    
    {\displaystyle d,}
   and let 
  
    
      
        s
        ∈
        
          
            R
          
          
            >
            0
          
        
        .
      
    
    {\displaystyle s\in \mathbb {R} _{>0}.}
   Then the packing measure 
  
    
      
        
          
            
              H
            
          
          
            s
          
        
        
          |
        
        
          
            B
          
        
      
    
    {\displaystyle {\cal {H}}^{s}|{\cal {B}}}
   is semifinite.


==== Involved example ====
The zero measure is sigma-finite and thus semifinite. In addition, the zero measure is clearly less than or equal to 
  
    
      
        μ
        .
      
    
    {\displaystyle \mu .}
   It can be shown there is a greatest measure with these two properties:

We say the semifinite part of 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   to mean the semifinite measure 
  
    
      
        
          μ
          
            sf
          
        
      
    
    {\displaystyle \mu _{\text{sf}}}
   defined in the above theorem. We give some nice, explicit formulas, which some authors may take as definition, for the semifinite part:

  
    
      
        
          μ
          
            sf
          
        
        =
        (
        sup
        {
        μ
        (
        B
        )
        :
        B
        ∈
        
          
            P
          
        
        (
        A
        )
        ∩
        
          μ
          
            pre
          
        
        (
        
          
            R
          
          
            ≥
            0
          
        
        )
        }
        
          )
          
            A
            ∈
            
              
                A
              
            
          
        
        .
      
    
    {\displaystyle \mu _{\text{sf}}=(\sup\{\mu (B):B\in {\cal {P}}(A)\cap \mu ^{\text{pre}}(\mathbb {R} _{\geq 0})\})_{A\in {\cal {A}}}.}
  

  
    
      
        
          μ
          
            sf
          
        
        =
        (
        sup
        {
        μ
        (
        A
        ∩
        B
        )
        :
        B
        ∈
        
          μ
          
            pre
          
        
        (
        
          
            R
          
          
            ≥
            0
          
        
        )
        }
        
          )
          
            A
            ∈
            
              
                A
              
            
          
        
        }
        .
      
    
    {\displaystyle \mu _{\text{sf}}=(\sup\{\mu (A\cap B):B\in \mu ^{\text{pre}}(\mathbb {R} _{\geq 0})\})_{A\in {\cal {A}}}\}.}
  

  
    
      
        
          μ
          
            sf
          
        
        =
        μ
        
          
            |
          
          
            
              μ
              
                pre
              
            
            (
            
              
                R
              
              
                >
                0
              
            
            )
          
        
        ∪
        {
        A
        ∈
        
          
            A
          
        
        :
        sup
        {
        μ
        (
        B
        )
        :
        B
        ∈
        
          
            P
          
        
        (
        A
        )
        }
        =
        +
        ∞
        }
        ×
        {
        +
        ∞
        }
        ∪
        {
        A
        ∈
        
          
            A
          
        
        :
        sup
        {
        μ
        (
        B
        )
        :
        B
        ∈
        
          
            P
          
        
        (
        A
        )
        }
        <
        +
        ∞
        }
        ×
        {
        0
        }
        .
      
    
    {\displaystyle \mu _{\text{sf}}=\mu |_{\mu ^{\text{pre}}(\mathbb {R} _{>0})}\cup \{A\in {\cal {A}}:\sup\{\mu (B):B\in {\cal {P}}(A)\}=+\infty \}\times \{+\infty \}\cup \{A\in {\cal {A}}:\sup\{\mu (B):B\in {\cal {P}}(A)\}<+\infty \}\times \{0\}.}
  Since 
  
    
      
        
          μ
          
            sf
          
        
      
    
    {\displaystyle \mu _{\text{sf}}}
   is semifinite, it follows that if 
  
    
      
        μ
        =
        
          μ
          
            sf
          
        
      
    
    {\displaystyle \mu =\mu _{\text{sf}}}
   then 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is semifinite. It is also evident that if 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is semifinite then 
  
    
      
        μ
        =
        
          μ
          
            sf
          
        
        .
      
    
    {\displaystyle \mu =\mu _{\text{sf}}.}
  


==== Non-examples ====
Every 
  
    
      
        0
        −
        ∞
      
    
    {\displaystyle 0-\infty }
   measure that is not the zero measure is not semifinite. (Here, we say 
  
    
      
        0
        −
        ∞
      
    
    {\displaystyle 0-\infty }
   measure to mean a measure whose range lies in 
  
    
      
        {
        0
        ,
        +
        ∞
        }
      
    
    {\displaystyle \{0,+\infty \}}
  : 
  
    
      
        (
        ∀
        A
        ∈
        
          
            A
          
        
        )
        (
        μ
        (
        A
        )
        ∈
        {
        0
        ,
        +
        ∞
        }
        )
        .
      
    
    {\displaystyle (\forall A\in {\cal {A}})(\mu (A)\in \{0,+\infty \}).}
  ) Below we give examples of 
  
    
      
        0
        −
        ∞
      
    
    {\displaystyle 0-\infty }
   measures that are not zero measures.

Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be nonempty, let 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\cal {A}}}
   be a 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  -algebra on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   let 
  
    
      
        f
        :
        X
        →
        {
        0
        ,
        +
        ∞
        }
      
    
    {\displaystyle f:X\to \{0,+\infty \}}
   be not the zero function, and let 
  
    
      
        μ
        =
        (
        
          ∑
          
            x
            ∈
            A
          
        
        f
        (
        x
        )
        
          )
          
            A
            ∈
            
              
                A
              
            
          
        
        .
      
    
    {\displaystyle \mu =(\sum _{x\in A}f(x))_{A\in {\cal {A}}}.}
   It can be shown that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is a measure.

  
    
      
        μ
        =
        {
        (
        ∅
        ,
        0
        )
        }
        ∪
        (
        
          
            A
          
        
        ∖
        {
        ∅
        }
        )
        ×
        {
        +
        ∞
        }
        .
      
    
    {\displaystyle \mu =\{(\emptyset ,0)\}\cup ({\cal {A}}\setminus \{\emptyset \})\times \{+\infty \}.}
  
  
    
      
        X
        =
        {
        0
        }
        ,
      
    
    {\displaystyle X=\{0\},}
   
  
    
      
        
          
            A
          
        
        =
        {
        ∅
        ,
        X
        }
        ,
      
    
    {\displaystyle {\cal {A}}=\{\emptyset ,X\},}
   
  
    
      
        μ
        =
        {
        (
        ∅
        ,
        0
        )
        ,
        (
        X
        ,
        +
        ∞
        )
        }
        .
      
    
    {\displaystyle \mu =\{(\emptyset ,0),(X,+\infty )\}.}
  
Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be uncountable, let 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\cal {A}}}
   be a 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  -algebra on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   let 
  
    
      
        
          
            C
          
        
        =
        {
        A
        ∈
        
          
            A
          
        
        :
        A
        
           is countable
        
        }
      
    
    {\displaystyle {\cal {C}}=\{A\in {\cal {A}}:A{\text{ is countable}}\}}
   be the countable elements of 
  
    
      
        
          
            A
          
        
        ,
      
    
    {\displaystyle {\cal {A}},}
   and let 
  
    
      
        μ
        =
        
          
            C
          
        
        ×
        {
        0
        }
        ∪
        (
        
          
            A
          
        
        ∖
        
          
            C
          
        
        )
        ×
        {
        +
        ∞
        }
        .
      
    
    {\displaystyle \mu ={\cal {C}}\times \{0\}\cup ({\cal {A}}\setminus {\cal {C}})\times \{+\infty \}.}
   It can be shown that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is a measure.


==== Involved non-example ====
Measures that are not semifinite are very wild when restricted to certain sets. Every measure is, in a sense, semifinite once its 
  
    
      
        0
        −
        ∞
      
    
    {\displaystyle 0-\infty }
   part (the wild part) is taken away.

We say the 
  
    
      
        
          0
          −
          ∞
        
      
    
    {\displaystyle \mathbf {0-\infty } }
   part of 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   to mean the measure 
  
    
      
        
          μ
          
            0
            −
            ∞
          
        
      
    
    {\displaystyle \mu _{0-\infty }}
   defined in the above theorem. Here is an explicit formula for 
  
    
      
        
          μ
          
            0
            −
            ∞
          
        
      
    
    {\displaystyle \mu _{0-\infty }}
  : 
  
    
      
        
          μ
          
            0
            −
            ∞
          
        
        =
        (
        sup
        {
        μ
        (
        B
        )
        −
        
          μ
          
            sf
          
        
        (
        B
        )
        :
        B
        ∈
        
          
            P
          
        
        (
        A
        )
        ∩
        
          μ
          
            sf
          
          
            pre
          
        
        (
        
          
            R
          
          
            ≥
            0
          
        
        )
        }
        
          )
          
            A
            ∈
            
              
                A
              
            
          
        
        .
      
    
    {\displaystyle \mu _{0-\infty }=(\sup\{\mu (B)-\mu _{\text{sf}}(B):B\in {\cal {P}}(A)\cap \mu _{\text{sf}}^{\text{pre}}(\mathbb {R} _{\geq 0})\})_{A\in {\cal {A}}}.}
  


==== Results regarding semifinite measures ====
Let 
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
   be 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   or 
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   and let 
  
    
      
        T
        :
        
          L
          
            
              F
            
          
          
            ∞
          
        
        (
        μ
        )
        →
        
          
            (
            
              
                L
                
                  
                    F
                  
                
                
                  1
                
              
              (
              μ
              )
            
            )
          
          
            ∗
          
        
        :
        g
        ↦
        
          T
          
            g
          
        
        =
        
          
            (
            
              ∫
              f
              g
              d
              μ
            
            )
          
          
            f
            ∈
            
              L
              
                
                  F
                
              
              
                1
              
            
            (
            μ
            )
          
        
        .
      
    
    {\displaystyle T:L_{\mathbb {F} }^{\infty }(\mu )\to \left(L_{\mathbb {F} }^{1}(\mu )\right)^{*}:g\mapsto T_{g}=\left(\int fgd\mu \right)_{f\in L_{\mathbb {F} }^{1}(\mu )}.}
   Then 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is semifinite if and only if 
  
    
      
        T
      
    
    {\displaystyle T}
   is injective. (This result has import in the study of the dual space of 
  
    
      
        
          L
          
            1
          
        
        =
        
          L
          
            
              F
            
          
          
            1
          
        
        (
        μ
        )
      
    
    {\displaystyle L^{1}=L_{\mathbb {F} }^{1}(\mu )}
  .)
Let 
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
   be 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   or 
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   and let 
  
    
      
        
          
            T
          
        
      
    
    {\displaystyle {\cal {T}}}
   be the topology of convergence in measure on 
  
    
      
        
          L
          
            
              F
            
          
          
            0
          
        
        (
        μ
        )
        .
      
    
    {\displaystyle L_{\mathbb {F} }^{0}(\mu ).}
   Then 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is semifinite if and only if 
  
    
      
        
          
            T
          
        
      
    
    {\displaystyle {\cal {T}}}
   is Hausdorff.
(Johnson) Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a set, let 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\cal {A}}}
   be a sigma-algebra on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   let 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   be a measure on 
  
    
      
        
          
            A
          
        
        ,
      
    
    {\displaystyle {\cal {A}},}
   let 
  
    
      
        Y
      
    
    {\displaystyle Y}
   be a set, let 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\cal {B}}}
   be a sigma-algebra on 
  
    
      
        Y
        ,
      
    
    {\displaystyle Y,}
   and let 
  
    
      
        ν
      
    
    {\displaystyle \nu }
   be a measure on 
  
    
      
        
          
            B
          
        
        .
      
    
    {\displaystyle {\cal {B}}.}
   If 
  
    
      
        μ
        ,
        ν
      
    
    {\displaystyle \mu ,\nu }
   are both not a 
  
    
      
        0
        −
        ∞
      
    
    {\displaystyle 0-\infty }
   measure, then both 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   and 
  
    
      
        ν
      
    
    {\displaystyle \nu }
   are semifinite if and only if 
  
    
      
        (
        μ
        
          ×
          
            cld
          
        
        ν
        )
      
    
    {\displaystyle (\mu \times _{\text{cld}}\nu )}
  
  
    
      
        (
        A
        ×
        B
        )
        =
        μ
        (
        A
        )
        ν
        (
        B
        )
      
    
    {\displaystyle (A\times B)=\mu (A)\nu (B)}
   for all 
  
    
      
        A
        ∈
        
          
            A
          
        
      
    
    {\displaystyle A\in {\cal {A}}}
   and 
  
    
      
        B
        ∈
        
          
            B
          
        
        .
      
    
    {\displaystyle B\in {\cal {B}}.}
   (Here, 
  
    
      
        μ
        
          ×
          
            cld
          
        
        ν
      
    
    {\displaystyle \mu \times _{\text{cld}}\nu }
   is the measure defined in Theorem 39.1 in Berberian '65.)


=== Localizable measures ===
Localizable measures are a special case of semifinite measures and a generalization of sigma-finite measures.
Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a set, let 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\cal {A}}}
   be a sigma-algebra on 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   and let 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   be a measure on 
  
    
      
        
          
            A
          
        
        .
      
    
    {\displaystyle {\cal {A}}.}
  

Let 
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
   be 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   or 
  
    
      
        
          C
        
        ,
      
    
    {\displaystyle \mathbb {C} ,}
   and let 
  
    
      
        T
        :
        
          L
          
            
              F
            
          
          
            ∞
          
        
        (
        μ
        )
        →
        
          
            (
            
              
                L
                
                  
                    F
                  
                
                
                  1
                
              
              (
              μ
              )
            
            )
          
          
            ∗
          
        
        :
        g
        ↦
        
          T
          
            g
          
        
        =
        
          
            (
            
              ∫
              f
              g
              d
              μ
            
            )
          
          
            f
            ∈
            
              L
              
                
                  F
                
              
              
                1
              
            
            (
            μ
            )
          
        
        .
      
    
    {\displaystyle T:L_{\mathbb {F} }^{\infty }(\mu )\to \left(L_{\mathbb {F} }^{1}(\mu )\right)^{*}:g\mapsto T_{g}=\left(\int fgd\mu \right)_{f\in L_{\mathbb {F} }^{1}(\mu )}.}
   Then 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is localizable if and only if 
  
    
      
        T
      
    
    {\displaystyle T}
   is bijective (if and only if 
  
    
      
        
          L
          
            
              F
            
          
          
            ∞
          
        
        (
        μ
        )
      
    
    {\displaystyle L_{\mathbb {F} }^{\infty }(\mu )}
   ""is"" 
  
    
      
        
          L
          
            
              F
            
          
          
            1
          
        
        (
        μ
        
          )
          
            ∗
          
        
      
    
    {\displaystyle L_{\mathbb {F} }^{1}(\mu )^{*}}
  ).


=== s-finite measures ===

A measure is said to be s-finite if it is a countable sum of bounded measures. S-finite measures are more general than sigma-finite ones and have applications in the theory of stochastic processes.


== Non-measurable sets ==

If the axiom of choice is assumed to be true, it can be proved that not all subsets of Euclidean space are Lebesgue measurable; examples of such sets include the Vitali set, and the non-measurable sets postulated by the Hausdorff paradox and the Banach–Tarski paradox.


== Generalizations ==
For certain purposes, it is useful to have a ""measure"" whose values are not restricted to the non-negative reals or infinity. For instance, a countably additive set function with values in the (signed) real numbers is called a signed measure, while such a function with values in the complex numbers is called a complex measure. Observe, however, that complex measure is necessarily of finite variation, hence complex measures include finite signed measures but not, for example, the Lebesgue measure.
Measures that take values in Banach spaces have been studied extensively. A measure that takes values in the set of self-adjoint projections on a Hilbert space is called a projection-valued measure; these are used in functional analysis for the spectral theorem. When it is necessary to distinguish the usual measures which take non-negative values from generalizations, the term positive measure is used. Positive measures are closed under conical combination but not general linear combination, while signed measures are the linear closure of positive measures.
Another generalization is the finitely additive measure, also known as a content. This is the same as a measure except that instead of requiring countable additivity we require only finite additivity. Historically, this definition was used first. It turns out that in general, finitely additive measures are connected with notions such as Banach limits, the dual of 
  
    
      
        
          L
          
            ∞
          
        
      
    
    {\displaystyle L^{\infty }}
   and the Stone–Čech compactification. All these are linked in one way or another to the axiom of choice.  Contents remain useful in certain technical problems in geometric measure theory; this is the theory of Banach measures.
A charge is a generalization in both directions: it is a finitely additive, signed measure. (Cf. ba space for information about bounded charges, where we say a charge is bounded to mean its range its a bounded subset of R.)


== See also ==


== Notes ==


== Bibliography ==


== References ==


== External links ==

""Measure"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Tutorial: Measure Theory for Dummies"
3b8ae149e1,Plane (mathematics),"In mathematics, a plane is a two-dimensional space or flat surface that extends indefinitely. 
A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. 
When working exclusively in two-dimensional Euclidean space, the definite article is used, so the Euclidean plane refers to the whole space. 
Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional or planar space.


== Euclidean plane ==


=== Embedding in three-dimensional space ===


== Elliptic plane ==


== Projective plane ==


== Further generalizations ==
In addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.
At one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.
The plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.
Differential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.
In the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.
In the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.
In addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere tangent to the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point. This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature.
Alternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)


== Topological and differential geometric notions ==
The one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the ""north pole"" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.
The plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.


== See also ==
Affine plane
Hyperbolic plane
Geometric space


== References =="
e2c4179f8b,Lemma (mathematics),"In mathematics, informal logic and argument mapping, a lemma (plural lemmas or lemmata) is a generally minor, proven proposition which is used as a stepping stone to a larger result. For that reason, it is also known as a ""helping theorem"" or an ""auxiliary theorem"". In many cases, a lemma derives its importance from the theorem it aims to prove; however, a lemma can also turn out to be more important than originally thought. The word ""lemma"" derives from the Ancient Greek λῆμμα (""anything which is received"", such as a gift, profit, or a bribe). 


== Comparison with theorem ==
There is no formal distinction between a lemma and a theorem, only one of intention (see Theorem terminology). However, a lemma can be considered a minor result whose sole purpose is to help prove a more substantial theorem – a step in the direction of proof.


== Well-known lemmas ==
A good stepping stone can lead to many others. Some powerful results in mathematics are known as lemmas, first named for their originally minor purpose. These include, among others:

While these results originally seemed too simple or too technical to warrant independent interest, they have eventually turned out to be central to the theories in which they occur.


== See also ==


== Notes ==


== References ==


== External links ==
Doron Zeilberger, Opinion 82: A Good Lemma is Worth a Thousand TheoremsThis article incorporates material from Lemma on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License."
3289364bbc,Homology (mathematics),"In mathematics, homology is a general way of associating a sequence of algebraic objects, such as abelian groups or modules, with other mathematical objects such as topological spaces.  Homology groups were originally defined in algebraic topology.  Similar constructions are available in a wide variety of other contexts, such as abstract algebra, groups, Lie algebras, Galois theory, and algebraic geometry.
The original motivation for defining homology groups was the observation that two shapes can be distinguished by examining their holes.  For instance, a circle is not a disk because the circle has a hole through it while the disk is solid, and the ordinary sphere is not a circle because the sphere encloses a two-dimensional hole while the circle encloses a one-dimensional hole.  However, because a hole is ""not there"", it is not immediately obvious how to define a hole or how to distinguish different kinds of holes.  Homology was originally a rigorous mathematical method for defining and categorizing holes in a manifold.  Loosely speaking, a cycle is a closed submanifold, a boundary is a cycle which is also the boundary of a submanifold, and a homology class (which represents a hole) is an equivalence class of cycles modulo boundaries. A homology class is thus represented by a cycle which is not the boundary of any submanifold: the cycle represents a hole, namely a hypothetical manifold whose boundary would be that cycle, but which is ""not there"".
There are many different homology theories.  A particular type of mathematical object, such as a topological space or a group, may have one or more associated homology theories.  When the underlying object has a geometric interpretation as topological spaces do, the nth homology group represents behavior in dimension n.  Most homology groups or modules may be formulated as derived functors on appropriate abelian categories, measuring the failure of a functor to be exact.  From this abstract perspective, homology groups are determined by objects of a derived category.


== Background ==


=== Origins ===
Homology theory can be said to start with the Euler polyhedron formula, or Euler characteristic. This was followed by Riemann's definition of genus and n-fold connectedness numerical invariants in 1857 and Betti's proof in 1871 of the independence of ""homology numbers"" from the choice of basis.Homology itself was developed as a way to analyse and classify manifolds according to their cycles – closed loops (or more generally submanifolds) that can be drawn on a given n dimensional manifold but not continuously deformed into each other. These cycles are also sometimes thought of as cuts which can be glued back together, or as zippers which can be fastened and unfastened. Cycles are classified by dimension. For example, a line drawn on a surface represents a 1-cycle, a closed loop or 
  
    
      
        
          S
          
            1
          
        
      
    
    {\displaystyle S^{1}}
   (1-manifold), while a surface cut through a three-dimensional manifold is a 2-cycle.


=== Surfaces ===

On the ordinary sphere 
  
    
      
        
          S
          
            2
          
        
      
    
    {\displaystyle S^{2}}
  , the cycle b in the diagram can be shrunk to the pole, and even the equatorial great circle a can be shrunk in the same way. The Jordan curve theorem shows that any arbitrary cycle such as c can be similarly shrunk to a point. All cycles on the sphere can therefore be continuously transformed into each other and belong to the same homology class. They are said to be homologous to zero. Cutting a manifold along a cycle homologous to zero separates the manifold into two or more components. For example, cutting the sphere along a produces two hemispheres.
This is not generally true of cycles on other surfaces. The torus 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T^{2}}
   has cycles which cannot be continuously deformed into each other, for example in the diagram none of the cycles a, b or c can be deformed into one another. In particular, cycles a and b cannot be shrunk to a point whereas cycle c can, thus making it homologous to zero.
If the torus surface is cut along both a and b, it can be opened out and flattened into a rectangle or, more conveniently, a square. One opposite pair of sides represents the cut along a, and the other opposite pair represents the cut along b.
The edges of the square may then be glued back together in different ways. The square can be twisted to allow edges to meet in the opposite direction, as shown by the arrows in the diagram. Up to symmetry, there are four distinct ways of gluing the sides, each creating a different surface:

  
    
      
        
          K
          
            2
          
        
      
    
    {\displaystyle K^{2}}
   is the Klein bottle, which is a torus with a twist in it (In the square diagram, the twist can be seen as the reversal of the bottom arrow). It is a theorem that the re-glued surface must self-intersect (when immersed in Euclidean 3-space). Like the torus, cycles a and b cannot be shrunk while c can be. But unlike the torus, following b forwards right round and back reverses left and right, because b happens to cross over the twist given to one join. If an equidistant cut on one side of b is made, it returns on the other side and goes round the surface a second time before returning to its starting point, cutting out a twisted Möbius strip. Because local left and right can be arbitrarily re-oriented in this way, the surface as a whole is said to be non-orientable.
The projective plane 
  
    
      
        
          P
          
            2
          
        
      
    
    {\displaystyle P^{2}}
   has both joins twisted. The uncut form, generally represented as the Boy surface, is visually complex, so a hemispherical embedding is shown in the diagram, in which antipodal points around the rim such as A and A′ are identified as the same point. Again, a and b are non-shrinkable while c is. But this time, both a and b reverse left and right.
Cycles can be joined or added together, as a and b on the torus were when it was cut open and flattened down. In the Klein bottle diagram, a goes round one way and −a goes round the opposite way. If a is thought of as a cut, then  −a can be thought of as a gluing operation. Making a cut and then re-gluing it does not change the surface, so a + (−a) = 0.
But now consider two a-cycles. Since the Klein bottle is nonorientable, you can transport one of them all the way round the bottle (along the b-cycle), and it will come back as −a. This is because the Klein bottle is made from a cylinder, whose a-cycle ends are glued together with opposite orientations. Hence 2a = a + a = a + (−a) = 0. This phenomenon is called torsion. Similarly, in the projective plane, following the unshrinkable cycle b round twice remarkably creates a trivial cycle which can be shrunk to a point; that is, b + b = 0. Because b must be followed around twice to achieve a zero cycle, the surface is said to have a torsion coefficient of 2. However, following a b-cycle around twice in the Klein bottle gives simply b + b = 2b, since this cycle lives in a torsion-free homology class. This corresponds to the fact that in the fundamental polygon of the Klein bottle, only one pair of sides is glued with a twist, whereas in the projective plane both sides are twisted.
A square is a contractible topological space, which implies that it has trivial homology.  Consequently, additional cuts disconnect it. The square is not the only shape in the plane that can be glued into a surface.  Gluing opposite sides of an octagon, for example, produces a surface with two holes.  In fact, all closed surfaces can be produced by gluing the sides of some polygon and all even-sided polygons (2n-gons) can be glued to make different manifolds. Conversely, a closed surface with n non-zero classes can be cut into a 2n-gon. Variations are also possible, for example a hexagon may also be glued to form a torus.The first recognisable theory of homology was published by Henri Poincaré in his seminal paper ""Analysis situs"", J. Ecole polytech. (2) 1. 1–121 (1895). The paper introduced homology classes and relations. The possible configurations of orientable cycles are classified by the Betti numbers of the manifold (Betti numbers are a refinement of the Euler characteristic).  Classifying the non-orientable cycles requires additional information about torsion coefficients.The complete classification of 1- and 2-manifolds is given in the table.

Notes
For a non-orientable surface, a hole is equivalent to two cross-caps.
Any 2-manifold is the connected sum of g tori and c projective planes. For the sphere 
  
    
      
        
          S
          
            2
          
        
      
    
    {\displaystyle S^{2}}
  , g = c = 0.


=== Generalization ===
A manifold with boundary or open manifold is topologically distinct from a closed manifold and can be created by making a cut in any suitable closed manifold. For example the disk or 2-ball 
  
    
      
        
          B
          
            2
          
        
      
    
    {\displaystyle B^{2}}
   is bounded by a circle 
  
    
      
        
          S
          
            1
          
        
      
    
    {\displaystyle S^{1}}
  . It may be created by cutting a trivial cycle in any 2-manifold and keeping the piece removed, by piercing the sphere and stretching the puncture wide, or by cutting the projective plane. It can also be seen as filling-in the circle in the plane.
When two cycles can be continuously deformed into each other, then cutting along one produces the same shape as cutting along the other, up to some bending and stretching.  In this case the two cycles are said to be homologous or to lie in the same homology class.  Additionally, if one cycle can be continuously deformed into a combination of other cycles, then cutting along the initial cycle is the same as cutting along the combination of other cycles.  For example, cutting along a figure 8 is equivalent to cutting along its two lobes.  In this case, the figure 8 is said to be homologous to the sum of its lobes.
Two open manifolds with similar boundaries (up to some bending and stretching) may be glued together to form a new manifold which is their connected sum.
This geometric analysis of manifolds is not rigorous. In a search for increased rigour, Poincaré went on to develop the simplicial homology of a triangulated manifold and to create what is now called a chain complex.  These chain complexes (since greatly generalized) form the basis for most modern treatments of homology.
In such treatments a cycle need not be continuous: a 0-cycle is a set of points, and cutting along this cycle corresponds to puncturing the manifold.  A 1-cycle corresponds to a set of closed loops (an image of the 1-manifold 
  
    
      
        
          S
          
            1
          
        
      
    
    {\displaystyle S^{1}}
  ).  On a surface, cutting along a 1-cycle yields either disconnected pieces or a simpler shape.  A 2-cycle corresponds to a collection of embedded surfaces such as a sphere or a torus, and so on.
Emmy Noether and, independently, Leopold Vietoris and Walther Mayer further developed the theory of algebraic homology groups in the period 1925–28. The new combinatorial topology formally treated topological classes as abelian groups.  Homology groups are finitely generated abelian groups, and homology classes are elements of these groups. The Betti numbers of the manifold are the rank of the free part of the homology group, and the non-orientable cycles are described by the torsion part.
The subsequent spread of homology groups brought a change of terminology and viewpoint from ""combinatorial topology"" to ""algebraic topology"".  Algebraic homology remains the primary method of classifying manifolds.


== Informal examples ==
The homology of a topological space X is a set of topological invariants of X represented by its homology groups

where the 
  
    
      
        
          k
          
            
              t
              h
            
          
        
      
    
    {\displaystyle k^{\rm {th}}}
   homology group 
  
    
      
        
          H
          
            k
          
        
        (
        X
        )
      
    
    {\displaystyle H_{k}(X)}
   describes, informally, the number of holes in X with a k-dimensional boundary. A 0-dimensional-boundary hole is simply a gap between two components. Consequently, 
  
    
      
        
          H
          
            0
          
        
        (
        X
        )
      
    
    {\displaystyle H_{0}(X)}
   describes the path-connected components of X.

A one-dimensional sphere 
  
    
      
        
          S
          
            1
          
        
      
    
    {\displaystyle S^{1}}
   is a circle. It has a single connected component and a one-dimensional-boundary hole, but no higher-dimensional holes. The corresponding homology groups are given as

where 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   is the group of integers and 
  
    
      
        {
        0
        }
      
    
    {\displaystyle \{0\}}
   is the trivial group. The group 
  
    
      
        
          H
          
            1
          
        
        
          (
          
            S
            
              1
            
          
          )
        
        =
        
          Z
        
      
    
    {\displaystyle H_{1}\left(S^{1}\right)=\mathbb {Z} }
   represents a finitely-generated abelian group, with a single generator representing the one-dimensional hole contained in a circle.A two-dimensional sphere 
  
    
      
        
          S
          
            2
          
        
      
    
    {\displaystyle S^{2}}
   has a single connected component, no one-dimensional-boundary holes, a two-dimensional-boundary hole, and no higher-dimensional holes. The corresponding homology groups are
In general for an n-dimensional sphere 
  
    
      
        
          S
          
            n
          
        
        ,
      
    
    {\displaystyle S^{n},}
  the homology groups are

A two-dimensional ball 
  
    
      
        
          B
          
            2
          
        
      
    
    {\displaystyle B^{2}}
   is a solid disc. It has a single path-connected component, but in contrast to the circle, has no higher-dimensional holes. The corresponding homology groups are all trivial except for 
  
    
      
        
          H
          
            0
          
        
        
          (
          
            B
            
              2
            
          
          )
        
        =
        
          Z
        
      
    
    {\displaystyle H_{0}\left(B^{2}\right)=\mathbb {Z} }
  . In general, for an n-dimensional ball 
  
    
      
        
          B
          
            n
          
        
        ,
      
    
    {\displaystyle B^{n},}
  
The torus is defined as a product of two circles 
  
    
      
        T
        =
        
          S
          
            1
          
        
        ×
        
          S
          
            1
          
        
      
    
    {\displaystyle T=S^{1}\times S^{1}}
  . The torus has a single path-connected component, two independent one-dimensional holes (indicated by circles in red and blue) and one two-dimensional hole as the interior of the torus. The corresponding homology groups are
The two independent 1-dimensional holes form independent generators in a finitely-generated abelian group, expressed as the product group 
  
    
      
        
          Z
        
        ×
        
          Z
        
        .
      
    
    {\displaystyle \mathbb {Z} \times \mathbb {Z} .}
   
For the projective plane P, a simple computation shows (where 
  
    
      
        
          
            Z
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {Z} _{2}}
   is the cyclic group of order 2):

  
    
      
        
          H
          
            0
          
        
        (
        P
        )
        =
        
          Z
        
      
    
    {\displaystyle H_{0}(P)=\mathbb {Z} }
   corresponds, as in the previous examples, to the fact that there is a single connected component. 
  
    
      
        
          H
          
            1
          
        
        (
        P
        )
        =
        
          
            Z
          
          
            2
          
        
      
    
    {\displaystyle H_{1}(P)=\mathbb {Z} _{2}}
   is a new phenomenon: intuitively, it corresponds to the fact that there is a single non-contractible ""loop"", but if we do the loop twice, it becomes contractible to zero. This phenomenon is called torsion.


== Construction of homology groups ==
The following text describes a general algorithm for constructing the homology groups. It may be easier for the reader to look at some simple examples first: graph homology and simplicial homology.
The general construction begins with an object such as a topological space X, on which one first defines a chain complex C(X) encoding information about X. A chain complex is a sequence of abelian groups or modules 
  
    
      
        
          C
          
            0
          
        
        ,
        
          C
          
            1
          
        
        ,
        
          C
          
            2
          
        
        ,
        …
      
    
    {\displaystyle C_{0},C_{1},C_{2},\ldots }
  . connected by homomorphisms 
  
    
      
        
          ∂
          
            n
          
        
        :
        
          C
          
            n
          
        
        →
        
          C
          
            n
            −
            1
          
        
        ,
      
    
    {\displaystyle \partial _{n}:C_{n}\to C_{n-1},}
   which are called boundary operators. That is,

  
    
      
        ⋯
        
          
            
              ⟶
              
            
            
              ∂
              
                n
                +
                1
              
            
          
        
        
          C
          
            n
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                n
              
            
          
        
        
          C
          
            n
            −
            1
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                n
                −
                1
              
            
          
        
        ⋯
        
          
            
              ⟶
              
            
            
              ∂
              
                2
              
            
          
        
        
          C
          
            1
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                1
              
            
          
        
        
          C
          
            0
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                0
              
            
          
        
        0
      
    
    {\displaystyle \dotsb {\overset {\partial _{n+1}}{\longrightarrow \,}}C_{n}{\overset {\partial _{n}}{\longrightarrow \,}}C_{n-1}{\overset {\partial _{n-1}}{\longrightarrow \,}}\dotsb {\overset {\partial _{2}}{\longrightarrow \,}}C_{1}{\overset {\partial _{1}}{\longrightarrow \,}}C_{0}{\overset {\partial _{0}}{\longrightarrow \,}}0}
  where 0 denotes the trivial group and 
  
    
      
        
          C
          
            i
          
        
        ≡
        0
      
    
    {\displaystyle C_{i}\equiv 0}
   for i < 0. It is also required that the composition of any two consecutive boundary operators be trivial. That is, for all n,

  
    
      
        
          ∂
          
            n
          
        
        ∘
        
          ∂
          
            n
            +
            1
          
        
        =
        
          0
          
            n
            +
            1
            ,
            n
            −
            1
          
        
        ,
      
    
    {\displaystyle \partial _{n}\circ \partial _{n+1}=0_{n+1,n-1},}
  i.e., the constant map sending every element of 
  
    
      
        
          C
          
            n
            +
            1
          
        
      
    
    {\displaystyle C_{n+1}}
   to the group identity in 
  
    
      
        
          C
          
            n
            −
            1
          
        
        .
      
    
    {\displaystyle C_{n-1}.}
  
The statement that the boundary of a boundary is trivial is equivalent to the statement that 
  
    
      
        
          i
          m
        
        (
        
          ∂
          
            n
            +
            1
          
        
        )
        ⊆
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle \mathrm {im} (\partial _{n+1})\subseteq \ker(\partial _{n})}
  , where 
  
    
      
        
          i
          m
        
        (
        
          ∂
          
            n
            +
            1
          
        
        )
      
    
    {\displaystyle \mathrm {im} (\partial _{n+1})}
   denotes the image of the boundary operator and 
  
    
      
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle \ker(\partial _{n})}
   its kernel. Elements of 
  
    
      
        
          B
          
            n
          
        
        (
        X
        )
        =
        
          i
          m
        
        (
        
          ∂
          
            n
            +
            1
          
        
        )
      
    
    {\displaystyle B_{n}(X)=\mathrm {im} (\partial _{n+1})}
   are called boundaries and elements of 
  
    
      
        
          Z
          
            n
          
        
        (
        X
        )
        =
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle Z_{n}(X)=\ker(\partial _{n})}
   are called cycles.
Since each chain group Cn is abelian all its subgroups are normal. Then because 
  
    
      
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle \ker(\partial _{n})}
   is a subgroup of Cn, 
  
    
      
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle \ker(\partial _{n})}
   is abelian, and since 
  
    
      
        
          i
          m
        
        (
        
          ∂
          
            n
            +
            1
          
        
        )
        ⊆
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle \mathrm {im} (\partial _{n+1})\subseteq \ker(\partial _{n})}
   therefore 
  
    
      
        
          i
          m
        
        (
        
          ∂
          
            n
            +
            1
          
        
        )
      
    
    {\displaystyle \mathrm {im} (\partial _{n+1})}
   is a normal subgroup of 
  
    
      
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
      
    
    {\displaystyle \ker(\partial _{n})}
  . Then one can create the quotient group

  
    
      
        
          H
          
            n
          
        
        (
        X
        )
        :=
        ker
        ⁡
        (
        
          ∂
          
            n
          
        
        )
        
          /
        
        
          i
          m
        
        (
        
          ∂
          
            n
            +
            1
          
        
        )
        =
        
          Z
          
            n
          
        
        (
        X
        )
        
          /
        
        
          B
          
            n
          
        
        (
        X
        )
        ,
      
    
    {\displaystyle H_{n}(X):=\ker(\partial _{n})/\mathrm {im} (\partial _{n+1})=Z_{n}(X)/B_{n}(X),}
  called the nth homology group of X. The elements of Hn(X) are called homology classes. Each homology class is an equivalence class over cycles and two cycles in the same homology class are said to be homologous.A chain complex is said to be exact if the image of the (n+1)th map is always equal to the kernel of the nth map. The homology groups of X therefore measure ""how far"" the chain complex associated to X is from being exact.The reduced homology groups of a chain complex C(X) are defined as homologies of the augmented chain complex

  
    
      
        ⋯
        
          
            
              ⟶
              
            
            
              ∂
              
                n
                +
                1
              
            
          
        
        
          C
          
            n
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                n
              
            
          
        
        
          C
          
            n
            −
            1
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                n
                −
                1
              
            
          
        
        ⋯
        
          
            
              ⟶
              
            
            
              ∂
              
                2
              
            
          
        
        
          C
          
            1
          
        
        
          
            
              ⟶
              
            
            
              ∂
              
                1
              
            
          
        
        
          C
          
            0
          
        
        
          
            
              ⟶
              
            
            ϵ
          
        
        
          Z
        
        
          ⟶
          
        
        0
      
    
    {\displaystyle \dotsb {\overset {\partial _{n+1}}{\longrightarrow \,}}C_{n}{\overset {\partial _{n}}{\longrightarrow \,}}C_{n-1}{\overset {\partial _{n-1}}{\longrightarrow \,}}\dotsb {\overset {\partial _{2}}{\longrightarrow \,}}C_{1}{\overset {\partial _{1}}{\longrightarrow \,}}C_{0}{\overset {\epsilon }{\longrightarrow \,}}\mathbb {Z} {\longrightarrow \,}0}
  where the boundary operator 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   is

  
    
      
        ϵ
        
          (
          
            
              ∑
              
                i
              
            
            
              n
              
                i
              
            
            
              σ
              
                i
              
            
          
          )
        
        =
        
          ∑
          
            i
          
        
        
          n
          
            i
          
        
      
    
    {\displaystyle \epsilon \left(\sum _{i}n_{i}\sigma _{i}\right)=\sum _{i}n_{i}}
  for a combination 
  
    
      
        ∑
        
          n
          
            i
          
        
        
          σ
          
            i
          
        
        ,
      
    
    {\displaystyle \sum n_{i}\sigma _{i},}
   of points 
  
    
      
        
          σ
          
            i
          
        
        ,
      
    
    {\displaystyle \sigma _{i},}
   which are the fixed generators of C0. The reduced homology groups 
  
    
      
        
          
            
              
                H
                ~
              
            
          
          
            i
          
        
        (
        X
        )
      
    
    {\displaystyle {\tilde {H}}_{i}(X)}
   coincide with 
  
    
      
        
          H
          
            i
          
        
        (
        X
        )
      
    
    {\displaystyle H_{i}(X)}
   for 
  
    
      
        i
        ≠
        0.
      
    
    {\displaystyle i\neq 0.}
   The extra 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   in the chain complex represents the unique map 
  
    
      
        [
        ∅
        ]
        ⟶
        X
      
    
    {\displaystyle [\emptyset ]\longrightarrow X}
   from the empty simplex to X.
Computing the cycle 
  
    
      
        
          Z
          
            n
          
        
        (
        X
        )
      
    
    {\displaystyle Z_{n}(X)}
   and boundary 
  
    
      
        
          B
          
            n
          
        
        (
        X
        )
      
    
    {\displaystyle B_{n}(X)}
   groups is usually rather difficult since they have a very large number of generators. On the other hand, there are tools which make the task easier.
The simplicial homology groups Hn(X) of a simplicial complex X are defined using the simplicial chain complex C(X), with Cn(X) the free abelian group generated by the n-simplices of X. See simplicial homology for details.
The singular homology groups Hn(X) are defined for any topological space X, and agree with the simplicial homology groups for a simplicial complex.
Cohomology groups are formally similar to homology groups: one starts with a cochain complex, which is the same as a chain complex but whose arrows, now denoted 
  
    
      
        
          d
          
            n
          
        
        ,
      
    
    {\displaystyle d_{n},}
   point in the direction of increasing n rather than decreasing n; then the groups 
  
    
      
        ker
        ⁡
        
          (
          
            d
            
              n
            
          
          )
        
        =
        
          Z
          
            n
          
        
        (
        X
        )
      
    
    {\displaystyle \ker \left(d^{n}\right)=Z^{n}(X)}
   of cocycles and 
  
    
      
        
          i
          m
        
        
          (
          
            d
            
              n
              −
              1
            
          
          )
        
        =
        
          B
          
            n
          
        
        (
        X
        )
      
    
    {\displaystyle \mathrm {im} \left(d^{n-1}\right)=B^{n}(X)}
   of coboundaries follow from the same description. The nth cohomology group of X is then the quotient group

  
    
      
        
          H
          
            n
          
        
        (
        X
        )
        =
        
          Z
          
            n
          
        
        (
        X
        )
        
          /
        
        
          B
          
            n
          
        
        (
        X
        )
        ,
      
    
    {\displaystyle H^{n}(X)=Z^{n}(X)/B^{n}(X),}
  in analogy with the nth homology group.


== Homology vs. homotopy ==
Homotopy groups are similar to homology groups in that they can represent ""holes"" in a topological space. There is a close connection between the first homotopy group 
  
    
      
        
          π
          
            1
          
        
        (
        X
        )
      
    
    {\displaystyle \pi _{1}(X)}
   and the first homology group 
  
    
      
        
          H
          
            1
          
        
        (
        X
        )
      
    
    {\displaystyle H_{1}(X)}
  : the latter is the abelianization of the former. Hence, it is said that ""homology is a commutative alternative to homotopy"".: 4:00  The higher homotopy groups are abelian and are related to homology groups by the Hurewicz theorem, but can be vastly more complicated. For instance, the homotopy groups of spheres are poorly understood and are not known in general, in contrast to the straightforward description given above for the homology groups.
As an example, let X be the figure eight. Its first homotopy group 
  
    
      
        
          π
          
            1
          
        
        (
        X
        )
      
    
    {\displaystyle \pi _{1}(X)}
   is the group of directed loops starting and ending at a predetermined point (e.g. its center). It is equivalent to the free group of rank 2, which is not commutative: looping around the leftmost cycle and then around the rightmost cycle is different than looping around the rightmost cycle and then looping around the leftmost cycle. In contrast, its first homology group 
  
    
      
        
          H
          
            1
          
        
        (
        X
        )
      
    
    {\displaystyle H_{1}(X)}
   is the group of cuts made in a surface. This group is commutative, since (informally) cutting the leftmost cycle and then the rightmost cycle leads to the same result as cutting the rightmost cycle and then the leftmost cycle.


== Types of homology ==
The different types of homology theory arise from functors mapping from various categories of mathematical objects to the category of chain complexes. In each case the composition of the functor from objects to chain complexes and the functor from chain complexes to homology groups defines the overall homology functor for the theory.


=== Simplicial homology ===

The motivating example comes from algebraic topology: the simplicial homology of a simplicial complex X. Here the chain group Cn is the free abelian group or module whose generators are the n-dimensional oriented simplexes of X. The orientation is captured by ordering the complex's vertices and expressing an oriented simplex 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   as an n-tuple 
  
    
      
        (
        σ
        [
        0
        ]
        ,
        σ
        [
        1
        ]
        ,
        …
        ,
        σ
        [
        n
        ]
        )
      
    
    {\displaystyle (\sigma [0],\sigma [1],\dots ,\sigma [n])}
   of its vertices listed in increasing order (i.e. 
  
    
      
        σ
        [
        0
        ]
        <
        σ
        [
        1
        ]
        <
        ⋯
        <
        σ
        [
        n
        ]
      
    
    {\displaystyle \sigma [0]<\sigma [1]<\cdots <\sigma [n]}
   in the complex's vertex ordering, where 
  
    
      
        σ
        [
        i
        ]
      
    
    {\displaystyle \sigma [i]}
   is the 
  
    
      
        i
      
    
    {\displaystyle i}
  th vertex appearing in the tuple). The mapping 
  
    
      
        
          ∂
          
            n
          
        
      
    
    {\displaystyle \partial _{n}}
   from Cn to Cn−1 is called the boundary mapping and sends the simplex

  
    
      
        σ
        =
        (
        σ
        [
        0
        ]
        ,
        σ
        [
        1
        ]
        ,
        …
        ,
        σ
        [
        n
        ]
        )
      
    
    {\displaystyle \sigma =(\sigma [0],\sigma [1],\dots ,\sigma [n])}
  to the formal sum

  
    
      
        
          ∂
          
            n
          
        
        (
        σ
        )
        =
        
          ∑
          
            i
            =
            0
          
          
            n
          
        
        (
        −
        1
        
          )
          
            i
          
        
        
          (
          
            σ
            [
            0
            ]
            ,
            …
            ,
            σ
            [
            i
            −
            1
            ]
            ,
            σ
            [
            i
            +
            1
            ]
            ,
            …
            ,
            σ
            [
            n
            ]
          
          )
        
        ,
      
    
    {\displaystyle \partial _{n}(\sigma )=\sum _{i=0}^{n}(-1)^{i}\left(\sigma [0],\dots ,\sigma [i-1],\sigma [i+1],\dots ,\sigma [n]\right),}
  which is considered 0 if 
  
    
      
        n
        =
        0.
      
    
    {\displaystyle n=0.}
   This behavior on the generators induces a homomorphism on all of Cn as follows. Given an element 
  
    
      
        c
        ∈
        
          C
          
            n
          
        
      
    
    {\displaystyle c\in C_{n}}
  , write it as the sum of generators 
  
    
      
        c
        =
        
          ∑
          
            
              σ
              
                i
              
            
            ∈
            
              X
              
                n
              
            
          
        
        
          m
          
            i
          
        
        
          σ
          
            i
          
        
        ,
      
    
    {\textstyle c=\sum _{\sigma _{i}\in X_{n}}m_{i}\sigma _{i},}
   where 
  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
   is the set of n-simplexes in X and the mi are coefficients from the ring Cn is defined over (usually integers, unless otherwise specified). Then define

  
    
      
        
          ∂
          
            n
          
        
        (
        c
        )
        =
        
          ∑
          
            
              σ
              
                i
              
            
            ∈
            
              X
              
                n
              
            
          
        
        
          m
          
            i
          
        
        
          ∂
          
            n
          
        
        (
        
          σ
          
            i
          
        
        )
        .
      
    
    {\displaystyle \partial _{n}(c)=\sum _{\sigma _{i}\in X_{n}}m_{i}\partial _{n}(\sigma _{i}).}
  The dimension of the n-th homology of X turns out to be the number of ""holes"" in X at dimension n. It may be computed by putting matrix representations of these boundary mappings in Smith normal form.


=== Singular homology ===

Using simplicial homology example as a model, one can define a singular homology for any topological space X. A chain complex for X is defined by taking Cn to be the free abelian group (or free module) whose generators are all continuous maps from n-dimensional simplices into X. The homomorphisms ∂n arise from the boundary maps of simplices.


=== Group homology ===

In abstract algebra, one uses homology to define derived functors, for example the Tor functors. Here one starts with some covariant additive functor F and some module X. The chain complex for X is defined as follows: first find a free module 
  
    
      
        
          F
          
            1
          
        
      
    
    {\displaystyle F_{1}}
   and a surjective homomorphism 
  
    
      
        
          p
          
            1
          
        
        :
        
          F
          
            1
          
        
        →
        X
        .
      
    
    {\displaystyle p_{1}:F_{1}\to X.}
   Then one finds a free module 
  
    
      
        
          F
          
            2
          
        
      
    
    {\displaystyle F_{2}}
   and a surjective homomorphism 
  
    
      
        
          p
          
            2
          
        
        :
        
          F
          
            2
          
        
        →
        ker
        ⁡
        
          (
          
            p
            
              1
            
          
          )
        
        .
      
    
    {\displaystyle p_{2}:F_{2}\to \ker \left(p_{1}\right).}
   Continuing in this fashion, a sequence of free modules 
  
    
      
        
          F
          
            n
          
        
      
    
    {\displaystyle F_{n}}
   and homomorphisms 
  
    
      
        
          p
          
            n
          
        
      
    
    {\displaystyle p_{n}}
   can be defined. By applying the functor F to this sequence, one obtains a chain complex; the homology 
  
    
      
        
          H
          
            n
          
        
      
    
    {\displaystyle H_{n}}
   of this complex depends only on F and X and is, by definition, the n-th derived functor of F, applied to X.
A common use of group (co)homology 
  
    
      
        
          H
          
            2
          
        
        (
        G
        ,
        M
        )
      
    
    {\displaystyle H^{2}(G,M)}
  is to classify the possible extension groups E which contain a given G-module M as a normal subgroup and have a given quotient group G, so that 
  
    
      
        G
        =
        E
        
          /
        
        M
        .
      
    
    {\displaystyle G=E/M.}
  


=== Other homology theories ===


== Homology functors ==
Chain complexes form a category: A morphism from the chain complex (
  
    
      
        
          d
          
            n
          
        
        :
        
          A
          
            n
          
        
        →
        
          A
          
            n
            −
            1
          
        
      
    
    {\displaystyle d_{n}:A_{n}\to A_{n-1}}
  ) to the chain complex (
  
    
      
        
          e
          
            n
          
        
        :
        
          B
          
            n
          
        
        →
        
          B
          
            n
            −
            1
          
        
      
    
    {\displaystyle e_{n}:B_{n}\to B_{n-1}}
  ) is a sequence of homomorphisms 
  
    
      
        
          f
          
            n
          
        
        :
        
          A
          
            n
          
        
        →
        
          B
          
            n
          
        
      
    
    {\displaystyle f_{n}:A_{n}\to B_{n}}
   such that 
  
    
      
        
          f
          
            n
            −
            1
          
        
        ∘
        
          d
          
            n
          
        
        =
        
          e
          
            n
          
        
        ∘
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{n-1}\circ d_{n}=e_{n}\circ f_{n}}
   for all n. The n-th homology Hn can be viewed as a covariant functor from the category of chain complexes to the category of abelian groups (or modules).
If the chain complex depends on the object X in a covariant manner (meaning that any morphism 
  
    
      
        X
        →
        Y
      
    
    {\displaystyle X\to Y}
   induces a morphism from the chain complex of  X  to the chain complex of Y), then the Hn are covariant functors from the category that X belongs to into the category of abelian groups (or modules).
The only difference between homology and cohomology is that in cohomology the chain complexes depend in a contravariant manner on X, and that therefore the homology groups (which are called cohomology groups in this context and denoted by Hn) form contravariant functors from the category that X belongs to into the category of abelian groups or modules.


== Properties ==
If (
  
    
      
        
          d
          
            n
          
        
        :
        
          A
          
            n
          
        
        →
        
          A
          
            n
            −
            1
          
        
      
    
    {\displaystyle d_{n}:A_{n}\to A_{n-1}}
  ) is a chain complex such that all but finitely many An are zero, and the others are finitely generated abelian groups (or finite-dimensional vector spaces), then we can define the Euler characteristic

  
    
      
        χ
        =
        ∑
        (
        −
        1
        
          )
          
            n
          
        
        
        
          r
          a
          n
          k
        
        (
        
          A
          
            n
          
        
        )
      
    
    {\displaystyle \chi =\sum (-1)^{n}\,\mathrm {rank} (A_{n})}
  (using the rank in the case of abelian groups and the Hamel dimension in the case of vector spaces). It turns out that the Euler characteristic can also be computed on the level of homology:

  
    
      
        χ
        =
        ∑
        (
        −
        1
        
          )
          
            n
          
        
        
        
          r
          a
          n
          k
        
        (
        
          H
          
            n
          
        
        )
      
    
    {\displaystyle \chi =\sum (-1)^{n}\,\mathrm {rank} (H_{n})}
  and, especially in algebraic topology, this provides two ways to compute the important invariant 
  
    
      
        χ
      
    
    {\displaystyle \chi }
   for the object X which gave rise to the chain complex.
Every short exact sequence

  
    
      
        0
        →
        A
        →
        B
        →
        C
        →
        0
      
    
    {\displaystyle 0\rightarrow A\rightarrow B\rightarrow C\rightarrow 0}
  of chain complexes gives rise to a long exact sequence of homology groups

  
    
      
        ⋯
        →
        
          H
          
            n
          
        
        (
        A
        )
        →
        
          H
          
            n
          
        
        (
        B
        )
        →
        
          H
          
            n
          
        
        (
        C
        )
        →
        
          H
          
            n
            −
            1
          
        
        (
        A
        )
        →
        
          H
          
            n
            −
            1
          
        
        (
        B
        )
        →
        
          H
          
            n
            −
            1
          
        
        (
        C
        )
        →
        
          H
          
            n
            −
            2
          
        
        (
        A
        )
        →
        ⋯
      
    
    {\displaystyle \cdots \to H_{n}(A)\to H_{n}(B)\to H_{n}(C)\to H_{n-1}(A)\to H_{n-1}(B)\to H_{n-1}(C)\to H_{n-2}(A)\to \cdots }
  All maps in this long exact sequence are induced by the maps between the chain complexes, except for the maps 
  
    
      
        
          H
          
            n
          
        
        (
        C
        )
        →
        
          H
          
            n
            −
            1
          
        
        (
        A
        )
      
    
    {\displaystyle H_{n}(C)\to H_{n-1}(A)}
   The latter are called connecting homomorphisms and are provided by the zig-zag lemma.  This lemma can be applied to homology in numerous ways that aid in calculating homology groups, such as the theories of relative homology and Mayer-Vietoris sequences.


== Applications ==


=== Application in pure mathematics ===
Notable theorems proved using homology include the following:

The Brouwer fixed point theorem: If f is any continuous map from the ball Bn to itself, then there is a fixed point 
  
    
      
        a
        ∈
        
          B
          
            n
          
        
      
    
    {\displaystyle a\in B^{n}}
   with 
  
    
      
        f
        (
        a
        )
        =
        a
        .
      
    
    {\displaystyle f(a)=a.}
  
Invariance of domain: If U is an open subset of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   and 
  
    
      
        f
        :
        U
        →
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle f:U\to \mathbb {R} ^{n}}
   is an injective continuous map, then 
  
    
      
        V
        =
        f
        (
        U
        )
      
    
    {\displaystyle V=f(U)}
   is open and f is a homeomorphism between U and V.
The Hairy ball theorem: any continuous vector field on the 2-sphere (or more generally, the 2k-sphere for any 
  
    
      
        k
        ≥
        1
      
    
    {\displaystyle k\geq 1}
  ) vanishes at some point.
The Borsuk–Ulam theorem: any continuous function from an n-sphere into Euclidean n-space maps some pair of antipodal points to the same point. (Two points on a sphere are called antipodal if they are in exactly opposite directions from the sphere's center.)
Invariance of dimension: if non-empty open subsets 
  
    
      
        U
        ⊆
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle U\subseteq \mathbb {R} ^{m}}
   and 
  
    
      
        V
        ⊆
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle V\subseteq \mathbb {R} ^{n}}
   are homeomorphic, then 
  
    
      
        m
        =
        n
        .
      
    
    {\displaystyle m=n.}
  


=== Application in science and engineering ===
In topological data analysis, data sets are regarded as a point cloud sampling of a manifold or algebraic variety embedded in Euclidean space. By linking nearest neighbor points in the cloud into a triangulation, a simplicial approximation of the manifold is created and its simplicial homology may be calculated. Finding techniques to robustly calculate homology using various triangulation strategies over multiple length scales is the topic of persistent homology.In sensor networks, sensors may communicate information via an ad-hoc network that dynamically changes in time. To understand the global context of this set of local measurements and communication paths, it is useful to compute the homology of the network topology to evaluate, for instance, holes in coverage.In dynamical systems theory in physics, Poincaré was one of the first to consider the interplay between the invariant manifold of a dynamical system and its topological invariants. Morse theory relates the dynamics of a gradient flow on a manifold to, for example, its homology. Floer homology extended this to infinite-dimensional manifolds. The KAM theorem established that periodic orbits can follow complex trajectories; in particular, they may form braids that can be investigated using Floer homology.In one class of finite element methods, boundary-value problems for differential equations involving the Hodge-Laplace operator may need to be solved on topologically nontrivial domains, for example, in electromagnetic simulations. In these simulations, solution is aided by fixing the cohomology class of the solution based on the chosen boundary conditions and the homology of the domain. FEM domains can be triangulated, from which the simplicial homology can be calculated.


== Software ==
Various software packages have been developed for the purposes of computing homology groups of finite cell complexes. Linbox is a C++ library for performing fast matrix operations, including Smith normal form; it interfaces with both Gap and Maple. Chomp, CAPD::Redhom and Perseus are also written in C++. All three implement pre-processing algorithms based on simple-homotopy equivalence and discrete Morse theory to perform homology-preserving reductions of the input cell complexes before resorting to matrix algebra. Kenzo is written in Lisp, and in addition to homology it may also be used to generate presentations of homotopy groups of finite simplicial complexes. Gmsh includes a homology solver for finite element meshes, which can generate Cohomology bases directly usable by finite element software.


== See also ==
Betti number
Cycle space
De Rham cohomology
Eilenberg–Steenrod axioms
Extraordinary homology theory
Homological algebra
Homological conjectures in commutative algebra
Homological connectivity
Homological dimension
Homotopy group
Künneth theorem
List of cohomology theories - also has a list of homology theories
Poincaré duality


== Notes ==


== References ==
Cartan, Henri Paul; Eilenberg, Samuel (1956). Homological Algebra. Princeton mathematical series. Vol. 19. Princeton University Press. ISBN 9780674079779. OCLC 529171.
Eilenberg, Samuel; Moore, J.C. (1965). Foundations of relative homological algebra. Memoirs of the American Mathematical Society number. Vol. 55. American Mathematical Society. ISBN 9780821812556. OCLC 1361982.
Gowers, Timothy; Barrow-Green, June; Leader, Imre, eds. (2010), The Princeton Companion to Mathematics, Princeton University Press, ISBN 9781400830398.
Hatcher, A. (2002), Algebraic Topology, Cambridge University Press, ISBN 0-521-79540-0. Detailed discussion of homology theories for simplicial complexes and manifolds, singular homology, etc.
Hilton, Peter (1988), ""A Brief, Subjective History of Homology and Homotopy Theory in This Century"", Mathematics Magazine, Mathematical Association of America, 60 (5): 282–291, doi:10.1080/0025570X.1988.11977391, JSTOR 2689545
Richeson, D. (2008), Euler's Gem: The Polyhedron Formula and the Birth of Topology, Princeton University.
Spanier, Edwin H. (1966), Algebraic Topology, Springer, p. 155, ISBN 0-387-90646-0.
Stillwell, John (1993), Classical Topology and Combinatorial Group Theory, Springer, doi:10.1007/978-1-4612-4372-4_6, ISBN 978-0-387-97970-0.
Teicher, M., ed. (1999), The Heritage of Emmy Noether, Israel Mathematical Conference Proceedings, Bar-Ilan University/American Mathematical Society/Oxford University Press, ISBN 978-0-19-851045-1, OCLC 223099225
Weibel, Charles A. (1999), ""28. History of Homological Algebra"" (PDF),  in James, I. M. (ed.), History of Topology, Elsevier, ISBN 9780080534077.


== External links ==
Homology group at Encyclopaedia of Mathematics
[1] N.J. Windberger intro to algebraic topology, last six lectures with an easy intro to homology
[2] Algebraic topology Allen Hatcher - Chapter 2 on homology"
c4ca302e98,Decision mathematics,
32abad9512,Indian mathematics,"Indian mathematics emerged in the Indian subcontinent from 1200 BCE until the end of the 18th century. In the classical period of Indian mathematics (400 CE to 1200 CE), important contributions were made by scholars like Aryabhata, Brahmagupta, Bhaskara II, and Varāhamihira. The decimal number system in use today was first recorded in Indian mathematics. Indian mathematicians made early contributions to the study of the concept of zero as a number, negative numbers, arithmetic, and algebra. In addition, trigonometry
was further advanced in India, and, in particular, the modern definitions of sine and cosine were developed there. These mathematical concepts were transmitted to the Middle East, China, and Europe and led to further developments that now form the foundations of many areas of mathematics.
Ancient and medieval Indian mathematical works, all composed in Sanskrit, usually consisted of a section of sutras in which a set of rules or problems were stated with great economy in verse in order to aid memorization by a student.  This was followed by a second section consisting of a prose commentary (sometimes multiple commentaries by different scholars) that explained the problem in more detail and provided justification for the solution.  In the prose section, the form (and therefore its memorization) was not considered so important as the ideas involved. All mathematical works were orally transmitted until approximately 500 BCE; thereafter, they were transmitted both orally and in manuscript form.  The oldest extant mathematical document produced on the Indian subcontinent is the birch bark Bakhshali Manuscript, discovered in 1881 in the village of Bakhshali, near Peshawar (modern day Pakistan) and is likely from the 7th century CE.A later landmark in Indian mathematics was the development of the series expansions for trigonometric functions (sine, cosine, and arc tangent) by mathematicians of the Kerala school in the 15th century CE.  Their remarkable work, completed two centuries before the invention of calculus in Europe, provided what is now considered the first example of a power series (apart from geometric series). However, they did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.


== Prehistory ==
Excavations at Harappa, Mohenjo-daro and other sites of the Indus Valley civilisation have uncovered evidence of the use of ""practical mathematics"". The people of the Indus Valley Civilization manufactured bricks whose dimensions were in the proportion 4:2:1, considered favourable for the stability of a brick structure. They used a standardised system of weights based on the ratios: 1/20, 1/10, 1/5, 1/2, 1, 2, 5, 10, 20, 50, 100, 200, and 500, with the unit weight equaling approximately 28 grams (and approximately equal to the English ounce or Greek uncia).  They mass-produced weights in regular geometrical shapes, which included hexahedra, barrels, cones, and cylinders, thereby demonstrating knowledge of basic geometry.The inhabitants of Indus civilisation also tried to standardise measurement of length to a high degree of accuracy. They designed a ruler—the Mohenjo-daro ruler—whose unit of length (approximately 1.32 inches or 3.4 centimetres) was divided into ten equal parts.  Bricks manufactured in ancient Mohenjo-daro often had dimensions that were integral multiples of this unit of length.Hollow cylindrical objects made of shell and found at Lothal (2200 BCE) and Dholavira are demonstrated to have the ability to measure angles in a plane, as well as to determine the position of stars for navigation.


== Vedic period ==


=== Samhitas and Brahmanas ===
The religious texts of the Vedic Period provide evidence for the use of large numbers. By the time of the Yajurvedasaṃhitā- (1200–900 BCE), numbers as high as 1012 were being included in the texts. For example, the mantra (sacred recitation) at the end of the annahoma (""food-oblation rite"") performed during the aśvamedha, and uttered just before-, during-, and just after sunrise, invokes powers of ten from a hundred to a trillion:
Hail to śata (""hundred,"" 102), hail to sahasra (""thousand,"" 103), hail to ayuta (""ten thousand,"" 104), hail to niyuta (""hundred thousand,"" 105), hail to prayuta (""million,"" 106), hail to arbuda (""ten million,"" 107), hail to nyarbuda (""hundred million,"" 108), hail to samudra (""billion,"" 109, literally ""ocean""), hail to madhya (""ten billion,"" 1010, literally ""middle""), hail to anta (""hundred billion,"" 1011, lit., ""end""), hail to parārdha (""one trillion,"" 1012 lit., ""beyond parts""), hail to the uṣas (dawn) , hail to the vyuṣṭi (twilight), hail to udeṣyat (the one which is going to rise), hail to udyat (the one which is rising), hail udita (to the one which has just risen), hail to svarga (the heaven), hail to martya (the world), hail to all.
The solution to partial fraction was known to the Rigvedic People as states in the purush Sukta (RV 10.90.4):

With three-fourths Puruṣa went up: one-fourth of him again was here.
The Satapatha Brahmana (c. 7th century BCE) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.


=== Śulba Sūtras ===

The Śulba Sūtras (literally, ""Aphorisms of the Chords"" in Vedic Sanskrit) (c. 700–400 BCE) list rules for the construction of sacrificial fire altars. Most mathematical problems considered in the  Śulba Sūtras spring from ""a single theological requirement,"" that of constructing fire altars which have different shapes but occupy the same area.  The altars were required to be constructed of five layers of burnt brick, with the further condition that each layer consist of 200 bricks and that no two adjacent layers have congruent arrangements of bricks.
According to (Hayashi 2005, p. 363), the Śulba Sūtras contain  ""the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians."" The diagonal rope (akṣṇayā-rajju) of an oblong (rectangle) produces both which the flank (pārśvamāni) and the horizontal (tiryaṇmānī) <ropes> produce separately."" Since the statement is a sūtra, it is necessarily compressed and what the ropes produce is not elaborated on, but the context clearly implies the square areas constructed on their lengths, and would have been explained so by the teacher to the student.They contain lists of Pythagorean triples, which are particular cases of Diophantine equations. They also contain statements (that with hindsight we know to be approximate) about squaring the circle and ""circling the square.""Baudhayana (c. 8th century BCE) composed the Baudhayana Sulba Sutra, the best-known Sulba Sutra, which contains examples of simple Pythagorean triples, such as: (3, 4, 5), (5, 12, 13), (8, 15, 17), (7, 24, 25), and (12, 35, 37), as well as a statement of the Pythagorean theorem for the sides of a square: ""The rope which is stretched across the diagonal of a square produces an area double the size of the original square."" It also contains the general statement of the Pythagorean theorem (for the sides of a rectangle): ""The rope stretched along the length of the diagonal of a rectangle makes an area which the vertical and horizontal sides make together."" Baudhayana gives an expression for the square root of two:

  
    
      
        
          
            2
          
        
        ≈
        1
        +
        
          
            1
            3
          
        
        +
        
          
            1
            
              3
              ⋅
              4
            
          
        
        −
        
          
            1
            
              3
              ⋅
              4
              ⋅
              34
            
          
        
        =
        1.4142156
        …
      
    
    {\displaystyle {\sqrt {2}}\approx 1+{\frac {1}{3}}+{\frac {1}{3\cdot 4}}-{\frac {1}{3\cdot 4\cdot 34}}=1.4142156\ldots }
  The expression is accurate up to five decimal places, the true value being 1.41421356... This expression is similar in structure to the expression found on a Mesopotamian tablet from the Old Babylonian period (1900–1600 BCE):

  
    
      
        
          
            2
          
        
        ≈
        1
        +
        
          
            24
            60
          
        
        +
        
          
            51
            
              60
              
                2
              
            
          
        
        +
        
          
            10
            
              60
              
                3
              
            
          
        
        =
        1.41421297
        …
      
    
    {\displaystyle {\sqrt {2}}\approx 1+{\frac {24}{60}}+{\frac {51}{60^{2}}}+{\frac {10}{60^{3}}}=1.41421297\ldots }
  which expresses √2 in the sexagesimal system, and which is also accurate up to 5 decimal places.
According to mathematician S. G. Dani, the Babylonian cuneiform tablet Plimpton 322 written c. 1850 BCE ""contains fifteen Pythagorean triples with quite large entries, including (13500, 12709, 18541) which is a primitive triple, indicating, in particular, that there was sophisticated understanding on the topic"" in Mesopotamia in 1850 BCE.  ""Since these tablets predate the Sulbasutras period by several centuries, taking into account the contextual appearance of some of the triples, it is reasonable to expect that similar understanding would have been there in India."" Dani goes on to say:

As the main objective of the Sulvasutras was to describe the constructions of altars and the geometric principles involved in them, the subject of Pythagorean triples, even if it had been well understood may still not have featured in the Sulvasutras.  The occurrence of the triples in the Sulvasutras is comparable to mathematics that one may encounter in an introductory book on architecture or another similar applied area, and would not correspond directly to the overall knowledge on the topic at that time. Since, unfortunately, no other contemporaneous sources have been found it may never be possible to settle this issue satisfactorily.
In all, three Sulba Sutras were composed.  The remaining two, the Manava Sulba Sutra composed by Manava (fl. 750–650 BCE) and the Apastamba Sulba Sutra, composed by Apastamba (c. 600 BCE), contained results similar to the Baudhayana Sulba Sutra.

Vyakarana
An important landmark of the Vedic period was the work of Sanskrit grammarian, Pāṇini (c. 520–460 BCE). His grammar includes early use of Boolean logic, of the null operator, and of context free grammars, and includes a precursor of the Backus–Naur form (used in the description programming languages).


== Pingala (300 BCE – 200 BCE) ==
Among the scholars of the post-Vedic period who contributed to mathematics, the most notable is Pingala (piṅgalá) (fl. 300–200 BCE), a music theorist who authored the Chhandas Shastra (chandaḥ-śāstra, also Chhandas Sutra chhandaḥ-sūtra), a Sanskrit treatise on prosody.  There is evidence that in his work on the enumeration of syllabic combinations, Pingala stumbled upon both Pascal's triangle and binomial coefficients, although he did not have knowledge of the binomial theorem itself. Pingala's work also contains the basic ideas of Fibonacci numbers (called maatraameru). Although the Chandah sutra hasn't survived in its entirety, a 10th-century commentary on it by Halāyudha has.  Halāyudha, who refers to the Pascal triangle as Meru-prastāra (literally ""the staircase to Mount Meru""), has this to say:

Draw a square. Beginning at half the square, draw two other similar squares below it; below these two, three other squares, and so on. The marking should be started by putting 1 in the first square. Put 1 in each of the two squares of the second line. In the third line put 1 in the two squares at the ends and, in the middle square, the sum of the digits in the two squares lying above it.  In the fourth line put 1 in the two squares at the ends. In the middle ones put the sum of the digits in the two squares above each. Proceed in this way. Of these lines, the second gives the combinations with one syllable, the third the combinations with two syllables, ...
The text also indicates that Pingala was aware of the combinatorial identity:

  
    
      
        
          
            
              (
            
            
              n
              0
            
            
              )
            
          
        
        +
        
          
            
              (
            
            
              n
              1
            
            
              )
            
          
        
        +
        
          
            
              (
            
            
              n
              2
            
            
              )
            
          
        
        +
        ⋯
        +
        
          
            
              (
            
            
              n
              
                n
                −
                1
              
            
            
              )
            
          
        
        +
        
          
            
              (
            
            
              n
              n
            
            
              )
            
          
        
        =
        
          2
          
            n
          
        
      
    
    {\displaystyle {n \choose 0}+{n \choose 1}+{n \choose 2}+\cdots +{n \choose n-1}+{n \choose n}=2^{n}}
  KātyāyanaKātyāyana (c. 3rd century BCE) is notable for being the last of the Vedic mathematicians. He wrote the Katyayana Sulba Sutra, which presented much geometry, including the general Pythagorean theorem and a computation of the square root of 2 correct to five decimal places.


== Jain mathematics (400 BCE – 200 CE) ==
Although Jainism as a religion and philosophy predates its most famous exponent, the great Mahaviraswami (6th century BCE), most Jain texts on mathematical topics were composed after the 6th century BCE. Jain mathematicians are important historically as crucial links between the mathematics of the Vedic period and that of the ""classical period.""
A significant historical contribution of Jain mathematicians lay in their freeing Indian mathematics from its religious and ritualistic constraints. In particular, their fascination with the enumeration of very large numbers and infinities led them to classify numbers into three classes: enumerable, innumerable and infinite.  Not content with a simple notion of infinity, their texts define five different types of infinity: the infinite in one direction, the infinite in two directions, the infinite in area, the infinite everywhere, and the infinite perpetually. In addition, Jain mathematicians devised notations for simple powers (and exponents) of numbers like squares and cubes, which enabled them to define simple algebraic equations (beejganita samikaran).  Jain mathematicians were apparently also the first to use the word shunya (literally void in Sanskrit) to refer to zero.  More than a millennium later, their appellation became the English word ""zero"" after a tortuous journey of translations and transliterations from India to Europe. (See Zero: Etymology.)
In addition to Surya Prajnapti, important Jain works on mathematics included the Sthananga Sutra (c. 300 BCE – 200 CE); the Anuyogadwara Sutra (c. 200 BCE – 100 CE), which includes the earliest known description of factorials in Indian mathematics; and the Satkhandagama (c. 2nd century CE).  Important Jain mathematicians included Bhadrabahu (d. 298 BCE), the author of two astronomical works, the Bhadrabahavi-Samhita and a commentary on the Surya Prajinapti; Yativrisham Acharya (c. 176 BCE), who authored a mathematical text called Tiloyapannati; and Umasvati (c. 150 BCE), who, although better known for his influential writings on Jain philosophy and metaphysics, composed a mathematical work called Tattwarthadhigama-Sutra Bhashya.


== Oral tradition ==
Mathematicians of ancient and early medieval India were almost all Sanskrit pandits (paṇḍita ""learned man""), who were trained in Sanskrit language and literature, and possessed ""a common stock of knowledge in grammar (vyākaraṇa), exegesis (mīmāṃsā) and logic (nyāya)."" Memorisation of ""what is heard"" (śruti in Sanskrit) through recitation played a major role in the transmission of sacred texts in ancient India.  Memorisation and recitation was also used to transmit philosophical and literary works, as well as treatises on ritual and grammar.  Modern scholars of ancient India have noted the ""truly remarkable achievements of the Indian pandits who have preserved enormously bulky texts orally for millennia.""


=== Styles of memorisation ===
Prodigious energy was expended by ancient Indian culture in ensuring that these texts were transmitted from generation to generation with inordinate fidelity. For example, memorisation of the sacred Vedas included up to eleven forms of recitation of the same text.  The texts were subsequently ""proof-read"" by comparing the different recited versions.  Forms of recitation included the jaṭā-pāṭha (literally ""mesh recitation"") in which every two adjacent words in the text were first recited in their original order, then repeated in the reverse order, and finally repeated in the original order. The recitation thus proceeded as:

In another form of recitation, dhvaja-pāṭha (literally ""flag recitation"") a sequence of N words were recited (and memorised) by pairing the first two and last two words and then proceeding as:

The most complex form of recitation, ghana-pāṭha (literally ""dense recitation""), according to (Filliozat 2004, p. 139), took the form:

That these methods have been effective, is testified to by the preservation of the most ancient Indian religious text, the Ṛgveda (c. 1500 BCE), as a single text, without any variant readings. Similar methods were used for memorising mathematical texts, whose transmission remained exclusively oral until the end of the Vedic period (c. 500 BCE).


=== The Sutra genre ===
Mathematical activity in ancient India began as a part of a ""methodological reflexion"" on the sacred Vedas, which took the form of works called Vedāṇgas, or, ""Ancillaries of the Veda"" (7th–4th century BCE). The need to conserve the sound of sacred text by use of śikṣā (phonetics) and chhandas (metrics); to conserve its meaning by use of vyākaraṇa (grammar) and nirukta (etymology); and to correctly perform the rites at the correct time by the use of kalpa (ritual) and jyotiṣa (astrology), gave rise to the six disciplines of the Vedāṇgas. Mathematics arose as a part of the last two disciplines, ritual and astronomy (which also included astrology).
Since the Vedāṇgas immediately preceded the use of writing in ancient India, they formed the last of the exclusively oral literature.  They were expressed in a highly compressed mnemonic form, the sūtra (literally, ""thread""):

The knowers of the sūtra know it as having few phonemes, being devoid of ambiguity, containing the essence, facing everything, being without pause and unobjectionable.
Extreme brevity was achieved through multiple means, which included using ellipsis ""beyond the tolerance of natural language,"" using technical names instead of longer descriptive names, abridging lists by only mentioning the first and last entries, and using markers and variables. The sūtras create the impression that communication through the text was ""only a part of the whole instruction.  The rest of the instruction must have been transmitted by the so-called Guru-shishya parampara, 'uninterrupted succession from teacher (guru) to the student (śisya),' and it was not open to the general public"" and perhaps even kept secret. The brevity achieved in a sūtra is demonstrated in the following example from the Baudhāyana Śulba Sūtra (700 BCE).

The domestic fire-altar in the Vedic period was required by ritual to have a square base and be constituted of five layers of bricks with 21 bricks in each layer.  One method of constructing the altar was to divide one side of the square into three equal parts using a cord or rope, to next divide the transverse (or perpendicular) side into seven equal parts, and thereby sub-divide the square into 21 congruent rectangles.  The bricks were then designed to be of the shape of the constituent rectangle and the layer was created.  To form the next layer, the same formula was used, but the bricks were arranged transversely. The process was then repeated three more times (with alternating directions) in order to complete the construction.  In the Baudhāyana Śulba Sūtra, this procedure is described in the following words:

II.64. After dividing the quadri-lateral in seven, one divides the transverse [cord] in three.II.65. In another layer one places the [bricks] North-pointing.
According to (Filliozat 2004, p. 144), the officiant constructing the altar has only a few tools and materials at his disposal: a cord (Sanskrit, rajju, f.), two pegs (Sanskrit, śanku, m.), and clay to make the bricks (Sanskrit, iṣṭakā, f.).  Concision is achieved in the sūtra, by not explicitly mentioning what the adjective ""transverse"" qualifies; however, from the feminine form of the (Sanskrit) adjective used, it is easily inferred to qualify ""cord.""  Similarly, in the second stanza, ""bricks""  are not explicitly mentioned, but inferred again by the feminine plural form of ""North-pointing.""  Finally, the first stanza, never explicitly says that the first layer of bricks are oriented in the east–west direction, but that too is implied by the explicit mention of ""North-pointing"" in the second stanza; for, if the orientation was meant to be the same in the two layers, it would either not be mentioned at all or be only mentioned in the first stanza.  All these inferences are made by the officiant as he recalls the formula from his memory.


== The written tradition: prose commentary ==
With the increasing complexity of mathematics and other exact sciences, both writing and computation were required. Consequently, many mathematical works began to be written down in manuscripts that were then copied and re-copied from generation to generation.

India today is estimated to have about thirty million manuscripts, the largest body of handwritten reading material anywhere in the world.  The literate culture of Indian science goes back to at least the fifth century B.C. ... as is shown by the elements of Mesopotamian omen literature and astronomy that entered India at that time and (were) definitely not ... preserved orally.
The earliest mathematical prose commentary was that on the work, Āryabhaṭīya (written 499 CE), a work on astronomy and mathematics.  The mathematical portion of the Āryabhaṭīya was composed of 33 sūtras (in verse form) consisting of mathematical statements or rules, but without any proofs. However, according to (Hayashi 2003, p. 123), ""this does not necessarily mean that their authors did not prove them.  It was probably a matter of style of exposition.""  From the time of Bhaskara I (600 CE onwards), prose commentaries increasingly began to include some derivations (upapatti).  Bhaskara I's commentary on the Āryabhaṭīya, had the following structure:
Rule ('sūtra') in verse by Āryabhaṭa
Commentary by Bhāskara I, consisting of:
Elucidation of rule (derivations were still rare then, but became more common later)
Example (uddeśaka) usually in verse.
Setting (nyāsa/sthāpanā) of the numerical data.
Working (karana) of the solution.
Verification (pratyayakaraṇa, literally ""to make conviction"") of the answer.  These became rare by the 13th century, derivations or proofs being favoured by then.Typically, for any mathematical topic, students in ancient India first memorised the sūtras, which, as explained earlier, were ""deliberately inadequate"" in explanatory details (in order to pithily convey the bare-bone mathematical rules).  The students then worked through the topics of the prose commentary by writing (and drawing diagrams) on chalk- and dust-boards (i.e. boards covered with dust).  The latter activity, a staple of mathematical work, was to later prompt mathematician-astronomer, Brahmagupta (fl. 7th century CE), to characterise astronomical computations as ""dust work"" (Sanskrit: dhulikarman).


== Numerals and the decimal number system ==
It is well known that the decimal place-value system in use today was first recorded in India, then transmitted to the Islamic world, and eventually to Europe. The Syrian bishop Severus Sebokht wrote in the mid-7th century CE about the ""nine signs"" of the Indians for expressing numbers. However, how, when, and where the first decimal place value system was invented is not so clear.The earliest extant script used in India was the Kharoṣṭhī script used in the Gandhara culture of the north-west. It is thought to be of Aramaic origin and it was in use from the 4th century BCE to the 4th century CE.  Almost contemporaneously, another script, the Brāhmī script, appeared on much of the sub-continent, and would later become the foundation of many scripts of South Asia and South-east Asia. Both scripts had numeral symbols and numeral systems, which were initially not based on a place-value system.The earliest surviving evidence of decimal place value numerals in India and southeast Asia is from the middle of the first millennium CE. A copper plate from Gujarat, India mentions the date 595 CE, written in a decimal place value notation, although there is some doubt as to the authenticity of the plate. Decimal numerals recording the years 683 CE have also been found in stone inscriptions in Indonesia and Cambodia, where Indian cultural influence was substantial.There are older textual sources, although the extant manuscript copies of these texts are from much later dates. Probably the earliest such source is the work of the Buddhist philosopher Vasumitra dated likely to the 1st century CE. Discussing the counting pits of merchants, Vasumitra remarks, ""When [the same] clay counting-piece is in the place of units, it is denoted as one, when in hundreds, one hundred."" Although such references seem to imply that his readers had knowledge of a decimal place value representation, the ""brevity of their allusions and the ambiguity of their dates, however, do not solidly establish the chronology of the development of this concept.""A third decimal representation was employed in a verse composition technique, later labelled Bhuta-sankhya (literally, ""object numbers"") used by early Sanskrit authors of technical books. Since many early technical works were composed in verse, numbers were often represented by objects in the natural or religious world that correspondence to them; this allowed a many-to-one correspondence for each number and made verse composition easier. According to (Plofker 2009), the number 4, for example, could be represented by the word ""Veda"" (since there were four of these religious texts), the number 32 by the word ""teeth"" (since a full set consists of 32), and the number 1 by ""moon"" (since there is only one moon). So, Veda/teeth/moon would correspond to the decimal numeral 1324, as the convention for numbers was to enumerate their digits from right to left. The earliest reference employing object numbers is a c. 269 CE Sanskrit text, Yavanajātaka (literally ""Greek horoscopy"") of Sphujidhvaja, a versification of an earlier (c. 150 CE) Indian prose adaptation of a lost work of Hellenistic astrology. Such use seems to make the case that by the mid-3rd century CE, the decimal place value system was familiar, at least to readers of astronomical and astrological texts in India.
It has been hypothesized that the Indian decimal place value system was based on the symbols used on Chinese counting boards from as early as the middle of the first millennium BCE. According to (Plofker 2009), These counting boards, like the Indian counting pits, ..., had a decimal place value structure ... Indians may well have learned of these decimal place value ""rod numerals"" from Chinese Buddhist pilgrims or other travelers, or they may have developed the concept independently from their earlier non-place-value system; no documentary evidence survives to confirm either conclusion.""


== Bakhshali Manuscript ==
The oldest extant mathematical manuscript in India is the Bakhshali Manuscript, a birch bark manuscript written in ""Buddhist hybrid Sanskrit"" in the Śāradā script, which was used in the northwestern region of the Indian subcontinent between the 8th and 12th centuries CE. The manuscript was discovered in 1881 by a farmer while digging in a stone enclosure in the village of Bakhshali, near Peshawar (then in British India and now in Pakistan).  Of unknown authorship and now preserved in the Bodleian Library in the University of Oxford, the manuscript has been dated recently as 224 AD- 383 AD.The surviving manuscript has seventy leaves, some of which are in fragments. Its mathematical content consists of rules and examples, written in verse, together with prose commentaries, which include solutions to the examples. The topics treated include arithmetic (fractions, square roots, profit and loss, simple interest, the rule of three, and regula falsi) and algebra (simultaneous linear equations and quadratic equations), and arithmetic progressions.  In addition, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also ""employs a decimal place value system with a dot for zero."" Many of its problems are of a category known as 'equalisation problems' that lead to systems of linear equations.  One example from Fragment III-5-3v is the following:

One merchant has seven asava horses, a second has nine haya horses, and a third has ten camels.  They are equally well off in the value of their animals if each gives two animals, one to each of the others.  Find the price of each animal and the total value for the animals possessed by each merchant.
The prose commentary accompanying the example solves the problem by converting it to three (under-determined) equations in four unknowns and assuming that the prices are all integers.In 2017, three samples from the manuscript were shown by radiocarbon dating to come from three different centuries: from 224 to 383 AD, 680-779 AD, and 885-993 AD. It is not known how fragments from different centuries came to be packaged together.


== Classical period (400–1600) ==
This period is often known as the golden age of Indian Mathematics. This period saw mathematicians such as Aryabhata, Varahamihira, Brahmagupta, Bhaskara I, Mahavira, Bhaskara II, Madhava of Sangamagrama and Nilakantha Somayaji give broader and clearer shape to many branches of mathematics. Their contributions would spread to Asia, the Middle East, and eventually to Europe.  Unlike Vedic mathematics, their works included both astronomical and mathematical contributions.  In fact, mathematics of that period was included in the 'astral science' (jyotiḥśāstra) and consisted of three sub-disciplines: mathematical sciences (gaṇita or tantra), horoscope astrology (horā or jātaka) and divination (saṃhitā). This tripartite division is seen in Varāhamihira's 6th century compilation—Pancasiddhantika (literally panca, ""five,"" siddhānta, ""conclusion of deliberation"", dated 575 CE)—of five earlier works, Surya Siddhanta, Romaka Siddhanta, Paulisa Siddhanta, Vasishtha Siddhanta and Paitamaha Siddhanta, which were adaptations of still earlier works of Mesopotamian, Greek, Egyptian, Roman and Indian astronomy.  As explained earlier, the main texts were composed in Sanskrit verse, and were followed by prose commentaries.


=== Fifth and sixth centuries ===
Surya SiddhantaThough its authorship is unknown, the Surya Siddhanta (c. 400) contains the roots of modern trigonometry. Because it contains many words of foreign origin, some authors consider that it was written under the influence of Mesopotamia and Greece.This ancient text uses the following as trigonometric functions for the first time:
Sine (Jya).
Cosine (Kojya).
Inverse sine (Otkram jya).It also contains the earliest uses of:
Tangent.
Secant.Later Indian mathematicians such as Aryabhata made references to this text, while later Arabic and Latin translations were very influential in Europe and the Middle East.

Chhedi calendarThis Chhedi calendar (594) contains an early use of the modern place-value Hindu–Arabic numeral system now used universally.

Aryabhata IAryabhata (476–550) wrote the Aryabhatiya. He described the important fundamental principles of mathematics in 332 shlokas. The treatise contained:

Quadratic equations
Trigonometry
The value of π, correct to 4 decimal places.Aryabhata also wrote the Arya Siddhanta, which is now lost. Aryabhata's contributions include:
Trigonometry:
(See also : Aryabhata's sine table)

Introduced the trigonometric functions.
Defined the sine (jya) as the modern relationship between half an angle and half a chord.
Defined the cosine (kojya).
Defined the versine (utkrama-jya).
Defined the inverse sine (otkram jya).
Gave methods of calculating their approximate numerical values.
Contains the earliest tables of sine, cosine and versine values, in 3.75° intervals from 0° to 90°, to 4 decimal places of accuracy.
Contains the trigonometric formula sin(n + 1)x − sin nx = sin nx − sin(n − 1)x − (1/225)sin nx.
Spherical trigonometry.Arithmetic:

Continued fractions.Algebra:

Solutions of simultaneous quadratic equations.
Whole number solutions of linear equations by a method equivalent to the modern method.
General solution of the indeterminate linear equation .Mathematical astronomy:

Accurate calculations for astronomical constants, such as the:
Solar eclipse.
Lunar eclipse.
The formula for the sum of the cubes, which was an important step in the development of integral calculus.VarahamihiraVarahamihira (505–587) produced the Pancha Siddhanta (The Five Astronomical Canons). He made important contributions to trigonometry, including sine and cosine tables to 4 decimal places of accuracy and the following formulas relating sine and cosine functions:

  
    
      
        
          sin
          
            2
          
        
        ⁡
        (
        x
        )
        +
        
          cos
          
            2
          
        
        ⁡
        (
        x
        )
        =
        1
      
    
    {\displaystyle \sin ^{2}(x)+\cos ^{2}(x)=1}
  

  
    
      
        sin
        ⁡
        (
        x
        )
        =
        cos
        ⁡
        
          (
          
            
              
                π
                2
              
            
            −
            x
          
          )
        
      
    
    {\displaystyle \sin(x)=\cos \left({\frac {\pi }{2}}-x\right)}
  

  
    
      
        
          
            
              1
              −
              cos
              ⁡
              (
              2
              x
              )
            
            2
          
        
        =
        
          sin
          
            2
          
        
        ⁡
        (
        x
        )
      
    
    {\displaystyle {\frac {1-\cos(2x)}{2}}=\sin ^{2}(x)}
  


=== Seventh and eighth centuries ===

In the 7th century, two separate fields, arithmetic (which included measurement) and algebra, began to emerge in Indian mathematics.  The two fields would later be called pāṭī-gaṇita (literally ""mathematics of algorithms"") and bīja-gaṇita (lit. ""mathematics of seeds,"" with ""seeds""—like the seeds of plants—representing unknowns with the potential to generate, in this case, the solutions of equations). Brahmagupta, in his astronomical work Brāhma Sphuṭa Siddhānta (628 CE), included two chapters (12 and 18) devoted to these fields.  Chapter 12, containing 66 Sanskrit verses, was divided into two sections:  ""basic operations"" (including cube roots, fractions, ratio and proportion, and barter) and ""practical mathematics"" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral:Brahmagupta's theorem: If a cyclic quadrilateral has diagonals that are perpendicular to each other, then the perpendicular line drawn from the point of intersection of the diagonals to any side of the quadrilateral always bisects the opposite side.
Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalisation of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).
Brahmagupta's formula: The area, A, of a cyclic quadrilateral with sides of lengths a, b, c, d, respectively, is given by

  
    
      
        A
        =
        
          
            (
            s
            −
            a
            )
            (
            s
            −
            b
            )
            (
            s
            −
            c
            )
            (
            s
            −
            d
            )
          
        
        
      
    
    {\displaystyle A={\sqrt {(s-a)(s-b)(s-c)(s-d)}}\,}
  where s, the semiperimeter, given by 
  
    
      
        s
        =
        
          
            
              a
              +
              b
              +
              c
              +
              d
            
            2
          
        
        .
      
    
    {\displaystyle s={\frac {a+b+c+d}{2}}.}
  
Brahmagupta's Theorem on rational triangles: A triangle with rational sides 
  
    
      
        a
        ,
        b
        ,
        c
      
    
    {\displaystyle a,b,c}
   and rational area is of the form:

  
    
      
        a
        =
        
          
            
              u
              
                2
              
            
            v
          
        
        +
        v
        ,
         
         
        b
        =
        
          
            
              u
              
                2
              
            
            w
          
        
        +
        w
        ,
         
         
        c
        =
        
          
            
              u
              
                2
              
            
            v
          
        
        +
        
          
            
              u
              
                2
              
            
            w
          
        
        −
        (
        v
        +
        w
        )
      
    
    {\displaystyle a={\frac {u^{2}}{v}}+v,\ \ b={\frac {u^{2}}{w}}+w,\ \ c={\frac {u^{2}}{v}}+{\frac {u^{2}}{w}}-(v+w)}
  for some rational numbers 
  
    
      
        u
        ,
        v
        ,
      
    
    {\displaystyle u,v,}
   and 
  
    
      
        w
      
    
    {\displaystyle w}
  .Chapter 18 contained 103 Sanskrit verses which began with rules for arithmetical operations involving zero and negative numbers and is considered the first systematic treatment of the subject.  The rules (which included 
  
    
      
        a
        +
        0
        =
         
        a
      
    
    {\displaystyle a+0=\ a}
   and 
  
    
      
        a
        ×
        0
        =
        0
      
    
    {\displaystyle a\times 0=0}
  ) were all correct, with one exception: 
  
    
      
        
          
            0
            0
          
        
        =
        0
      
    
    {\displaystyle {\frac {0}{0}}=0}
  . Later in the chapter, he gave the first explicit (although still not completely general) solution of the quadratic equation:

  
    
      
         
        a
        
          x
          
            2
          
        
        +
        b
        x
        =
        c
      
    
    {\displaystyle \ ax^{2}+bx=c}
  To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value.
This is equivalent to:

  
    
      
        x
        =
        
          
            
              
                
                  4
                  a
                  c
                  +
                  
                    b
                    
                      2
                    
                  
                
              
              −
              b
            
            
              2
              a
            
          
        
      
    
    {\displaystyle x={\frac {{\sqrt {4ac+b^{2}}}-b}{2a}}}
  Also in chapter 18, Brahmagupta was able to make progress in finding (integral) solutions of Pell's equation,

  
    
      
         
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        =
        1
        ,
      
    
    {\displaystyle \ x^{2}-Ny^{2}=1,}
  where 
  
    
      
        N
      
    
    {\displaystyle N}
   is a nonsquare integer.  He did this by discovering the following identity:Brahmagupta's Identity: 
  
    
      
         
        (
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        )
        (
        
          x
          
            ′
            
              2
            
          
        
        −
        N
        
          y
          
            ′
            
              2
            
          
        
        )
        =
        (
        x
        
          x
          ′
        
        +
        N
        y
        
          y
          ′
        
        
          )
          
            2
          
        
        −
        N
        (
        x
        
          y
          ′
        
        +
        
          x
          ′
        
        y
        
          )
          
            2
          
        
      
    
    {\displaystyle \ (x^{2}-Ny^{2})(x'^{2}-Ny'^{2})=(xx'+Nyy')^{2}-N(xy'+x'y)^{2}}
  
which was a generalisation of an earlier identity of Diophantus: Brahmagupta used his identity to prove the following lemma:Lemma (Brahmagupta): If 
  
    
      
        x
        =
        
          x
          
            1
          
        
        ,
         
         
        y
        =
        
          y
          
            1
          
        
         
         
      
    
    {\displaystyle x=x_{1},\ \ y=y_{1}\ \ }
   is a solution of 
  
    
      
         
         
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        =
        
          k
          
            1
          
        
        ,
      
    
    {\displaystyle \ \ x^{2}-Ny^{2}=k_{1},}
   and,

  
    
      
        x
        =
        
          x
          
            2
          
        
        ,
         
         
        y
        =
        
          y
          
            2
          
        
         
         
      
    
    {\displaystyle x=x_{2},\ \ y=y_{2}\ \ }
   is a solution of 
  
    
      
         
         
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        =
        
          k
          
            2
          
        
        ,
      
    
    {\displaystyle \ \ x^{2}-Ny^{2}=k_{2},}
  , then:

  
    
      
        x
        =
        
          x
          
            1
          
        
        
          x
          
            2
          
        
        +
        N
        
          y
          
            1
          
        
        
          y
          
            2
          
        
        ,
         
         
        y
        =
        
          x
          
            1
          
        
        
          y
          
            2
          
        
        +
        
          x
          
            2
          
        
        
          y
          
            1
          
        
         
         
      
    
    {\displaystyle x=x_{1}x_{2}+Ny_{1}y_{2},\ \ y=x_{1}y_{2}+x_{2}y_{1}\ \ }
   is a solution of 
  
    
      
         
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        =
        
          k
          
            1
          
        
        
          k
          
            2
          
        
      
    
    {\displaystyle \ x^{2}-Ny^{2}=k_{1}k_{2}}
  He then used this lemma to both generate infinitely many (integral) solutions of Pell's equation, given one solution, and state the following theorem:
Theorem (Brahmagupta): If the equation 
  
    
      
         
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        =
        k
      
    
    {\displaystyle \ x^{2}-Ny^{2}=k}
   has an integer solution for any one of 
  
    
      
         
        k
        =
        ±
        4
        ,
        ±
        2
        ,
        −
        1
      
    
    {\displaystyle \ k=\pm 4,\pm 2,-1}
   then Pell's equation:

  
    
      
         
        
          x
          
            2
          
        
        −
        N
        
          y
          
            2
          
        
        =
        1
      
    
    {\displaystyle \ x^{2}-Ny^{2}=1}
  also has an integer solution.Brahmagupta did not actually prove the theorem, but rather worked out examples using his method.  The first example he presented was:Example (Brahmagupta): Find integers 
  
    
      
         
        x
        ,
         
        y
         
      
    
    {\displaystyle \ x,\ y\ }
   such that:

  
    
      
         
        
          x
          
            2
          
        
        −
        92
        
          y
          
            2
          
        
        =
        1
      
    
    {\displaystyle \ x^{2}-92y^{2}=1}
  In his commentary, Brahmagupta added, ""a person solving this problem within a year is a mathematician."" The solution he provided was:

  
    
      
         
        x
        =
        1151
        ,
         
        y
        =
        120
      
    
    {\displaystyle \ x=1151,\ y=120}
  Bhaskara IBhaskara I (c. 600–680) expanded the work of Aryabhata in his books titled Mahabhaskariya, Aryabhatiya-bhashya and Laghu-bhaskariya. He produced:

Solutions of indeterminate equations.
A rational approximation of the sine function.
A formula for calculating the sine of an acute angle without the use of a table, correct to two decimal places.


=== Ninth to twelfth centuries ===
VirasenaVirasena (8th century) was a Jain mathematician in the court of Rashtrakuta King Amoghavarsha of Manyakheta, Karnataka. He wrote the Dhavala, a commentary on Jain mathematics, which:

Deals with the concept of ardhaccheda, the number of times a number could be halved, and lists various rules involving this operation. This coincides with the binary logarithm when applied to powers of two, but differs on other numbers, more closely resembling the 2-adic order.
The same concept for base 3 (trakacheda) and base 4 (caturthacheda).Virasena also gave:

The derivation of the volume of a frustum by a sort of infinite procedure.It is thought that much of the mathematical material in the Dhavala can attributed to previous writers, especially Kundakunda, Shamakunda, Tumbulura, Samantabhadra and Bappadeva and date who wrote between 200 and 600 CE.
MahaviraMahavira Acharya (c. 800–870) from Karnataka, the last of the notable Jain mathematicians, lived in the 9th century and was patronised by the Rashtrakuta king Amoghavarsha. He wrote a book titled Ganit Saar Sangraha on numerical mathematics, and also wrote treatises about a wide range of mathematical topics. These include the mathematics of:

Zero
Squares
Cubes
square roots, cube roots, and the series extending beyond these
Plane geometry
Solid geometry
Problems relating to the casting of shadows
Formulae derived to calculate the area of an ellipse and quadrilateral inside a circle.Mahavira also:

Asserted that the square root of a negative number did not exist
Gave the sum of a series whose terms are squares of an arithmetical progression, and gave empirical rules for area and perimeter of an ellipse.
Solved cubic equations.
Solved quartic equations.
Solved some quintic equations and higher-order polynomials.
Gave the general solutions of the higher order polynomial equations:

  
    
      
         
        a
        
          x
          
            n
          
        
        =
        q
      
    
    {\displaystyle \ ax^{n}=q}
  

  
    
      
        a
        
          
            
              
                x
                
                  n
                
              
              −
              1
            
            
              x
              −
              1
            
          
        
        =
        p
      
    
    {\displaystyle a{\frac {x^{n}-1}{x-1}}=p}
  
Solved indeterminate quadratic equations.
Solved indeterminate cubic equations.
Solved indeterminate higher order equations.ShridharaShridhara (c. 870–930), who lived in Bengal, wrote the books titled Nav Shatika, Tri Shatika and Pati Ganita. He gave:

A good rule for finding the volume of a sphere.
The formula for solving quadratic equations.The Pati Ganita is a work on arithmetic and measurement. It deals with various operations, including:

Elementary operations
Extracting square and cube roots.
Fractions.
Eight rules given for operations involving zero.
Methods of summation of different arithmetic and geometric series, which were to become standard references in later works.ManjulaAryabhata's differential equations were elaborated in the 10th century by Manjula (also Munjala), who realised that the expression

  
    
      
         
        sin
        ⁡
        
          w
          ′
        
        −
        sin
        ⁡
        w
      
    
    {\displaystyle \ \sin w'-\sin w}
  could be approximately expressed as

  
    
      
         
        (
        
          w
          ′
        
        −
        w
        )
        cos
        ⁡
        w
      
    
    {\displaystyle \ (w'-w)\cos w}
  He understood the concept of differentiation after solving the differential equation that resulted from substituting this expression into Aryabhata's differential equation.
Aryabhata IIAryabhata II (c. 920–1000) wrote a commentary on Shridhara, and an astronomical treatise Maha-Siddhanta. The Maha-Siddhanta has 18 chapters, and discusses:

Numerical mathematics (Ank Ganit).
Algebra.
Solutions of indeterminate equations (kuttaka).ShripatiShripati Mishra (1019–1066) wrote the books Siddhanta Shekhara, a major work on astronomy in 19 chapters, and Ganit Tilaka, an incomplete arithmetical treatise in 125 verses based on a work by Shridhara. He worked mainly on:

Permutations and combinations.
General solution of the simultaneous indeterminate linear equation.He was also the author of Dhikotidakarana, a work of twenty verses on:

Solar eclipse.
Lunar eclipse.The Dhruvamanasa is a work of 105 verses on:

Calculating planetary longitudes
eclipses.
planetary transits.Nemichandra Siddhanta ChakravatiNemichandra Siddhanta Chakravati (c. 1100) authored a mathematical treatise titled Gome-mat Saar.

Bhaskara IIBhāskara II (1114–1185) was a mathematician-astronomer who wrote a number of important treatises, namely the Siddhanta Shiromani, Lilavati, Bijaganita, Gola Addhaya, Griha Ganitam and Karan Kautoohal. A number of his contributions were later transmitted to the Middle East and Europe. His contributions include:
Arithmetic:

Interest computation
Arithmetical and geometrical progressions
Plane geometry
Solid geometry
The shadow of the gnomon
Solutions of combinations
Gave a proof for division by zero being infinity.Algebra:

The recognition of a positive number having two square roots.
Surds.
Operations with products of several unknowns.
The solutions of:
Quadratic equations.
Cubic equations.
Quartic equations.
Equations with more than one unknown.
Quadratic equations with more than one unknown.
The general form of Pell's equation using the chakravala method.
The general indeterminate quadratic equation using the chakravala method.
Indeterminate cubic equations.
Indeterminate quartic equations.
Indeterminate higher-order polynomial equations.Geometry:

Gave a proof of the Pythagorean theorem.Calculus:

Conceived of differential calculus.
Discovered the derivative.
Discovered the differential coefficient.
Developed differentiation.
Stated Rolle's theorem, a special case of the mean value theorem (one of the most important theorems of calculus and analysis).
Derived the differential of the sine function.
Computed π, correct to five decimal places.
Calculated the length of the Earth's revolution around the Sun to 9 decimal places.Trigonometry:

Developments of spherical trigonometry
The trigonometric formulas:

  
    
      
         
        sin
        ⁡
        (
        a
        +
        b
        )
        =
        sin
        ⁡
        (
        a
        )
        cos
        ⁡
        (
        b
        )
        +
        sin
        ⁡
        (
        b
        )
        cos
        ⁡
        (
        a
        )
      
    
    {\displaystyle \ \sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)}
  

  
    
      
         
        sin
        ⁡
        (
        a
        −
        b
        )
        =
        sin
        ⁡
        (
        a
        )
        cos
        ⁡
        (
        b
        )
        −
        sin
        ⁡
        (
        b
        )
        cos
        ⁡
        (
        a
        )
      
    
    {\displaystyle \ \sin(a-b)=\sin(a)\cos(b)-\sin(b)\cos(a)}
  


== Kerala mathematics (1300–1600) ==

The Kerala school of astronomy and mathematics was founded by Madhava of Sangamagrama in Kerala, South India and included among its members: Parameshvara, Neelakanta Somayaji, Jyeshtadeva, Achyuta Pisharati, Melpathur Narayana Bhattathiri and Achyuta Panikkar. It flourished between the 14th and 16th centuries and the original discoveries of the school seems to have ended with Narayana Bhattathiri (1559–1632).  In attempting to solve astronomical problems, the Kerala school astronomers independently created a number of important mathematics concepts.  The most important results, series expansion for trigonometric functions, were given in Sanskrit verse in a book by Neelakanta called Tantrasangraha and a commentary on this work called Tantrasangraha-vakhya of unknown authorship.  The theorems were stated without proof, but proofs for the series for sine, cosine, and inverse tangent were provided a century later in the work Yuktibhāṣā (c.1500–c.1610), written in Malayalam, by Jyesthadeva.Their discovery of these three important series expansions of calculus—several centuries before calculus was developed in Europe by Isaac Newton and Gottfried Leibniz—was an achievement.  However, the Kerala School did not invent calculus, because, while they were able to develop Taylor series expansions for the important trigonometric functions, differentiation, term by term integration, convergence tests, iterative methods for solutions of non-linear equations, and the theory that the area under a curve is its integral, they developed neither a theory of differentiation or integration, nor the fundamental theorem of calculus. The results obtained by the Kerala school include:

The (infinite) geometric series: 
  
    
      
        
          
            1
            
              1
              −
              x
            
          
        
        =
        1
        +
        x
        +
        
          x
          
            2
          
        
        +
        
          x
          
            3
          
        
        +
        
          x
          
            4
          
        
        +
        ⋯
        
           for 
        
        
          |
        
        x
        
          |
        
        <
        1
      
    
    {\displaystyle {\frac {1}{1-x}}=1+x+x^{2}+x^{3}+x^{4}+\cdots {\text{ for }}|x|<1}
  
A semi-rigorous proof (see ""induction"" remark below) of the result: 
  
    
      
        
          1
          
            p
          
        
        +
        
          2
          
            p
          
        
        +
        ⋯
        +
        
          n
          
            p
          
        
        ≈
        
          
            
              n
              
                p
                +
                1
              
            
            
              p
              +
              1
            
          
        
      
    
    {\displaystyle 1^{p}+2^{p}+\cdots +n^{p}\approx {\frac {n^{p+1}}{p+1}}}
   for large n.
Intuitive use of mathematical induction, however, the inductive hypothesis was not formulated or employed in proofs.
Applications of ideas from (what was to become) differential and integral calculus to obtain (Taylor–Maclaurin) infinite series for sin x, cos x, and arctan x. The Tantrasangraha-vakhya gives the series in verse, which when translated to mathematical notation, can be written as:
  
    
      
        r
        arctan
        ⁡
        
          (
          
            
              y
              x
            
          
          )
        
        =
        
          
            1
            1
          
        
        ⋅
        
          
            
              r
              y
            
            x
          
        
        −
        
          
            1
            3
          
        
        ⋅
        
          
            
              r
              
                y
                
                  3
                
              
            
            
              x
              
                3
              
            
          
        
        +
        
          
            1
            5
          
        
        ⋅
        
          
            
              r
              
                y
                
                  5
                
              
            
            
              x
              
                5
              
            
          
        
        −
        ⋯
        ,
        
           where 
        
        y
        
          /
        
        x
        ≤
        1.
      
    
    {\displaystyle r\arctan \left({\frac {y}{x}}\right)={\frac {1}{1}}\cdot {\frac {ry}{x}}-{\frac {1}{3}}\cdot {\frac {ry^{3}}{x^{3}}}+{\frac {1}{5}}\cdot {\frac {ry^{5}}{x^{5}}}-\cdots ,{\text{ where }}y/x\leq 1.}
  

  
    
      
        r
        sin
        ⁡
        x
        =
        x
        −
        x
        
          
            
              x
              
                2
              
            
            
              (
              
                2
                
                  2
                
              
              +
              2
              )
              
                r
                
                  2
                
              
            
          
        
        +
        x
        
          
            
              x
              
                2
              
            
            
              (
              
                2
                
                  2
                
              
              +
              2
              )
              
                r
                
                  2
                
              
            
          
        
        ⋅
        
          
            
              x
              
                2
              
            
            
              (
              
                4
                
                  2
                
              
              +
              4
              )
              
                r
                
                  2
                
              
            
          
        
        −
        ⋯
      
    
    {\displaystyle r\sin x=x-x{\frac {x^{2}}{(2^{2}+2)r^{2}}}+x{\frac {x^{2}}{(2^{2}+2)r^{2}}}\cdot {\frac {x^{2}}{(4^{2}+4)r^{2}}}-\cdots }
  

  
    
      
        r
        −
        cos
        ⁡
        x
        =
        r
        
          
            
              x
              
                2
              
            
            
              (
              
                2
                
                  2
                
              
              −
              2
              )
              
                r
                
                  2
                
              
            
          
        
        −
        r
        
          
            
              x
              
                2
              
            
            
              (
              
                2
                
                  2
                
              
              −
              2
              )
              
                r
                
                  2
                
              
            
          
        
        
          
            
              x
              
                2
              
            
            
              (
              
                4
                
                  2
                
              
              −
              4
              )
              
                r
                
                  2
                
              
            
          
        
        +
        ⋯
        ,
      
    
    {\displaystyle r-\cos x=r{\frac {x^{2}}{(2^{2}-2)r^{2}}}-r{\frac {x^{2}}{(2^{2}-2)r^{2}}}{\frac {x^{2}}{(4^{2}-4)r^{2}}}+\cdots ,}
  
where, for r = 1, the series reduces to the standard power series for these trigonometric functions, for example:

  
    
      
        sin
        ⁡
        x
        =
        x
        −
        
          
            
              x
              
                3
              
            
            
              3
              !
            
          
        
        +
        
          
            
              x
              
                5
              
            
            
              5
              !
            
          
        
        −
        
          
            
              x
              
                7
              
            
            
              7
              !
            
          
        
        +
        ⋯
      
    
    {\displaystyle \sin x=x-{\frac {x^{3}}{3!}}+{\frac {x^{5}}{5!}}-{\frac {x^{7}}{7!}}+\cdots }
  
and

  
    
      
        cos
        ⁡
        x
        =
        1
        −
        
          
            
              x
              
                2
              
            
            
              2
              !
            
          
        
        +
        
          
            
              x
              
                4
              
            
            
              4
              !
            
          
        
        −
        
          
            
              x
              
                6
              
            
            
              6
              !
            
          
        
        +
        ⋯
      
    
    {\displaystyle \cos x=1-{\frac {x^{2}}{2!}}+{\frac {x^{4}}{4!}}-{\frac {x^{6}}{6!}}+\cdots }
  Use of rectification (computation of length) of the arc of a circle to give a proof of these results.  (The later method of Leibniz, using quadrature, i.e. computation of area under the arc of the circle, was not used.)
Use of the series expansion of 
  
    
      
        arctan
        ⁡
        x
      
    
    {\displaystyle \arctan x}
   to obtain the Leibniz formula for π:
  
    
      
        
          
            π
            4
          
        
        =
        1
        −
        
          
            1
            3
          
        
        +
        
          
            1
            5
          
        
        −
        
          
            1
            7
          
        
        +
        ⋯
      
    
    {\displaystyle {\frac {\pi }{4}}=1-{\frac {1}{3}}+{\frac {1}{5}}-{\frac {1}{7}}+\cdots }
  A rational approximation of error for the finite sum of their series of interest.  For example, the error, 
  
    
      
        
          f
          
            i
          
        
        (
        n
        +
        1
        )
      
    
    {\displaystyle f_{i}(n+1)}
  , (for n odd, and i = 1, 2, 3) for the series:
  
    
      
        
          
            π
            4
          
        
        ≈
        1
        −
        
          
            1
            3
          
        
        +
        
          
            1
            5
          
        
        −
        ⋯
        +
        (
        −
        1
        
          )
          
            (
            n
            −
            1
            )
            
              /
            
            2
          
        
        
          
            1
            n
          
        
        +
        (
        −
        1
        
          )
          
            (
            n
            +
            1
            )
            
              /
            
            2
          
        
        
          f
          
            i
          
        
        (
        n
        +
        1
        )
      
    
    {\displaystyle {\frac {\pi }{4}}\approx 1-{\frac {1}{3}}+{\frac {1}{5}}-\cdots +(-1)^{(n-1)/2}{\frac {1}{n}}+(-1)^{(n+1)/2}f_{i}(n+1)}
  

  
    
      
        
          where 
        
        
          f
          
            1
          
        
        (
        n
        )
        =
        
          
            1
            
              2
              n
            
          
        
        ,
         
        
          f
          
            2
          
        
        (
        n
        )
        =
        
          
            
              n
              
                /
              
              2
            
            
              
                n
                
                  2
                
              
              +
              1
            
          
        
        ,
         
        
          f
          
            3
          
        
        (
        n
        )
        =
        
          
            
              (
              n
              
                /
              
              2
              
                )
                
                  2
                
              
              +
              1
            
            
              (
              
                n
                
                  2
                
              
              +
              5
              )
              n
              
                /
              
              2
            
          
        
        .
      
    
    {\displaystyle {\text{where }}f_{1}(n)={\frac {1}{2n}},\ f_{2}(n)={\frac {n/2}{n^{2}+1}},\ f_{3}(n)={\frac {(n/2)^{2}+1}{(n^{2}+5)n/2}}.}
  Manipulation of error term to derive a faster converging series for 
  
    
      
        π
      
    
    {\displaystyle \pi }
  :
  
    
      
        
          
            π
            4
          
        
        =
        
          
            3
            4
          
        
        +
        
          
            1
            
              
                3
                
                  3
                
              
              −
              3
            
          
        
        −
        
          
            1
            
              
                5
                
                  3
                
              
              −
              5
            
          
        
        +
        
          
            1
            
              
                7
                
                  3
                
              
              −
              7
            
          
        
        −
        ⋯
      
    
    {\displaystyle {\frac {\pi }{4}}={\frac {3}{4}}+{\frac {1}{3^{3}-3}}-{\frac {1}{5^{3}-5}}+{\frac {1}{7^{3}-7}}-\cdots }
  Using the improved series to derive a rational expression, 104348/33215 for π correct up to nine decimal places, i.e. 3.141592653.
Use of an intuitive notion of limit to compute these results.
A semi-rigorous (see remark on limits above) method of differentiation of some trigonometric functions. However, they did not formulate the notion of a function, or have knowledge of the exponential or logarithmic functions.The works of the Kerala school were first written up for the Western world by Englishman C.M. Whish in 1835.  According to Whish, the Kerala mathematicians had ""laid the foundation for a complete system of fluxions"" and these works abounded ""with fluxional forms and series to be found in no work of foreign countries.""However, Whish's results were almost completely neglected, until over a century later, when the discoveries of the Kerala school were investigated again by C. Rajagopal and his associates.  Their work includes commentaries on the proofs of the arctan series in Yuktibhāṣā given in two papers, a commentary on the Yuktibhāṣā's proof of the sine and cosine series and two papers that provide the Sanskrit verses of the Tantrasangrahavakhya for the series for arctan, sin, and cosine (with English translation and commentary).Narayana Pandit is a 14th century mathematician who composed two important mathematical works, an arithmetical treatise, Ganita Kaumudi, and an algebraic treatise, Bijganita Vatamsa. Narayana is also thought to be the author of an elaborate commentary of Bhaskara II's Lilavati, titled Karmapradipika (or Karma-Paddhati). Madhava of Sangamagrama (c. 1340–1425) was the founder of the Kerala School.  Although it is possible that he wrote Karana Paddhati a work written sometime between 1375 and 1475, all we really know of his work comes from works of later scholars.
Parameshvara (c. 1370–1460) wrote commentaries on the works of Bhaskara I, Aryabhata and Bhaskara II. His Lilavati Bhasya, a commentary on Bhaskara II's Lilavati, contains one of his important discoveries: a version of the mean value theorem.  Nilakantha Somayaji (1444–1544) composed the Tantra Samgraha (which 'spawned' a later anonymous commentary Tantrasangraha-vyakhya and a further commentary by the name Yuktidipaika, written in 1501).  He elaborated and extended the contributions of Madhava.
Citrabhanu (c. 1530) was a 16th-century mathematician from Kerala who gave integer solutions to 21 types of systems of two simultaneous algebraic equations in two unknowns. These types are all the possible pairs of equations of the following seven forms:

  
    
      
        
          
            
              
              
                x
                +
                y
                =
                a
                ,
                 
                x
                −
                y
                =
                b
                ,
                 
                x
                y
                =
                c
                ,
                
                  x
                  
                    2
                  
                
                +
                
                  y
                  
                    2
                  
                
                =
                d
                ,
              
            
            
              
              
                
                  x
                  
                    2
                  
                
                −
                
                  y
                  
                    2
                  
                
                =
                e
                ,
                 
                
                  x
                  
                    3
                  
                
                +
                
                  y
                  
                    3
                  
                
                =
                f
                ,
                 
                
                  x
                  
                    3
                  
                
                −
                
                  y
                  
                    3
                  
                
                =
                g
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&x+y=a,\ x-y=b,\ xy=c,x^{2}+y^{2}=d,\\[8pt]&x^{2}-y^{2}=e,\ x^{3}+y^{3}=f,\ x^{3}-y^{3}=g\end{aligned}}}
  For each case, Citrabhanu gave an explanation and justification of his rule as well as an example. Some of his explanations are algebraic, while others are geometric.  Jyesthadeva (c. 1500–1575) was another member of the Kerala School.  His key work was the Yukti-bhāṣā (written in Malayalam, a regional language of Kerala).  Jyesthadeva presented proofs of most mathematical theorems and infinite series earlier discovered by Madhava and other Kerala School mathematicians.


== Charges of Eurocentrism ==
It has been suggested that Indian contributions to mathematics have not been given due acknowledgement in modern history and that many discoveries and inventions by Indian mathematicians are presently culturally attributed to their Western counterparts, as a result of Eurocentrism. According to G. G. Joseph's take on ""Ethnomathematics"":

[Their work] takes on board some of the objections raised about the classical Eurocentric trajectory. The awareness [of Indian and Arabic mathematics] is all too likely to be tempered with dismissive rejections of their importance compared to Greek mathematics. The contributions from other civilisations – most notably China and India, are perceived either as borrowers from Greek sources or having made only minor contributions to mainstream mathematical development. An openness to more recent research findings, especially in the case of Indian and Chinese mathematics, is sadly missing""
The historian of mathematics, Florian Cajori, suggested that he and others ""suspect that Diophantus got his first glimpse of algebraic knowledge from India."" However, he also wrote that ""it is certain that portions of Hindu mathematics are of Greek origin"".More recently, as discussed in the above section, the infinite series of calculus for trigonometric functions (rediscovered by Gregory, Taylor, and Maclaurin in the late 17th century) were described in India, by mathematicians of the Kerala school, remarkably some two centuries earlier. Some scholars have recently suggested that knowledge of these results might have been transmitted to Europe through the trade route from Kerala by traders and Jesuit missionaries. Kerala was in continuous contact with China and Arabia, and, from around 1500, with Europe.  The existence of communication routes and a suitable chronology certainly make such a transmission a possibility. However, there is no direct evidence by way of relevant manuscripts that such a transmission actually took place. According to David Bressoud, ""there is no evidence that the Indian work of series was known beyond India, or even outside of Kerala, until the nineteenth century.""Both Arab and Indian scholars made discoveries before the 17th century that are now considered a part of calculus. However, they did not, as Newton and Leibniz did, ""combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today."" The intellectual careers of both Newton and Leibniz are well-documented and there is no indication of their work not being their own; however, it is not known with certainty whether the immediate predecessors of Newton and Leibniz, ""including, in particular, Fermat and Roberval, learned of some of the ideas of the Islamic and Indian mathematicians through sources we are not now aware."" This is an active area of current research, especially in the manuscript collections of Spain and Maghreb.  This research is being pursued, among other places, at the CNRS.


== See also ==


== Notes ==


== References ==
Bourbaki, Nicolas (1998), Elements of the History of Mathematics, Berlin, Heidelberg, and New York: Springer-Verlag, 301 pages, ISBN 978-3-540-64767-6.
Boyer, C. B.; Merzback (fwd. by Isaac Asimov), U. C. (1991), History of Mathematics, New York: John Wiley and Sons, 736 pages, ISBN 978-0-471-54397-8.
Bressoud, David (2002), ""Was Calculus Invented in India?"", The College Mathematics Journal, 33 (1): 2–13, doi:10.2307/1558972, JSTOR 1558972.
Bronkhorst, Johannes (2001), ""Panini and Euclid: Reflections on Indian Geometry"", Journal of Indian Philosophy, Springer Netherlands, 29 (1–2): 43–80, doi:10.1023/A:1017506118885, S2CID 115779583.
Burnett, Charles (2006), ""The Semantics of Indian Numerals in Arabic, Greek and Latin"", Journal of Indian Philosophy, Springer-Netherlands, 34 (1–2): 15–30, doi:10.1007/s10781-005-8153-z, S2CID 170783929.
Burton, David M. (1997), The History of Mathematics: An Introduction, The McGraw-Hill Companies, Inc., pp. 193–220.
Cooke, Roger (2005), The History of Mathematics: A Brief Course, New York: Wiley-Interscience, 632 pages, ISBN 978-0-471-44459-6.
Dani, S. G. (25 July 2003), ""On the Pythagorean triples in the Śulvasūtras"" (PDF), Current Science, 85 (2): 219–224.
Datta, Bibhutibhusan (December 1931), ""Early Literary Evidence of the Use of the Zero in India"", The American Mathematical Monthly, 38 (10): 566–572, doi:10.2307/2301384, JSTOR 2301384.
Datta, Bibhutibhusan; Singh, Avadesh Narayan (1962), History of Hindu Mathematics : A source book, Bombay: Asia Publishing House.
De Young, Gregg (1995), ""Euclidean Geometry in the Mathematical Tradition of Islamic India"", Historia Mathematica, 22 (2): 138–153, doi:10.1006/hmat.1995.1014.
Kim Plofker (2007), ""mathematics, South Asian"", Encyclopaedia Britannica Online, pp. 1–12, retrieved 18 May 2007.
Filliozat, Pierre-Sylvain (2004), ""Ancient Sanskrit Mathematics: An Oral Tradition and a Written Literature"",  in Chemla, Karine; Cohen, Robert S.; Renn, Jürgen;  et al. (eds.), History of Science, History of Text (Boston Series in the Philosophy of Science), Dordrecht: Springer Netherlands, 254 pages, pp. 137–157, pp. 360–375, doi:10.1007/1-4020-2321-9_7, ISBN 978-1-4020-2320-0.
Fowler, David (1996), ""Binomial Coefficient Function"", The American Mathematical Monthly, 103 (1): 1–17, doi:10.2307/2975209, JSTOR 2975209.
Hayashi, Takao (1995), The Bakhshali Manuscript, An ancient Indian mathematical treatise, Groningen: Egbert Forsten, 596 pages, ISBN 978-90-6980-087-5.
Hayashi, Takao (1997), ""Aryabhata's Rule and Table of Sine-Differences"", Historia Mathematica, 24 (4): 396–406, doi:10.1006/hmat.1997.2160.
Hayashi, Takao (2003), ""Indian Mathematics"",  in Grattan-Guinness, Ivor (ed.), Companion Encyclopedia of the History and Philosophy of the Mathematical Sciences, vol. 1, Baltimore, MD: The Johns Hopkins University Press, pp. 118–130, ISBN 978-0-8018-7396-6.
Hayashi, Takao (2005), ""Indian Mathematics"",  in Flood, Gavin (ed.), The Blackwell Companion to Hinduism, Oxford: Basil Blackwell, 616 pages, pp. 360–375, pp. 360–375, ISBN 978-1-4051-3251-0.
Henderson, David W. (2000), ""Square roots in the Sulba Sutras"",  in Gorini, Catherine A. (ed.), Geometry at Work: Papers in Applied Geometry, vol. 53, Washington DC: Mathematical Association of America Notes, pp. 39–45, ISBN 978-0-88385-164-7.
Joseph, G. G. (2000), The Crest of the Peacock: The Non-European Roots of Mathematics, Princeton, NJ: Princeton University Press, 416 pages, ISBN 978-0-691-00659-8.
Katz, Victor J. (1995), ""Ideas of Calculus in Islam and India"", Mathematics Magazine, 68 (3): 163–174, doi:10.2307/2691411, JSTOR 2691411.
Katz, Victor J., ed. (2007), The Mathematics of Egypt, Mesopotamia, China, India, and Islam: A Sourcebook, Princeton, NJ: Princeton University Press, pp. 385–514, ISBN 978-0-691-11485-9.
Keller, Agathe (2005), ""Making diagrams speak, in Bhāskara I's commentary on the Aryabhaṭīya"" (PDF), Historia Mathematica, 32 (3): 275–302, doi:10.1016/j.hm.2004.09.001.
Kichenassamy, Satynad (2006), ""Baudhāyana's rule for the quadrature of the circle"", Historia Mathematica, 33 (2): 149–183, doi:10.1016/j.hm.2005.05.001.
Neugebauer, Otto; Pingree, David, eds. (1970), The Pañcasiddhāntikā of Varāhamihira, Copenhagen. New edition with translation and commentary, (2 Vols.).
Pingree, David (1971), ""On the Greek Origin of the Indian Planetary Model Employing a Double Epicycle"", Journal of Historical Astronomy, 2 (1): 80–85, Bibcode:1971JHA.....2...80P, doi:10.1177/002182867100200202, S2CID 118053453.
Pingree, David (1973), ""The Mesopotamian Origin of Early Indian Mathematical Astronomy"", Journal of Historical Astronomy, 4 (1): 1–12, Bibcode:1973JHA.....4....1P, doi:10.1177/002182867300400102, S2CID 125228353.
Pingree, David, ed. (1978), The Yavanajātaka of Sphujidhvaja, Harvard Oriental Series 48 (2 vols.), Edited, translated and commented by D. Pingree, Cambridge, MA.
Pingree, David (1988), ""Reviewed Work(s): The Fidelity of Oral Tradition and the Origins of Science by Frits Staal"", Journal of the American Oriental Society, 108 (4): 637–638, doi:10.2307/603154, JSTOR 603154.
Pingree, David (1992), ""Hellenophilia versus the History of Science"", Isis, 83 (4): 554–563, Bibcode:1992Isis...83..554P, doi:10.1086/356288, JSTOR 234257, S2CID 68570164
Pingree, David (2003), ""The logic of non-Western science: mathematical discoveries in medieval India"", Daedalus, 132 (4): 45–54, doi:10.1162/001152603771338779, S2CID 57559157.
Plofker, Kim (1996), ""An Example of the Secant Method of Iterative Approximation in a Fifteenth-Century Sanskrit Text"", Historia Mathematica, 23 (3): 246–256, doi:10.1006/hmat.1996.0026.
Plofker, Kim (2001), ""The ""Error"" in the Indian ""Taylor Series Approximation"" to the Sine"", Historia Mathematica, 28 (4): 283–295, doi:10.1006/hmat.2001.2331.
Plofker, K. (2007), ""Mathematics of India"",  in Katz, Victor J. (ed.), The Mathematics of Egypt, Mesopotamia, China, India, and Islam: A Sourcebook, Princeton, NJ: Princeton University Press, pp. 385–514, ISBN 978-0-691-11485-9.
Plofker, Kim (2009), Mathematics in India: 500 BCE–1800 CE, Princeton, NJ: Princeton University Press, ISBN 978-0-691-12067-6.
Price, John F. (2000), ""Applied geometry of the Sulba Sutras"" (PDF),  in Gorini, Catherine A. (ed.), Geometry at Work: Papers in Applied Geometry, vol. 53, Washington DC: Mathematical Association of America Notes, pp. 46–58, ISBN 978-0-88385-164-7, archived from the original (PDF) on 27 September 2007, retrieved 20 May 2007.
Roy, Ranjan (1990), ""Discovery of the Series Formula for 
  
    
      
        π
      
    
    {\displaystyle \pi }
   by Leibniz, Gregory, and Nilakantha"", Mathematics Magazine, 63 (5): 291–306, doi:10.2307/2690896, JSTOR 2690896.
Singh, A. N. (1936), ""On the Use of Series in Hindu Mathematics"", Osiris, 1 (1): 606–628, doi:10.1086/368443, JSTOR 301627, S2CID 144760421
Staal, Frits (1986), ""The Fidelity of Oral Tradition and the Origins of Science"", Mededelingen der Koninklijke Nederlandse Akademie von Wetenschappen, Afd. Letterkunde, New Series, Amsterdam: North Holland Publishing Company, 49 (8).
Staal, Frits (1995), ""The Sanskrit of science"", Journal of Indian Philosophy, Springer Netherlands, 23 (1): 73–127, doi:10.1007/BF01062067, S2CID 170755274.
Staal, Frits (1999), ""Greek and Vedic Geometry"", Journal of Indian Philosophy, 27 (1–2): 105–127, doi:10.1023/A:1004364417713, S2CID 170894641.
Staal, Frits (2001), ""Squares and oblongs in the Veda"", Journal of Indian Philosophy, Springer Netherlands, 29 (1–2): 256–272, doi:10.1023/A:1017527129520, S2CID 170403804.
Staal, Frits (2006), ""Artificial Languages Across Sciences and Civilisations"", Journal of Indian Philosophy, Springer Netherlands, 34 (1): 89–141, doi:10.1007/s10781-005-8189-0, S2CID 170968871.
Stillwell, John (2004), Mathematics and its History, Undergraduate Texts in Mathematics (2 ed.), Springer, Berlin and New York, 568 pages, doi:10.1007/978-1-4684-9281-1, ISBN 978-0-387-95336-6.
Thibaut, George (1984) [1875], Mathematics in the Making in Ancient India: reprints of 'On the Sulvasutras' and 'Baudhyayana Sulva-sutra', Calcutta and Delhi: K. P. Bagchi and Company (orig. Journal of the Asiatic Society of Bengal), 133 pages.
van der Waerden, B. L. (1983), Geometry and Algebra in Ancient Civilisations, Berlin and New York: Springer, 223 pages, ISBN 978-0-387-12159-8
van der Waerden, B. L. (1988), ""On the Romaka-Siddhānta"", Archive for History of Exact Sciences, 38 (1): 1–11, doi:10.1007/BF00329976, S2CID 189788738
van der Waerden, B. L. (1988), ""Reconstruction of a Greek table of chords"", Archive for History of Exact Sciences, 38 (1): 23–38, Bibcode:1988AHES...38...23V, doi:10.1007/BF00329978, S2CID 189793547
Van Nooten, B. (1993), ""Binary numbers in Indian antiquity"", Journal of Indian Philosophy, Springer Netherlands, 21 (1): 31–50, doi:10.1007/BF01092744, S2CID 171039636
Whish, Charles (1835), ""On the Hindú Quadrature of the Circle, and the infinite Series of the proportion of the circumference to the diameter exhibited in the four S'ástras, the Tantra Sangraham, Yucti Bháshá, Carana Padhati, and Sadratnamála"", Transactions of the Royal Asiatic Society of Great Britain and Ireland, 3 (3): 509–523, doi:10.1017/S0950473700001221, JSTOR 25581775
Yano, Michio (2006), ""Oral and Written Transmission of the Exact Sciences in Sanskrit"", Journal of Indian Philosophy, Springer Netherlands, 34 (1–2): 143–160, doi:10.1007/s10781-005-8175-6, S2CID 170679879


== Further reading ==


=== Source books in Sanskrit ===
Keller, Agathe (2006), Expounding the Mathematical Seed. Vol. 1: The Translation: A Translation of Bhaskara I on the Mathematical Chapter of the Aryabhatiya, Basel, Boston, and Berlin: Birkhäuser Verlag, 172 pages, ISBN 978-3-7643-7291-0.
Keller, Agathe (2006), Expounding the Mathematical Seed. Vol. 2: The Supplements: A Translation of Bhaskara I on the Mathematical Chapter of the Aryabhatiya, Basel, Boston, and Berlin: Birkhäuser Verlag, 206 pages, ISBN 978-3-7643-7292-7.
Sarma, K. V., ed. (1976), Āryabhaṭīya of Āryabhaṭa with the commentary of Sūryadeva Yajvan, critically edited with Introduction and Appendices, New Delhi: Indian National Science Academy.
Sen, S. N.; Bag, A. K., eds. (1983), The Śulbasūtras of Baudhāyana, Āpastamba, Kātyāyana and Mānava, with Text, English Translation and Commentary, New Delhi: Indian National Science Academy.
Shukla, K. S., ed. (1976), Āryabhaṭīya of Āryabhaṭa with the commentary of Bhāskara I and Someśvara, critically edited with Introduction, English Translation, Notes, Comments and Indexes, New Delhi: Indian National Science Academy.
Shukla, K. S., ed. (1988), Āryabhaṭīya of Āryabhaṭa, critically edited with Introduction, English Translation, Notes, Comments and Indexes, in collaboration with K.V. Sarma, New Delhi: Indian National Science Academy.


== External links ==

Science and Mathematics in India
An overview of Indian mathematics, MacTutor History of Mathematics Archive, St Andrews University, 2000.
Indian Mathematicians
Index of Ancient Indian mathematics, MacTutor History of Mathematics Archive, St Andrews University, 2004.
Indian Mathematics: Redressing the balance, Student Projects in the History of Mathematics.  Ian Pearce.  MacTutor History of Mathematics Archive, St Andrews University, 2002.
Indian Mathematics on In Our Time at the BBC
InSIGHT 2009, a workshop on traditional Indian sciences for school children conducted by the Computer Science department of Anna University, Chennai, India.
Mathematics in ancient India by R. Sridharan
Combinatorial methods in ancient India 
Mathematics before S. Ramanujan"
248b271d6f,Mathematical sciences,"The mathematical sciences are a group of areas of study that includes, in addition to mathematics, those academic disciplines that are primarily mathematical in nature but may not be universally considered subfields of mathematics proper.
Statistics, for example, is mathematical in its methods but grew out of bureaucratic and scientific observations, which merged with inverse probability and then grew through applications in some areas of physics, biometrics, and the social sciences to become its own separate, though closely allied, field. Theoretical astronomy, theoretical physics, theoretical and applied mechanics, continuum mechanics, mathematical chemistry, actuarial science, computer science, computational science, data science, operations research, quantitative biology, control theory, econometrics, geophysics and mathematical geosciences are likewise other fields often considered part of the mathematical sciences.
Some institutions offer degrees in mathematical sciences (e.g. the United States Military Academy, Stanford University, and University of Khartoum) or applied mathematical sciences (for example, the University of Rhode Island).


== See also ==
Exact sciences – Sciences that admit of absolute precision in their results
Formal science – Branch of science
Relationship between mathematics and physics – Study of how mathematics and physics relate to each other


== References ==


== External links ==
Division of Mathematical Sciences at the National Science Foundation, including a list of disciplinary areas supported
Faculty of Mathematical Sciences at University of Khartoum, offers academic degrees in Mathematics, Computer Sciences and Statistics
Programs of the Mathematical Sciences Research Institute
Research topics studied at the Isaac Newton Institute for Mathematical Sciences
Mathematical Sciences in the U.S. FY 2016 Budget; a report from the AAAS"
48e7a551fc,Expression (mathematics),"In mathematics, an expression or mathematical expression is a finite combination of symbols that is well-formed according to rules that depend on the context.  Mathematical symbols can designate numbers (constants), variables, operations, functions, brackets, punctuation, and grouping to help determine order of operations and other aspects of logical syntax.
Many authors distinguish an expression from a formula, the former denoting a mathematical object, and the latter denoting a statement about mathematical objects. For example, 
  
    
      
        8
        x
        −
        5
      
    
    {\displaystyle 8x-5}
   is an expression, while 
  
    
      
        8
        x
        −
        5
        ≥
        5
        x
        −
        8
      
    
    {\displaystyle 8x-5\geq 5x-8}
   is a formula. However, in modern mathematics, and in particular in computer algebra, formulas are viewed as expressions that can be evaluated to true or false, depending on the values that are given to the variables occurring in the expressions. For example 
  
    
      
        8
        x
        −
        5
        ≥
        5
        x
        −
        8
      
    
    {\displaystyle 8x-5\geq 5x-8}
   takes the value false if x is given a value less than –1, and the value true otherwise.


== Examples ==
The use of expressions ranges from the simple:

  
    
      
        3
        +
        8
      
    
    {\displaystyle 3+8}
  
  
    
      
        8
        x
        −
        5
      
    
    {\displaystyle 8x-5}
     (linear polynomial)
  
    
      
        7
        
          
            
              x
            
            
              2
            
          
        
        +
        4
        x
        −
        10
      
    
    {\displaystyle 7{{x}^{2}}+4x-10}
     (quadratic polynomial)
  
    
      
        
          
            
              x
              −
              1
            
            
              
                
                  
                    x
                  
                  
                    2
                  
                
              
              +
              12
            
          
        
      
    
    {\displaystyle {\frac {x-1}{{{x}^{2}}+12}}}
     (rational fraction)to the complex: 

  
    
      
        f
        (
        a
        )
        +
        
          ∑
          
            k
            =
            1
          
          
            n
          
        
        
          
            
            
              
                
                  1
                  
                    k
                    !
                  
                
              
              
                
                  
                    d
                    
                      k
                    
                  
                  
                    d
                    
                      t
                      
                        k
                      
                    
                  
                
              
            
            |
          
          
            t
            =
            0
          
        
        f
        (
        u
        (
        t
        )
        )
        +
        
          ∫
          
            0
          
          
            1
          
        
        
          
            
              (
              1
              −
              t
              
                )
                
                  n
                
              
            
            
              n
              !
            
          
        
        
          
            
              d
              
                n
                +
                1
              
            
            
              d
              
                t
                
                  n
                  +
                  1
                
              
            
          
        
        f
        (
        u
        (
        t
        )
        )
        
        d
        t
        .
      
    
    {\displaystyle f(a)+\sum _{k=1}^{n}\left.{\frac {1}{k!}}{\frac {d^{k}}{dt^{k}}}\right|_{t=0}f(u(t))+\int _{0}^{1}{\frac {(1-t)^{n}}{n!}}{\frac {d^{n+1}}{dt^{n+1}}}f(u(t))\,dt.}
  


== Syntax versus semantics ==


=== Syntax ===

An expression is a syntactic construct. It must be well-formed: the allowed operators must have the correct number of inputs in the correct places, the characters that make up these inputs must be valid, have a clear order of operations, etc. Strings of symbols that violate the rules of syntax are not well-formed and are not valid mathematical expressions.
For example, in the usual notation of arithmetic, the expression 1 + 2 × 3 is well-formed, but the following expression is not:

  
    
      
        ×
        4
        )
        x
        +
        ,
        
          /
        
        y
      
    
    {\displaystyle \times 4)x+,/y}
  .


=== Semantics ===

Semantics is the study of meaning. Formal semantics is about attaching meaning to expressions.
In algebra, an expression may be used to designate a value, which might depend on values assigned to variables occurring in the expression. The determination of this value depends on the semantics attached to the symbols of the expression.  The choice of semantics depends on the context of the expression.  The same syntactic expression 1 + 2 × 3 can have different values (mathematically 7, but also 9), depending on the order of operations implied by the context (See also Operations § Calculators).
The semantic rules may declare that certain expressions do not designate any value (for instance when they involve division by 0); such expressions are said to have an undefined value, but they are well-formed expressions nonetheless. In general the meaning of expressions is not limited to designating values; for instance, an expression might designate a condition, or an equation that is to be solved, or it can be viewed as an object in its own right that can be manipulated according to certain rules. Certain expressions that designate a value simultaneously express a condition that is assumed to hold, for instance those involving the operator 
  
    
      
        ⊕
      
    
    {\displaystyle \oplus }
   to designate an internal direct sum.


=== Formal languages and lambda calculus ===

Formal languages allow formalizing the concept of well-formed expressions.
In the 1930s, a new type of expressions, called lambda expressions, were introduced by Alonzo Church and Stephen Kleene for formalizing functions and their evaluation. They form the basis for lambda calculus, a formal system used in mathematical logic and the theory of programming languages.
The equivalence of two lambda expressions is undecidable. This is also the case for the expressions representing real numbers, which are built from the integers by using the arithmetical operations, the logarithm and the exponential (Richardson's theorem).


== Variables ==
Many mathematical expressions include variables. Any variable can be classified as being either a free variable or a bound variable.
For a given combination of values for the free variables, an expression may be evaluated, although for some combinations of values of the free variables, the value of the expression may be undefined. Thus an expression represents a function whose inputs are the values assigned to the free variables and whose output is the resulting value of the expression.For example, the expression

  
    
      
        x
        
          /
        
        y
      
    
    {\displaystyle x/y}
  evaluated for x = 10, y = 5, will give 2; but it is undefined for y = 0.
The evaluation of an expression is dependent on the definition of the mathematical operators and on the system of values that is its context.
Two expressions are said to be equivalent if, for each combination of values for the free variables, they have the same output, i.e., they represent the same function. Example:
The expression

  
    
      
        
          ∑
          
            n
            =
            1
          
          
            3
          
        
        (
        2
        n
        x
        )
      
    
    {\displaystyle \sum _{n=1}^{3}(2nx)}
  has free variable x, bound variable n, constants 1, 2, and 3, two occurrences of an implicit multiplication operator, and a summation operator. The expression is equivalent to the simpler expression 12x. The value for x = 3 is 36.


== See also ==


== Notes ==


== References ==
Redden, John (2011). ""Elementary Algebra"". Flat World Knowledge. Archived from the original on 2014-11-15. Retrieved 2012-03-18."
ee6b34d10f,Antiparallel (mathematics),"In geometry, two lines 
  
    
      
        
          l
          
            1
          
        
      
    
    {\displaystyle l_{1}}
   and 
  
    
      
        
          l
          
            2
          
        
      
    
    {\displaystyle l_{2}}
   are antiparallel with respect to a given line 
  
    
      
        m
      
    
    {\displaystyle m}
   if they each make congruent angles with 
  
    
      
        m
      
    
    {\displaystyle m}
   in opposite senses. More generally, lines 
  
    
      
        
          l
          
            1
          
        
      
    
    {\displaystyle l_{1}}
   and 
  
    
      
        
          l
          
            2
          
        
      
    
    {\displaystyle l_{2}}
   are antiparallel with respect to another pair of lines 
  
    
      
        
          m
          
            1
          
        
      
    
    {\displaystyle m_{1}}
   and 
  
    
      
        
          m
          
            2
          
        
      
    
    {\displaystyle m_{2}}
   if they are antiparallel with respect to the angle bisector of 
  
    
      
        
          m
          
            1
          
        
      
    
    {\displaystyle m_{1}}
   and 
  
    
      
        
          m
          
            2
          
        
        .
      
    
    {\displaystyle m_{2}.}
  
In any cyclic quadrilateral, any two opposite sides are antiparallel with respect to the other two sides.


== Relations ==
The line joining the feet to two altitudes of a triangle is antiparallel to the third side. (any cevians which 'see' the third side with the same angle create antiparallel lines)
The tangent to a triangle's circumcircle at a vertex is antiparallel to the opposite side.
The radius of the circumcircle at a vertex is perpendicular to all lines antiparallel to the opposite sides.


== References ==

A.B. Ivanov, Encyclopaedia of Mathematics - ISBN 1-4020-0609-8
Weisstein, Eric W. ""Antiparallel."" From MathWorld—A Wolfram Web Resource. [1]"
b200ecc043,Mathematical game,"A mathematical game is a game whose rules, strategies, and outcomes are defined by clear mathematical parameters. Often, such games have simple rules and match procedures, such as tic-tac-toe and dots and boxes. Generally, mathematical games need not be conceptually intricate to involve deeper computational underpinnings. For example, even though the rules of Mancala are relatively basic, the game can be rigorously analyzed through the lens of combinatorial game theory.Mathematical games differ sharply from mathematical puzzles in that mathematical puzzles require specific mathematical expertise to complete, whereas mathematical games do not require a deep knowledge of mathematics to play. Often, the arithmetic core of mathematical games is not readily apparent to players untrained to note the statistical or mathematical aspects.Some mathematical games are of deep interest in the field of recreational mathematics.When studying a game's core mathematics, arithmetic theory is generally of higher utility than actively playing or observing the game itself. To analyze a game numerically, it is particularly useful to study the rules of the game insofar as they can yield equations or relevant formulas. This is frequently done to determine winning strategies or to distinguish if the game has a solution.


== List of games ==
Sometimes it is not immediately obvious that a particular game involves chance. Often a card game is described as ""pure strategy"" and such, but a game with any sort of random shuffling or face-down dealing of cards should not be considered to be ""no chance"".  Several abstract strategy games are listed below:


=== Lattice board ===
Angels and Devils
Arimaa
Checkers (English draughts)
Checkers variants
Chess
Chess variants
Chomp
Domineering
Dots and boxes
Go
Go variants
Gomoku
Hex
Hexapawn
L game
Othello
Pente
Philosopher's football
Rhythmomachy
Tak
Tic-tac-toe
Tic-tac-toe variants


=== Non-lattice boards and other games ===
Graph pebbling
Hackenbush
Chopsticks (hand game)
Mancala
Nim
Sim
Sprouts
Four fours


=== Chance involved or imperfect information ===
24
Prisoner's dilemma


== See also ==
Games of skill


== References =="
61c8ca1b0c,Encyclopedia of Mathematics,"The Encyclopedia of Mathematics (also EOM and formerly Encyclopaedia of Mathematics) is a large reference work in mathematics.


== Overview ==
The 2002 version contains more than 8,000 entries covering most areas of mathematics at a graduate level, and the presentation is technical in nature. The encyclopedia is edited by Michiel Hazewinkel and was published by Kluwer Academic Publishers until 2003, when Kluwer became part of Springer. The CD-ROM contains animations and three-dimensional objects.
The encyclopedia has been translated from the Soviet Matematicheskaya entsiklopediya (1977) originally edited by Ivan Matveevich Vinogradov and extended with comments and three supplements adding several thousand articles.
Until November 29, 2011, a static version of the encyclopedia could be browsed online free of charge online. This URL now redirects to the new wiki incarnation of the EOM.


== Encyclopedia of Mathematics wiki ==
A new dynamic version of the encyclopedia is now available as a public wiki online. This new wiki is a collaboration between Springer and the European Mathematical Society. This new version of the encyclopedia includes the entire contents of the previous online version, but all entries can now be publicly updated to include the newest advancements in mathematics. All entries will be monitored for content accuracy by members of an editorial board selected by the European Mathematical Society.


== Versions ==

Vinogradov, I. M. (Ed.), Matematicheskaya entsiklopediya, Moscow, Sov. Entsiklopediya, 1977.
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics (set), Kluwer, 1994 (ISBN 1-55608-010-7).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 1 (A–B), Kluwer, 1987 (ISBN 1-55608-000-X).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 2 (C), Kluwer, 1988 (ISBN 1-55608-001-8).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 3 (D–Fey), Kluwer, 1989 (ISBN 1-55608-002-6).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 4 (Fib–H), Kluwer, 1989 (ISBN 1-55608-003-4).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 5 (I–Lit), Kluwer, 1990 (ISBN 1-55608-004-2).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 6 (Lob–Opt), Kluwer, 1990 (ISBN 1-55608-005-0).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 7 (Orb–Ray), Kluwer, 1991 (ISBN 1-55608-006-9).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 8 (Rea–Sti), Kluwer, 1992 (ISBN 1-55608-007-7).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 9 (Sto–Zyg), Kluwer, 1993 (ISBN 1-55608-008-5).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Vol. 10 (Index), Kluwer, 1994 (ISBN 1-55608-009-3).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Supplement I, Kluwer, 1997 (ISBN 0-7923-4709-9).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Supplement II, Kluwer, 2000 (ISBN 0-7923-6114-8).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics, Supplement III, Kluwer, 2002 (ISBN 1-4020-0198-3).
Hazewinkel, M. (Ed.), Encyclopaedia of Mathematics on CD-ROM, Kluwer, 1998 (ISBN 0-7923-4805-2).
Encyclopedia of Mathematics, public wiki monitored by an editorial board under the management of the European Mathematical Society.


== See also ==
List of online encyclopedias


== References ==


== External links ==

Official website
Publications by M. Hazewinkel at ResearchGate"
63570bb8b9,Map (mathematics),"In mathematics, a map or mapping is a function in its general sense.  These terms may have originated as from the process of making a geographical map: mapping the Earth surface to a sheet of paper.The term map may be used to distinguish some special types of functions, such as homomorphisms. For example, a linear map is a homomorphism of vector spaces, while the term linear function may have this meaning or it may mean a linear polynomial. In category theory, a map may refer to a morphism. The term transformation can be used interchangeably, but transformation often refers to a function from a set to itself. There are also a few less common uses in logic and graph theory.


== Maps as functions ==

In many branches of mathematics, the term map is used to mean a function, sometimes with a specific property of particular importance to that branch. For instance, a ""map"" is a ""continuous function"" in topology, a ""linear transformation"" in linear algebra, etc.
Some authors, such as Serge Lang, use ""function"" only to refer to maps in which the codomain is a set of numbers (i.e. a subset of R or C), and reserve the term mapping for more general functions.
Maps of certain kinds are the subjects of many important theories. These include homomorphisms in abstract algebra, isometries in geometry, operators in analysis and representations in group theory.In the theory of dynamical systems, a map denotes an evolution function used to create discrete dynamical systems. 
A partial map is a partial function. Related terms such as domain, codomain, injective, and continuous can be applied equally to maps and functions, with the same meaning. All these usages can be applied to ""maps"" as general functions or as functions with special properties.


== As morphisms ==

In category theory, ""map"" is often used as a synonym for ""morphism"" or ""arrow"", which is a structure-respecting function and thus may imply more structure than ""function"" does. For example, a morphism 
  
    
      
        f
        :
        
        X
        →
        Y
      
    
    {\displaystyle f:\,X\to Y}
   in a concrete category (i.e. a morphism that can be viewed as a function) carries with it the information of its domain (the source 
  
    
      
        X
      
    
    {\displaystyle X}
   of the morphism) and its codomain (the target 
  
    
      
        Y
      
    
    {\displaystyle Y}
  ). In the widely used definition of a function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
  , 
  
    
      
        f
      
    
    {\displaystyle f}
   is a subset of 
  
    
      
        X
        ×
        Y
      
    
    {\displaystyle X\times Y}
   consisting of all the pairs 
  
    
      
        (
        x
        ,
        f
        (
        x
        )
        )
      
    
    {\displaystyle (x,f(x))}
   for 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
  . In this sense, the function does not capture the set 
  
    
      
        Y
      
    
    {\displaystyle Y}
   that is used as the codomain; only the range 
  
    
      
        f
        (
        X
        )
      
    
    {\displaystyle f(X)}
   is determined by the function.


== See also ==
Apply function – Function that maps a function and its arguments to the function value
Arrow notation – e.g., 
  
    
      
        x
        ↦
        x
        +
        1
      
    
    {\displaystyle x\mapsto x+1}
  , also known as map
Bijection, injection and surjection – Properties of mathematical functions
Homeomorphism – Mapping which preserves all topological properties of a given space
List of chaotic maps
Maplet arrow (↦) – commonly pronounced ""maps to""
Mapping class group – Group of isotopy classes of a topological automorphism group
Permutation group – Group whose operation is composition of permutations
Regular map (algebraic geometry) – Morphism of algebraic varieties


== References ==


=== Works cited ===
Halmos, Paul R. (1970). Naive Set Theory. Springer-Verlag. ISBN 978-0-387-90092-6.


== External links =="
640c6a368d,List of letters used in mathematics and science,"This list is about the meanings of the letters used in mathematics, science and engineering. SI units are indicated in parentheses.  For the Unicode blocksee Mathematical Alphanumeric Symbols.Latin and Greek letters are used in mathematics, science, engineering, and other areas where mathematical notation is used as symbols for constants, special functions, and also conventionally for variables representing certain quantities.

Some common conventions:Intensive quantities in physics are usually denoted with minusculeswhile extensive are denoted with capital letters.
Most symbols are written in italics.
Vectors can be denoted in boldface.
Sets of numbers are typically bold or blackboard bold.


== Latin ==


== Greek ==


== More ==


== See also ==
Blackboard bold letters used in mathematics
Greek letters used in mathematics, science, and engineering
Latin letters used in mathematics"
cd9b3b726f,Finite mathematics,"In mathematics education, Finite Mathematics is a syllabus in college and university mathematics that is independent of calculus. A course in precalculus may be a prerequisite for Finite Mathematics.
Contents of the course include an eclectic selection of topics often applied in social science and business, such as finite probability spaces, matrix multiplication, Markov processes, finite graphs, or mathematical models. These topics were used in Finite Mathematics courses at Dartmouth College (home of Tuck School of Business) as developed by John G. Kemeny, Gerald L. Thompson, and J. Laurie Snell and published by Prentice-Hall. Other publishers followed with their own topics. With the arrival of software to facilitate computations, teaching and usage shifted from a broad-spectrum Finite Mathematics with paper and pen, into development and usage of software.


== Textbooks ==
1957: Kemeny, Thompson, Snell, Introduction to Finite Mathematics, (2nd edition 1966) Prentice-Hall
1959: Hazelton Mirkil & Kemeny, Thompson, Snell, Finite Mathematical Structures, Prentice-Hall
1962: Arthur Schliefer Jr. & Kemeny, Thompson, Snell, Finite Mathematics with Business Applications, Prentice-Hall
1969: Marvin Marcus, A Survey of Finite Mathematics, Houghton-Mifflin
1970: Guillermo Owen, Mathematics for Social and Management Sciences, Finite Mathematics, W. B. Saunders
1970: Irving Allen Dodes, Finite Mathematics: A Liberal Arts Approach, McGraw-Hill
1971: A.W. Goodman & J. S. Ratti, Finite Mathematics with Applications, Macmillan
1971: J. Conrad Crown & Marvin L. Bittinger, Finite Mathematics: a modeling approach, (2nd edition 1981) Addison-Wesley
1977: Robert F. Brown & Brenda W. Brown, Applied Finite Mathematics, Wadsworth Publishing
1980: L.J. Goldstein, David I. Schneider, Martha Siegel, Finite Mathematics and Applications, (7th edition 2001) Prentice-Hall
1981: John J. Costello, Spenser O. Gowdy, Agnes M. Rash, Finite Mathematics with Applications, Harcourt, Brace, Jovanovich
1982: James Radlow, Understanding Finite Mathematics, PWS Publishers
1984: Daniel Gallin, Finite Mathematics, Scott Foresman
1984: Gary G. Gilbert & Donald O. Koehler, Applied Finite Mathematics, McGraw-Hill
1984: Frank S. Budnick, Finite Mathematics with Applications in Management and the Social Sciences, McGraw Hill
2015: Chris P. Tsokos & Rebecca D. Wooton, The Joy of Finite Mathematics, Academic Press


== See also ==
Business mathematics § Undergraduate
Discrete mathematics
Finite geometry
Finite group, Finite ring, Finite field
Finite topological space


== References =="
30c7b3eeaf,Limit (mathematics),"In mathematics, a limit is the value that a function (or sequence) approaches as the input (or index) approaches some value. Limits are essential to calculus and mathematical analysis, and are used to define continuity, derivatives, and integrals.
The concept of a limit of a sequence is further generalized to the concept of a limit of a topological net, and is closely related to limit and direct limit in category theory.
In formulas, a limit of a function is usually written as

  
    
      
        
          lim
          
            x
            →
            c
          
        
        f
        (
        x
        )
        =
        L
        ,
      
    
    {\displaystyle \lim _{x\to c}f(x)=L,}
  (although a few authors use ""Lt"" instead of ""lim"")
and is read as ""the limit of f of x as x approaches c equals L"". The fact that a function f approaches the limit L as x approaches c is sometimes denoted by a right arrow (→ or 
  
    
      
        →
      
    
    {\displaystyle \rightarrow }
  ), as in

  
    
      
        f
        (
        x
        )
        →
        L
        
           as 
        
        x
        →
        c
        ,
      
    
    {\displaystyle f(x)\to L{\text{ as }}x\to c,}
  which reads ""
  
    
      
        f
      
    
    {\displaystyle f}
   of 
  
    
      
        x
      
    
    {\displaystyle x}
   tends to 
  
    
      
        L
      
    
    {\displaystyle L}
   as 
  
    
      
        x
      
    
    {\displaystyle x}
   tends to 
  
    
      
        c
      
    
    {\displaystyle c}
  "".


== History ==
Grégoire de Saint-Vincent gave the first definition of limit (terminus) of a geometric series in his work Opus Geometricum (1647): ""The terminus of a progression is the end of the series, which none progression can reach, even not if she is continued in infinity, but which she can approach nearer than a given segment.""The modern definition of a limit goes back to Bernard Bolzano who, in 1817, developed the basics of the epsilon-delta technique to define continuous functions. However, his work remained unknown to other mathematicians until thirty years after his death.Augustin-Louis Cauchy in 1821, followed by Karl Weierstrass, formalized the definition of the limit of a function which became known as the (ε, δ)-definition of limit.
The modern notation of placing the arrow below the limit symbol is due to G. H. Hardy, who introduced it in his book A Course of Pure Mathematics in 1908.


== Types of limits ==


=== In sequences ===


==== Real numbers ====

The expression 0.999... should be interpreted as the limit of the sequence 0.9, 0.99, 0.999, ... and so on. This sequence can be rigorously shown to have the limit 1, and therefore this expression is meaningfully interpreted as having the value 1.Formally, suppose a1, a2, … is a sequence of real numbers. When the limit of the sequence exists, the real number L is the limit of this sequence if and only if for every real number ε > 0, there exists a natural number N such that for all n > N, we have |an − L| < ε.
The notation

is often used, and which is read as

""The limit of an as n approaches infinity equals L""The formal definition intuitively means that eventually, all elements of the sequence get arbitrarily close to the limit, since the absolute value |an − L| is the distance between an and L. 
Not every sequence has a limit. If it does, then it is called convergent, and if it does not, then it is divergent. One can show that a convergent sequence has only one limit.
The limit of a sequence and the limit of a function are closely related. On one hand, the limit as n approaches infinity of a sequence {an} is simply the limit at infinity of a function a(n)—defined on the natural numbers {n}. On the other hand, if X is the domain of a function f(x) and if the limit as n approaches infinity of f(xn) is L for every arbitrary sequence of points {xn} in {X – {x0}} which converges to x0, then the limit of the function f(x) as x approaches x0 is L. One such sequence would be {x0 + 1/n}.


==== Infinity as a limit ====
There is also a notion of having a limit ""at infinity"", as opposed to at some finite 
  
    
      
        L
      
    
    {\displaystyle L}
  . A sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   is said to ""tend to infinity"" if, for each real number 
  
    
      
        M
        >
        0
      
    
    {\displaystyle M>0}
  , known as the bound, there exists an integer 
  
    
      
        N
      
    
    {\displaystyle N}
   such that for each 
  
    
      
        n
        >
        N
      
    
    {\displaystyle n>N}
  , 

That is, for every possible bound, the magnitude of the sequence eventually exceeds the bound. This is often written 
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        =
        ∞
      
    
    {\displaystyle \lim _{n\rightarrow \infty }a_{n}=\infty }
   or simply 
  
    
      
        
          a
          
            n
          
        
        →
        ∞
      
    
    {\displaystyle a_{n}\rightarrow \infty }
  . Such sequences are also called unbounded.
It is possible for a sequence to be divergent, but not tend to infinity. Such sequences are called oscillatory. An example of an oscillatory sequence is 
  
    
      
        
          a
          
            n
          
        
        =
        (
        −
        1
        
          )
          
            n
          
        
      
    
    {\displaystyle a_{n}=(-1)^{n}}
  .
For the real numbers, there are corresponding notions of tending to positive infinity and negative infinity, by removing the modulus sign from the above definition:

defines tending to positive infinity, while 

defines tending to negative infinity.
Sequences which do not tend to infinity are called bounded. Sequences which do not tend to positive infinity are called bounded above, while those which do not tend to negative infinity are bounded below.


==== Metric space ====
The discussion of sequences above is for sequences of real numbers. The notion of limits can be defined for sequences valued in more abstract spaces. One example of a more abstract space is metric spaces. If 
  
    
      
        M
      
    
    {\displaystyle M}
   is a metric space with distance function 
  
    
      
        d
      
    
    {\displaystyle d}
  , and 
  
    
      
        {
        
          a
          
            n
          
        
        
          }
          
            n
            ≥
            0
          
        
      
    
    {\displaystyle \{a_{n}\}_{n\geq 0}}
   is a sequence in 
  
    
      
        M
      
    
    {\displaystyle M}
  , then the limit (when it exists) of the sequence is an element 
  
    
      
        a
        ∈
        M
      
    
    {\displaystyle a\in M}
   such that, given 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
  , there exists an 
  
    
      
        N
      
    
    {\displaystyle N}
   such that for each 
  
    
      
        n
        >
        N
      
    
    {\displaystyle n>N}
  , the equation

is satisfied.
An equivalent statement is that 
  
    
      
        
          a
          
            n
          
        
        →
        a
      
    
    {\displaystyle a_{n}\rightarrow a}
   if the sequence of real numbers 
  
    
      
        d
        (
        a
        ,
        
          a
          
            n
          
        
        )
        →
        0
      
    
    {\displaystyle d(a,a_{n})\rightarrow 0}
  .


===== Example: ℝn =====
An important example is the space of 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional real vectors, with elements 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        ⋯
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {x} =(x_{1},\cdots ,x_{n})}
   where each of the 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   are real, an example of a suitable distance function is the Euclidean distance, defined by

The sequence of points 
  
    
      
        {
        
          
            x
          
          
            n
          
        
        
          }
          
            n
            ≥
            0
          
        
      
    
    {\displaystyle \{\mathbf {x} _{n}\}_{n\geq 0}}
   converges to 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   if the limit exists and 
  
    
      
        
          |
        
        
          
            x
          
          
            n
          
        
        −
        
          x
        
        
          |
        
        →
        0
      
    
    {\displaystyle |\mathbf {x} _{n}-\mathbf {x} |\rightarrow 0}
  .


==== Topological space ====
In some sense the most abstract space in which limits can be defined are topological spaces. If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a topological space with topology 
  
    
      
        τ
      
    
    {\displaystyle \tau }
  , and 
  
    
      
        {
        
          a
          
            n
          
        
        
          }
          
            n
            ≥
            0
          
        
      
    
    {\displaystyle \{a_{n}\}_{n\geq 0}}
   is a sequence in 
  
    
      
        X
      
    
    {\displaystyle X}
  , then the limit (when it exists) of the sequence is a point 
  
    
      
        a
        ∈
        X
      
    
    {\displaystyle a\in X}
   such that, given a (open) neighborhood 
  
    
      
        U
        ∈
        τ
      
    
    {\displaystyle U\in \tau }
   of 
  
    
      
        a
      
    
    {\displaystyle a}
  , there exists an 
  
    
      
        N
      
    
    {\displaystyle N}
   such that for every 
  
    
      
        n
        >
        N
      
    
    {\displaystyle n>N}
  ,

is satisfied.


==== Function space ====
This section deals with the idea of limits of sequences of functions, not to be confused with the idea of limits of functions, discussed below.
The field of functional analysis partly seeks to identify useful notions of convergence on function spaces. For example, consider the space of functions from a generic set 
  
    
      
        E
      
    
    {\displaystyle E}
   to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  . Given a sequence of functions 
  
    
      
        {
        
          f
          
            n
          
        
        
          }
          
            n
            >
            0
          
        
      
    
    {\displaystyle \{f_{n}\}_{n>0}}
   such that each is a function 
  
    
      
        
          f
          
            n
          
        
        :
        E
        →
        
          R
        
      
    
    {\displaystyle f_{n}:E\rightarrow \mathbb {R} }
  , suppose that there exists a function such that for each 
  
    
      
        x
        ∈
        E
      
    
    {\displaystyle x\in E}
  ,

Then the sequence 
  
    
      
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{n}}
   is said to  converge pointwise to 
  
    
      
        f
      
    
    {\displaystyle f}
  . However, such sequences can exhibit unexpected behavior. For example, it is possible to construct a sequence of continuous functions which has a discontinuous pointwise limit. 
Another notion of convergence is uniform convergence. The uniform distance between two functions 
  
    
      
        f
        ,
        g
        :
        E
        →
        
          R
        
      
    
    {\displaystyle f,g:E\rightarrow \mathbb {R} }
   is the maximum difference between the two functions as the argument 
  
    
      
        x
        ∈
        E
      
    
    {\displaystyle x\in E}
   is varied. That is, 

Then the sequence 
  
    
      
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{n}}
   is said to uniformly converge or have a uniform limit of 
  
    
      
        f
      
    
    {\displaystyle f}
   if 
  
    
      
        
          f
          
            n
          
        
        →
        f
      
    
    {\displaystyle f_{n}\rightarrow f}
   with respect to this distance. The uniform limit has ""nicer"" properties than the pointwise limit. For example, the uniform limit of a sequence of continuous functions is continuous.
Many different notions of convergence can be defined on function spaces. This is sometimes dependent on the  regularity of the space. Prominent examples of function spaces with some notion of convergence are Lp spaces and Sobolev space.


=== In functions ===

Suppose f is a real-valued function and c is a real number. Intuitively speaking, the expression

  
    
      
        
          lim
          
            x
            →
            c
          
        
        f
        (
        x
        )
        =
        L
      
    
    {\displaystyle \lim _{x\to c}f(x)=L}
  means that f(x) can be made to be as close to L as desired, by making x sufficiently close to c.  In that case, the above equation can be read as ""the limit of f of x, as x approaches c, is L"".
Formally, the definition of the ""limit of 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   as 
  
    
      
        x
      
    
    {\displaystyle x}
   approaches 
  
    
      
        c
      
    
    {\displaystyle c}
  "" is given as follows. The limit is a real number 
  
    
      
        L
      
    
    {\displaystyle L}
   so that, given an arbitrary real number 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
   (thought of as the ""error""), there is a 
  
    
      
        δ
        >
        0
      
    
    {\displaystyle \delta >0}
   such that, for any 
  
    
      
        x
      
    
    {\displaystyle x}
   satisfying 
  
    
      
        0
        <
        
          |
        
        x
        −
        c
        
          |
        
        <
        δ
      
    
    {\displaystyle 0<|x-c|<\delta }
  , it holds that 
  
    
      
        
          |
        
        f
        (
        x
        )
        −
        L
        
          |
        
        <
        ϵ
      
    
    {\displaystyle |f(x)-L|<\epsilon }
  . This is known as the (ε, δ)-definition of limit. 
The inequality 
  
    
      
        0
        <
        
          |
        
        x
        −
        c
        
          |
        
      
    
    {\displaystyle 0<|x-c|}
   is used to exclude 
  
    
      
        c
      
    
    {\displaystyle c}
   from the set of points under consideration, but some authors do not include this in their definition of limits, replacing 
  
    
      
        0
        <
        
          |
        
        x
        −
        c
        
          |
        
        <
        δ
      
    
    {\displaystyle 0<|x-c|<\delta }
   with simply 
  
    
      
        
          |
        
        x
        −
        c
        
          |
        
        <
        δ
      
    
    {\displaystyle |x-c|<\delta }
  . This replacement is equivalent to additionally requiring that 
  
    
      
        f
      
    
    {\displaystyle f}
   be continuous at 
  
    
      
        c
      
    
    {\displaystyle c}
  .
It can be proven that there is an equivalent definition which makes manifest the connection between limits of sequences and limits of functions. The equivalent definition is given as follows. First observe that for every sequence 
  
    
      
        {
        
          x
          
            n
          
        
        }
      
    
    {\displaystyle \{x_{n}\}}
   in the domain of 
  
    
      
        f
      
    
    {\displaystyle f}
  , there is an associated sequence 
  
    
      
        {
        f
        (
        
          x
          
            n
          
        
        )
        }
      
    
    {\displaystyle \{f(x_{n})\}}
  , the image of the sequence under 
  
    
      
        f
      
    
    {\displaystyle f}
  . The limit is a real number 
  
    
      
        L
      
    
    {\displaystyle L}
   so that, for all sequences 
  
    
      
        
          x
          
            n
          
        
        →
        c
      
    
    {\displaystyle x_{n}\rightarrow c}
  , the associated sequence 
  
    
      
        f
        (
        
          x
          
            n
          
        
        )
        →
        L
      
    
    {\displaystyle f(x_{n})\rightarrow L}
  .


==== One-sided limit ====

It is possible to define the notion of having a ""left-handed"" limit (""from below""), and a notion of a ""right-handed"" limit (""from above""). These need not agree. An example is given by the positive indicator function, 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \rightarrow \mathbb {R} }
  , defined such that 
  
    
      
        f
        (
        x
        )
        =
        0
      
    
    {\displaystyle f(x)=0}
   if 
  
    
      
        x
        ≤
        0
      
    
    {\displaystyle x\leq 0}
  , and 
  
    
      
        f
        (
        x
        )
        =
        1
      
    
    {\displaystyle f(x)=1}
   if 
  
    
      
        x
        >
        0
      
    
    {\displaystyle x>0}
  . At 
  
    
      
        x
        =
        0
      
    
    {\displaystyle x=0}
  , the function has a ""left-handed limit"" of 0, a ""right-handed limit"" of 1, and its limit does not exist. Symbolically, this can be stated as

  
    
      
        
          lim
          
            x
            →
            
              c
              
                −
              
            
          
        
        f
        (
        x
        )
        =
        0
      
    
    {\displaystyle \lim _{x\to c^{-}}f(x)=0}
  , and 
  
    
      
        
          lim
          
            x
            →
            
              c
              
                +
              
            
          
        
        f
        (
        x
        )
        =
        1
      
    
    {\displaystyle \lim _{x\to c^{+}}f(x)=1}
  , and from this it can be deduced 
  
    
      
        
          lim
          
            x
            →
            c
          
        
        f
        (
        x
        )
      
    
    {\displaystyle \lim _{x\to c}f(x)}
   doesn't exist, because 
  
    
      
        
          lim
          
            x
            →
            
              c
              
                −
              
            
          
        
        f
        (
        x
        )
        ≠
        
          lim
          
            x
            →
            
              c
              
                +
              
            
          
        
        f
        (
        x
        )
      
    
    {\displaystyle \lim _{x\to c^{-}}f(x)\neq \lim _{x\to c^{+}}f(x)}
  .


==== Infinity in limits of functions ====
It is possible to define the notion of ""tending to infinity"" in the domain of 
  
    
      
        f
      
    
    {\displaystyle f}
  ,

In this expression, the infinity is considered to be signed: either 
  
    
      
        +
        ∞
      
    
    {\displaystyle +\infty }
   or 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  . The ""limit of f as x tends to positive infinity"" is defined as follows. It is a real number 
  
    
      
        L
      
    
    {\displaystyle L}
   such that, given any real 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
  , there exists an 
  
    
      
        M
        >
        0
      
    
    {\displaystyle M>0}
   so that if 
  
    
      
        x
        >
        M
      
    
    {\displaystyle x>M}
  , 
  
    
      
        
          |
        
        f
        (
        x
        )
        −
        L
        
          |
        
        <
        ϵ
      
    
    {\displaystyle |f(x)-L|<\epsilon }
  . Equivalently, for any sequence 
  
    
      
        
          x
          
            n
          
        
        →
        +
        ∞
      
    
    {\displaystyle x_{n}\rightarrow +\infty }
  , we have 
  
    
      
        f
        (
        
          x
          
            n
          
        
        )
        →
        L
      
    
    {\displaystyle f(x_{n})\rightarrow L}
  .
It is also possible to define the notion of ""tending to infinity"" in the value of 
  
    
      
        f
      
    
    {\displaystyle f}
  ,

The definition is given as follows. Given any real number 
  
    
      
        M
        >
        0
      
    
    {\displaystyle M>0}
  , there is a 
  
    
      
        δ
        >
        0
      
    
    {\displaystyle \delta >0}
   so that for 
  
    
      
        0
        <
        
          |
        
        x
        −
        c
        
          |
        
        <
        δ
      
    
    {\displaystyle 0<|x-c|<\delta }
  , the absolute value of the function 
  
    
      
        
          |
        
        f
        (
        x
        )
        
          |
        
        >
        M
      
    
    {\displaystyle |f(x)|>M}
  . Equivalently, for any sequence 
  
    
      
        
          x
          
            n
          
        
        →
        c
      
    
    {\displaystyle x_{n}\rightarrow c}
  , the sequence 
  
    
      
        f
        (
        
          x
          
            n
          
        
        )
        →
        ∞
      
    
    {\displaystyle f(x_{n})\rightarrow \infty }
  .


=== Nonstandard analysis ===
In non-standard analysis (which involves a hyperreal enlargement of the number system), the limit of a sequence 
  
    
      
        (
        
          a
          
            n
          
        
        )
      
    
    {\displaystyle (a_{n})}
   can be expressed as the standard part of the value 
  
    
      
        
          a
          
            H
          
        
      
    
    {\displaystyle a_{H}}
   of the natural extension of the sequence at an infinite hypernatural index n=H.  Thus,

  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        =
        st
        ⁡
        (
        
          a
          
            H
          
        
        )
        .
      
    
    {\displaystyle \lim _{n\to \infty }a_{n}=\operatorname {st} (a_{H}).}
  Here, the standard part function ""st"" rounds off each finite hyperreal number to the nearest real number (the difference between them is infinitesimal). This formalizes the natural intuition that for ""very large"" values of the index, the terms in the sequence are ""very close"" to the limit value of the sequence. Conversely, the standard part of a hyperreal 
  
    
      
        a
        =
        [
        
          a
          
            n
          
        
        ]
      
    
    {\displaystyle a=[a_{n}]}
   represented in the ultrapower construction by a Cauchy sequence 
  
    
      
        (
        
          a
          
            n
          
        
        )
      
    
    {\displaystyle (a_{n})}
  , is simply the limit of that sequence:

  
    
      
        st
        ⁡
        (
        a
        )
        =
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        .
      
    
    {\displaystyle \operatorname {st} (a)=\lim _{n\to \infty }a_{n}.}
  In this sense, taking the limit and taking the standard part are equivalent procedures.


=== Limit sets ===


==== Limit set of a sequence ====
Let 
  
    
      
        {
        
          a
          
            n
          
        
        
          }
          
            n
            >
            0
          
        
      
    
    {\displaystyle \{a_{n}\}_{n>0}}
   be a sequence in a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
  . For concreteness, 
  
    
      
        X
      
    
    {\displaystyle X}
   can be thought of as 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , but the definitions hold more generally. The limit set is the set of points such that if there is a convergent subsequence 
  
    
      
        {
        
          a
          
            
              n
              
                k
              
            
          
        
        
          }
          
            k
            >
            0
          
        
      
    
    {\displaystyle \{a_{n_{k}}\}_{k>0}}
   with 
  
    
      
        
          a
          
            
              n
              
                k
              
            
          
        
        →
        a
      
    
    {\displaystyle a_{n_{k}}\rightarrow a}
  , then 
  
    
      
        a
      
    
    {\displaystyle a}
   belongs to the limit set. In this context, such an 
  
    
      
        a
      
    
    {\displaystyle a}
   is sometimes called a limit point.
A use of this notion is to characterize the ""long-term behavior"" of oscillatory sequences. For example, consider the sequence 
  
    
      
        
          a
          
            n
          
        
        =
        (
        −
        1
        
          )
          
            n
          
        
      
    
    {\displaystyle a_{n}=(-1)^{n}}
  . Starting from n=1, the first few terms of this sequence are 
  
    
      
        −
        1
        ,
        +
        1
        ,
        −
        1
        ,
        +
        1
        ,
        ⋯
      
    
    {\displaystyle -1,+1,-1,+1,\cdots }
  . It can be checked that it is oscillatory, so has no limit, but has limit points 
  
    
      
        {
        −
        1
        ,
        +
        1
        }
      
    
    {\displaystyle \{-1,+1\}}
  .


==== Limit set of a trajectory ====
This notion is used in dynamical systems, to study limits of trajectories. Defining a trajectory to be a function 
  
    
      
        γ
        :
        
          R
        
        →
        X
      
    
    {\displaystyle \gamma :\mathbb {R} \rightarrow X}
  , the point 
  
    
      
        γ
        (
        t
        )
      
    
    {\displaystyle \gamma (t)}
   is thought of as the ""position"" of the trajectory at ""time"" 
  
    
      
        t
      
    
    {\displaystyle t}
  . The limit set of a trajectory is defined as follows. To any sequence of increasing times 
  
    
      
        {
        
          t
          
            n
          
        
        }
      
    
    {\displaystyle \{t_{n}\}}
  , there is an associated sequence of positions 
  
    
      
        {
        
          x
          
            n
          
        
        }
        =
        {
        γ
        (
        
          t
          
            n
          
        
        )
        }
      
    
    {\displaystyle \{x_{n}\}=\{\gamma (t_{n})\}}
  . If 
  
    
      
        x
      
    
    {\displaystyle x}
   is the limit set of the sequence 
  
    
      
        {
        
          x
          
            n
          
        
        }
      
    
    {\displaystyle \{x_{n}\}}
   for any sequence of increasing times, then 
  
    
      
        x
      
    
    {\displaystyle x}
   is a limit set of the trajectory.
Technically, this is the 
  
    
      
        ω
      
    
    {\displaystyle \omega }
  -limit set. The corresponding limit set for sequences of decreasing time is called the 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  -limit set.
An illustrative example is the circle trajectory: 
  
    
      
        γ
        (
        t
        )
        =
        (
        cos
        ⁡
        (
        t
        )
        ,
        sin
        ⁡
        (
        t
        )
        )
      
    
    {\displaystyle \gamma (t)=(\cos(t),\sin(t))}
  . This has no unique limit, but for each 
  
    
      
        θ
        ∈
        
          R
        
      
    
    {\displaystyle \theta \in \mathbb {R} }
  , the point 
  
    
      
        (
        cos
        ⁡
        (
        θ
        )
        ,
        sin
        ⁡
        (
        θ
        )
        )
      
    
    {\displaystyle (\cos(\theta ),\sin(\theta ))}
   is a limit point, given by the sequence of times 
  
    
      
        
          t
          
            n
          
        
        =
        θ
        +
        2
        π
        n
      
    
    {\displaystyle t_{n}=\theta +2\pi n}
  . But the limit points need not be attained on the trajectory. The trajectory 
  
    
      
        γ
        (
        t
        )
        =
        t
        
          /
        
        (
        1
        +
        t
        )
        (
        cos
        ⁡
        (
        t
        )
        ,
        sin
        ⁡
        (
        t
        )
        )
      
    
    {\displaystyle \gamma (t)=t/(1+t)(\cos(t),\sin(t))}
   also has the unit circle as its limit set.


== Uses ==
Limits are used to define a number of important concepts in analysis.


=== Series ===

A particular expression of interest which is formalized as the limit of a sequence is sums of infinite series. These are ""infinite sums"" of real numbers, generally written as

This is defined through limits as follows: given a sequence of real numbers 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
  , the sequence of partial sums is defined by

If the limit of the sequence 
  
    
      
        {
        
          s
          
            n
          
        
        }
      
    
    {\displaystyle \{s_{n}\}}
   exists, the value of the expression 
  
    
      
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          a
          
            n
          
        
      
    
    {\displaystyle \sum _{n=1}^{\infty }a_{n}}
   is defined to be the limit. Otherwise, the series is said to be divergent.
A classic example is the Basel problem, where 
  
    
      
        
          a
          
            n
          
        
        =
        1
        
          /
        
        
          n
          
            2
          
        
      
    
    {\displaystyle a_{n}=1/n^{2}}
  . Then 

However, while for sequences there is essentially a unique notion of convergence, for series there are different notions of convergence. This is due to the fact that the expression 
  
    
      
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          a
          
            n
          
        
      
    
    {\displaystyle \sum _{n=1}^{\infty }a_{n}}
   does not discriminate between different orderings of the sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
  , while the convergence properties of the sequence of partial sums can depend on the ordering of the sequence. 
A series which converges for all orderings is called unconditionally convergent. It can be proven to be equivalent to absolute convergence. This is defined as follows. A series is absolutely convergent if 
  
    
      
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          |
        
        
          a
          
            n
          
        
        
          |
        
      
    
    {\displaystyle \sum _{n=1}^{\infty }|a_{n}|}
   is well defined. Furthermore, all possible orderings give the same value.
Otherwise, the series is conditionally convergent. A surprising result for conditionally convergent series is the Riemann series theorem: depending on the ordering, the partial sums can be made to converge to any real number, as well as 
  
    
      
        ±
        ∞
      
    
    {\displaystyle \pm \infty }
  .


==== Power series ====

A useful application of the theory of sums of series is for power series. These are sums of series of the form

Often 
  
    
      
        z
      
    
    {\displaystyle z}
   is thought of as a complex number, and a suitable notion of convergence of complex sequences is needed. The set of values of 
  
    
      
        z
        ∈
        
          C
        
      
    
    {\displaystyle z\in \mathbb {C} }
   for which the series sum converges is a circle, with its radius known as the radius of convergence.


=== Continuity of a function at a point ===
The definition of continuity at a point is given through limits.
The above definition of a limit is true even if 
  
    
      
        f
        (
        c
        )
        ≠
        L
      
    
    {\displaystyle f(c)\neq L}
  . Indeed, the function f need not even be defined at c. However, if 
  
    
      
        f
        (
        c
        )
      
    
    {\displaystyle f(c)}
   is defined and is equal to 
  
    
      
        L
      
    
    {\displaystyle L}
  , then the function is said to be continuous at the point 
  
    
      
        c
      
    
    {\displaystyle c}
  .
Equivalently, the function is continuous at 
  
    
      
        c
      
    
    {\displaystyle c}
   if 
  
    
      
        f
        (
        x
        )
        →
        f
        (
        c
        )
      
    
    {\displaystyle f(x)\rightarrow f(c)}
   as 
  
    
      
        x
        →
        c
      
    
    {\displaystyle x\rightarrow c}
  , or in terms of sequences, whenever 
  
    
      
        
          x
          
            n
          
        
        →
        c
      
    
    {\displaystyle x_{n}\rightarrow c}
  , then 
  
    
      
        f
        (
        
          x
          
            n
          
        
        )
        →
        f
        (
        c
        )
      
    
    {\displaystyle f(x_{n})\rightarrow f(c)}
  .
An example of a limit where 
  
    
      
        f
      
    
    {\displaystyle f}
   is not defined at 
  
    
      
        c
      
    
    {\displaystyle c}
   is given below.
Consider the function

then f(1) is not defined (see Indeterminate form), yet as x moves arbitrarily close to 1, f(x) correspondingly approaches 2:
Thus, f(x) can be made arbitrarily close to the limit of 2—just by making x sufficiently close to 1.
In other words,

This can also be calculated algebraically, as 
  
    
      
        
          
            
              
                x
                
                  2
                
              
              −
              1
            
            
              x
              −
              1
            
          
        
        =
        
          
            
              (
              x
              +
              1
              )
              (
              x
              −
              1
              )
            
            
              x
              −
              1
            
          
        
        =
        x
        +
        1
      
    
    {\textstyle {\frac {x^{2}-1}{x-1}}={\frac {(x+1)(x-1)}{x-1}}=x+1}
   for all real numbers x ≠ 1.
Now, since x + 1 is continuous in x at 1, we can now plug in 1 for x, leading to the equation

In addition to limits at finite values, functions can also have limits at infinity. For example, consider the function

where:

f(100) = 1.9900
f(1000) = 1.9990
f(10000) = 1.9999As x becomes extremely large, the value of f(x) approaches 2, and the value of f(x) can be made as close to 2 as one could wish—by making x sufficiently large. So in this case, the limit of f(x) as x approaches infinity is 2, or in mathematical notation,


=== Continuous functions ===
An important class of functions when considering limits are continuous functions. These are precisely those functions which preserve limits, in the sense that if 
  
    
      
        f
      
    
    {\displaystyle f}
   is a continuous function, then whenever 
  
    
      
        
          a
          
            n
          
        
        →
        a
      
    
    {\displaystyle a_{n}\rightarrow a}
   in the domain of 
  
    
      
        f
      
    
    {\displaystyle f}
  , then the limit 
  
    
      
        f
        (
        
          a
          
            n
          
        
        )
      
    
    {\displaystyle f(a_{n})}
   exists and furthermore is 
  
    
      
        f
        (
        a
        )
      
    
    {\displaystyle f(a)}
  .
In the most general setting of topological spaces, a short proof is given below:
Let 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\rightarrow Y}
   be a continuous function between topological spaces 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  . By definition, for each open set 
  
    
      
        V
      
    
    {\displaystyle V}
   in 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , the preimage 
  
    
      
        
          f
          
            −
            1
          
        
        (
        V
        )
      
    
    {\displaystyle f^{-1}(V)}
   is open in 
  
    
      
        X
      
    
    {\displaystyle X}
  .
Now suppose 
  
    
      
        
          a
          
            n
          
        
        →
        a
      
    
    {\displaystyle a_{n}\rightarrow a}
   is a sequence with limit 
  
    
      
        a
      
    
    {\displaystyle a}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
  . Then 
  
    
      
        f
        (
        
          a
          
            n
          
        
        )
      
    
    {\displaystyle f(a_{n})}
   is a sequence in 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , and 
  
    
      
        f
        (
        a
        )
      
    
    {\displaystyle f(a)}
   is some point.
Choose a neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        f
        (
        a
        )
      
    
    {\displaystyle f(a)}
  . Then 
  
    
      
        
          f
          
            −
            1
          
        
        (
        V
        )
      
    
    {\displaystyle f^{-1}(V)}
   is an open set (by continuity of 
  
    
      
        f
      
    
    {\displaystyle f}
  ) which in particular contains 
  
    
      
        a
      
    
    {\displaystyle a}
  , and therefore 
  
    
      
        
          f
          
            −
            1
          
        
        (
        V
        )
      
    
    {\displaystyle f^{-1}(V)}
   is a neighborhood of 
  
    
      
        a
      
    
    {\displaystyle a}
  . By the convergence of 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   to 
  
    
      
        a
      
    
    {\displaystyle a}
  , there exists an 
  
    
      
        N
      
    
    {\displaystyle N}
   such that for 
  
    
      
        n
        >
        N
      
    
    {\displaystyle n>N}
  , we have 
  
    
      
        
          a
          
            n
          
        
        ∈
        
          f
          
            −
            1
          
        
        (
        V
        )
      
    
    {\displaystyle a_{n}\in f^{-1}(V)}
  .
Then applying 
  
    
      
        f
      
    
    {\displaystyle f}
   to both sides gives that, for the same 
  
    
      
        N
      
    
    {\displaystyle N}
  , for each 
  
    
      
        n
        >
        N
      
    
    {\displaystyle n>N}
   we have 
  
    
      
        f
        (
        
          a
          
            n
          
        
        )
        ∈
        V
      
    
    {\displaystyle f(a_{n})\in V}
  . Originally 
  
    
      
        V
      
    
    {\displaystyle V}
   was an arbitrary neighborhood of 
  
    
      
        f
        (
        a
        )
      
    
    {\displaystyle f(a)}
  , so 
  
    
      
        f
        (
        
          a
          
            n
          
        
        )
        →
        f
        (
        a
        )
      
    
    {\displaystyle f(a_{n})\rightarrow f(a)}
  . This concludes the proof.
In real analysis, for the more concrete case of real-valued functions defined on a subset 
  
    
      
        E
        ⊂
        
          R
        
      
    
    {\displaystyle E\subset \mathbb {R} }
  , that is, 
  
    
      
        f
        :
        E
        →
        
          R
        
      
    
    {\displaystyle f:E\rightarrow \mathbb {R} }
  , a continuous function may also be defined as a function which is continuous at every point of its domain.


=== Limit points ===
In topology, limits are used to define limit points of a subset of a topological space, which in turn give a useful characterization of closed sets.
In a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
  , consider a subset 
  
    
      
        S
      
    
    {\displaystyle S}
  . A point 
  
    
      
        a
      
    
    {\displaystyle a}
   is called a limit point if there is a sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   in 
  
    
      
        S
        ∖
        {
        a
        }
      
    
    {\displaystyle S\backslash \{a\}}
   such that 
  
    
      
        
          a
          
            n
          
        
        →
        a
      
    
    {\displaystyle a_{n}\rightarrow a}
  .
The reason why 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   is defined to be in 
  
    
      
        S
        ∖
        {
        a
        }
      
    
    {\displaystyle S\backslash \{a\}}
   rather than just 
  
    
      
        S
      
    
    {\displaystyle S}
   is illustrated by the following example. Take 
  
    
      
        X
        =
        
          R
        
      
    
    {\displaystyle X=\mathbb {R} }
   and 
  
    
      
        S
        =
        [
        0
        ,
        1
        ]
        ∪
        {
        2
        }
      
    
    {\displaystyle S=[0,1]\cup \{2\}}
  . Then 
  
    
      
        2
        ∈
        S
      
    
    {\displaystyle 2\in S}
  , and therefore is the limit of the constant sequence 
  
    
      
        2
        ,
        2
        ,
        ⋯
      
    
    {\displaystyle 2,2,\cdots }
  . But 
  
    
      
        2
      
    
    {\displaystyle 2}
   is not a limit point of 
  
    
      
        S
      
    
    {\displaystyle S}
  .
A closed set, which is defined to be the complement of an open set, is equivalently any set 
  
    
      
        C
      
    
    {\displaystyle C}
   which contains all its limit points.


=== Derivative ===

The derivative is defined formally as a limit. In the scope of real analysis, the derivative is first defined for real functions 
  
    
      
        f
      
    
    {\displaystyle f}
   defined on a subset 
  
    
      
        E
        ⊂
        
          R
        
      
    
    {\displaystyle E\subset \mathbb {R} }
  . The derivative at 
  
    
      
        x
        ∈
        E
      
    
    {\displaystyle x\in E}
   is defined as follows. If the limit

as 
  
    
      
        h
        →
        0
      
    
    {\displaystyle h\rightarrow 0}
   exists, then the derivative at 
  
    
      
        x
      
    
    {\displaystyle x}
   is this limit.
Equivalently, it is the limit as 
  
    
      
        y
        →
        x
      
    
    {\displaystyle y\rightarrow x}
   of

If the derivative exists, it is commonly denoted by 
  
    
      
        
          f
          ′
        
        (
        x
        )
      
    
    {\displaystyle f'(x)}
  .


== Properties ==


=== Sequences of real numbers ===
For sequences of real numbers, a number of properties can be proven. Suppose 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   and 
  
    
      
        {
        
          b
          
            n
          
        
        }
      
    
    {\displaystyle \{b_{n}\}}
   are two sequences converging to 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   respectively.

Sum of limits is equal to limit of sum

Product of limits is equal to limit of product

Inverse of limit is equal to limit of inverse (as long as 
  
    
      
        a
        ≠
        0
      
    
    {\displaystyle a\neq 0}
  )
Equivalently, the function 
  
    
      
        f
        (
        x
        )
        =
        1
        
          /
        
        x
      
    
    {\displaystyle f(x)=1/x}
   is continuous about nonzero 
  
    
      
        x
      
    
    {\displaystyle x}
  .


==== Cauchy sequences ====

A property of convergent sequences of real numbers is that they are Cauchy sequences. The definition of a Cauchy sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   is that for every real number 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
  , there is an 
  
    
      
        N
      
    
    {\displaystyle N}
   such that whenever 
  
    
      
        m
        ,
        n
        >
        N
      
    
    {\displaystyle m,n>N}
  , 

Informally, for any arbitrarily small error 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  , it is possible to find an interval of diameter 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   such that eventually the sequence is contained within the interval. 
Cauchy sequences are closely related to convergent sequences. In fact, for sequences of real numbers they are equivalent: any Cauchy sequence is convergent. 
In general metric spaces, it continues to hold that convergent sequences are also Cauchy. But the converse is not true: not every Cauchy sequence is convergent in a general metric space. A classic counterexample is the rational numbers, 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  , with the usual distance. The sequence of decimal approximations to 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
  , truncated at the 
  
    
      
        n
      
    
    {\displaystyle n}
  th decimal place is a Cauchy sequence, but does not converge in 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  .
A metric space in which every Cauchy sequence is also convergent, that is, Cauchy sequences are equivalent to convergent sequences, is known as a complete metric space.
One reason Cauchy sequences can be ""easier to work with"" than convergent sequences is that they are a property of the sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   alone, while convergent sequences require not just the sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   but also the limit of the sequence 
  
    
      
        a
      
    
    {\displaystyle a}
  .


=== Order of convergence ===
Beyond whether or not a sequence 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
   converges to a limit 
  
    
      
        a
      
    
    {\displaystyle a}
  , it is possible to describe how fast a sequence converges to a limit. One way to quantify this is using the order of convergence of a sequence.
A formal definition of order of convergence can be stated as follows. Suppose 
  
    
      
        {
        
          a
          
            n
          
        
        
          }
          
            n
            >
            0
          
        
      
    
    {\displaystyle \{a_{n}\}_{n>0}}
   is a sequence of real numbers which is convergent with limit 
  
    
      
        a
      
    
    {\displaystyle a}
  . Furthermore, 
  
    
      
        
          a
          
            n
          
        
        ≠
        a
      
    
    {\displaystyle a_{n}\neq a}
   for all 
  
    
      
        n
      
    
    {\displaystyle n}
  . If positive constants 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   and 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   exist such that

then 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   is said to converge to 
  
    
      
        a
      
    
    {\displaystyle a}
   with order of convergence 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  . The constant 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is known as the asymptotic error constant.
Order of convergence is used for example the field of numerical analysis, in error analysis.


=== Computability ===
Limits can be difficult to compute. There exist limit expressions whose modulus of convergence is undecidable. In recursion theory, the limit lemma proves that it is possible to encode undecidable problems using limits.There are several theorems or tests that indicate whether the limit exists. These are known as convergence tests. Examples include the ratio test and the squeeze theorem. However they may not tell how to compute the limit.


== See also ==
Asymptotic analysis: a method of describing limiting behavior
Big O notation: used to describe the limiting behavior of a function when the argument tends towards a particular value or infinity
Banach limit defined on the Banach space 
  
    
      
        
          ℓ
          
            ∞
          
        
      
    
    {\displaystyle \ell ^{\infty }}
   that extends the usual limits.
Convergence of random variables
Convergent matrix
Limit in category theory
Direct limit
Inverse limit
Limit of a function
One-sided limit: either of the two limits of functions of a real variable x, as x approaches a point from above or below
List of limits: list of limits for common functions
Squeeze theorem: finds a limit of a function via comparison with two other functions
Limit superior and limit inferior
Modes of convergence
An annotated index


== Notes ==


== References ==
Apostol, Tom M. (1974), Mathematical Analysis (2nd ed.), Menlo Park: Addison-Wesley, LCCN 72011473


== External links =="
c6e2e05fbe,Duality (mathematics),"In mathematics, a duality translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of A is B, then the dual of B is A. Such involutions sometimes have fixed points, so that the dual of A is A itself. For example, Desargues' theorem is self-dual in this sense under the standard duality in projective geometry.
In mathematical contexts, duality has numerous meanings. It has been described as ""a very pervasive and important concept in (modern) mathematics"" and ""an important general theme that has manifestations in almost every area of mathematics"".Many mathematical dualities between objects of two types correspond to pairings, bilinear functions from an object of one type and another object of the second type to some family of scalars. For instance, linear algebra duality corresponds in this way to bilinear maps from pairs of vector spaces to scalars, the duality between distributions and the associated test functions corresponds to the pairing in which one integrates a distribution against a test function, and Poincaré duality corresponds similarly to intersection number, viewed as a pairing between submanifolds of a given manifold.From a category theory viewpoint, duality can also be seen as a functor, at least in the realm of vector spaces. This functor assigns to each space its dual space, and the pullback construction assigns to each arrow f: V → W its dual f∗: W∗ → V∗.


== Introductory examples ==
In the words of Michael Atiyah,

Duality in mathematics is not a theorem, but a ""principle"".
The following list of examples shows the common features of many dualities, but also indicates that the precise meaning of duality may vary from case to case.


=== Complement of a subset ===
A simple, maybe the most simple, duality arises from considering subsets of a fixed set S. To any subset A ⊆ S, the complement Ac consists of all those elements in S that are not contained in A. It is again a subset of S. Taking the complement has the following properties:

Applying it twice gives back the original set, i.e., (Ac)c = A. This is referred to by saying that the operation of taking the complement is an involution.
An inclusion of sets A ⊆ B is turned into an inclusion in the opposite direction Bc ⊆ Ac.
Given two subsets A and B of S, A is contained in Bc if and only if B is contained in Ac.This duality appears in topology as a duality between open and closed subsets of some fixed topological space X: a subset U of X is closed if and only if its complement in X is open.  Because of this, many theorems about closed sets are dual to theorems about open sets. For example, any union of open sets is open, so dually, any intersection of closed sets is closed.  The interior of a set is the largest open set contained in it, and the closure of the set is the smallest closed set that contains it.  Because of the duality, the complement of the interior of any set U is equal to the closure of the complement of U.


=== Dual cone ===

A duality in geometry is provided by the dual cone construction. Given a set 
  
    
      
        C
      
    
    {\displaystyle C}
   of points in the plane 
  
    
      
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {R} ^{2}}
   (or more generally points in 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  ), the dual cone is defined as the set 
  
    
      
        
          C
          
            ∗
          
        
        ⊆
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle C^{*}\subseteq \mathbb {R} ^{2}}
   consisting of those points 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        )
      
    
    {\displaystyle (x_{1},x_{2})}
   satisfying

for all points 
  
    
      
        (
        
          c
          
            1
          
        
        ,
        
          c
          
            2
          
        
        )
      
    
    {\displaystyle (c_{1},c_{2})}
   in 
  
    
      
        C
      
    
    {\displaystyle C}
  , as illustrated in the diagram.
Unlike for the complement of sets mentioned above, it is not in general true that applying the dual cone construction twice gives back the original set 
  
    
      
        C
      
    
    {\displaystyle C}
  . Instead, 
  
    
      
        
          C
          
            ∗
            ∗
          
        
      
    
    {\displaystyle C^{**}}
   is the smallest cone containing 
  
    
      
        C
      
    
    {\displaystyle C}
   which may be bigger than 
  
    
      
        C
      
    
    {\displaystyle C}
  . Therefore this duality is weaker than the one above, in that

Applying the operation twice gives back a possibly bigger set: for all 
  
    
      
        C
      
    
    {\displaystyle C}
  , 
  
    
      
        C
      
    
    {\displaystyle C}
   is contained in 
  
    
      
        
          C
          
            ∗
            ∗
          
        
      
    
    {\displaystyle C^{**}}
  . (For some 
  
    
      
        C
      
    
    {\displaystyle C}
  , namely the cones, the two are actually equal.)The other two properties carry over without change:

It is still true that an inclusion 
  
    
      
        C
        ⊆
        D
      
    
    {\displaystyle C\subseteq D}
   is turned into an inclusion in the opposite direction (
  
    
      
        
          D
          
            ∗
          
        
        ⊆
        
          C
          
            ∗
          
        
      
    
    {\displaystyle D^{*}\subseteq C^{*}}
  ).
Given two subsets 
  
    
      
        C
      
    
    {\displaystyle C}
   and 
  
    
      
        D
      
    
    {\displaystyle D}
   of the plane, 
  
    
      
        C
      
    
    {\displaystyle C}
   is contained in 
  
    
      
        
          D
          
            ∗
          
        
      
    
    {\displaystyle D^{*}}
   if and only if 
  
    
      
        D
      
    
    {\displaystyle D}
   is contained in 
  
    
      
        
          C
          
            ∗
          
        
      
    
    {\displaystyle C^{*}}
  .


=== Dual vector space ===
A very important example of a duality arises in linear algebra by associating to any vector space V its dual vector space V*. Its elements are the linear functionals 
  
    
      
        φ
        :
        V
        →
        K
      
    
    {\displaystyle \varphi :V\to K}
  , where K is the field over which V is defined.
The three properties of the dual cone carry over to this type of duality by replacing subsets of 
  
    
      
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {R} ^{2}}
   by vector space and inclusions of such subsets by linear maps. That is:

Applying the operation of taking the dual vector space twice gives another vector space V**. There is always a map V → V**. For some V, namely precisely the finite-dimensional vector spaces, this map is an isomorphism.
A linear map V → W gives rise to a map in the opposite direction (W* → V*).
Given two vector spaces V and W, the maps from V to W* correspond to the maps from W to V*.A particular feature of this duality is that V and V* are isomorphic for certain objects, namely finite-dimensional vector spaces. However, this is in a sense a lucky coincidence, for giving such an isomorphism requires a certain choice, for example the choice of a basis of V. This is also true in the case if V is a Hilbert space, via the Riesz representation theorem.


=== Galois theory ===
In all the dualities discussed before, the dual of an object is of the same kind as the object itself. For example, the dual of a vector space is again a vector space. Many duality statements are not of this kind. Instead, such dualities reveal a close relation between objects of seemingly different nature. One example of such a more general duality is from Galois theory. For a fixed Galois extension K / F, one may associate the Galois group Gal(K/E) to any intermediate field E (i.e., F ⊆ E ⊆ K). This group is a subgroup of the Galois group G = Gal(K/F). Conversely, to any such subgroup H ⊆ G there is the fixed field KH consisting of elements fixed by the elements in H.
Compared to the above, this duality has the following features:

An extension F ⊆ F′ of intermediate fields gives rise to an inclusion of Galois groups in the opposite direction: Gal(K/F′) ⊆ Gal(K/F).
Associating Gal(K/E) to E and KH to H are inverse to each other. This is the content of the fundamental theorem of Galois theory.


== Order-reversing dualities ==

Given a poset P = (X, ≤) (short for partially ordered set; i.e., a set that has a notion of ordering but in which two elements cannot necessarily be placed in order relative to each other), the dual poset Pd = (X, ≥) comprises the same ground set but the converse relation. Familiar examples of dual partial orders include

the subset and superset relations ⊂ and ⊃ on any collection of sets, such as the subsets of a fixed set S. This gives rise to the first example of a duality mentioned above.
the divides and multiple-of relations on the integers.
the descendant-of and ancestor-of relations on the set of humans.A duality transform is an involutive antiautomorphism f of a partially ordered set S, that is, an order-reversing involution f : S → S. In several important cases these simple properties determine the transform uniquely up to some simple symmetries. For example, if f1, f2 are two duality transforms then their composition is an order automorphism of S; thus, any two duality transforms differ only by an order automorphism. For example, all order automorphisms of a power set S = 2R are induced by permutations of R.
A concept defined for a partial order P will correspond to a dual concept on the dual poset Pd. For instance, a minimal element of P will be a maximal element of Pd: minimality and maximality are dual concepts in order theory. Other pairs of dual concepts are upper and lower bounds, lower sets and upper sets, and ideals and filters.
In topology, open sets and closed sets are dual concepts: the complement of an open set is closed, and vice versa. In matroid theory, the family of sets complementary to the independent sets of a given matroid themselves form another matroid, called the dual matroid.


== Dimension-reversing dualities ==

There are many distinct but interrelated dualities in which geometric or topological objects correspond to other objects of the same type, but with a reversal of the dimensions of the features of the objects. A classical example of this is the duality of the Platonic solids, in which the cube and the octahedron form a dual pair, the dodecahedron and the icosahedron form a dual pair, and the tetrahedron is self-dual.  The dual polyhedron of any of these polyhedra may be formed as the convex hull of the center points of each face of the primal polyhedron, so the vertices of the dual correspond one-for-one with the faces of the primal. Similarly, each edge of the dual corresponds to an edge of the primal, and each face of the dual corresponds to a vertex of the primal. These correspondences are incidence-preserving: if two parts of the primal polyhedron touch each other, so do the corresponding two parts of the dual polyhedron. More generally, using the concept of polar reciprocation, any convex polyhedron, or more generally any convex polytope, corresponds to a dual polyhedron or dual polytope, with an i-dimensional feature of an n-dimensional polytope corresponding to an (n − i − 1)-dimensional feature of the dual polytope. The incidence-preserving nature of the duality is reflected in the fact that the face lattices of the primal and dual polyhedra or polytopes are themselves order-theoretic duals. Duality of polytopes and order-theoretic duality are both involutions: the dual polytope of the dual polytope of any polytope is the original polytope, and reversing all order-relations twice returns to the original order. Choosing a different center of polarity leads to geometrically different dual polytopes, but all have the same combinatorial structure.

From any three-dimensional polyhedron, one can form a planar graph, the graph of its vertices and edges. The dual polyhedron has a dual graph, a graph with one vertex for each face of the polyhedron and with one edge for every two adjacent faces. The same concept of planar graph duality may be generalized to graphs that are drawn in the plane but that do not come from a three-dimensional polyhedron, or more generally to graph embeddings on surfaces of higher genus: one may draw a dual graph by placing one vertex within each region bounded by a cycle of edges in the embedding, and drawing an edge connecting any two regions that share a boundary edge. An important example of this type comes from computational geometry: the duality for any finite set S of points in the plane between the Delaunay triangulation of S and the Voronoi diagram of S. As with dual polyhedra and dual polytopes, the duality of graphs on surfaces is a dimension-reversing involution: each vertex in the primal embedded graph corresponds to a region of the dual embedding, each edge in the primal is crossed by an edge in the dual, and each region of the primal corresponds to a vertex of the dual. The dual graph depends on how the primal graph is embedded: different planar embeddings of a single graph may lead to different dual graphs. Matroid duality is an algebraic extension of planar graph duality, in the sense that the dual matroid of the graphic matroid of a planar graph is isomorphic to the graphic matroid of the dual graph.
A kind of geometric duality also occurs in optimization theory, but not one that reverses dimensions. A linear program may be specified by a system of real variables (the coordinates for a point in Euclidean space 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  ), a system of linear constraints (specifying that the point lie in a halfspace; the intersection of these halfspaces is a convex polytope, the feasible region of the program), and a linear function (what to optimize). Every linear program has a dual problem with the same optimal solution, but the variables in the dual problem correspond to constraints in the primal problem and vice versa.


== Duality in logic and set theory ==
In logic, functions or relations A and B are considered dual if A(¬x) = ¬B(x), where ¬ is logical negation. The basic duality of this type is the duality of the ∃ and ∀ quantifiers in classical logic. These are dual because ∃x.¬P(x) and ¬∀x.P(x) are equivalent for all predicates P in classical logic: if there exists an x for which P fails to hold, then it is false that P holds for all x (but the converse does not hold constructively). From this fundamental logical duality follow several others:

A formula is said to be satisfiable in a certain model if there are assignments to its free variables that render it true; it is valid if every assignment to its free variables makes it true. Satisfiability and validity are dual because the invalid formulas are precisely those whose negations are satisfiable, and the unsatisfiable formulas are those whose negations are valid.  This can be viewed as a special case of the previous item, with the quantifiers ranging over interpretations.
In classical logic, the ∧ and ∨ operators are dual in this sense, because (¬x ∧ ¬y) and ¬(x ∨ y) are equivalent.  This means that for every theorem of classical logic there is an equivalent dual theorem.  De Morgan's laws are examples.  More generally, ∧ (¬ xi) = ¬∨ xi.  The left side is true if and only if ∀i.¬xi, and the right side if and only if ¬∃i.xi.
In modal logic, □p means that the proposition p is ""necessarily"" true, and ◊p that p is ""possibly"" true.  Most interpretations of modal logic assign dual meanings to these two operators.  For example in Kripke semantics, ""p is possibly true"" means ""there exists some world W such that p is true in W"", while ""p is necessarily true"" means ""for all worlds W, p is true in W"".  The duality of □ and ◊ then follows from the analogous duality of ∀ and ∃. Other dual modal operators behave similarly. For example, temporal logic has operators denoting ""will be true at some time in the future"" and ""will be true at all times in the future"" which are similarly dual.Other analogous dualities follow from these:

Set-theoretic union and intersection are dual under the set complement operator ⋅C. That is, AC ∩ BC = (A ∪ B)C, and more generally, ∩ ACα = (∪ Aα)C. This follows from the duality of ∀ and ∃: an element x is a member of ∩ ACα if and only if ∀α.¬x ∈ Aα, and is a member of (∪ Aα)C if and only if ¬∃α. x ∈ Aα.


== Dual objects ==
A group of dualities can be described by endowing, for any mathematical object X, the set of morphisms Hom (X, D) into some fixed object D, with a structure similar to that of X. This is sometimes called internal Hom. In general, this yields a true duality only for specific choices of D, in which case X* = Hom (X, D) is referred to as the dual of X. There is always a map from X to the bidual, that is to say, the dual of the dual,

It assigns to some x ∈ X the map that associates to any map f : X → D (i.e., an element in Hom(X, D)) the value f(x).
Depending on the concrete duality considered and also depending on the object X, this map may or may not be an isomorphism.


=== Dual vector spaces revisited ===
The construction of the dual vector space

mentioned in the introduction is an example of such a duality. Indeed, the set of morphisms, i.e., linear maps, forms a vector space in its own right. The map V → V** mentioned above is always injective. It is surjective, and therefore an isomorphism, if and only if the dimension of V is finite. This fact characterizes finite-dimensional vector spaces without referring to a basis.


==== Isomorphisms of V and V∗ and inner product spaces ====
A vector space V is isomorphic to V∗ precisely if V is finite-dimensional. In this case, such an isomorphism is equivalent to a non-degenerate bilinear form

In this case V is called an inner product space.
For example, if K is the field of real or complex numbers, any positive definite bilinear form gives rise to such an isomorphism. In Riemannian geometry, V is taken to be the tangent space of a manifold and such positive bilinear forms are called Riemannian metrics. Their purpose is to measure angles and distances. Thus, duality is a foundational basis of this branch of geometry. Another application of inner product spaces is the Hodge star which provides a correspondence between the elements of the exterior algebra. For an n-dimensional vector space, the Hodge star operator maps k-forms to (n − k)-forms. This can be used to formulate Maxwell's equations. In this guise, the duality inherent in the inner product space exchanges the role of magnetic and electric fields.


==== Duality in projective geometry ====

In some projective planes, it is possible to find geometric transformations that map each point of the projective plane to a line, and each line of the projective plane to a point, in an incidence-preserving way. For such planes there arises a general principle of duality in projective planes: given any theorem in such a plane projective geometry, exchanging the terms ""point"" and ""line"" everywhere results in a new, equally valid theorem. A simple example is that the statement ""two points determine a unique line, the line passing through these points"" has the dual statement that ""two lines determine a unique point, the intersection point of these two lines"". For further examples, see Dual theorems.
A conceptual explanation of this phenomenon in some planes (notably field planes) is offered by the dual vector space. In fact, the points in the projective plane 
  
    
      
        
          
            R
            P
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {RP} ^{2}}
   correspond to one-dimensional subvector spaces 
  
    
      
        V
        ⊂
        
          
            R
          
          
            3
          
        
      
    
    {\displaystyle V\subset \mathbb {R} ^{3}}
   while the lines in the projective plane correspond to subvector spaces 
  
    
      
        W
      
    
    {\displaystyle W}
   of dimension 2. The duality in such projective geometries stems from assigning to a one-dimensional 
  
    
      
        V
      
    
    {\displaystyle V}
   the subspace of 
  
    
      
        (
        
          
            R
          
          
            3
          
        
        
          )
          
            ∗
          
        
      
    
    {\displaystyle (\mathbb {R} ^{3})^{*}}
   consisting of those linear maps 
  
    
      
        f
        :
        
          
            R
          
          
            3
          
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} ^{3}\to \mathbb {R} }
   which satisfy 
  
    
      
        f
        (
        V
        )
        =
        0
      
    
    {\displaystyle f(V)=0}
  . As a consequence of the dimension formula of linear algebra, this space is two-dimensional, i.e., it corresponds to a line in the projective plane associated to 
  
    
      
        (
        
          
            R
          
          
            3
          
        
        
          )
          
            ∗
          
        
      
    
    {\displaystyle (\mathbb {R} ^{3})^{*}}
  .
The (positive definite) bilinear form

yields an identification of this projective plane with the 
  
    
      
        
          
            R
            P
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {RP} ^{2}}
  . Concretely, the duality assigns to 
  
    
      
        V
        ⊂
        
          
            R
          
          
            3
          
        
      
    
    {\displaystyle V\subset \mathbb {R} ^{3}}
   its orthogonal 
  
    
      
        
          {
          
            w
            ∈
            
              
                R
              
              
                3
              
            
            ,
            ⟨
            v
            ,
            w
            ⟩
            =
            0
            
               for all 
            
            v
            ∈
            V
          
          }
        
      
    
    {\displaystyle \left\{w\in \mathbb {R} ^{3},\langle v,w\rangle =0{\text{ for all }}v\in V\right\}}
  . The explicit formulas in duality in projective geometry arise by means of this identification.


=== Topological vector spaces and Hilbert spaces ===
In the realm of topological vector spaces, a similar construction exists, replacing the dual by the topological dual vector space. There are several notions of topological dual space, and each of them gives rise to a certain concept of duality. A topological vector space 
  
    
      
        X
      
    
    {\displaystyle X}
   that is canonically isomorphic to its bidual 
  
    
      
        
          X
          ″
        
      
    
    {\displaystyle X''}
   is called a reflexive space:

Examples:

As in the finite-dimensional case, on each Hilbert space H its inner product ⟨⋅, ⋅⟩ defines a map  which is a bijection due to the Riesz representation theorem. As a corollary, every Hilbert space is a reflexive Banach space.
The dual normed space of an Lp-space is Lq where 1/p + 1/q = 1 provided that 1 ≤ p < ∞, but the dual of L∞ is bigger than L1. Hence L1 is not reflexive.
Distributions are linear functionals on appropriate spaces of functions. They are an important technical means in the theory of partial differential equations (PDE): instead of solving a PDE directly, it may be easier to first solve the PDE in the ""weak sense"", i.e., find a distribution that satisfies the PDE and, second, to show that the solution must, in fact, be a function. All the standard spaces of distributions — 
  
    
      
        
          
            
              D
            
          
          ′
        
        (
        U
        )
      
    
    {\displaystyle {\mathcal {D}}'(U)}
  , 
  
    
      
        
          
            
              S
            
          
          ′
        
        (
        
          
            R
          
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {S}}'(\mathbb {R} ^{n})}
  , 
  
    
      
        
          
            
              C
            
          
          
            ∞
          
        
        (
        U
        
          )
          ′
        
      
    
    {\displaystyle {\mathcal {C}}^{\infty }(U)'}
   — are reflexive locally convex spaces.


=== Further dual objects ===
The dual lattice of a lattice L is given by
which is used in the construction of toric varieties. The Pontryagin dual of locally compact topological groups G is given by

continuous group homomorphisms with values in the circle (with multiplication of complex numbers as group operation).


== Dual categories ==


=== Opposite category and adjoint functors ===
In another group of dualities, the objects of one theory are translated into objects of another theory and the maps between objects in the first theory are translated into morphisms in the second theory, but with direction reversed. Using the parlance of category theory, this amounts to a contravariant functor between two categories C and D:

which for any two objects X and Y of C gives a map

That functor may or may not be an equivalence of categories. There are various situations, where such a functor is an equivalence between the opposite category Cop of C, and D. Using a duality of this type, every statement in the first theory can be translated into a ""dual"" statement in the second theory, where the direction of all arrows has to be reversed. Therefore, any duality between categories C and D is formally the same as an equivalence between C and Dop (Cop and D). However, in many circumstances the opposite categories have no inherent meaning, which makes duality an additional, separate concept.A category that is equivalent to its dual is called self-dual. An example of self-dual category is the category of Hilbert spaces.Many category-theoretic notions come in pairs in the sense that they correspond to each other while considering the opposite category. For example, Cartesian products Y1 × Y2 and disjoint unions Y1 ⊔ Y2 of sets are dual to each other in the sense that

and

for any set X. This is a particular case of a more general duality phenomenon, under which limits in a category C correspond to colimits in the opposite category Cop; further concrete examples of this are epimorphisms vs. monomorphism, in particular factor modules (or groups etc.) vs. submodules, direct products vs. direct sums (also called coproducts to emphasize the duality aspect). Therefore, in some cases, proofs of certain statements can be halved, using such a duality phenomenon. Further notions displaying related by such a categorical duality are projective and injective modules in homological algebra, fibrations and cofibrations in topology and more generally model categories.Two functors F: C → D and G: D → C are adjoint if for all objects c in C and d in D

in a natural way.  Actually, the correspondence of limits and colimits is an example of adjoints, since there is an adjunction

between the colimit functor that assigns to any diagram in C indexed by some category I its colimit and the diagonal functor that maps any object c of C to the constant diagram which has c at all places. Dually,


=== Spaces and functions ===
Gelfand duality is a duality between commutative C*-algebras A and compact Hausdorff spaces X is the same: it assigns to X the space of continuous functions (which vanish at infinity) from X to C, the complex numbers. Conversely, the space X can be reconstructed from A as the spectrum of A. Both Gelfand and Pontryagin duality can be deduced in a largely formal, category-theoretic way.In a similar vein there is a duality in algebraic geometry between commutative rings and affine schemes: to every commutative ring A there is an affine spectrum, Spec A. Conversely, given an affine scheme S, one gets back a ring by taking global sections of the structure sheaf OS. In addition, ring homomorphisms are in one-to-one correspondence with morphisms of affine schemes, thereby there is an equivalence

(Commutative rings)op ≅ (affine schemes)Affine schemes are the local building blocks of schemes. The previous result therefore tells that the local theory of schemes is the same as commutative algebra, the study of commutative rings.
Noncommutative geometry draws inspiration from Gelfand duality and studies noncommutative C*-algebras as if they were functions on some imagined space. Tannaka–Krein duality is a non-commutative analogue of Pontryagin duality.


=== Galois connections ===
In a number of situations, the two categories which are dual to each other are actually arising from partially ordered sets, i.e., there is some notion of an object ""being smaller"" than another one. A duality that respects the orderings in question is known as a Galois connection. An example is the standard duality in Galois theory mentioned in the introduction: a bigger field extension corresponds—under the mapping that assigns to any extension L ⊃ K (inside some fixed bigger field Ω) the Galois group Gal (Ω / L) —to a smaller group.The collection of all open subsets of a topological space X forms a complete Heyting algebra. There is a duality, known as Stone duality, connecting sober spaces and spatial locales.

Birkhoff's representation theorem relating distributive lattices and partial orders


=== Pontryagin duality ===
Pontryagin duality gives a duality on the category of locally compact abelian groups: given any such group G, the character group

χ(G) = Hom (G, S1)given by continuous group homomorphisms from G to the circle group S1 can be endowed with the compact-open topology. Pontryagin duality states that the character group is again locally compact abelian and that

G ≅ χ(χ(G)).Moreover, discrete groups correspond to compact abelian groups; finite groups correspond to finite groups. On the one hand, Pontryagin is a special case of Gelfand duality. On the other hand, it is the conceptual reason of Fourier analysis, see below.


== Analytic dualities ==
In analysis, problems are frequently solved by passing to the dual description of functions and operators.
Fourier transform switches between functions on a vector space and its dual:

and conversely

If f is an L2-function on R or RN, say, then so is 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\widehat {f}}}
   and 
  
    
      
        f
        (
        −
        x
        )
        =
        
          
            
              
                
                  f
                  ^
                
              
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle f(-x)={\widehat {\widehat {f}}}(x)}
  . Moreover, the transform interchanges operations of multiplication and convolution on the corresponding function spaces. A conceptual explanation of the Fourier transform is obtained by the aforementioned Pontryagin duality, applied to the locally compact groups R (or RN etc.): any character of R is given by ξ ↦ e−2πixξ. The dualizing character of Fourier transform has many other manifestations, for example, in alternative descriptions of quantum mechanical systems in terms of coordinate and momentum representations.

Laplace transform is similar to Fourier transform and interchanges operators of multiplication by polynomials with constant coefficient linear differential operators.
Legendre transformation is an important analytic duality which switches between velocities in Lagrangian mechanics and momenta in Hamiltonian mechanics.


== Homology and cohomology ==
Theorems showing that certain objects of interest are the dual spaces (in the sense of linear algebra) of other objects of interest are often called dualities. Many of these dualities are given by a bilinear pairing of two K-vector spaces

A ⊗ B → K.For perfect pairings, there is, therefore, an isomorphism of A to the dual of B.


=== Poincaré duality ===
Poincaré duality of a smooth compact complex manifold X is given by a pairing of singular cohomology with C-coefficients (equivalently, sheaf cohomology of the constant sheaf C)

Hi(X) ⊗ H2n−i(X) → C,where n is the (complex) dimension of X. Poincaré duality can also be expressed as a relation of singular homology and de Rham cohomology, by asserting that the map

  
    
      
        (
        γ
        ,
        ω
        )
        ↦
        
          ∫
          
            γ
          
        
        ω
      
    
    {\displaystyle (\gamma ,\omega )\mapsto \int _{\gamma }\omega }
  (integrating a differential k-form over an 2n−k-(real) -dimensional cycle) is a perfect pairing.
Poincaré duality also reverses dimensions; it corresponds to the fact that, if a topological manifold is represented as a cell complex, then the dual of the complex (a higher-dimensional generalization of the planar graph dual) represents the same manifold. In Poincaré duality, this homeomorphism is reflected in an isomorphism of the kth homology group and the (n − k)th cohomology group.


=== Duality in algebraic and arithmetic geometry ===
The same duality pattern holds for a smooth projective variety over a separably closed field, using l-adic cohomology with Qℓ-coefficients instead. This is further generalized to possibly singular varieties, using intersection cohomology instead, a duality called Verdier duality. Serre duality or coherent duality are similar to the statements above, but applies to cohomology of coherent sheaves instead.With increasing level of generality, it turns out, an increasing amount of technical background is helpful or necessary to understand these theorems: the modern formulation of these dualities can be done using derived categories and certain direct and inverse image functors of sheaves (with respect to the classical analytical topology on manifolds for Poincaré duality, l-adic sheaves and the étale topology in the second case, and with respect to coherent sheaves for coherent duality).
Yet another group of similar duality statements is encountered in arithmetics: étale cohomology of finite, local  and global fields (also known as Galois cohomology, since étale cohomology over a field is equivalent to group cohomology of the (absolute) Galois group of the field) admit similar pairings. The absolute Galois group G(Fq) of a finite field, for example, is isomorphic to 
  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
      
    
    {\displaystyle {\widehat {\mathbf {Z} }}}
  , the profinite completion of Z, the integers. Therefore, the perfect pairing (for any G-module M)

Hn(G, M) × H1−n (G, Hom (M, Q/Z)) → Q/Zis a direct consequence of Pontryagin duality of finite groups. For local and global fields, similar statements exist (local duality and global or Poitou–Tate duality).


== See also ==


== Notes ==


== References ==


=== Duality in general ===
Atiyah, Michael (2007). ""Duality in Mathematics and Physics lecture notes from the Institut de Matematica de la Universitat de Barcelona (IMUB)"" (PDF).
Kostrikin, A. I. (2001) [1994], ""Duality"", Encyclopedia of Mathematics, EMS Press.
Gowers, Timothy (2008), ""III.19 Duality"", The Princeton Companion to Mathematics, Princeton University Press, pp. 187–190.
Cartier, Pierre (2001), ""A mad day's work: from Grothendieck to Connes and Kontsevich. The evolution of concepts of space and symmetry"", Bulletin of the American Mathematical Society, New Series, 38 (4): 389–408, doi:10.1090/S0273-0979-01-00913-2, ISSN 0002-9904, MR 1848254 (a non-technical overview about several aspects of geometry, including dualities)


=== Duality in algebraic topology ===
James C. Becker and Daniel Henry Gottlieb, A History of Duality in Algebraic Topology


=== Specific dualities ===
Artstein-Avidan, Shiri; Milman, Vitali (2008), ""The concept of duality for measure projections of convex bodies"", Journal of Functional Analysis, 254 (10): 2648–66, doi:10.1016/j.jfa.2007.11.008. Also author's site.
Artstein-Avidan, Shiri; Milman, Vitali (2007), ""A characterization of the concept of duality"", Electronic Research Announcements in Mathematical Sciences, 14: 42–59, archived from the original on 2011-07-24, retrieved 2009-05-30. Also author's site.
Dwyer, William G.; Spaliński, Jan (1995), ""Homotopy theories and model categories"", Handbook of algebraic topology, Amsterdam: North-Holland, pp. 73–126, MR 1361887
Fulton, William (1993), Introduction to toric varieties, Princeton University Press, ISBN 978-0-691-00049-7
Griffiths, Phillip; Harris, Joseph (1994), Principles of algebraic geometry, Wiley Classics Library, New York: John Wiley & Sons, ISBN 978-0-471-05059-9, MR 1288523
Hartshorne, Robin (1966), Residues and Duality, Lecture Notes in Mathematics, vol. 20, Springer-Verlag, pp. 20–48, ISBN 978-3-540-34794-1
Hartshorne, Robin (1977), Algebraic Geometry, Springer-Verlag, ISBN 978-0-387-90244-9, MR 0463157, OCLC 13348052
Iversen, Birger (1986), Cohomology of sheaves, Universitext, Springer-Verlag, ISBN 978-3-540-16389-3, MR 0842190
Joyal, André; Street, Ross (1991), ""An introduction to Tannaka duality and quantum groups"" (PDF), Category theory, Lecture Notes in Mathematics, vol. 1488, Springer-Verlag, pp. 413–492, doi:10.1007/BFb0084235, ISBN 978-3-540-46435-8, MR 1173027
Lam, Tsit-Yuen (1999), Lectures on modules and rings, Graduate Texts in Mathematics, vol. 189, Springer-Verlag, ISBN 978-0-387-98428-5, MR 1653294
Lang, Serge (2002), Algebra, Graduate Texts in Mathematics, vol. 211, Springer-Verlag, ISBN 978-0-387-95385-4, MR 1878556
Loomis, Lynn H. (1953), An introduction to abstract harmonic analysis, D. Van Nostrand, pp. x+190, hdl:2027/uc1.b4250788
Mac Lane, Saunders (1998), Categories for the Working Mathematician (2nd ed.), Springer-Verlag, ISBN 978-0-387-98403-2
Mazur, Barry (1973), ""Notes on étale cohomology of number fields"", Annales Scientifiques de l'École Normale Supérieure, Série 4, 6 (4): 521–552, doi:10.24033/asens.1257, ISSN 0012-9593, MR 0344254
Milne, James S. (1980), Étale cohomology, Princeton University Press, ISBN 978-0-691-08238-7
Milne, James S. (2006), Arithmetic duality theorems (2nd ed.), Charleston, South Carolina: BookSurge, LLC, ISBN 978-1-4196-4274-6, MR 2261462
Negrepontis, Joan W. (1971), ""Duality in analysis from the point of view of triples"", Journal of Algebra, 19 (2): 228–253, doi:10.1016/0021-8693(71)90105-0, ISSN 0021-8693, MR 0280571
Veblen, Oswald; Young, John Wesley (1965), Projective geometry. Vols. 1, 2, Blaisdell Publishing Co. Ginn and Co., MR 0179666
Weibel, Charles A. (1994), An introduction to homological algebra, Cambridge University Press, ISBN 978-0-521-55987-4, MR 1269324
Edwards, R. E. (1965). Functional analysis. Theory and applications. New York: Holt, Rinehart and Winston. ISBN 0030505356."
e1d6d86cf8,Lists of mathematics topics,"Lists of mathematics topics cover a variety of topics related to mathematics. Some of these lists link to hundreds of articles; some link only to a few. The template to the right includes links to alphabetical lists of all mathematical articles. This article brings together the same content organized in a manner better suited for browsing.
Lists cover aspects of basic and advanced mathematics, methodology, mathematical statements, integrals, general concepts, mathematical objects, and reference tables.
They also cover equations named after people, societies, mathematicians, journals, and meta-lists.
The purpose of this list is not similar to that of the Mathematics Subject Classification formulated by the American Mathematical Society. Many mathematics journals ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The subject codes so listed are used by the two major reviewing databases, Mathematical Reviews and Zentralblatt MATH. This list has some items that would not fit in such a classification, such as list of exponential topics and list of factorial and binomial topics, which may surprise the reader with the diversity of their coverage.


== Basic mathematics ==
This branch is typically taught in secondary education or in the first year of university.

Outline of arithmetic
Outline of discrete mathematics
List of calculus topics
List of geometry topics
Outline of geometry
List of trigonometry topics
Outline of trigonometry
List of trigonometric identities
List of logarithmic identities
List of integrals of logarithmic functions
List of set identities and relations
List of topics in logic


== Areas of advanced mathematics ==

As a rough guide, this list is divided into pure and applied sections although in reality, these branches are overlapping and intertwined.


=== Pure mathematics ===


==== Algebra ====
Algebra includes the study of algebraic structures, which are sets and operations defined on these sets satisfying certain axioms. The field of algebra is further divided according to which structure is studied; for instance, group theory concerns an algebraic structure called group.

Outline of algebra
Glossary of field theory
Glossary of group theory
Glossary of linear algebra
Glossary of ring theory
List of abstract algebra topics
List of algebraic structures
List of Boolean algebra topics
List of category theory topics
List of cohomology theories
List of commutative algebra topics
List of homological algebra topics
List of group theory topics
List of representation theory topics
List of linear algebra topics
List of reciprocity laws


==== Calculus and analysis ====

Calculus studies the computation of limits, derivatives, and integrals of functions of real numbers, and in particular studies instantaneous rates of change. Analysis evolved from calculus.

Glossary of tensor theory
List of complex analysis topics
List of functional analysis topics
List of vector spaces in mathematics
List of integration and measure theory topics
List of harmonic analysis topics
List of Fourier analysis topics
List of mathematical series
List of multivariable calculus topics
List of q-analogs
List of real analysis topics
List of variational topics
See also Dynamical systems and differential equations section below.


==== Geometry and topology ====

Geometry is initially the study of spatial figures like circles and cubes, though it has been generalized considerably. Topology developed from geometry; it looks at those properties that do not change even when the figures are deformed by stretching and bending, like dimension.

Glossary of differential geometry and topology
Glossary of general topology
Glossary of Riemannian and metric geometry
Glossary of scheme theory
List of algebraic geometry topics
List of algebraic surfaces
List of algebraic topology topics
List of cohomology theories
List of circle topics
List of topics related to pi
List of curves topics
List of differential geometry topics
List of general topology topics
List of geometric shapes
List of geometric topology topics
List of geometry topics
List of knot theory topics
List of Lie group topics
List of mathematical properties of points
List of topology topics
List of topologies
Topological property
List of triangle topics


==== Combinatorics ====
Combinatorics concerns the study of discrete (and usually finite) objects. Aspects include ""counting"" the objects satisfying certain criteria (enumerative combinatorics), deciding when the criteria can be met, and constructing and analyzing objects meeting the criteria (as in combinatorial designs and matroid theory), finding ""largest"", ""smallest"", or ""optimal"" objects (extremal combinatorics and combinatorial optimization), and finding algebraic structures these objects may have (algebraic combinatorics).

Outline of combinatorics
Glossary of graph theory
List of graph theory topics


==== Logic ====

Logic is the foundation that underlies mathematical logic and the rest of mathematics. It tries to formalize valid reasoning. In particular, it attempts to define what constitutes a proof.

List of Boolean algebra topics
List of first-order theories
List of large cardinal properties
List of mathematical logic topics
List of set theory topics
Glossary of order theory


==== Number theory ====
The branch of mathematics deals with the properties and relationships of numbers, especially positive integers.
Number theory is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss said, ""Mathematics is the queen of the sciences—and number theory is the queen of mathematics.""
Number theory also studies the natural, or whole, numbers. One of the central concepts in number theory is that of the prime number, and there are many questions about primes that appear simple but whose resolution continues to elude mathematicians.

List of algebraic number theory topics
List of number theory topics
List of recreational number theory topics
Glossary of arithmetic and Diophantine geometry
List of prime numbers—not just a table, but a list of various kinds of prime numbers (each with an accompanying table)
List of zeta functions


=== Applied mathematics ===


==== Dynamical systems and differential equations ====

A differential equation is an equation involving an unknown function and its derivatives.
In a dynamical system, a fixed rule describes the time dependence of a point in a geometrical space. The mathematical models used to describe the swinging of a clock pendulum, the flow of water in a pipe, or the number of fish each spring in a lake are examples of dynamical systems.

List of dynamical systems and differential equations topics
List of nonlinear partial differential equations
List of partial differential equation topics


==== Mathematical physics ====
Mathematical physics is concerned with ""the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories"".1
List of mathematical topics in classical mechanics
List of mathematical topics in quantum theory
List of mathematical topics in relativity
List of string theory topics
Index of wave articles


==== Theory of computation ====

The fields of mathematics and computing intersect both in computer science, the study of algorithms and data structures, and in scientific computing, the study of algorithmic methods for solving problems in mathematics, science, and engineering.

List of algorithm general topics
List of computability and complexity topics
Lists for computational topics in geometry and graphics
List of combinatorial computational geometry topics
List of computer graphics and descriptive geometry topics
List of numerical computational geometry topics
List of computer vision topics
List of formal language and literal string topics
List of numerical analysis topics
List of terms relating to algorithms and data structures


==== Information theory and signal processing ====
Information theory is a branch of applied mathematics and Social science involving the quantification of information. Historically, information theory was developed to find fundamental limits on compressing and reliably communicating data.
Signal processing is the analysis, interpretation, and manipulation of signals. Signals of interest include sound, images, biological signals such as ECG, radar signals, and many others. Processing of such signals includes filtering, storage and reconstruction, separation of information from noise, compression, and feature extraction.

List of algebraic coding theory topics
List of information theory topics
List of cryptography topics


==== Probability and statistics ====

Probability theory is the formalization and study of the mathematics of uncertain events or knowledge. The related field of mathematical statistics develops statistical theory with mathematics. Statistics, the science concerned with collecting and analyzing data, is an autonomous discipline (and not a subdiscipline of applied mathematics).

Catalog of articles in probability theory
List of probability topics
List of stochastic processes topics
List of probability distributions
List of statistics topics
Outline of regression analysis


==== Game theory ====
Game theory is a branch of mathematics that uses models to study interactions with formalized incentive structures (""games""). It has applications in a variety of fields, including economics, anthropology , political science, social psychology and military strategy.

Glossary of game theory
List of games in game theory


==== Operations research ====
Operations research is the study and use of mathematical models, statistics, and algorithms to aid in decision-making, typically with the goal of improving or optimizing the performance of real-world systems.

List of knapsack problems
List of network theory topics


== Methodology ==
List of graphical methods
List of mathematics-based methods
List of rules of inference


== Mathematical statements ==
A mathematical statement amounts to a proposition or assertion of some mathematical fact, formula, or construction. Such statements include axioms and the theorems that may be proved from them, conjectures that may be unproven or even unprovable, and also algorithms for computing the answers to questions that can be expressed mathematically.

List of algorithms
List of axioms
List of conjectures
List of conjectures by Paul Erdős
Combinatorial principles
List of equations
List of formulae involving pi
List of representations of e
List of inequalities
List of lemmas
List of mathematical identities
List of mathematical proofs
List of theorems


== General concepts ==
List of convexity topics
List of dualities
List of exceptional set concepts
List of exponential topics
List of factorial and binomial topics
List of fractal topics
List of logarithm topics
List of mathematical properties of points
List of numeral system topics
List of order topics
List of partition topics
List of permutation topics
List of polynomial topics
List of properties of sets of reals
List of transforms


== Mathematical objects ==
Among mathematical objects are numbers, functions, sets, a great variety of things called ""spaces"" of one kind or another, algebraic structures such as rings, groups, or fields, and many other things.

List of mathematical examples
List of algebraic surfaces
List of curves
List of complex reflection groups
List of complexity classes
List of examples in general topology
List of finite simple groups
List of Fourier-related transforms
List of manifolds
List of mathematical constants
List of mathematical functions
List of mathematical knots and links
List of mathematical shapes
List of mathematical spaces
List of matrices
List of numbers
List of polygons, polyhedra and polytopes
List of regular polytopes
List of simple Lie groups
List of small groups
List of special functions and eponyms
List of surfaces
Table of Lie groups


== Equations named after people ==
Scientific equations named after people


== About mathematics ==
List of letters used in mathematics and science
List of mathematical societies
List of mathematics competitions
List of mathematics history topics
List of publications in mathematics
List of mathematics journals


=== Mathematicians ===

Mathematicians study and research in all the different areas of mathematics. The publication of new discoveries in mathematics continues at an immense rate in hundreds of scientific journals, many of them devoted to mathematics and many devoted to subjects to which mathematics is applied (such as theoretical computer science and theoretical physics).

List of films about mathematicians
List of game theorists
List of geometers
List of logicians
List of mathematicians
List of mathematical probabilists
List of statisticians


=== Work of particular mathematicians ===

List of things named after Niels Henrik Abel
List of things named after George Airy
List of things named after Jean d'Alembert
List of things named after Archimedes
List of things named after Vladimir Arnold
List of things named after Emil Artin
List of things named after Stefan Banach
List of things named after Thomas Bayes
List of things named after members of the Bernoulli family
List of things named after Jakob Bernoulli
List of things named after Friedrich Bessel
List of things named after Élie Cartan
List of things named after Augustin-Louis Cauchy
List of things named after Arthur Cayley
List of things named after Pafnuty Chebyshev
List of things named after John Horton Conway
List of things named after Richard Dedekind
List of things named after Pierre Deligne
List of things named after Peter Gustav Lejeune Dirichlet
List of things named after Albert Einstein
List of things named after Euclid
List of things named after Leonhard Euler
List of things named after Paul Erdős
List of things named after Pierre de Fermat
List of things named after Fibonacci
List of things named after Joseph Fourier
List of things named after Erik Fredholm
List of things named after Ferdinand Georg Frobenius
List of things named after Carl Friedrich Gauss
List of things named after Évariste Galois
List of things named after Hermann Grassmann
List of things named after Alexander Grothendieck
List of things named after Jacques Hadamard
List of things named after William Rowan Hamilton
List of things named after Erich Hecke
List of things named after Eduard Heine
List of things named after Charles Hermite
List of things named after David Hilbert
List of things named after W. V. D. Hodge
List of things named after Carl Gustav Jacob Jacobi
List of things named after Johannes Kepler
List of things named after Felix Klein
List of things named after Joseph-Louis Lagrange
List of things named after Johann Lambert
List of things named after Pierre-Simon Laplace
List of things named after Adrien-Marie Legendre
List of things named after Gottfried Leibniz
List of things named after Sophus Lie
List of things named after Joseph Liouville
List of things named after Andrey Markov
List of things named after John Milnor
List of things named after Hermann Minkowski
List of things named after John von Neumann
List of things named after Isaac Newton
List of things named after Emmy Noether
List of things named after Henri Poincaré
List of things named after Siméon Denis Poisson
List of things named after Pythagoras
List of things named after Srinivasa Ramanujan
List of things named after Bernhard Riemann
List of things named after Issai Schur
List of things named after Anatoliy Skorokhod
List of things named after George Gabriel Stokes
List of things named after Jean-Pierre Serre
List of things named after James Joseph Sylvester
List of things named after Alfred Tarski
List of things named after Alan Turing
List of things named after Stanislaw Ulam
List of things named after Karl Weierstrass
List of things named after André Weil
List of things named after Hermann Weyl
List of things named after Norbert Wiener
List of things named after Ernst Witt


== Reference tables ==
List of mathematical reference tables
List of moments of inertia
Table of derivatives


=== Integrals ===
In calculus, the integral of a function is a generalization of area, mass, volume, sum, and total. The following pages list the integrals of many different functions.

Lists of integrals
List of integrals of exponential functions
List of integrals of hyperbolic functions
List of integrals of inverse hyperbolic functions
List of integrals of inverse trigonometric functions
List of integrals of irrational functions
List of integrals of logarithmic functions
List of integrals of rational functions
List of integrals of trigonometric functions


== Journals ==
List of mathematics journals
List of mathematics education journals
Category:History of science journals
Category:Philosophy of science literature


== Meta-lists ==
Glossary of mathematical symbols
List of important publications in mathematics
List of important publications in statistics
List of mathematical theories
List of mathematics categories
List of mathematical symbols by subject
Table of logic symbols
Table of mathematical symbols


== See also ==
Areas of mathematics
Glossary of areas of mathematics
Outline of mathematics
Timeline of women in mathematics


== Others ==
Lists of unsolved problems in mathematics
List of order theory topics
List of topics related to π


== Notes ==
^Note 1 :  Definition from the Journal of Mathematical Physics [1].


== External links and references ==
2000 Mathematics Subject Classification from the American Mathematical Society, scheme authors find many mathematics research journals asking them to use to classify their submissions; those published then include these classifications.
The Mathematical Atlas
Maths Formula"
55366e596b,Support (mathematics),"In mathematics, the support of a real-valued function 
  
    
      
        f
      
    
    {\displaystyle f}
   is the subset of the function domain containing the elements which are not mapped to zero. If the domain of 
  
    
      
        f
      
    
    {\displaystyle f}
   is a topological space, then the support of 
  
    
      
        f
      
    
    {\displaystyle f}
   is instead defined as the smallest closed set containing all points not mapped to zero. This concept is used very widely in mathematical analysis.


== Formulation ==
Suppose that 
  
    
      
        f
        :
        X
        →
        
          R
        
      
    
    {\displaystyle f:X\to \mathbb {R} }
   is a real-valued function whose domain is an arbitrary set 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   The set-theoretic support of 
  
    
      
        f
        ,
      
    
    {\displaystyle f,}
   written 
  
    
      
        supp
        ⁡
        (
        f
        )
        ,
      
    
    {\displaystyle \operatorname {supp} (f),}
   is the set of points in 
  
    
      
        X
      
    
    {\displaystyle X}
   where 
  
    
      
        f
      
    
    {\displaystyle f}
   is non-zero:

The support of 
  
    
      
        f
      
    
    {\displaystyle f}
   is the smallest subset of 
  
    
      
        X
      
    
    {\displaystyle X}
   with the property that 
  
    
      
        f
      
    
    {\displaystyle f}
   is zero on the subset's complement. If 
  
    
      
        f
        (
        x
        )
        =
        0
      
    
    {\displaystyle f(x)=0}
   for all but a finite number of points 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
   then 
  
    
      
        f
      
    
    {\displaystyle f}
   is said to have finite support.
If the set 
  
    
      
        X
      
    
    {\displaystyle X}
   has an additional structure (for example, a topology), then the support of 
  
    
      
        f
      
    
    {\displaystyle f}
   is defined in an analogous way as the smallest subset of 
  
    
      
        X
      
    
    {\displaystyle X}
   of an appropriate type such that 
  
    
      
        f
      
    
    {\displaystyle f}
   vanishes in an appropriate sense on its complement. The notion of support also extends in a natural way to functions taking values in more general sets than 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   and to other objects, such as measures or distributions.


== Closed support ==
The most common situation occurs when 
  
    
      
        X
      
    
    {\displaystyle X}
   is a topological space (such as the real line or 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space) and 
  
    
      
        f
        :
        X
        →
        
          R
        
      
    
    {\displaystyle f:X\to \mathbb {R} }
   is a continuous real (or complex)-valued function. In this case, the support of 
  
    
      
        f
      
    
    {\displaystyle f}
  , 
  
    
      
        supp
        ⁡
        (
        f
        )
      
    
    {\displaystyle \operatorname {supp} (f)}
  , or the closed support of 
  
    
      
        f
      
    
    {\displaystyle f}
  , is defined topologically as the closure (taken in 
  
    
      
        X
      
    
    {\displaystyle X}
  ) of the subset of 
  
    
      
        X
      
    
    {\displaystyle X}
   where 
  
    
      
        f
      
    
    {\displaystyle f}
   is non-zero that is,

Since the intersection of closed sets is closed, 
  
    
      
        supp
        ⁡
        (
        f
        )
      
    
    {\displaystyle \operatorname {supp} (f)}
   is the intersection of all closed sets that contain the set-theoretic support of 
  
    
      
        f
        .
      
    
    {\displaystyle f.}
  
For example, if 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} }
   is the function defined by

then 
  
    
      
        supp
        ⁡
        (
        f
        )
      
    
    {\displaystyle \operatorname {supp} (f)}
  , the support of 
  
    
      
        f
      
    
    {\displaystyle f}
  , or the closed support of 
  
    
      
        f
      
    
    {\displaystyle f}
  , is the closed interval 
  
    
      
        [
        −
        1
        ,
        1
        ]
        ,
      
    
    {\displaystyle [-1,1],}
   since 
  
    
      
        f
      
    
    {\displaystyle f}
   is non-zero on the open interval 
  
    
      
        (
        −
        1
        ,
        1
        )
      
    
    {\displaystyle (-1,1)}
   and the closure of this set is 
  
    
      
        [
        −
        1
        ,
        1
        ]
        .
      
    
    {\displaystyle [-1,1].}
  
The notion of closed support is usually applied to continuous functions, but the definition makes sense for arbitrary real or complex-valued functions on a topological space, and some authors do not require that 
  
    
      
        f
        :
        X
        →
        
          R
        
      
    
    {\displaystyle f:X\to \mathbb {R} }
   (or 
  
    
      
        f
        :
        X
        →
        
          C
        
      
    
    {\displaystyle f:X\to \mathbb {C} }
  ) be continuous.


== Compact support ==
Functions with compact support on a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   are those whose closed support is a compact subset of 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   If 
  
    
      
        X
      
    
    {\displaystyle X}
   is the real line, or 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space, then a function has compact support if and only if it has bounded support, since a subset of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   is compact if and only if it is closed and bounded.
For example, the function 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} }
   defined above is a continuous function with compact support 
  
    
      
        [
        −
        1
        ,
        1
        ]
        .
      
    
    {\displaystyle [-1,1].}
   If 
  
    
      
        f
        :
        
          
            R
          
          
            n
          
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} }
   is a smooth function then because 
  
    
      
        f
      
    
    {\displaystyle f}
   is identically 
  
    
      
        0
      
    
    {\displaystyle 0}
   on the open subset 
  
    
      
        
          
            R
          
          
            n
          
        
        ∖
        supp
        ⁡
        (
        f
        )
        ,
      
    
    {\displaystyle \mathbb {R} ^{n}\smallsetminus \operatorname {supp} (f),}
   all of 
  
    
      
        f
      
    
    {\displaystyle f}
  's partial derivatives of all orders are also identically 
  
    
      
        0
      
    
    {\displaystyle 0}
   on 
  
    
      
        
          
            R
          
          
            n
          
        
        ∖
        supp
        ⁡
        (
        f
        )
        .
      
    
    {\displaystyle \mathbb {R} ^{n}\smallsetminus \operatorname {supp} (f).}
   
The condition of compact support is stronger than the condition of vanishing at infinity. For example, the function 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} }
   defined by

vanishes at infinity, since 
  
    
      
        f
        (
        x
        )
        →
        0
      
    
    {\displaystyle f(x)\to 0}
   as 
  
    
      
        
          |
        
        x
        
          |
        
        →
        ∞
        ,
      
    
    {\displaystyle |x|\to \infty ,}
   but its support 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   is not compact.
Real-valued compactly supported smooth functions on a Euclidean space are called bump functions. Mollifiers are an important special case of bump functions as they can be used in distribution theory to create sequences of smooth functions approximating nonsmooth (generalized) functions, via convolution.
In good cases, functions with compact support are dense in the space of functions that vanish at infinity, but this property requires some technical work to justify in a given example. As an intuition for more complex examples, and in the language of limits, for any 
  
    
      
        ε
        >
        0
        ,
      
    
    {\displaystyle \varepsilon >0,}
   any function 
  
    
      
        f
      
    
    {\displaystyle f}
   on the real line 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   that vanishes at infinity can be approximated by choosing an appropriate compact subset 
  
    
      
        C
      
    
    {\displaystyle C}
   of 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   such that

for all 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
   where 
  
    
      
        
          I
          
            C
          
        
      
    
    {\displaystyle I_{C}}
   is the indicator function of 
  
    
      
        C
        .
      
    
    {\displaystyle C.}
   Every continuous function on a compact topological space has compact support since every closed subset of a compact space is indeed compact.


== Essential support ==
If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a topological measure space with a Borel measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   (such as 
  
    
      
        
          
            R
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{n},}
   or a Lebesgue measurable subset of 
  
    
      
        
          
            R
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{n},}
   equipped with Lebesgue measure), then one typically identifies functions that are equal 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  -almost everywhere. In that case, the essential support of a measurable function 
  
    
      
        f
        :
        X
        →
        
          R
        
      
    
    {\displaystyle f:X\to \mathbb {R} }
   written 
  
    
      
        
          e
          s
          s
          
          s
          u
          p
          p
        
        ⁡
        (
        f
        )
        ,
      
    
    {\displaystyle \operatorname {ess\,supp} (f),}
   is defined to be the smallest closed subset 
  
    
      
        F
      
    
    {\displaystyle F}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   such that 
  
    
      
        f
        =
        0
      
    
    {\displaystyle f=0}
   
  
    
      
        μ
      
    
    {\displaystyle \mu }
  -almost everywhere outside 
  
    
      
        F
        .
      
    
    {\displaystyle F.}
   Equivalently, 
  
    
      
        
          e
          s
          s
          
          s
          u
          p
          p
        
        ⁡
        (
        f
        )
      
    
    {\displaystyle \operatorname {ess\,supp} (f)}
   is the complement of the largest open set on which 
  
    
      
        f
        =
        0
      
    
    {\displaystyle f=0}
   
  
    
      
        μ
      
    
    {\displaystyle \mu }
  -almost everywhere
The essential support of a function 
  
    
      
        f
      
    
    {\displaystyle f}
   depends on the measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   as well as on 
  
    
      
        f
        ,
      
    
    {\displaystyle f,}
   and it may be strictly smaller than the closed support. For example, if 
  
    
      
        f
        :
        [
        0
        ,
        1
        ]
        →
        
          R
        
      
    
    {\displaystyle f:[0,1]\to \mathbb {R} }
   is the Dirichlet function that is 
  
    
      
        0
      
    
    {\displaystyle 0}
   on irrational numbers and 
  
    
      
        1
      
    
    {\displaystyle 1}
   on rational numbers, and 
  
    
      
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle [0,1]}
   is equipped with Lebesgue measure, then the support of 
  
    
      
        f
      
    
    {\displaystyle f}
   is the entire interval 
  
    
      
        [
        0
        ,
        1
        ]
        ,
      
    
    {\displaystyle [0,1],}
   but the essential support of 
  
    
      
        f
      
    
    {\displaystyle f}
   is empty, since 
  
    
      
        f
      
    
    {\displaystyle f}
   is equal almost everywhere to the zero function.
In analysis one nearly always wants to use the essential support of a function, rather than its closed support, when the two sets are different, so 
  
    
      
        
          e
          s
          s
          
          s
          u
          p
          p
        
        ⁡
        (
        f
        )
      
    
    {\displaystyle \operatorname {ess\,supp} (f)}
   is often written simply as 
  
    
      
        supp
        ⁡
        (
        f
        )
      
    
    {\displaystyle \operatorname {supp} (f)}
   and referred to as the support.


== Generalization ==
If 
  
    
      
        M
      
    
    {\displaystyle M}
   is an arbitrary set containing zero, the concept of support is immediately generalizable to functions 
  
    
      
        f
        :
        X
        →
        M
        .
      
    
    {\displaystyle f:X\to M.}
    Support may also be defined for any algebraic structure with identity (such as a group, monoid, or composition algebra), in which the identity element assumes the role of zero. For instance, the family 
  
    
      
        
          
            Z
          
          
            
              N
            
          
        
      
    
    {\displaystyle \mathbb {Z} ^{\mathbb {N} }}
   of functions from the natural numbers to the integers is the uncountable set of integer sequences.  The subfamily 
  
    
      
        
          {
          
            f
            ∈
            
              
                Z
              
              
                
                  N
                
              
            
            :
            f
            
               has finite support 
            
          
          }
        
      
    
    {\displaystyle \left\{f\in \mathbb {Z} ^{\mathbb {N} }:f{\text{ has finite support }}\right\}}
   is the countable set of all integer sequences that have only finitely many nonzero entries.
Functions of finite support are used in defining algebraic structures such as group rings and free abelian groups.


== In probability and measure theory ==

In probability theory, the support of a probability distribution can be loosely thought of as the closure of the set of possible values of a random variable having that distribution. There are, however, some subtleties to consider when dealing with general distributions defined on a sigma algebra, rather than on a topological space.
More formally, if 
  
    
      
        X
        :
        Ω
        →
        
          R
        
      
    
    {\displaystyle X:\Omega \to \mathbb {R} }
   is a random variable on 
  
    
      
        (
        Ω
        ,
        
          
            F
          
        
        ,
        P
        )
      
    
    {\displaystyle (\Omega ,{\mathcal {F}},P)}
   then the support of 
  
    
      
        X
      
    
    {\displaystyle X}
   is the smallest closed set 
  
    
      
        
          R
          
            X
          
        
        ⊆
        
          R
        
      
    
    {\displaystyle R_{X}\subseteq \mathbb {R} }
   such that 
  
    
      
        P
        
          (
          
            X
            ∈
            
              R
              
                X
              
            
          
          )
        
        =
        1.
      
    
    {\displaystyle P\left(X\in R_{X}\right)=1.}
  
In practice however, the support of a discrete random variable 
  
    
      
        X
      
    
    {\displaystyle X}
   is often defined as the set 
  
    
      
        
          R
          
            X
          
        
        =
        {
        x
        ∈
        
          R
        
        :
        P
        (
        X
        =
        x
        )
        >
        0
        }
      
    
    {\displaystyle R_{X}=\{x\in \mathbb {R} :P(X=x)>0\}}
   and the support of a continuous random variable 
  
    
      
        X
      
    
    {\displaystyle X}
   is defined as the set 
  
    
      
        
          R
          
            X
          
        
        =
        {
        x
        ∈
        
          R
        
        :
        
          f
          
            X
          
        
        (
        x
        )
        >
        0
        }
      
    
    {\displaystyle R_{X}=\{x\in \mathbb {R} :f_{X}(x)>0\}}
   where 
  
    
      
        
          f
          
            X
          
        
        (
        x
        )
      
    
    {\displaystyle f_{X}(x)}
   is a probability density function of 
  
    
      
        X
      
    
    {\displaystyle X}
   (the set-theoretic support).Note that the word support can refer to the logarithm of the likelihood of a probability density function.


== Support of a distribution ==
It is possible also to talk about the support of a distribution, such as the Dirac delta function 
  
    
      
        δ
        (
        x
        )
      
    
    {\displaystyle \delta (x)}
   on the real line. In that example, we can consider test functions 
  
    
      
        F
        ,
      
    
    {\displaystyle F,}
   which are smooth functions with support not including the point 
  
    
      
        0.
      
    
    {\displaystyle 0.}
   Since 
  
    
      
        δ
        (
        F
        )
      
    
    {\displaystyle \delta (F)}
   (the distribution 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   applied as linear functional to 
  
    
      
        F
      
    
    {\displaystyle F}
  ) is 
  
    
      
        0
      
    
    {\displaystyle 0}
   for such functions, we can say that the support of 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   is 
  
    
      
        {
        0
        }
      
    
    {\displaystyle \{0\}}
   only. Since measures (including probability measures) on the real line are special cases of distributions, we can also speak of the support of a measure in the same way.
Suppose that 
  
    
      
        f
      
    
    {\displaystyle f}
   is a distribution, and that 
  
    
      
        U
      
    
    {\displaystyle U}
   is an open set in Euclidean space such that, for all test functions 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   such that the support of 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   is contained in 
  
    
      
        U
        ,
      
    
    {\displaystyle U,}
   
  
    
      
        f
        (
        ϕ
        )
        =
        0.
      
    
    {\displaystyle f(\phi )=0.}
   Then 
  
    
      
        f
      
    
    {\displaystyle f}
   is said to vanish on 
  
    
      
        U
        .
      
    
    {\displaystyle U.}
   Now, if 
  
    
      
        f
      
    
    {\displaystyle f}
   vanishes on an arbitrary family 
  
    
      
        
          U
          
            α
          
        
      
    
    {\displaystyle U_{\alpha }}
   of open sets, then for any test function 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   supported in 
  
    
      
        ⋃
        
          U
          
            α
          
        
        ,
      
    
    {\displaystyle \bigcup U_{\alpha },}
   a simple argument based on the compactness of the support of 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   and a partition of unity shows that 
  
    
      
        f
        (
        ϕ
        )
        =
        0
      
    
    {\displaystyle f(\phi )=0}
   as well. Hence we can define the support of 
  
    
      
        f
      
    
    {\displaystyle f}
   as the complement of the largest open set on which 
  
    
      
        f
      
    
    {\displaystyle f}
   vanishes. For example, the support of the Dirac delta is 
  
    
      
        {
        0
        }
        .
      
    
    {\displaystyle \{0\}.}
  


== Singular support ==
In Fourier analysis in particular, it is interesting to study the singular support of a distribution. This has the intuitive interpretation as the set of points at which a distribution fails to be a smooth function.
For example, the Fourier transform of the Heaviside step function can, up to constant factors, be considered to be 
  
    
      
        1
        
          /
        
        x
      
    
    {\displaystyle 1/x}
   (a function) except at 
  
    
      
        x
        =
        0.
      
    
    {\displaystyle x=0.}
   While 
  
    
      
        x
        =
        0
      
    
    {\displaystyle x=0}
   is clearly a special point, it is more precise to say that the transform of the distribution has singular support 
  
    
      
        {
        0
        }
      
    
    {\displaystyle \{0\}}
  : it cannot accurately be expressed as a function in relation to test functions with support including 
  
    
      
        0.
      
    
    {\displaystyle 0.}
   It can be expressed as an application of a Cauchy principal value improper integral.
For distributions in several variables, singular supports allow one to define wave front sets and understand Huygens' principle in terms of mathematical analysis. Singular supports may also be used to understand phenomena special to distribution theory, such as attempts to 'multiply' distributions (squaring the Dirac delta function fails – essentially because the singular supports of the distributions to be multiplied should be disjoint).


== Family of supports ==
An abstract notion of family of supports on a topological space 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   suitable for sheaf theory, was defined by Henri Cartan. In extending Poincaré duality to manifolds that are not compact, the 'compact support' idea enters naturally on one side of the duality; see for example Alexander–Spanier cohomology.
Bredon, Sheaf Theory (2nd edition, 1997) gives these definitions. A family 
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   of closed subsets of 
  
    
      
        X
      
    
    {\displaystyle X}
   is a family of supports, if it is down-closed and closed under finite union. Its extent is the union over 
  
    
      
        Φ
        .
      
    
    {\displaystyle \Phi .}
   A paracompactifying family of supports that satisfies further that any 
  
    
      
        Y
      
    
    {\displaystyle Y}
   in 
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   is, with the subspace topology, a paracompact space; and has some 
  
    
      
        Z
      
    
    {\displaystyle Z}
   in 
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   which is a neighbourhood. If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a locally compact space, assumed Hausdorff the family of all compact subsets satisfies the further conditions, making it paracompactifying.


== See also ==
Bounded function – A mathematical function the set of whose values are bounded
Bump function – Smooth and compactly supported function
Support of a module
Titchmarsh convolution theorem


== Citations ==


== References ==
Rudin, Walter (1991). Functional Analysis. International Series in Pure and Applied Mathematics. Vol. 8 (Second ed.). New York, NY: McGraw-Hill Science/Engineering/Math. ISBN 978-0-07-054236-5. OCLC 21163277.
Trèves, François (2006) [1967]. Topological Vector Spaces, Distributions and Kernels. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-45352-1. OCLC 853623322."
1f94940014,Invariant (mathematics),"In mathematics, an invariant is a property of a mathematical object (or a class of mathematical objects) which remains unchanged after operations or transformations of a certain type are applied to the objects. The particular class of objects and type of transformations are usually indicated by the context in which the term is used. For example, the area of a triangle is an invariant with respect to isometries of the Euclidean plane. The phrases ""invariant under"" and ""invariant to"" a transformation are both used. More generally, an invariant with respect to an equivalence relation is a property that is constant on each equivalence class.Invariants are used in diverse areas of mathematics such as geometry, topology, algebra and discrete mathematics. Some important classes of transformations are defined by an invariant they leave unchanged. For example, conformal maps are defined as transformations of the plane that preserve angles. The discovery of invariants is an important step in the process of classifying mathematical objects.


== Examples ==
A simple example of invariance is expressed in our ability to count. For a finite set of objects of any kind, there is a number to which we always arrive, regardless of the order in which we count the objects in the set. The quantity—a cardinal number—is associated with the set, and is invariant under the process of counting.
An identity is an equation that remains true for all values of its variables. There are also inequalities that remain true when the values of their variables change.
The distance between two points on a number line is not changed by adding the same quantity to both numbers. On the other hand, multiplication does not have this same property, as distance is not invariant under multiplication.
Angles and ratios of distances are invariant under scalings, rotations, translations and reflections. These transformations produce similar shapes, which is the basis of trigonometry. In contrast, angles and ratios are not invariant under non-uniform scaling (such as stretching). The sum of a triangle's interior angles (180°) is invariant under all the above operations.  As another example, all circles are similar: they can be transformed into each other and the ratio of the circumference to the diameter is invariant (denoted by the Greek letter π (pi)).
Some more complicated examples:

The real part and the absolute value of a complex number are invariant under complex conjugation.
The degree of a polynomial is invariant under a linear change of variables.
The dimension and homology groups of a topological object are invariant under homeomorphism.
The number of fixed points of a dynamical system is invariant under many mathematical operations.
Euclidean distance is invariant under orthogonal transformations.
Euclidean area is invariant under linear maps which have determinant ±1 (see Equiareal map § Linear transformations).
Some invariants of projective transformations include collinearity of three or more points, concurrency of three or more lines, conic sections, and the cross-ratio.
The determinant, trace, eigenvectors, and eigenvalues of a linear endomorphism are invariant under a change of basis. In other words, the spectrum of a matrix is invariant under a change of basis.
The principal invariants of tensors do not change with rotation of the coordinate system (see Invariants of tensors).
The singular values of a matrix are invariant under orthogonal transformations.
Lebesgue measure is invariant under translations.
The variance of a probability distribution is invariant under translations of the real line. Hence the variance of a random variable is unchanged after the addition of a constant.
The fixed points of a transformation are the elements in the domain that are invariant under the transformation. They may, depending on the application, be called symmetric with respect to that transformation. For example, objects with translational symmetry are invariant under certain translations.
The integral 
  
    
      
        
          ∫
          
            M
          
        
        K
        
        d
        μ
      
    
    {\textstyle \int _{M}K\,d\mu }
   of the Gaussian curvature 
  
    
      
        K
      
    
    {\displaystyle K}
   of a two-dimensional Riemannian manifold 
  
    
      
        (
        M
        ,
        g
        )
      
    
    {\displaystyle (M,g)}
   is invariant under changes of the Riemannian metric 
  
    
      
        g
      
    
    {\displaystyle g}
  .  This is the Gauss–Bonnet theorem.


=== MU puzzle ===
The MU puzzle is a good example of a logical problem where determining an invariant is of use for an impossibility proof. The puzzle asks one to start with the word MI and transform it into the word MU, using in each step one of the following transformation rules:

If a string ends with an I, a U may be appended (xI → xIU)
The string after the M may be completely duplicated (Mx → Mxx)
Any three consecutive I's (III) may be replaced with a single U (xIIIy → xUy)
Any two consecutive U's may be removed (xUUy → xy)An example derivation (with superscripts indicating the applied rules) is

MI →2 MII →2 MIIII →3 MUI →2 MUIUI →1 MUIUIU →2 MUIUIUUIUIU →4 MUIUIIUIU → ...In light of this, one might wonder whether it is possible to convert MI into MU, using only these four transformation rules. One could spend many hours applying these transformation rules to strings. However, it might be quicker to find a property that is invariant to all rules (i.e. that isn't changed by any of them), and demonstrates that getting to MU is impossible. By looking at the puzzle from a logical standpoint, one might realize that the only way to get rid of any I's is to have three consecutive I's in the string. This makes the following invariant interesting to consider:

The number of I's in the string is not a multiple of 3.This is an invariant to the problem, if for each of the transformation rules the following holds: if the invariant held before applying the rule, it will also hold after applying it. Looking at the net effect of applying the rules on the number of I's and U's, one can see this actually is the case for all rules:

The table above shows clearly that the invariant holds for each of the possible transformation rules, which means that whichever rule one picks, at whatever state, if the number of I's was not a multiple of three before applying the rule, then it won't be afterwards either.
Given that there is a single I in the starting string MI, and one that is not a multiple of three, one can then conclude that it is impossible to go from MI to MU (as the number of I's will never be a multiple of three).


== Invariant set ==
A subset S of the domain U of a mapping T: U → U  is an invariant set under the mapping when 
  
    
      
        x
        ∈
        S
        
        ⟹
        
        T
        (
        x
        )
        ∈
        S
        .
      
    
    {\displaystyle x\in S\implies T(x)\in S.}
   Note that the elements of S are not fixed, even though the set S is fixed in the power set of U. (Some authors use the terminology setwise invariant, vs. pointwise invariant, to distinguish between these cases.)
For example, a circle is an invariant subset of the plane under a rotation about the circle's center. Further, a conical surface is invariant as a set under a homothety of space.
An invariant set of an operation T is also said to be stable under T. For example, the normal subgroups that are so important in group theory are those subgroups that are stable under the inner automorphisms of the ambient group.
In linear algebra, if a linear transformation T has an eigenvector v, then the line through 0 and v is an invariant set under T, in which case the eigenvectors span an invariant subspace which is stable under T.
When T is a screw displacement, the screw axis is an invariant line, though if the pitch is non-zero, T has no fixed points.


== Formal statement ==
The notion of invariance is formalized in three different ways in mathematics: via group actions, presentations, and deformation.


=== Unchanged under group action ===
Firstly, if one has a group G acting on a mathematical object (or set of objects) X, then one may ask which points x are unchanged, ""invariant"" under the group action, or under an element g of the group.
Frequently one will have a group acting on a set X, which leaves one to determine which objects in an associated set F(X) are invariant. For example, rotation in the plane about a point leaves the point about which it rotates invariant, while translation in the plane does not leave any points invariant, but does leave all lines parallel to the direction of translation invariant as lines. Formally, define the set of lines in the plane P as L(P); then a rigid motion of the plane takes lines to lines – the group of rigid motions acts on the set of lines – and one may ask which lines are unchanged by an action.
More importantly, one may define a function on a set, such as ""radius of a circle in the plane"", and then ask if this function is invariant under a group action, such as rigid motions.
Dual to the notion of invariants are coinvariants, also known as orbits, which formalizes the notion of congruence: objects which can be taken to each other by a group action. For example, under the group of rigid motions of the plane, the perimeter of a triangle is an invariant, while the set of triangles congruent to a given triangle is a coinvariant.
These are connected as follows: invariants are constant on coinvariants (for example, congruent triangles have the same perimeter), while two objects which agree in the value of one invariant may or may not be congruent (for example, two triangles with the same perimeter need not be congruent). In classification problems, one might seek to find a complete set of invariants, such that if two objects have the same values for this set of invariants, then they are congruent.
For example, triangles such that all three sides are equal are congruent under rigid motions, via SSS congruence, and thus the lengths of all three sides form a complete set of invariants for triangles. The three angle measures of a triangle are also invariant under rigid motions, but do not form a complete set as incongruent triangles can share the same angle measures. However, if one allows scaling in addition to rigid motions, then the AAA similarity criterion shows that this is a complete set of invariants.


=== Independent of presentation ===
Secondly, a function may be defined in terms of some presentation or decomposition of a mathematical object; for instance, the Euler characteristic of a cell complex is defined as the alternating sum of the number of cells in each dimension. One may forget the cell complex structure and look only at the underlying topological space (the manifold) – as different cell complexes give the same underlying manifold, one may ask if the function is independent of choice of presentation, in which case it is an intrinsically defined invariant. This is the case for the Euler characteristic, and a general method for defining and computing invariants is to define them for a given presentation, and then show that they are independent of the choice of presentation. Note that there is no notion of a group action in this sense.
The most common examples are:

The presentation of a manifold in terms of coordinate charts – invariants must be unchanged under change of coordinates.
Various manifold decompositions, as discussed for Euler characteristic.
Invariants of a presentation of a group.


=== Unchanged under perturbation ===
Thirdly, if one is studying an object which varies in a family, as is common in algebraic geometry and differential geometry, one may ask if the property is unchanged under perturbation (for example, if an object is constant on families or invariant under change of metric).


== Invariants in computer science ==

In computer science, an invariant is a logical assertion that is always held to be true during a certain phase of execution of a computer program. For example, a loop invariant is a condition that is true at the beginning and the end of every iteration of a loop.
Invariants are especially useful when reasoning about the correctness of a computer program. The theory of optimizing compilers, the methodology of design by contract, and formal methods for determining program correctness, all rely heavily on invariants.
Programmers often use assertions in their code to make invariants explicit. Some object oriented programming languages have a special syntax for specifying class invariants.


=== Automatic invariant detection in imperative programs ===
Abstract interpretation tools can compute simple invariants of given imperative computer programs. The kind of properties that can be found depend on the abstract domains used. Typical example properties are single integer variable ranges like 0<=x<1024, relations between several variables like 0<=i-j<2*n-1, and modulus information like y%4==0. Academic research prototypes also consider simple properties of pointer structures.More sophisticated invariants generally have to be provided manually.
In particular, when verifying an imperative program using the Hoare calculus, a loop invariant has to be provided manually for each loop in the program, which is one of the reasons that this approach is generally impractical for most programs.
In the context of the above MU puzzle example, there is currently no general automated tool that can detect that a derivation from MI to MU is impossible using only the rules 1–4. However, once the abstraction from the string to the number of its ""I""s has been made by hand, leading, for example, to the following C program, an abstract interpretation tool will be able to detect that ICount%3 can't be 0, and hence the ""while""-loop will never terminate.


== See also ==


== Notes ==


== References ==


== External links ==
""Applet: Visual Invariants in Sorting Algorithms"" by William Braynen in 1997"
ff98e7bfa9,Mathematical finance,"Mathematical finance, also known as quantitative finance and financial mathematics, is a field of applied mathematics, concerned with mathematical modeling of financial markets.
In general, there exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing on the one hand, and risk and portfolio management on the other.
Mathematical finance overlaps heavily with the fields of computational finance and financial engineering. The latter focuses on applications and modeling, often by help of stochastic asset models, while the former focuses, in addition to analysis, on building tools of implementation for the models. 
Also related is quantitative investing, which relies on statistical and numerical models (and lately machine learning) as opposed to traditional fundamental analysis when managing portfolios.
French mathematician Louis Bachelier's doctoral thesis, defended in 1900, is considered the first scholarly work on mathematical finance. But mathematical finance emerged as a discipline in the 1970s, following the work of Fischer Black, Myron Scholes and Robert Merton on option pricing theory. Mathematical investing originated from the research of mathematician Edward Thorp who used statistical methods to first invent card counting in blackjack and then applied its principles to modern systematic investing.The subject has a close relationship with the discipline of financial economics, which is concerned with much of the underlying theory that is involved in financial mathematics. While trained economists use complex economic models that are built on observed empirical relationships, in contrast, mathematical finance analysis will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input.
See: Valuation of options; Financial modeling; Asset pricing. 
The fundamental theorem of arbitrage-free pricing is one of the key theorems in mathematical finance, while the Black–Scholes equation and formula are amongst the key results.Today many universities offer degree and research programs in mathematical finance.


== History: Q versus P ==
There are two separate branches of finance that require advanced quantitative techniques: derivatives pricing, and risk and portfolio management. One of the main differences is that they use different probabilities such as the risk-neutral probability (or arbitrage-pricing probability), denoted by ""Q"", and the actual (or actuarial) probability, denoted by ""P"".


=== Derivatives pricing: the Q world ===

The goal of derivatives pricing is to determine the fair price of a given security in terms of more liquid securities whose price is determined by the law of supply and demand. The meaning of ""fair"" depends, of course, on whether one considers buying or selling the security. Examples of securities being priced are plain vanilla and exotic options, convertible bonds, etc.
Once a fair price has been determined, the sell-side trader can make a market on the security. Therefore, derivatives pricing is a complex ""extrapolation"" exercise to define the current market value of a security, which is then used by the sell-side community. 
Quantitative derivatives pricing was initiated by Louis Bachelier in The Theory of Speculation (""Théorie de la spéculation"", published 1900), with the introduction of the most basic and most influential of processes, Brownian motion, and its applications to the pricing of options. Brownian motion is derived using the Langevin equation and the discrete random walk. Bachelier modeled the time series of changes in the logarithm of stock prices as a random walk in which the short-term changes had a finite variance. This causes longer-term changes to follow a Gaussian distribution.The theory remained dormant until Fischer Black and Myron Scholes, along with fundamental contributions by Robert C. Merton, applied the second most influential process, the geometric Brownian motion, to option pricing. For this M. Scholes and R. Merton were awarded the 1997 Nobel Memorial Prize in Economic Sciences. Black was ineligible for the prize because of his death in 1995.The next important step was the fundamental theorem of asset pricing by Harrison and Pliska (1981), according to which the suitably normalized current price P0 of a security is arbitrage-free, and thus truly fair only if there exists a stochastic process Pt with constant expected value which describes its future evolution:

A process satisfying (1) is called a ""martingale"". A martingale does not reward risk. Thus the probability of the normalized security price process is called ""risk-neutral"" and is typically denoted by the blackboard font letter ""
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  "".
The relationship (1) must hold for all times t: therefore the processes used for derivatives pricing are naturally set in continuous time.
The quants who operate in the Q world of derivatives pricing are specialists with deep knowledge of the specific products they model.
Securities are priced individually, and thus the problems in the Q world are low-dimensional in nature. Calibration is one of the main challenges of the Q world: once a continuous-time parametric process has been calibrated to a set of traded securities through a relationship such as (1), a similar relationship is used to define the price of new derivatives.
The main quantitative tools necessary to handle continuous-time Q-processes are Itô's stochastic calculus, simulation and partial differential equations (PDEs).


=== Risk and portfolio management: the P world ===
Risk and portfolio management aims at modeling the statistically derived probability distribution of the market prices of all the securities at a given future investment horizon. 
This ""real"" probability distribution of the market prices is typically denoted by the blackboard font letter ""
  
    
      
        
          P
        
      
    
    {\displaystyle \mathbb {P} }
  "", as opposed to the ""risk-neutral"" probability ""
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  "" used in derivatives pricing. Based on the P distribution, the buy-side community takes decisions on which securities to purchase in order to improve the prospective profit-and-loss profile of their positions considered as a portfolio. Increasingly, elements of this process are automated; see Outline of finance § Quantitative investing for a listing of relevant articles.
For their pioneering work, Markowitz and Sharpe, along with Merton Miller, shared the 1990 Nobel Memorial Prize in Economic Sciences, for the first time ever awarded for a work in finance.
The portfolio-selection work of Markowitz and Sharpe introduced mathematics to investment management. With time, the mathematics has become more sophisticated. Thanks to Robert Merton and Paul Samuelson, one-period models were replaced by continuous time, Brownian-motion models, and the quadratic utility function implicit in mean–variance optimization was replaced by more general increasing, concave utility functions. Furthermore, in recent years the focus shifted toward estimation risk, i.e., the dangers of incorrectly assuming that advanced time series analysis alone can provide completely accurate estimates of the market parameters.
See Financial risk management § Investment management.
Much effort has gone into the study of financial markets and how prices vary with time. 
Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow Theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of ""technical analysis"" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics.


== Criticism ==

The aftermath of the financial crisis of 2009 as well as the multiple Flash Crashes of the early 2010s resulted in social uproars in the general population and ethical malaises in the scientific community which triggered noticeable changes in Quantitative Finance (QF).
More specifically, mathematical finance was instructed to change and become more realistic as opposed to more convenient. The concurrent rise of Big data and Data Science contributed to facilitating these changes. More specifically, in terms of defining new models, we saw a significant increase in the use of Machine Learning overtaking traditional Mathematical Finance models.Over the years, increasingly sophisticated mathematical models and derivative pricing strategies have been developed, but their credibility was damaged by the financial crisis of 2007–2010.
Contemporary practice of mathematical finance has been subjected to criticism from figures within the field notably by Paul Wilmott, and by Nassim Nicholas Taleb, in his book The Black Swan. Taleb claims that the prices of financial assets cannot be characterized by the simple models currently in use, rendering much of current practice at best irrelevant, and, at worst, dangerously misleading. Wilmott and Emanuel Derman published the Financial Modelers' Manifesto in January 2009 which addresses some of the most serious concerns.
Bodies such as the Institute for New Economic Thinking are now attempting to develop new theories and methods.In general, modeling the changes by distributions with finite variance is, increasingly, said to be inappropriate. In the 1960s it was discovered by Benoit Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy alpha-stable distributions. The scale of change, or volatility, depends on the length of the time interval to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation. But the problem is that it does not solve the problem as it makes parametrization much harder and risk control less reliable.Perhaps more fundamental: though mathematical finance models may generate a profit in the short-run, this type of modeling is often in conflict with a central tenet of modern macroeconomics, the Lucas critique - or rational expectations -  which states that observed relationships may not be structural in nature and thus may not be possible to exploit for public policy or for profit unless we have identified relationships using causal analysis and econometrics. Mathematical finance models do not, therefore, incorporate complex elements of human psychology that are critical to modeling modern macroeconomic movements such as the self-fulfilling panic that motivates bank runs.


== See also ==


=== Mathematical tools ===


=== Derivatives pricing ===


=== Portfolio modelling ===


=== Other ===


== Notes ==


== Further reading ==
Nicole El Karoui, ""The future of financial mathematics"", ParisTech Review, 6 September 2013
Harold Markowitz, ""Portfolio Selection"", The Journal of Finance, 7, 1952, pp. 77–91
William F. Sharpe, Investments, Prentice-Hall, 1985
Pierre Henry Labordere (2017). “Model-Free Hedging A Martingale Optimal Transport Viewpoint”. Chapman & Hall/ CRC."
46382effd1,Magnitude (mathematics),"In mathematics, the magnitude or size of a mathematical object is a property which determines whether the object is larger or smaller than other objects of the same kind. More formally, an object's magnitude is the displayed result of an ordering (or ranking)—of the class of objects to which it belongs.
In physics, magnitude can be defined as quantity or distance.


== History ==
The Greeks distinguished between several types of magnitude, including:

Positive fractions
Line segments (ordered by length)
Plane figures (ordered by area)
Solids (ordered by volume)
Angles (ordered by angular magnitude)They proved that the first two could not be the same, or even isomorphic systems of magnitude. They did not consider negative magnitudes to be meaningful, and magnitude is still primarily used in contexts in which zero is either the smallest size or less than all possible sizes.


== Numbers ==

The magnitude of any number 
  
    
      
        x
      
    
    {\displaystyle x}
   is usually called its absolute value or modulus, denoted by 
  
    
      
        
          |
        
        x
        
          |
        
      
    
    {\displaystyle |x|}
  .


=== Real numbers ===
The absolute value of a real number r is defined by:

  
    
      
        
          |
          r
          |
        
        =
        r
        ,
        
           if 
        
        r
        
           ≥ 
        
        0
      
    
    {\displaystyle \left|r\right|=r,{\text{ if }}r{\text{ ≥ }}0}
  

  
    
      
        
          |
          r
          |
        
        =
        −
        r
        ,
        
           if 
        
        r
        <
        0.
      
    
    {\displaystyle \left|r\right|=-r,{\text{ if }}r<0.}
  Absolute value may also be thought of as the number's distance from zero on the real number line. For example, the absolute value of both 70 and −70 is 70.


=== Complex numbers ===
A complex number z may be viewed as the position of a point P in a 2-dimensional space, called the complex plane. The absolute value (or modulus) of z may be thought of as the distance of P from the origin of that space. The formula for the absolute value of z = a + bi is similar to that for the Euclidean norm of a vector in a 2-dimensional Euclidean space:

  
    
      
        
          |
          z
          |
        
        =
        
          
            
              a
              
                2
              
            
            +
            
              b
              
                2
              
            
          
        
      
    
    {\displaystyle \left|z\right|={\sqrt {a^{2}+b^{2}}}}
  where the real numbers a and b are the real part and the imaginary part of z, respectively. For instance, the modulus of −3 + 4i is 
  
    
      
        
          
            (
            −
            3
            
              )
              
                2
              
            
            +
            
              4
              
                2
              
            
          
        
        =
        5
      
    
    {\displaystyle {\sqrt {(-3)^{2}+4^{2}}}=5}
  .  Alternatively, the magnitude of a complex number z may be defined as the square root of the product of itself and its complex conjugate, 
  
    
      
        
          
            
              z
              ¯
            
          
        
      
    
    {\displaystyle {\bar {z}}}
  , where for any complex number 
  
    
      
        z
        =
        a
        +
        b
        i
      
    
    {\displaystyle z=a+bi}
  , its complex conjugate is 
  
    
      
        
          
            
              z
              ¯
            
          
        
        =
        a
        −
        b
        i
      
    
    {\displaystyle {\bar {z}}=a-bi}
  . 

  
    
      
        
          |
          z
          |
        
        =
        
          
            z
            
              
                
                  z
                  ¯
                
              
            
          
        
        =
        
          
            (
            a
            +
            b
            i
            )
            (
            a
            −
            b
            i
            )
          
        
        =
        
          
            
              a
              
                2
              
            
            −
            a
            b
            i
            +
            a
            b
            i
            −
            
              b
              
                2
              
            
            
              i
              
                2
              
            
          
        
        =
        
          
            
              a
              
                2
              
            
            +
            
              b
              
                2
              
            
          
        
      
    
    {\displaystyle \left|z\right|={\sqrt {z{\bar {z}}}}={\sqrt {(a+bi)(a-bi)}}={\sqrt {a^{2}-abi+abi-b^{2}i^{2}}}={\sqrt {a^{2}+b^{2}}}}
  (where 
  
    
      
        
          i
          
            2
          
        
        =
        −
        1
      
    
    {\displaystyle i^{2}=-1}
  ).


== Vector spaces ==


=== Euclidean vector space ===

A Euclidean vector represents the position of a point P in a Euclidean space. Geometrically, it can be described as an arrow from the origin of the space (vector tail) to that point (vector tip). Mathematically, a vector x in an n-dimensional Euclidean space can be defined as an ordered list of n real numbers (the Cartesian coordinates of P): x = [x1, x2, ..., xn]. Its magnitude or length, denoted by 
  
    
      
        ‖
        x
        ‖
      
    
    {\displaystyle \|x\|}
  , is most commonly defined as its Euclidean norm (or Euclidean length):

  
    
      
        ‖
        
          x
        
        ‖
        =
        
          
            
              x
              
                1
              
              
                2
              
            
            +
            
              x
              
                2
              
              
                2
              
            
            +
            ⋯
            +
            
              x
              
                n
              
              
                2
              
            
          
        
        .
      
    
    {\displaystyle \|\mathbf {x} \|={\sqrt {x_{1}^{2}+x_{2}^{2}+\cdots +x_{n}^{2}}}.}
  For instance, in a 3-dimensional space, the magnitude of [3, 4, 12] is 13 because 
  
    
      
        
          
            
              3
              
                2
              
            
            +
            
              4
              
                2
              
            
            +
            
              12
              
                2
              
            
          
        
        =
        
          
            169
          
        
        =
        13.
      
    
    {\displaystyle {\sqrt {3^{2}+4^{2}+12^{2}}}={\sqrt {169}}=13.}
  
This is equivalent to the square root of the dot product of the vector with itself:

  
    
      
        ‖
        
          x
        
        ‖
        =
        
          
            
              x
            
            ⋅
            
              x
            
          
        
        .
      
    
    {\displaystyle \|\mathbf {x} \|={\sqrt {\mathbf {x} \cdot \mathbf {x} }}.}
  The Euclidean norm of a vector is just a special case of Euclidean distance: the distance between its tail and its tip. Two similar notations are used for the Euclidean norm of a vector x:

  
    
      
        
          ‖
          
            x
          
          ‖
        
        ,
      
    
    {\displaystyle \left\|\mathbf {x} \right\|,}
  

  
    
      
        
          |
          
            x
          
          |
        
        .
      
    
    {\displaystyle \left|\mathbf {x} \right|.}
  A disadvantage of the second notation is that it can also be used to denote the absolute value of scalars and the determinants of matrices, which introduces an element of ambiguity.


=== Normed vector spaces ===

By definition, all Euclidean vectors have a magnitude (see above). However, a vector in an abstract vector space does not possess a magnitude.
A vector space endowed with a norm, such as the Euclidean space, is called a normed vector space. The norm of a vector v in a normed vector space can be considered to be the magnitude of v.


=== Pseudo-Euclidean space ===
In a pseudo-Euclidean space, the magnitude of a vector is the value of the quadratic form for that vector.


== Logarithmic magnitudes ==
When comparing magnitudes, a logarithmic scale is often used. Examples include the loudness of a sound (measured in decibels), the brightness of a star, and the Richter scale of earthquake intensity. Logarithmic magnitudes can be negative, and cannot be added or subtracted meaningfully (since the relationship is non-linear).


== Order of magnitude ==

Orders of magnitude denote differences in numeric quantities, usually measurements, by a factor of 10—that is, a difference of one digit in the location of the decimal point.


== See also ==
Number sense
Vector notation
Set size


== References =="
341d134d42,Additional Mathematics,"Additional Mathematics is a qualification in mathematics, commonly taken by students in high-school (or GCSE exam takers in the United Kingdom). It is applied to a range of problems set out in a different format and wider content to the standard Mathematics at the same level.


== Additional Mathematics in Singapore ==
In Singapore, Additional Mathematics is an optional subject offered to pupils in secondary school—specifically those who have an aptitude in Mathematics and are in the Normal (Academic) stream  or Express stream. The syllabus covered is more in-depth as compared to Elementary Mathematics, with additional topics including Algebra binomial expansion, proofs in plane geometry, differential calculus and integral calculus. Additional Mathematics is also a prerequisite for students who are intending to offer H2 Mathematics and H2 Further Mathematics at A-level (if they choose to enter a Junior College after secondary school). Students without Additional Mathematics at the 'O' level will usually be offered H1 Mathematics instead.  


=== Examination Format ===
The syllabus was updated starting with the 2021 batch of candidates. There are two written papers, each comprising half of the weightage towards the subject. Each paper is 2 hours 15 minutes long and worth 90 marks. Paper 1 has 12 to 14 questions, while Paper 2 has 9 to 11 questions. Generally, Paper 2 would have a graph plotting question based on linear law.


== GCSE Additional Mathematics in Northern Ireland ==
In Northern Ireland, Additional Mathematics was offered as a GCSE subject by the local examination board, CCEA. There were two examination papers: one which tested topics in Pure Mathematics, and one which tested topics in Mechanics and Statistics. It was discontinued in 2014 and replaced with GCSE Further Mathematics—a new qualification whose level exceeds both those offered by GCSE Mathematics, and the analogous qualifications offered in England.


== Further Maths IGCSE and Additional Maths FSMQ in England ==
Starting from 2012, Edexcel and AQA have started a new course which is an IGCSE in Further Maths. Edexcel and AQA both offer completely different courses, with Edexcel including the calculation of solids formed through integration, and AQA not including integration. 
AQA's syllabus mainly offers further algebra, with the factor theorem and the more complex algebra such as algebraic fractions. It also offers differentiation up to—and including—the calculation of normals to a curve. AQA's syllabus also includes a wide selection of matrices work, which is an AS Further Mathematics topic. 
AQA's syllabus is much more famous than Edexcel's, mainly for its controversial decision to award an A* with Distinction (A^), a grade higher than the maximum possible grade in any Level 2 qualification; it is known colloquially as a Super A* or A**.
A new Additional Maths course from 2018 is OCR Level 3 FSMQ: Additional Maths (6993). In addition to algebra, coordinate geometry, Pythagorean theorem, trigonometry and calculus, which were on the previous specification, this course also includes:

'Enumeration' content, which expands the topic of the binomial distribution to include permutations and combinations
'Numerical methods’ content, which expands upon the informal graphical approximations in GCSE
'Exponentials and Logarithms’ content, which develops the growth and decay content and the graphs section of GCSE
'Sequences' content, which uses subscript notation to support the iterative work on numerical methods.


== Additional Mathematics in Malaysia ==
In Malaysia, Additional Mathematics is offered as an elective to upper secondary students within the public education system. This subject is included in the Sijil Pelajaran Malaysia examination.
Science stream students are required to apply for Additional Mathematics as one of the subjects in the Sijil Pelajaran Malaysia examination, while Additional Mathematics is an optional subject for students who are from arts or commerce streams. 
Additional Mathematics in Malaysia—also commonly known as Add Maths—can be organized into two learning packages: the Core Package, which includes geometry, algebra, calculus, trigonometry and statistics, and the Elective Package, which includes science and technology application and social science application. It covers various topics including: 

Format for Additional Mathematics Exam based on the Malaysia Certificate of Education is as follows:

Paper 1 (Duration: 2 Hours): Questions are categorised into Sections A and B and are tested based on the student's knowledge to grasp the concepts and formulae learned during their 2 years of learning. Section A consists of 12 questions in which all must all be answered, whereas Section B consists of 3 questions and students are given the choice to answer 2 of the three questions only. Each question may contain from zero to three subsets of questions with marks ranging from 2 to 8 marks. The total weighting of the paper is 80 marks and constitutes 44% of the grade.
Paper 2 (Duration: 2 hours 30 minutes): Questions are categorised into 3 sections: A, B and C. Section A contains 7 questions which must all be answered. Section B contains 4 questions where students are given the choice to answer 3 out of 4 of them. Section C contains 4 questions where students are only required to answer 2 out of 4 of the given questions. All Section C questions are based on the same chapters every year and are thus predictable. A question in Section C carries 10 marks with at 3 to 4 subquestions per question. This paper tests the student's ability to apply various concepts and formulae in real-life situations. The total weighting of the paper is 100 marks and constitutes 56% of the grade.In 2020, the first batch of students learning the new syllabus, KSSM, will receive new Form 4 textbooks with new chapters which contain certain topics from A-levels.


== Additional Mathematics in Mauritius ==
In Mauritius, Additional Mathematics, more commonly referred to as Add Maths, is offered in secondary school as an optional subject in the Arts Streams, and a compulsory subject in the Science, Technical and Economics Stream. This subject is included in the University of Cambridge International Examinations, with covered topics including functions, quadratic equations, differentiation and integration (calculus).


== Additional Mathematics in Hong Kong ==
In Hong Kong, the syllabus of HKCEE additional mathematics covered three main topics, algebra, calculus and analytic geometry. In algebra, the topics covered include mathematical induction, binomial theorem, quadratic equations, trigonometry, inequalities, 2D-vectors and complex number, whereas in calculus, the topics covered include limit, differentiation and integration.In the HKDSE, additional mathematics was replaced by Mathematics Extend Modules, while some topics, such as matrix and determinant, many of which are covered in the syllabus of HKALE pure mathematics and applied mathematics, are also included.


== See also ==
Advanced level mathematics
Further mathematics


== References ==


== External links ==
[Vault] (Online resources on higher mathematics)"
7d2b134738,Degeneracy (mathematics),"In mathematics, a degenerate case is a limiting case of a class of objects which appears to be qualitatively different from (and usually simpler than) the rest of the class, and the term degeneracy is the condition of being a degenerate case.The definitions of many classes of composite or structured objects often implicitly include inequalities. For example, the angles and the side lengths of a triangle are supposed to be positive. The limiting cases, where one or several of these inequalities become equalities, are degeneracies. In the case of triangles, one has a degenerate triangle if at least one side length or angle is zero. Equivalently, it becomes a ""line segment"".Often, the degenerate cases are the exceptional cases where changes to the usual dimension or the cardinality of the object (or of some part of it) occur. For example, a triangle is an object of dimension two, and a degenerate triangle is contained in a line, which makes its dimension one. This is similar to the case of a circle, whose dimension shrinks from two to zero as it degenerates into a point. As another example, the solution set of a system of equations that depends on parameters generally has a fixed cardinality and dimension, but cardinality and/or dimension may be different for some exceptional values, called degenerate cases. In such a degenerate case, the solution set is said to be degenerate.
For some classes of composite objects, the degenerate cases depend on the properties that are specifically studied. In particular, the class of objects may often be defined or characterized by systems of equations. In most scenarios, a given class of objects may be defined by several different systems of equations, and these different systems of equations may lead to different degenerate cases, while characterizing the same non-degenerate cases. This may be the reason for which there is no general definition of degeneracy, despite the fact that the concept is widely used and defined (if needed) in each specific situation.
A degenerate case thus has special features which makes it non-generic or special cases. However, not all non-generic or special cases are degenerate. For example, right triangles, isosceles triangles and equilateral triangles are non-generic and non-degenerate. In fact, degenerate cases often correspond to singularities, either in the object or in some configuration space. For example, a conic section is degenerate if and only if it has singular points (e.g., point, line, intersecting lines).


== In geometry ==


=== Conic section ===

A degenerate conic is a conic section (a second-degree plane curve, defined by a polynomial equation of degree two)  that fails to be an  irreducible curve.

A point is a degenerate circle, namely one with radius 0.
The line is a degenerate case of a parabola if the parabola resides on a tangent plane. In inversive geometry, a line is a degenerate case of a circle, with infinite radius.
Two parallel lines also form a degenerate parabola.
A line segment can be viewed as a degenerate case of an ellipse in which the semiminor axis goes to zero, the foci go to the endpoints, and the eccentricity goes to one.
A circle can be thought of as a degenerate ellipse, as the eccentricity approaches 0 and the foci merge.
An ellipse can also degenerate into a single point.
A hyperbola can degenerate into two lines crossing at a point, through a family of hyperbolae having those lines as common asymptotes.


=== Triangle ===

A degenerate triangle has collinear vertices and zero area, and thus coincides with a segment covered twice (if the three vertices are not all equal; otherwise, the triangle degenerates to a single point). If the three vertices are pairwise distinct, it has two 0° angles and one 180° angle. If two vertices are equal, it has one 0° angle and two undefined angles.


=== Rectangle ===
A line segment is a degenerate case of a rectangle which has a side of length 0.
For any non-empty subset 
  
    
      
        S
        ⊆
        {
        1
        ,
        2
        ,
        …
        ,
        n
        }
      
    
    {\displaystyle S\subseteq \{1,2,\ldots ,n\}}
  , there is a bounded, axis-aligned degenerate rectangle  where 
  
    
      
        
          x
        
        ≜
        
          [
          
            
              x
              
                1
              
            
            ,
            
              x
              
                2
              
            
            ,
            …
            ,
            
              x
              
                n
              
            
          
          ]
        
      
    
    {\displaystyle \mathbf {x} \triangleq \left[x_{1},x_{2},\ldots ,x_{n}\right]}
   and ai, bi, ci are constant (with ai ≤ bi for all i). The number of degenerate sides of R is the number of elements of the subset S. Thus, there may be as few as one degenerate ""side"" or as many as n (in which case R reduces to a singleton point).


=== Convex polygon ===
A convex polygon is degenerate if at least two consecutive sides coincide at least partially, or at least one side has zero length, or at least one angle is 180°. Thus a degenerate convex polygon of n sides looks like a polygon with fewer sides. In the case of triangles, this definition coincides with the one that has been given above.


=== Convex polyhedron ===
A convex polyhedron is degenerate if either two adjacent facets are coplanar or two edges are aligned. In the case of a tetrahedron, this is equivalent to saying that all of its vertices lie in the same plane, giving it a volume of zero.


=== Standard torus ===
In contexts where self-intersection is allowed, a double-covered sphere is a degenerate standard torus where the axis of revolution passes through the center of the generating circle, rather than outside it.
A torus degenerates to a circle when its minor radius goes to 0.


=== Sphere ===
When the radius of a sphere goes to zero, the resulting degenerate sphere of zero volume is a point.


=== Other ===
See general position for other examples.


== Elsewhere ==
A set containing a single point is a degenerate continuum.
Objects such as the digon and monogon can be viewed as degenerate cases of polygons: valid in a general abstract mathematical sense, but not part of the original Euclidean conception of polygons.
A random variable which can only take one value has a degenerate distribution; if that value is the real number 0, then its probability density is the Dirac delta function.
A root of a polynomial is sometimes said to be degenerate if it is a multiple root, since generically the n roots of an nth degree polynomial are all distinct. This usage carries over to eigenproblems: a degenerate eigenvalue is a multiple root of the characteristic polynomial.
In quantum mechanics, any such multiplicity in the eigenvalues of the Hamiltonian operator gives rise to degenerate energy levels. Usually any such degeneracy indicates some underlying symmetry in the system.


== See also ==
Degeneracy (graph theory)
Degenerate form
Trivial (mathematics)
Pathological (mathematics)
Vacuous truth


== References =="
033fa6db38,Mathematics Genealogy Project,"The Mathematics Genealogy Project (MGP) is a web-based database for the academic genealogy of mathematicians. As of 31 December 2021, it contained information on 274,575 mathematical scientists who contributed to research-level mathematics. For a typical mathematician, the project entry includes graduation year, thesis title (in its Mathematics Subject Classification), alma mater, doctoral advisor, and doctoral students.


== Origin of the database ==
The project grew out of founder Harry Coonce's desire to know the name of his advisor's advisor. Coonce was Professor of Mathematics at Minnesota State University, Mankato, at the time of the project's founding, and the project went online there in fall 1997. Coonce retired from Mankato in 1999, and in fall 2002 the university decided that it would no longer support the project. The project relocated at that time to North Dakota State University. Since 2003, the project has also operated under the auspices of the American Mathematical Society and in 2005 it received a grant from the Clay Mathematics Institute. Harry Coonce has been assisted by Mitchel T. Keller, Assistant Professor at Morningside College. Keller is currently the Managing Director of the project.


== Mission ==
The Mathematics Genealogy Mission statement: ""Throughout this project when we use the word ""mathematics"" or ""mathematician"" we mean that word in a very inclusive sense. Thus, all relevant data from statistics, computer science, philosophy or operations research is welcome.""


== Scope ==
The genealogy information is obtained from sources such as Dissertation Abstracts International and Notices of the American Mathematical Society, but may be supplied by anyone via the project's website. The searchable database contains the name of the mathematician, university which awarded the degree, year when the degree was awarded, title of the dissertation, names of the advisor and second advisor, a flag of the country where the degree was awarded, a listing of doctoral students, and a count of academic descendants. Some historically significant figures who lacked a doctoral degree are listed, notably Joseph-Louis Lagrange.


== Reliability and completeness ==
It has been noted that ""the data collected by the mathematics genealogy project are self-reported, so there is no guarantee that the observed genealogy network is a complete description of the mentorship network. In fact, 16,147 mathematicians do not have a recorded mentor, and of these, 8,336 do not have any recorded proteges."" Maimgren, Ottino and Amaral (2010) stated that ""for [mathematicians who graduated between 1900 and 1960] we believe that the graduation and mentorship record is the most reliable.""


== See also ==
Neurotree, Academic Family Tree


== References ==


== External links ==

Official website"
ca332a19a9,MacTutor History of Mathematics archive,"The MacTutor History of Mathematics archive is a website maintained by John J. O'Connor and Edmund F. Robertson and hosted by the University of St Andrews in Scotland. It contains detailed biographies on many historical and contemporary mathematicians, as well as information on famous curves and various topics in the history of mathematics.
The History of Mathematics archive was an outgrowth of Mathematical MacTutor system, a HyperCard database by the same authors, which won them the European Academic Software award in 1994. In the same year, they founded their web site. As of 2015, it has biographies on over 2800 mathematicians and scientists.In 2015, O'Connor and Robertson won the Hirst Prize of the London Mathematical Society for their work. The citation for the Hirst Prize calls the archive ""the most widely used and influential web-based resource in history of mathematics"".


== See also ==
Mathematics Genealogy Project
MathWorld
PlanetMath


== References ==


== External links ==
MacTutor History of Mathematics archive
Mathematical MacTutor system"
b5cf24c62e,List of mathematics competitions,"Mathematics competitions or mathematical olympiads are competitive events where participants complete a math test. These tests may require multiple choice or numeric answers, or a detailed written solution or proof.


== International mathematics competitions ==
Championnat International de Jeux Mathématiques et Logiques — for all ages, mainly for French-speaking countries, but participation is not limited by language.
China Girls Mathematical Olympiad (CGMO) — held annually for teams of girls representing different regions within China and a few other countries.
European Girls' Mathematical Olympiad (EGMO) — since April 2012
Integration Bee — competition in integral calculus held in various institutions of higher learning in the United States and some other countries
Interdisciplinary Contest in Modeling (ICM) — team contest for undergraduates
International Mathematical Modeling Challenge — team contest for high school students
International Mathematical Olympiad (IMO) — the oldest international Olympiad, occurring annually since 1959.
International Mathematics Competition for University Students (IMC) — international competition for undergraduate students.
Mathematical Contest in Modeling (MCM) — team contest for undergraduates
Mathematical Kangaroo — worldwide competition.
Mental Calculation World Cup — contest for the best mental calculators
Primary Mathematics World Contest (PMWC) — worldwide competition
Rocket City Math League (RCML) — Competition run by students at Virgil I. Grissom High School with levels ranging from Explorer (Pre-Algebra) to Discovery (Comprehensive)
Romanian Master of Mathematics and Sciences — Olympiad for the selection of the top 20 countries in the last IMO.
Tournament of the Towns — worldwide competition.


== Regional mathematics competitions ==
Asian Pacific Mathematics Olympiad (APMO) — Pacific rim
Balkan Mathematical Olympiad — for students from Balkan area
Baltic Way — Baltic area
ICAS-Mathematics (formerly Australasian Schools Mathematics Assessment)
Mediterranean Mathematics Competition. Olympiad for countries in the Mediterranean zone.
Nordic Mathematical Contest (NMC) — the five Nordic countries
North East Asian Mathematics Competition (NEAMC) — North-East Asia
Pan African Mathematics Olympiads (PAMO)
South East Asian Mathematics Competition (SEAMC) — South-East Asia
William Lowell Putnam Mathematical Competition — United States and Canada


== National mathematics olympiads ==


=== Australia ===
Australian Mathematics Competition


=== Bangladesh ===
Bangladesh Mathematical Olympiad (Jatio Gonit Utshob)


=== Belgium ===
Olympiade Mathématique Belge — competition for French-speaking students in Belgium
Vlaamse Wiskunde Olympiade — competition for Dutch-speaking students in Belgium


=== Brazil ===
Olimpíada Brasileira de Matemática (OBM) — national competition open to all students from sixth grade to university
Olimpíada Brasileira de Matemática das Escolas Públicas (OBMEP) — national competition open to public-school students from fourth grade to high school


=== Canada ===
Canadian Open Mathematics Challenge — Canada's premier national mathematics competition open to any student with an interest in and grasp of high school math and organised by Canadian Mathematical Society
Canadian Mathematical Olympiad — competition whose top performers represent Canada at the International Mathematical Olympiad
The Centre for Education in Mathematics and Computing (CEMC) based out of the University of Waterloo hosts long-standing national competitions for grade levels 7–12
MathChallengers (formerly MathCounts BC) — for eighth, ninth, and tenth grade students


=== France ===
Concours général — competition whose mathematics portion is open to twelfth grade students


=== Hong Kong ===
Hong Kong Mathematics Olympiad
Hong Kong Mathematical High Achievers Selection Contest — for students from Form 1 to Form 3
Pui Ching Invitational Mathematics Competition
Primary Mathematics World Contest
Global Mathematics Elite Competition


=== Hungary ===
Miklós Schweitzer Competition
Középiskolai Matematikai Lapok — correspondence competition for students from 9th–12th grade
National Secondary School Academic Competition - Mathematics


=== India ===
Indian National Mathematical Olympiad


=== Indonesia ===
National Science Olympiad (Olimpiade Sains Nasional) — includes mathematics along with various science topics


=== Kenya ===
Moi National Mathematics Contest — prepared and hosted by Mang'u High School but open to students from all Kenyan high schools


=== Nigeria ===
Cowbellpedia. This contest is sponsored by Promasidor Nigeria. It is open to students from eight to eighteen, at public and private schools in Nigeria.


=== Saudi Arabia ===
KFUPM mathematics olympiad – organized by King Fahd University of Petroleum and Minerals (KFUPM).


=== Singapore ===
Singapore Mathematical Olympiad (SMO) — organized by the Singapore Mathematical Society, the competition is open to all pre-university students in Singapore.


=== South Africa ===
University of Cape Town Mathematics Competition — open to students in grades 8 through 12 in the Western Cape province.


=== United States ===


==== National elementary school competitions (K–5) and higher ====
Math League (grades 4–12)
Mathematical Olympiads for Elementary and Middle Schools (MOEMS) (grades 4–6 and 7–8)


==== National middle school competitions (grades 6–8) and lower/higher ====
American Mathematics Contest 8 (AMC->8), formerly the American Junior High School Mathematics Examination (AJHSME)
Math League (grades 4–12)
MATHCOUNTS
Mathematical Olympiads for Elementary and Middle Schools (MOEMS)
Rocket City Math League (pre-algebra to calculus)
United States of America Mathematical Talent Search (USAMTS)


==== National high school competitions (grade 9–12) and lower ====
American Invitational Mathematics Examination (AIME)
American Mathematics Contest 10 (AMC10)
American Mathematics Contest 12 (AMC12), formerly the American High School Mathematics Examination (AHSME)
American Regions Mathematics League (ARML)
Harvard-MIT Mathematics Tournament (HMMT)
iTest
High School Mathematical Contest in Modeling (HiMCM)
Math League (grades 4–12)
Math-O-Vision (grades 9–12)
Math Prize for Girls
MathWorks Math Modeling Challenge
Mu Alpha Theta
United States of America Mathematical Olympiad (USAMO)
United States of America Mathematical Talent Search (USAMTS)
Rocket City Math League (pre-algebra to calculus)


==== National college competitions ====
AMATYC Mathematics Contest
Mathematical Contest in Modeling (MCM)
William Lowell Putnam Mathematical Competition


==== Regional competitions ====


== References =="
57c0e1414c,Mathematical beauty,"Mathematical beauty is the aesthetic pleasure derived from the abstractness, purity, simplicity, depth or orderliness of mathematics. Mathematicians may express this pleasure by describing mathematics (or, at least, some aspect of mathematics) as beautiful or describe mathematics as an art form, (a position taken by G. H. Hardy) or, at a minimum, as a creative activity. Comparisons are  made with music and poetry.


== In method ==
Mathematicians describe an especially pleasing method of proof as elegant. Depending on context, this may mean:

A proof that uses a minimum of additional assumptions or previous results.
A proof that is unusually succinct.
A proof that derives a result in a surprising way (e.g., from an apparently unrelated theorem or a collection of theorems).
A proof that is based on new and original insights.
A method of proof that can be easily generalized to solve a family of similar problems.In the search for an elegant proof, mathematicians often look for different independent ways to prove a result—as the first proof that is found can often be improved. The theorem for which the greatest number of different proofs have been discovered is possibly the Pythagorean theorem, with hundreds of proofs being published up to date. Another theorem that has been proved in many different ways is the theorem of quadratic reciprocity. In fact, Carl Friedrich Gauss alone had eight different proofs of this theorem, six of which he published.Conversely, results that are logically correct but involve laborious calculations, over-elaborate methods, highly conventional approaches or a large number of powerful axioms or previous results are usually not considered to be elegant, and may be even referred to as ugly or clumsy.


== In results ==

Some mathematicians see beauty in mathematical results that establish connections between two areas of mathematics that at first sight appear to be unrelated. These results are often described as deep. While it is difficult to find universal agreement on whether a result is deep, some examples are more commonly cited than others. One such example is Euler's identity:
This elegant expression ties together arguably the five most important mathematical constants (e, i, π, 1, and 0) with the two most common mathematical symbols (+, =). Euler's identity is a special case of Euler's formula, which the physicist Richard Feynman called ""our jewel"" and ""the most remarkable formula in mathematics"". Modern examples include the modularity theorem, which establishes an important connection between elliptic curves and modular forms (work on which led to the awarding of the Wolf Prize to Andrew Wiles and Robert Langlands), and ""monstrous moonshine"", which connects the Monster group to modular functions via string theory (for which Richard Borcherds was awarded the Fields Medal).
Other examples of deep results include unexpected insights into mathematical structures. For example, Gauss's Theorema Egregium is a deep theorem which relates a local phenomenon (curvature) to a global phenomenon (area) in a surprising way. In particular, the area of a triangle on a curved surface is proportional to the excess of the triangle and the proportionality is curvature. Another example is the fundamental theorem of calculus (and its vector versions including Green's theorem and Stokes' theorem).
The opposite of deep is trivial. A trivial theorem may be a result that can be derived in an obvious and straightforward way from other known results, or which applies only to a specific set of particular objects such as the empty set. In some occasions, however, a statement of a theorem can be original enough to be considered deep—even though its proof is fairly obvious.
In his 1940 essay  A Mathematician's Apology, G. H. Hardy suggested that a beautiful proof or result possesses ""inevitability"", ""unexpectedness"", and ""economy"".In 1997, Gian-Carlo Rota, disagreed with unexpectedness as a sufficient condition for beauty and proposed a counterexample:

A great many theorems of mathematics, when first published, appear to be surprising; thus for example some twenty years ago [from 1977] the proof of the existence of non-equivalent differentiable structures on spheres of high dimension was thought to be surprising, but it did not occur to anyone to call such a fact beautiful, then or now. 
In contrast, Monastyrsky wrote in 2001:

It is very difficult to find an analogous invention in the past to Milnor's beautiful construction of the different differential structures on the seven-dimensional sphere... The original proof of Milnor was not very constructive, but later E. Briscorn showed that these differential structures can be described in an extremely explicit and beautiful form.
This disagreement illustrates both the subjective nature of mathematical beauty and its connection with mathematical results: in this case, not only the existence of exotic spheres, but also a particular realization of them.


== In experience ==

Interest in pure mathematics that is separate from empirical study has been part of the experience of various civilizations, including that of the ancient Greeks, who ""did mathematics for the beauty of it"". The aesthetic pleasure that mathematical physicists tend to experience in Einstein's theory of general relativity has been attributed (by Paul Dirac, among others) to its ""great mathematical beauty"". The beauty of mathematics is experienced when the physical reality of objects are represented by mathematical models. Group theory, developed in the early 1800s for the sole purpose of solving polynomial equations, became a fruitful way of categorizing elementary particles—the building blocks of matter. Similarly, the study of knots provides important insights into string theory and loop quantum gravity.Some believe that in order to appreciate mathematics, one must engage in doing mathematics.For example, Math Circle is an after-school enrichment program where students do mathematics through games and activities; there are also some teachers that encourage student engagement by teaching mathematics in kinesthetic learning. In a general Math Circle lesson, students use pattern finding, observation, and exploration to make their own mathematical discoveries. For example, mathematical beauty arises in a Math Circle activity on symmetry designed for 2nd and 3rd graders, where students create their own snowflakes by folding a square piece of paper and cutting out designs of their choice along the edges of the folded paper. When the paper is unfolded, a symmetrical design reveals itself. In a day to day elementary school mathematics class, symmetry can be presented as such in an artistic manner where students see aesthetically pleasing results in mathematics.Some teachers prefer to use mathematical manipulatives to present mathematics in an aesthetically pleasing way. Examples of a manipulative include algebra tiles, cuisenaire rods, and pattern blocks. For example, one can teach the method of completing the square by using algebra tiles. Cuisenaire rods can be used to teach fractions, and pattern blocks can be used to teach geometry. Using mathematical manipulatives helps students gain a conceptual understanding that might not be seen immediately in written mathematical formulas.Another example of beauty in experience involves the use of origami. Origami, the art of paper folding, has aesthetic qualities and many mathematical connections. One can study the mathematics of paper folding by observing the crease pattern on unfolded origami pieces.Combinatorics, the study of counting, has artistic representations which some find mathematically beautiful. There are many visual examples which illustrate combinatorial concepts. Some of the topics and objects seen in combinatorics courses with visual representations include, among others Four color theorem, Young tableau, Permutohedron,  Graph theory,  Partition of a set.Brain imaging experiments conducted by Semir Zeki and his colleagues show that the experience of mathematical beauty has, as a neural correlate, activity in field A1 of the medial orbito-frontal cortex (mOFC) of the brain and that this activity is parametrically related to the declared intensity of beauty. The location of the activity is similar to the location of the activity that correlates with the experience of beauty from other sources, such as music or joy or sorrow. Moreover, mathematicians seem resistant to revising their judgment of the beauty of a mathematical formula in light of contradictory opinion given by their peers.


== In philosophy ==
Some mathematicians are of the opinion that the doing of mathematics is closer to discovery than invention, for example:

There is no scientific discoverer, no poet, no painter, no musician, who will not tell you that he found ready made his discovery or poem or picture—that it came to him from outside, and that he did not consciously create it from within.

These mathematicians believe that the detailed and precise results of mathematics may be reasonably taken to be true without any dependence on the universe in which we live. For example, they would argue that the theory of the natural numbers is fundamentally valid, in a way that does not require any specific context. Some mathematicians have extrapolated this viewpoint that mathematical beauty is truth further, in some cases becoming mysticism.
In Plato's philosophy there were two worlds, the physical one in which we live and another abstract world which contained unchanging truth, including mathematics. He believed that the physical world was a mere reflection of the more perfect abstract world.Hungarian mathematician Paul Erdős spoke of an imaginary book, in which God has written down all the most beautiful mathematical proofs. When Erdős wanted to express particular appreciation of a proof, he would exclaim ""This one's from The Book!""
Twentieth-century French philosopher Alain Badiou claimed that ontology is mathematics. Badiou also believes in deep connections between mathematics, poetry and philosophy.
In many cases, however, natural philosophers and other scientists who have made extensive use of mathematics have made leaps of inference between beauty and physical truth in ways that turned out to be erroneous. For example, at one stage in his life, Johannes Kepler believed that the proportions of the orbits of the then-known planets in the Solar System have been arranged by God to correspond to a concentric arrangement of the five Platonic solids, each orbit lying on the circumsphere of one polyhedron and the insphere of another. As there are exactly five Platonic solids, Kepler's hypothesis could only accommodate six planetary orbits and was disproved by the subsequent discovery of Uranus.


== In information theory ==
In the 1970s, Abraham Moles and Frieder Nake analyzed links between beauty, information processing, and information theory. In the 1990s, Jürgen Schmidhuber formulated a mathematical theory of observer-dependent subjective beauty based on algorithmic information theory: the most beautiful objects among subjectively comparable objects have short algorithmic descriptions (i.e., Kolmogorov complexity) relative to what the observer already knows. Schmidhuber explicitly distinguishes between beautiful and interesting. The latter corresponds to the first derivative of subjectively perceived beauty:
the observer continually tries to improve the predictability and compressibility of the observations by discovering regularities such as repetitions and symmetries and fractal self-similarity. Whenever the observer's learning process (possibly a predictive artificial neural network) leads to improved data compression such that the observation sequence can be described by fewer bits than before, the temporary interesting-ness of the data corresponds to the compression progress, and is proportional to the observer's internal curiosity reward.


== In the arts ==


=== Music ===
Examples of the use of mathematics in music include the stochastic music of Iannis Xenakis, the Fibonacci sequence in Tool's Lateralus, counterpoint of Johann Sebastian Bach, polyrhythmic structures (as in Igor Stravinsky's The Rite of Spring), the Metric modulation of Elliott Carter, permutation theory in serialism beginning with Arnold Schoenberg, and application of Shepard tones in Karlheinz Stockhausen's Hymnen.  They also include the application of Group theory to transformations in music in the theoretical writings of David Lewin.


=== Visual arts ===

Examples of the use of mathematics in the visual arts include applications of chaos theory and fractal geometry to computer-generated art, symmetry studies of Leonardo da Vinci, projective geometries in development of the perspective theory of Renaissance art, grids in Op art, optical geometry in the camera obscura of Giambattista della Porta, and multiple perspective in analytic cubism and futurism.
Sacred geometry is a field of its own, giving rise to countless art forms including some of the best known mystic symbols and religious motifs, and has a particularly rich history in Islamic architecture.  It also provides a means of meditation and comtemplation, for example the study of the Kaballah Sefirot (Tree Of Life) and Metatron's Cube; and also the act of drawing itself.
The Dutch graphic designer M. C. Escher created mathematically inspired woodcuts, lithographs, and mezzotints. These feature impossible constructions, explorations of infinity, architecture, visual paradoxes and tessellations.
Some painters and sculptors create work distorted with the mathematical principles of anamorphosis, including South African sculptor Jonty Hurwitz.
British constructionist artist John Ernest created reliefs and paintings inspired by group theory. A number of other British artists of the constructionist and systems schools of thought also draw on mathematics models and structures as a source of inspiration, including Anthony Hill and Peter Lowe. Computer-generated art is based on mathematical algorithms.


== Quotes by mathematicians ==
Bertrand Russell expressed his sense of mathematical beauty in these words:

Mathematics, rightly viewed, possesses not only truth, but supreme beauty—a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. The true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as poetry.

Paul Erdős expressed his views on the ineffability of mathematics when he said, ""Why are numbers beautiful? It's like asking why is Beethoven's Ninth Symphony beautiful. If you don't see why, someone can't tell you. I know numbers are beautiful. If they aren't beautiful, nothing is"".


== See also ==


== Notes ==


== References ==


== Further reading ==
Cellucci, Carlo (2015), ""Mathematical beauty, understanding, and discovery"", Foundations of Science, 20 (4): 339–355, doi:10.1007/s10699-014-9378-7, S2CID 120068870
Martin Gardner (April 1, 2007). ""Is Beauty Truth and Truth Beauty?"". Scientific American.
Stewart, Ian (2007). Why beauty is truth : a history of symmetry. New York: Basic Books, a member of the Perseus Books Group. ISBN 978-0-465-08236-0. OCLC 76481488.
Zeki, S.; Romaya, J. P.; Benincasa, D. M. T.; Atiyah, M. F. (2014), ""The experience of mathematical beauty and its neural correlates"", Frontiers in Human Neuroscience, 8: 68, doi:10.3389/fnhum.2014.00068, PMC 3923150, PMID 24592230


== External links ==
Mathematics, Poetry and Beauty
Is Mathematics Beautiful? cut-the-knot.org
Justin Mullins.com
Edna St. Vincent Millay (poet): Euclid alone has looked on beauty bare
Terence Tao, What is good mathematics?
Mathbeauty Blog
The Aesthetic Appeal collection at the Internet Archive
A Mathematical Romance Jim Holt December 5, 2013 issue of The New York Review of Books review of Love and Math: The Heart of Hidden Reality by Edward Frenkel"
81c561fb0e,Mathematical physics,"Mathematical physics refers to the development of mathematical methods for application to problems in physics. The Journal of Mathematical Physics defines the field as ""the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories"". An alternative definition would also include those mathematics that are inspired by physics (also known as physical mathematics).


== Scope ==
There are several distinct branches of mathematical physics, and these roughly correspond to particular historical periods.


=== Classical mechanics ===

The rigorous, abstract and advanced reformulation of Newtonian mechanics adopting the Lagrangian mechanics and the Hamiltonian mechanics even in the presence of constraints. Both formulations are embodied in analytical mechanics and lead to understanding the deep interplay of the notions of symmetry and conserved quantities during the dynamical evolution, as embodied within the most elementary formulation of Noether's theorem. These approaches and ideas have been extended to other areas of physics as statistical mechanics, continuum mechanics, classical field theory and quantum field theory. Moreover, they have provided several examples and ideas in differential geometry (e.g. several notions in symplectic geometry and vector bundle).


=== Partial differential equations ===

Following mathematics: the theory of partial differential equation, variational calculus, Fourier analysis, potential theory, and vector analysis are perhaps most closely associated with mathematical physics. These were developed intensively from the second half of the 18th century (by, for example, D'Alembert, Euler, and Lagrange) until the 1930s. Physical applications of these developments include hydrodynamics, celestial mechanics, continuum mechanics, elasticity theory, acoustics, thermodynamics, electricity, magnetism, and aerodynamics.


=== Quantum theory ===

The theory of atomic spectra (and, later, quantum mechanics) developed almost concurrently with some parts of the mathematical fields of linear algebra, the spectral theory of operators, operator algebras and more broadly, functional analysis. Nonrelativistic quantum mechanics includes Schrödinger operators, and it has connections to atomic and molecular physics. Quantum information theory is another subspecialty.


=== Relativity and quantum relativistic theories ===

The special and general theories of relativity require a rather different type of mathematics. This was group theory, which played an important role in both quantum field theory and differential geometry. This was, however, gradually supplemented by topology and functional analysis  in the mathematical description of cosmological as well as quantum field theory phenomena. In the mathematical description of these physical areas, some concepts in homological algebra and category theory are also important.


=== Statistical mechanics ===

Statistical mechanics forms a separate field, which includes the theory of phase transitions. It relies upon the Hamiltonian mechanics (or its quantum version) and it is closely related with the more mathematical ergodic theory and some parts of probability theory. There are increasing interactions between combinatorics and physics, in particular statistical physics.


== Usage ==
 
The usage of the term ""mathematical physics"" is sometimes idiosyncratic. Certain parts of mathematics that initially arose from the development of physics are not, in fact, considered parts of mathematical physics, while other closely related fields are. For example, ordinary differential equations and symplectic geometry are generally viewed as purely mathematical disciplines, whereas dynamical systems and Hamiltonian mechanics belong to mathematical physics. John Herapath used the term for the title of his 1847 text on ""mathematical principles of natural philosophy""; the scope at that time being 
""the causes of heat, gaseous elasticity, gravitation, and other great phenomena of nature"".


=== Mathematical vs. theoretical physics ===
The term ""mathematical physics"" is sometimes used to denote research aimed at studying and solving problems in physics or thought experiments within a mathematically rigorous framework. In this sense, mathematical physics covers a very broad academic realm distinguished only by the blending of some mathematical aspect and physics theoretical aspect. Although related to theoretical physics, mathematical physics in this sense emphasizes the mathematical rigour of the similar type as found in mathematics.
On the other hand, theoretical physics emphasizes the links to observations and experimental physics, which often requires theoretical physicists (and mathematical physicists in the more general sense) to use heuristic, intuitive, or approximate arguments. Such arguments are not considered rigorous by mathematicians.
Such mathematical physicists primarily expand and elucidate physical theories. Because of the required level of mathematical rigour, these researchers often deal with questions that theoretical physicists have considered to be already solved. However, they can sometimes show that the previous solution was incomplete, incorrect, or simply too naïve. Issues about attempts to infer the second law of thermodynamics from statistical mechanics are examples. Other examples concern the subtleties involved with synchronisation procedures in special and general relativity (Sagnac effect and Einstein synchronisation).
The effort to put physical theories on a mathematically rigorous footing not only developed physics but also has influenced developments of some mathematical areas. For example, the development of quantum mechanics and some aspects of functional analysis parallel each other in many ways. The mathematical study of quantum mechanics, quantum field theory, and quantum statistical mechanics has motivated results in operator algebras. The attempt to construct a rigorous mathematical formulation of quantum field theory has also brought about some progress in fields such as representation theory.


== Prominent mathematical physicists ==


=== Before Newton ===
There is a tradition of mathematical analysis of nature that goes back to the ancient Greeks; examples include Euclid (Optics), Archimedes (On the Equilibrium of Planes, On Floating Bodies), and Ptolemy (Optics, Harmonics). Later, Islamic and Byzantine scholars built on these works, and these ultimately were reintroduced or became available to the West in the 12th century and during the Renaissance.
In the first decade of the 16th century, amateur astronomer Nicolaus Copernicus proposed heliocentrism, and published a treatise on it in 1543. He retained the Ptolemaic idea of epicycles, and merely sought to simplify astronomy by constructing simpler sets of epicyclic orbits. Epicycles consist of circles upon circles. According to Aristotelian physics, the circle was the perfect form of motion, and was the intrinsic motion of Aristotle's fifth element—the quintessence or universal essence known in Greek as aether for the English pure air—that was the pure substance beyond the sublunary sphere, and thus was celestial entities' pure composition. The German Johannes Kepler [1571–1630], Tycho Brahe's assistant, modified Copernican orbits to ellipses, formalized in the equations of Kepler's laws of planetary motion.
An enthusiastic atomist, Galileo Galilei in his 1623 book The Assayer asserted that the ""book of nature is written in mathematics"". His 1632 book, about his telescopic observations, supported heliocentrism. Having introduced experimentation, Galileo then refuted geocentric cosmology by refuting Aristotelian physics itself. Galileo's 1638 book Discourse on Two New Sciences established the law of equal free fall as well as the principles of inertial motion, founding the central concepts of what would become today's classical mechanics. By the Galilean law of inertia as well as the principle of Galilean invariance, also called Galilean relativity, for any object experiencing inertia, there is empirical justification for knowing only that it is at relative rest or relative motion—rest or motion with respect to another object.
René Descartes famously developed a complete system of heliocentric cosmology anchored on the principle of vortex motion, Cartesian physics, whose widespread acceptance brought the demise of Aristotelian physics. Descartes sought to formalize mathematical reasoning in science, and developed Cartesian coordinates for geometrically plotting locations in 3D space and marking their progressions along the flow of time.An older contemporary of Newton, Christiaan Huygens, was the first to idealize a physical problem by a set of parameters and the first to fully mathematize a mechanistic explanation of unobservable physical phenomena, and for these reasons Huygens is considered the first theoretical physicist and one of the founders of modern mathematical physics.


=== Newtonian and post Newtonian ===
In this era, important concepts in calculus such as the fundamental theorem of calculus (proved in 1668 by Scottish mathematician James Gregory) and finding extrema and minima of functions via differentiation using Fermat's theorem (by French mathematician Pierre de Fermat) were already known before Leibniz and Newton. Isaac Newton (1642–1727) developed some concepts in calculus (although Gottfried Wilhelm Leibniz developed similar concepts outside the context of physics) and Newton's method to solve problems in physics. He was extremely successful in his application of calculus to the theory of motion. Newton's theory of motion, shown in his Mathematical Principles of Natural Philosophy, published in 1687, modeled three Galilean laws of motion along with Newton's law of universal gravitation on a framework of absolute space—hypothesized by Newton as a physically real entity of Euclidean geometric structure extending infinitely in all directions—while presuming absolute time, supposedly justifying knowledge of absolute motion, the object's motion with respect to absolute space. The principle of Galilean invariance/relativity was merely implicit in Newton's theory of motion. Having ostensibly reduced the Keplerian celestial laws of motion as well as Galilean terrestrial laws of motion to a unifying force, Newton achieved great mathematical rigor, but with theoretical laxity.In the 18th century, the Swiss Daniel Bernoulli (1700–1782) made contributions to fluid dynamics, and vibrating strings. The Swiss Leonhard Euler (1707–1783) did special work in variational calculus, dynamics, fluid dynamics, and other areas. Also notable was the Italian-born Frenchman, Joseph-Louis Lagrange (1736–1813) for work in analytical mechanics: he formulated Lagrangian mechanics) and variational methods. A major contribution to the formulation of Analytical Dynamics called Hamiltonian dynamics was also made by the Irish physicist, astronomer and mathematician, William Rowan Hamilton (1805-1865). Hamiltonian dynamics had played an important role in the formulation of modern theories in physics, including field theory and quantum mechanics. The French mathematical physicist Joseph Fourier (1768 – 1830) introduced the notion of Fourier series to solve the heat equation, giving rise to a new approach to solving partial differential equations by means of integral transforms.
Into the early 19th century, following mathematicians in France, Germany and England had contributed to mathematical physics. The French Pierre-Simon Laplace (1749–1827) made paramount contributions to mathematical astronomy, potential theory. Siméon Denis Poisson (1781–1840) worked in analytical mechanics and potential theory. In Germany, Carl Friedrich Gauss (1777–1855) made key contributions to the theoretical foundations of electricity, magnetism, mechanics, and fluid dynamics. In England, George Green (1793-1841) published An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism in 1828, which in addition to its significant contributions to mathematics made early progress towards laying down the mathematical foundations of electricity and magnetism.
A couple of decades ahead of Newton's publication of a particle theory of light, the Dutch Christiaan Huygens (1629–1695) developed the wave theory of light, published in 1690. By 1804, Thomas Young's double-slit experiment revealed an interference pattern, as though light were a wave, and thus Huygens's wave theory of light, as well as Huygens's inference that light waves were vibrations of the luminiferous aether, was accepted. Jean-Augustin Fresnel modeled hypothetical behavior of the aether. The English physicist Michael Faraday introduced the theoretical concept of a field—not action at a distance. Mid-19th century, the Scottish James Clerk Maxwell (1831–1879) reduced electricity and magnetism to Maxwell's electromagnetic field theory, whittled down by others to the four Maxwell's equations. Initially, optics was found consequent of Maxwell's field. Later, radiation and then today's known electromagnetic spectrum were found also consequent of this electromagnetic field.
The English physicist Lord Rayleigh [1842–1919] worked on sound. The Irishmen William Rowan Hamilton (1805–1865), George Gabriel Stokes (1819–1903) and Lord Kelvin (1824–1907) produced several major works: Stokes was a leader in optics and fluid dynamics; Kelvin made substantial discoveries in thermodynamics; Hamilton did notable work on analytical mechanics, discovering a new and powerful approach nowadays known as Hamiltonian mechanics. Very relevant contributions to this approach are due to his German colleague mathematician Carl Gustav Jacobi (1804–1851) in particular referring to canonical transformations. The German Hermann von Helmholtz (1821–1894) made substantial contributions in the fields of electromagnetism, waves, fluids, and sound. In the United States, the pioneering work of Josiah Willard Gibbs (1839–1903) became the basis for statistical mechanics. Fundamental theoretical results in this area were achieved by the German Ludwig Boltzmann (1844-1906). Together, these individuals laid the foundations of electromagnetic theory, fluid dynamics, and statistical mechanics.


=== Relativistic ===
By the 1880s, there was a prominent paradox that an observer within Maxwell's electromagnetic field measured it at approximately constant speed, regardless of the observer's speed relative to other objects within the electromagnetic field. Thus, although the observer's speed was continually lost relative to the electromagnetic field, it was preserved relative to other objects in the electromagnetic field. And yet no violation of Galilean invariance within physical interactions among objects was detected. As Maxwell's electromagnetic field was modeled as oscillations of the aether, physicists inferred that motion within the aether resulted in aether drift, shifting the electromagnetic field, explaining the observer's missing speed relative to it. The Galilean transformation had been the mathematical process used to translate the positions in one reference frame to predictions of positions in another reference frame, all plotted on Cartesian coordinates, but this process was replaced by Lorentz transformation, modeled by the Dutch Hendrik Lorentz [1853–1928].
In 1887, experimentalists Michelson and Morley failed to detect aether drift, however. It was hypothesized that motion into the aether prompted aether's shortening, too, as modeled in the Lorentz contraction. It was hypothesized that the aether thus kept Maxwell's electromagnetic field aligned with the principle of Galilean invariance across all inertial frames of reference, while Newton's theory of motion was spared.
Austrian theoretical physicist and philosopher Ernst Mach criticized Newton's postulated absolute space.  Mathematician Jules-Henri Poincaré (1854–1912) questioned even absolute time. In 1905, Pierre Duhem published a devastating criticism of the foundation of Newton's theory of motion. Also in 1905, Albert Einstein (1879–1955) published his special theory of relativity, newly explaining both the electromagnetic field's invariance and Galilean invariance by discarding all hypotheses concerning aether, including the existence of aether itself.  Refuting the framework of Newton's theory—absolute space and absolute time—special relativity refers to relative space and relative time, whereby length contracts and time dilates along the travel pathway of an object.
In 1908, Einstein's former mathematics professor Hermann Minkowski modeled 3D space together with the 1D axis of time by treating the temporal axis like a fourth spatial dimension—altogether 4D spacetime—and declared the imminent demise of the separation of space and time. Einstein initially called this ""superfluous learnedness"", but later used Minkowski spacetime with great elegance in his general theory of relativity, extending invariance to all reference frames—whether perceived as inertial or as accelerated—and credited this to Minkowski, by then deceased. General relativity replaces Cartesian coordinates with Gaussian coordinates, and replaces Newton's claimed empty yet Euclidean space traversed instantly by Newton's vector of hypothetical gravitational force—an instant action at a distance—with a gravitational field. The gravitational field is Minkowski spacetime itself, the 4D topology of Einstein aether modeled on a Lorentzian manifold that ""curves"" geometrically, according to the Riemann curvature tensor. The concept of Newton's gravity: ""two masses attract each other"" replaced by the geometrical argument: ""mass transform curvatures of spacetime and free falling particles with mass move along a geodesic curve in the spacetime"" (Riemannian geometry already existed before the 1850s, by mathematicians Carl Friedrich Gauss and Bernhard Riemann in search for intrinsic geometry and non-Euclidean geometry.), in the vicinity of either mass or energy. (Under special relativity—a special case of general relativity—even massless energy exerts gravitational effect by its mass equivalence locally ""curving"" the geometry of the four, unified dimensions of space and time.)


=== Quantum ===
Another revolutionary development of the 20th century was quantum theory, which emerged from the seminal contributions of Max Planck (1856–1947) (on black-body radiation) and Einstein's work on the photoelectric effect.  In 1912, a mathematician Henri Poincare published Sur la théorie des quanta. He introduced the first non-naïve definition of quantization in this paper. The development of early quantum physics followed by a heuristic framework devised by Arnold Sommerfeld (1868–1951) and Niels Bohr (1885–1962), but this was soon replaced by the quantum mechanics developed by Max Born (1882–1970), Werner Heisenberg (1901–1976), Paul Dirac (1902–1984), Erwin Schrödinger (1887–1961), Satyendra Nath Bose (1894–1974), and Wolfgang Pauli (1900–1958). This revolutionary theoretical framework is based on a probabilistic interpretation of states, and evolution and measurements in terms of self-adjoint operators on an infinite-dimensional vector space. That is called Hilbert space (introduced by mathematicians David Hilbert (1862–1943), Erhard Schmidt(1876-1959) and Frigyes Riesz (1880-1956) in search of generalization of Euclidean space and study of integral equations), and rigorously defined within the axiomatic modern version by John von Neumann in his celebrated book Mathematical Foundations of Quantum Mechanics, where he built up a relevant part of modern functional analysis on Hilbert spaces, the spectral theory (introduced by David Hilbert who investigated quadratic forms with infinitely many variables. Many years later, it had been revealed that his spectral theory is associated with the spectrum of the hydrogen atom. He was surprised by this application.) in particular. Paul Dirac used algebraic constructions to produce a relativistic model for the electron, predicting its magnetic moment and the existence of its antiparticle, the positron.


=== List of prominent contributors to mathematical physics in the 20th century ===
Prominent contributors to the 20th century's mathematical physics include, (ordered by birth date) William Thomson (Lord Kelvin) [1824–1907], Oliver Heaviside [1850–1925], Jules Henri Poincaré [1854–1912] , David Hilbert [1862–1943], Arnold Sommerfeld [1868–1951], Constantin Carathéodory [1873–1950], Albert Einstein [1879–1955], Max Born [1882–1970], George David Birkhoff [1884-1944], Hermann Weyl [1885–1955], Satyendra Nath Bose [1894-1974], Norbert Wiener [1894–1964], John Lighton Synge [1897–1995], Wolfgang Pauli [1900–1958], Paul Dirac [1902–1984], Eugene Wigner [1902–1995], Andrey Kolmogorov [1903-1987], Lars Onsager [1903-1976], John von Neumann [1903–1957], Sin-Itiro Tomonaga [1906–1979], Hideki Yukawa [1907–1981], Nikolay Nikolayevich Bogolyubov [1909–1992], Subrahmanyan Chandrasekhar [1910-1995], Mark Kac [1914–1984], Julian Schwinger [1918–1994], Richard Phillips Feynman [1918–1988], Irving Ezra Segal [1918–1998], Ryogo Kubo [1920–1995], Arthur Strong Wightman [1922–2013], Chen-Ning Yang [1922– ], Rudolf Haag [1922–2016], Freeman John Dyson [1923–2020], Martin Gutzwiller [1925–2014], Abdus Salam [1926–1996], Jürgen Moser [1928–1999], Michael Francis Atiyah [1929–2019], Joel Louis Lebowitz [1930– ], Roger Penrose [1931– ], Elliott Hershel Lieb [1932– ], Yakir Aharonov [1932– ], Sheldon Glashow [1932– ], Steven Weinberg [1933–2021], Ludvig Dmitrievich Faddeev [1934–2017], David Ruelle [1935– ], Yakov Grigorevich Sinai [1935– ], Vladimir Igorevich Arnold [1937–2010], Arthur Michael Jaffe [1937–], Roman Wladimir Jackiw [1939– ], Leonard Susskind [1940– ], Rodney James Baxter [1940– ], Michael Victor Berry [1941- ], Giovanni Gallavotti [1941- ], Stephen William Hawking [1942–2018], 
John Michael Kosterlitz [1943-], Jerrold Eldon Marsden [1942–2010], Michael C. Reed [1942– ], Israel Michael Sigal [1945– ], Alexander Markovich Polyakov [1945– ], Barry Simon [1946– ], Herbert Spohn [1946– ], John Lawrence Cardy [1947– ], Giorgio Parisi [1948– ], Edward Witten [1951– ], 
 F. Duncan Haldane [1951-], Ashoke Sen [1956-] and Juan Martín Maldacena [1968– ].


== See also ==
International Association of Mathematical Physics
Notable publications in mathematical physics
List of mathematical physics journals
Gauge theory (mathematics)
Relationship between mathematics and physics
Theoretical, computational and philosophical physics


== Notes ==


== References ==
Zaslow, Eric (2005), Physmatics, arXiv:physics/0506153, Bibcode:2005physics...6153Z


== Further reading ==


=== Generic works ===
Allen, Jont (2020), An Invitation to Mathematical Physics and its History, Springer, ISBN 978-3-030-53758-6
Courant, Richard; Hilbert, David (1989), Methods of Mathematical Physics, Vol 1–2, Interscience Publishers
Françoise, Jean P.; Naber, Gregory L.; Tsun, Tsou S. (2006), Encyclopedia of Mathematical Physics, Elsevier, ISBN 978-0-1251-2660-1
Joos, Georg; Freeman, Ira M. (1987), Theoretical Physics (3rd ed.), Dover Publications, ISBN 0-486-65227-0
Kato, Tosio (1995), Perturbation Theory for Linear Operators (2nd ed.), Springer-Verlag, ISBN 3-540-58661-X
Margenau, Henry; Murphy, George M. (2009), The Mathematics of Physics and Chemistry (2nd ed.), Young Press, ISBN 978-1444627473
Masani, Pesi R. (1976–1986), Norbert Wiener: Collected Works with Commentaries, Vol 1–4, The MIT Press
Morse, Philip M.; Feshbach, Herman (1999), Methods of Theoretical Physics, Vol 1–2, McGraw Hill, ISBN 0-07-043316-X
Thirring, Walter E. (1978–1983), A Course in Mathematical Physics, Vol 1–4, Springer-Verlag
Tikhomirov, Vladimir M. (1991–1993), Selected Works of  A. N. Kolmogorov, Vol 1–3, Kluwer Academic Publishers
Titchmarsh, Edward C. (1985), The Theory of Functions (2nd ed.), Oxford University Press


=== Textbooks for undergraduate studies ===
Arfken, George B.; Weber, Hans J.; Harris, Frank E. (2013), Mathematical Methods for Physicists: A Comprehensive Guide (7th ed.), Academic Press, ISBN 978-0-12-384654-9, (Mathematical Methods for Physicists, Solutions for Mathematical Methods for Physicists (7th ed.), archive.org)
Bayın, Selçuk Ş. (2018), Mathematical Methods in Science and Engineering (2nd ed.), Wiley, ISBN 9781119425397
Boas, Mary L. (2006), Mathematical Methods in the Physical Sciences (3rd ed.), Wiley, ISBN 978-0-471-19826-0
Butkov, Eugene (1968), Mathematical Physics, Addison-Wesley
Hassani, Sadri (2009), Mathematical Methods for Students of Physics and Related Fields, (2nd ed.), New York, Springer, eISBN 978-0-387-09504-2
Jeffreys, Harold; Swirles Jeffreys, Bertha (1956), Methods of Mathematical Physics (3rd ed.), Cambridge University Press
Marsh, Adam (2018), Mathematics for Physics: An Illustrated Handbook, World Scientific, ISBN 978-981-3233-91-1
Mathews, Jon; Walker, Robert L. (1970), Mathematical Methods of Physics (2nd ed.), W. A. Benjamin, ISBN 0-8053-7002-1
Menzel, Donald H. (1961), Mathematical Physics, Dover Publications, ISBN 0-486-60056-4
Riley, Ken F.; Hobson, Michael P.; Bence, Stephen J. (2006), Mathematical Methods for Physics and Engineering (3rd ed.), Cambridge University Press, ISBN 978-0-521-86153-3
Stakgold, Ivar (2000), Boundary Value Problems of Mathematical Physics, Vol 1-2., Society for Industrial and Applied Mathematics, ISBN 0-89871-456-7
Starkovich, Steven P. (2021), The Structures of Mathematical Physics: An Introduction, Springer, ISBN 978-3-030-73448-0


=== Textbooks for graduate studies ===
Blanchard, Philippe; Brüning, Erwin (2015), Mathematical Methods in Physics: Distributions, Hilbert Space Operators, Variational Methods, and Applications in Quantum Physics (2nd ed.), Springer, ISBN 978-3-319-14044-5
Cahill, Kevin (2019), Physical Mathematics (2nd ed.), Cambridge University Press, ISBN 978-1-108-47003-2
Geroch, Robert (1985), Mathematical Physics, University of Chicago Press, ISBN 0-226-28862-5
Hassani, Sadri (2013), Mathematical Physics: A Modern Introduction to its Foundations (2nd ed.), Springer-Verlag, ISBN 978-3-319-01194-3
Marathe, Kishore (2010), Topics in Physical Mathematics, Springer-Verlag, ISBN 978-1-84882-938-1
Milstein, Grigori N.; Tretyakov, Michael V. (2021), Stochastic Numerics for Mathematical Physics (2nd ed.), Springer, ISBN 978-3-030-82039-8
Reed, Michael C.; Simon, Barry (1972–1981), Methods of Modern Mathematical Physics, Vol 1-4, Academic Press
Richtmyer, Robert D. (1978–1981), Principles of Advanced Mathematical Physics, Vol 1-2., Springer-Verlag
Rudolph, Gerd; Schmidt, Matthias (2013–2017), Differential Geometry and Mathematical Physics, Vol 1-2, Springer
Serov, Valery (2017), Fourier Series, Fourier Transform and Their Applications to Mathematical Physics, Springer, ISBN 978-3-319-65261-0
Simon, Barry (2015), A Comprehensive Course in Analysis, Vol 1-5, American Mathematical Society
Stakgold, Ivar; Holst, Michael (2011), Green's Functions and Boundary Value Problems (3rd ed.), Wiley, ISBN 978-0-470-60970-5
Stone, Michael; Goldbart, Paul (2009), Mathematics for Physics: A Guided Tour for Graduate Students, Cambridge University Press, ISBN 978-0-521-85403-0
Szekeres, Peter (2004), A Course in Modern Mathematical Physics: Groups, Hilbert Space and Differential Geometry, Cambridge University Press, ISBN 978-0-521-53645-5
Taylor, Michael E. (2011), Partial Differential Equations, Vol 1-3 (2nd ed.), Springer.
Whittaker, Edmund T.; Watson, George N. (1950), A Course of Modern Analysis: An Introduction to the General Theory of Infinite Processes and of Analytic Functions, with an Account of the Principal Transcendental Functions (4th ed.), Cambridge University Press


=== Specialized texts in classical physics ===
Abraham, Ralph; Marsden, Jerrold E. (2008), Foundations of Mechanics: A Mathematical Exposition of Classical Mechanics with an Introduction to the Qualitative Theory of Dynamical Systems (2nd ed.), AMS Chelsea Publishing, ISBN 978-0-8218-4438-0
Adam, John A. (2017), Rays, Waves, and Scattering: Topics in Classical Mathematical Physics, Princeton University Press., ISBN 978-0-691-14837-3
Arnold, Vladimir I. (1997), Mathematical Methods of Classical Mechanics (2nd ed.), Springer-Verlag, ISBN 0-387-96890-3
Bloom, Frederick (1993), Mathematical Problems of Classical Nonlinear Electromagnetic Theory, CRC Press, ISBN 0-582-21021-6
Boyer, Franck; Fabrie, Pierre (2013), Mathematical Tools for the Study of the Incompressible Navier-Stokes Equations and Related Models, Springer, ISBN 978-1-4614-5974-3
Colton, David; Kress, Rainer (2013), Integral Equation Methods in Scattering Theory, Society for Industrial and Applied Mathematics, ISBN 978-1-611973-15-0
Ciarlet, Philippe G. (1988–2000), Mathematical Elasticity, Vol 1–3, Elsevier
Galdi, Giovanni P. (2011), An Introduction to the Mathematical Theory of the Navier-Stokes Equations: Steady-State Problems (2nd ed.), Springer, ISBN 978-0-387-09619-3
Hanson, George W.; Yakovlev, Alexander B. (2002), Operator Theory for Electromagnetics: An Introduction, Springer, ISBN 978-1-4419-2934-1
Kirsch, Andreas; Hettlich, Frank (2015), The Mathematical Theory of Time-Harmonic Maxwell's Equations: Expansion-, Integral-, and Variational Methods, Springer, ISBN 978-3-319-11085-1
Knauf, Andreas (2018), Mathematical Physics: Classical Mechanics, Springer, ISBN 978-3-662-55772-3
Lechner, Kurt (2018), Classical Electrodynamics: A Modern Perspective, Springer, ISBN 978-3-319-91808-2
Marsden, Jerrold E.; Ratiu, Tudor S. (1999), Introduction to Mechanics and Symmetry: A Basic Exposition of Classical Mechanical Systems (2nd ed.), Springer, ISBN 978-1-4419-3143-6
Müller, Claus (1969), Foundations of the Mathematical Theory of Electromagnetic Waves, Springer-Verlag, ISBN 978-3-662-11775-0
Ramm, Alexander G. (2018), Scattering by Obstacles and Potentials, World Scientific, ISBN 9789813220966
Roach, Gary F.; Stratis, Ioannis G.; Yannacopoulos, Athanasios N. (2012), Mathematical Analysis of Deterministic and Stochastic Problems in Complex Media Electromagnetics, Princeton University Press, ISBN 978-0-691-14217-3


=== Specialized texts in modern physics ===
Baez, John C.; Muniain, Javier P. (1994), Gauge Fields, Knots, and Gravity, World Scientific, ISBN 981-02-2034-0
Blank, Jiří; Exner, Pavel; Havlíček, Miloslav (2008), Hilbert Space Operators in Quantum Physics (2nd ed.), Springer, ISBN 978-1-4020-8869-8
Engel, Eberhard; Dreizler, Reiner M. (2011), Density Functional Theory: An Advanced Course, Springer-Verlag, ISBN 978-3-642-14089-1
Glimm, James; Jaffe, Arthur (1987), Quantum Physics: A Functional Integral Point of View (2nd ed.), Springer-Verlag, ISBN 0-387-96477-0
Haag, Rudolf (1996), Local Quantum Physics: Fields, Particles, Algebras (2nd ed.), Springer-Verlag, ISBN 3-540-61049-9
Hall, Brian C. (2013), Quantum Theory for Mathematicians, Springer, ISBN 978-1-4614-7115-8
Hamilton, Mark J. D. (2017), Mathematical Gauge Theory: With Applications to the Standard Model of Particle Physics, Springer, ISBN 978-3-319-68438-3
Hawking, Stephen W.; Ellis, George F. R. (1973), The Large Scale Structure of Space-Time, Cambridge University Press, ISBN 0-521-20016-4
Jackiw, Roman (1995), Diverse Topics in Theoretical and Mathematical Physics, World Scientific, ISBN 9810216963
Landsman, Klaas (2017), Foundations of Quantum Theory: From Classical Concepts to Operator Algebras, Springer, ISBN 978-3-319-51776-6
Moretti, Valter (2018), Spectral Theory and Quantum Mechanics: Mathematical Foundations of Quantum Theories, Symmetries and Introduction to the Algebraic Formulation (2nd ed.), Springer, ISBN 978-3-319-70705-1
Robert, Didier; Combescure, Monique (2021), Coherent States and Applications in Mathematical Physics (2nd ed.), Springer, ISBN 978-3-030-70844-3
Tasaki, Hal (2020), Physics and mathematics of quantum many-body systems, Springer, ISBN 978-3-030-41265-4, OCLC 1154567924
Teschl, Gerald (2009), Mathematical Methods in Quantum Mechanics: With Applications to Schrödinger Operators, American Mathematical Society, ISBN 978-0-8218-4660-5
Thirring, Walter E. (2002), Quantum Mathematical Physics: Atoms, Molecules and Large Systems (2nd ed.), Springer-Verlag, ISBN 978-3-642-07711-1
von Neumann, John (2018), Mathematical Foundations of Quantum Mechanics, Princeton University Press, ISBN 978-0-691-17856-1
Weyl, Hermann (2014), The Theory of Groups and Quantum Mechanics, Martino Fine Books, ISBN 978-1614275800
Ynduráin, Francisco J. (2006), The Theory of Quark and Gluon Interactions (4th ed.), Springer, ISBN 978-3642069741
Zeidler, Eberhard (2006–2011), Quantum Field Theory: A Bridge Between Mathematicians and Physicists, Vol 1-3, Springer


== External links ==
 Media related to Mathematical physics at Wikimedia Commons"
78eb00d4f3,Transformation (function),"In mathematics, a transformation is a function f, usually with some geometrical underpinning, that maps a set X to itself, i.e. f : X → X.
Examples include linear transformations of vector spaces and geometric transformations, which include projective transformations, affine transformations, and specific affine transformations, such as rotations, reflections and translations.


== Partial transformations ==
While it is common to use the term transformation for any function of a set into itself (especially in terms like ""transformation semigroup"" and similar), there exists an alternative form of terminological convention in which the term ""transformation"" is reserved only for bijections. When such a narrow notion of transformation is generalized to partial functions, then a partial transformation is a function f: A → B, where both A and B are subsets of some set X.


== Algebraic structures ==
The set of all transformations on a given base set, together with function composition, forms a regular semigroup.


== Combinatorics ==
For a finite set of cardinality n, there are nn transformations and (n+1)n partial transformations.


== See also ==
Coordinate transformation
Data transformation (statistics)
Geometric transformation
Infinitesimal transformation
Linear transformation
Rigid transformation
Transformation geometry
Transformation semigroup
Transformation group
Transformation matrix


== References ==


== External links ==
 Media related to Transformation (function) at Wikimedia Commons"
fae4d2ca7d,Disk (mathematics),"In geometry, a disk (also spelled disc) is the region in a plane bounded by a circle. A disk is said to be closed if it contains the circle that constitutes its boundary, and open if it does not.For a radius, 
  
    
      
        r
      
    
    {\displaystyle r}
  , an open disk is usually denoted as 
  
    
      
        
          D
          
            r
          
        
      
    
    {\displaystyle D_{r}}
   and a closed disk is 
  
    
      
        
          
            
              D
              
                r
              
            
            ¯
          
        
      
    
    {\displaystyle {\overline {D_{r}}}}
  . However in the field of topology the closed disk is usually denoted as 
  
    
      
        
          D
          
            2
          
        
      
    
    {\displaystyle D^{2}}
   while the open disk is 
  
    
      
        Int
        ⁡
        
          D
          
            2
          
        
      
    
    {\displaystyle \operatorname {Int} D^{2}}
  .


== Formulas ==
In Cartesian coordinates, the open disk of center 
  
    
      
        (
        a
        ,
        b
        )
      
    
    {\displaystyle (a,b)}
   and radius R is given by the formula

  
    
      
        D
        =
        {
        (
        x
        ,
        y
        )
        ∈
        
          
            
              R
            
            
              2
            
          
        
        :
        (
        x
        −
        a
        
          )
          
            2
          
        
        +
        (
        y
        −
        b
        
          )
          
            2
          
        
        <
        
          R
          
            2
          
        
        }
      
    
    {\displaystyle D=\{(x,y)\in {\mathbb {R} ^{2}}:(x-a)^{2}+(y-b)^{2}<R^{2}\}}
  while the closed disk of the same center and radius is given by 

  
    
      
        
          
            D
            ¯
          
        
        =
        {
        (
        x
        ,
        y
        )
        ∈
        
          
            
              R
            
            
              2
            
          
        
        :
        (
        x
        −
        a
        
          )
          
            2
          
        
        +
        (
        y
        −
        b
        
          )
          
            2
          
        
        ≤
        
          R
          
            2
          
        
        }
        .
      
    
    {\displaystyle {\overline {D}}=\{(x,y)\in {\mathbb {R} ^{2}}:(x-a)^{2}+(y-b)^{2}\leq R^{2}\}.}
  The area of a closed or open disk of radius R is πR2 (see area of a disk).


== Properties ==
The disk has circular symmetry.The open disk and the closed disk are not topologically equivalent (that is, they are not homeomorphic), as they have different topological properties from each other. For instance, every closed disk is compact whereas every open disk is not compact. However from the viewpoint of algebraic topology they share many properties: both of them are contractible and so are homotopy equivalent to a single point. This implies that their fundamental groups are trivial, and all homology groups are trivial except the 0th one, which is isomorphic to Z. The Euler characteristic of a point (and therefore also that of a closed or open disk) is 1.Every continuous map from the closed disk to itself has at least one fixed point (we don't require the map to be bijective or even surjective); this is the case n=2 of the Brouwer fixed point theorem. The statement is false for the open disk:Consider for example the function

  
    
      
        f
        (
        x
        ,
        y
        )
        =
        
          (
          
            
              
                
                  x
                  +
                  
                    
                      1
                      −
                      
                        y
                        
                          2
                        
                      
                    
                  
                
                2
              
            
            ,
            y
          
          )
        
      
    
    {\displaystyle f(x,y)=\left({\frac {x+{\sqrt {1-y^{2}}}}{2}},y\right)}
  
which maps every point of the open unit disk to another point on the open unit disk to the right of the given one. But for the closed unit disk it fixes every point on the half circle 
  
    
      
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        =
        1
        ,
        x
        >
        0.
      
    
    {\displaystyle x^{2}+y^{2}=1,x>0.}
  


== As a statistical distribution ==
A uniform distribution on a unit circular disk is occasionally encountered in statistics. It most commonly occurs in operations research in the mathematics of urban planning, where it may be used to model a population within a city. Other uses may take advantage of the fact that it is a distribution for which it is easy to compute the probability that a given set of linear inequalities will be satisfied. (Gaussian distributions in the plane require numerical quadrature.)
""An ingenious argument via elementary functions"" shows the mean Euclidean distance between two points in the disk to be 128/45π ≈ 0.90541, while direct integration in polar coordinates shows the mean squared distance to be 1.
If we are given an arbitrary location at a distance q from the center of the disk, it is also of interest to determine the average distance b(q) from points in the distribution to this location and the average square of such distances. The latter value can be computed directly as q2+1/2.


=== Average distance to an arbitrary internal point ===
To find b(q) we need to look separately at the cases in which the location is internal or external, i.e. in which q ≶ 1, and we find that in both cases the result can only be expressed in terms of complete elliptic integrals.
If we consider an internal location, our aim (looking at the diagram) is to compute the expected value of r under a distribution whose density is 1/π for 0 ≤ r ≤ s(θ), integrating in polar coordinates centered on the fixed location for which the area of a cell is r dr dθ ; hence

Here s(θ) can be found in terms of q and θ using the Law of cosines. The steps needed to evaluate the integral, together with several references, will be found in the paper by Lew et al.; the result is that

where K and E are complete elliptic integrals of the first and second kinds. b(0) = 2/3; b(1) = 32/9π ≈ 1.13177.


=== Average distance to an arbitrary external point ===
Turning to an external location, we can set up the integral in a similar way, this time obtaining

where the law of cosines tells us that s+(θ) and s–(θ) are the roots for s of the equation

Hence

We may substitute u = q sinθ  to get 

using standard integrals.Hence again b(1) = 32/9π, while also


== See also ==
Unit disk, a disk with radius one
Annulus (mathematics), the region between two concentric circles
Ball (mathematics), the usual term for the 3-dimensional analogue of a disk
Disk algebra, a space of functions on a disk
Disk segment
Orthocentroidal disk, containing certain centers of a triangle


== References =="
04a65d79e3,Informal mathematics,"Informal mathematics, also called naïve mathematics, has historically been the predominant form of mathematics at most times and in most cultures, and is the subject of modern ethno-cultural studies of mathematics. The philosopher Imre Lakatos in his Proofs and Refutations aimed to sharpen the formulation of informal mathematics, by reconstructing its role in nineteenth century mathematical debates and concept formation, opposing the predominant assumptions of mathematical formalism.  Informality may not discern between statements given by inductive reasoning (as in approximations which are deemed ""correct"" merely because they are useful), and statements derived by deductive reasoning.


== Terminology ==
Informal mathematics means any informal mathematical practices, as used in everyday life, or by aboriginal or ancient peoples, without historical or geographical limitation. Modern mathematics, exceptionally from that point of view, emphasizes formal and strict proofs of all statements from given axioms. This can usefully be called therefore formal mathematics. Informal practices are usually understood intuitively and justified with examples—there are no axioms. This is of direct interest in anthropology and psychology: it casts light on the perceptions and agreements of other cultures.  It is also of interest in developmental psychology as it reflects a naïve understanding of the relationships between numbers and things. Another term used for informal mathematics is folk mathematics, which is ambiguous; the mathematical folklore article is dedicated to the usage of that term among professional mathematicians.
The field of naïve physics is concerned with similar understandings of physics. People use mathematics and physics in everyday life, without really understanding (or caring) how mathematical and physical ideas were historically derived and justified.


== History ==
There has long been a standard account of the development of geometry in ancient Egypt, followed by Greek mathematics and the emergence of deductive logic. The modern sense of the term mathematics, as meaning only those systems justified with reference to axioms, is however an anachronism if read back into history. Several ancient societies built impressive mathematical systems and carried out complex calculations based on proofless heuristics and practical approaches. Mathematical facts were accepted on a pragmatic basis. Empirical methods, as in science, provided the justification for a given technique. Commerce, engineering, calendar creation and the prediction of eclipses and stellar progression were practiced by ancient cultures on at least three continents. N.C. Ghosh included informal mathematics in the list of Folk Mathematics.


== See also ==
Folk psychology
Mathematical Platonism
Pseudomathematics
Ethnomathematics
Numeracy


== References =="
19da92ea2f,Greek mathematics,"Greek mathematics refers to mathematics texts and ideas stemming from the Archaic through the Hellenistic and Roman periods, mostly attested from the late 7th century BC to the 6th century AD, around the shores of the Mediterranean. Greek mathematicians lived in cities spread over the entire region, from Anatolia to Italy and North Africa, but were united by Greek culture and the Greek language. The development of mathematics as a theoretical discipline and the use of proofs is an important difference between Greek mathematics and those of preceding civilizations.


== Origins and etymology ==
Greek mathēmatikē (""mathematics"") derives from the Ancient Greek: μάθημα, romanized: máthēma, Attic Greek: [má.tʰɛː.ma] Koine Greek: [ˈma.θi.ma], from the verb manthanein, ""to learn"". Strictly speaking, a máthēma could be any branch of learning, or anything learnt; however, since antiquity certain mathēmata (mainly arithmetic, geometry, astronomy, and harmonics) were granted special status.The origins of Greek mathematics are not well documented. The earliest advanced civilizations in Greece and Europe were the Minoan and later Mycenaean civilizations, both of which flourished during the 2nd millennium BC. While these civilizations possessed writing and were capable of advanced engineering, including four-story palaces with drainage and beehive tombs, they left behind no mathematical documents.
Though no direct evidence is available, it is generally thought that the neighboring Babylonian and Egyptian civilizations had an influence on the younger Greek tradition. Unlike the flourishing of Greek literature in the span of 800 to 600 BC, not much is known about Greek mathematics in this early period—nearly all of the information was passed down through later authors, beginning in the mid-4th century BC.


== Archaic and Classical periods ==

Greek mathematics allegedly began with Thales of Miletus (c. 624–548 BC). Very little is known about his life, although it is generally agreed that he was one of the Seven Wise Men of Greece. According to Proclus, he traveled to Babylon from where he learned mathematics and other subjects, coming up with the proof of what is now called Thales' Theorem.An equally enigmatic figure is Pythagoras of Samos (c. 580–500 BC), who supposedly visited Egypt and Babylon, and ultimately settled in Croton, Magna Graecia, where he started a kind of brotherhood. Pythagoreans supposedly believed that ""all is number"" and were keen in looking for mathematical relations between numbers and things. Pythagoras himself was given credit for many later discoveries, including the construction of the five regular solids. However, Aristotle refused to attribute anything specifically to Pythagoras and only discussed the work of the Pythagoreans as a group.Almost half of the material in Euclid's Elements is customarily attributed to the Pythagoreans, including the discovery of irrationals, attributed to Hippasus (c. 530–450 BC) and Theodorus (fl. 450 BC). The greatest mathematician associated with the group, however, may have been Archytas (c. 435-360 BC), who solved the problem of doubling the cube, identified the harmonic mean, and possibly contributed to optics and mechanics. Other mathematicians active in this period, not fully affiliated with any school, include Hippocrates of Chios (c. 470–410 BC), Theaetetus (c. 417–369 BC), and Eudoxus (c. 408–355 BC).
Greek mathematics also drew the attention of philosophers during the Classical period. Plato (c. 428–348 BC), the founder of the Platonic Academy, mentions mathematics in several of his dialogues. While not considered a mathematician, Plato seems to have been influenced by Pythagorean ideas about number and believed that the elements of matter could be broken down into geometric solids. He also believed that geometrical proportions bound the cosmos together rather than physical or mechanical forces. Aristotle (c. 384–322 BC), the founder of the Peripatetic school, often used mathematics to illustrate many of his theories, as when he used geometry in his theory of the rainbow and the theory of proportions in his analysis of motion. Much of the knowledge about ancient Greek mathematics in this period is thanks to records referenced by Aristotle in his own works.


== Hellenistic and Roman periods ==

The Hellenistic era began in the late 4th century BC, following Alexander the Great's conquest of the Eastern Mediterranean, Egypt, Mesopotamia, the Iranian plateau, Central Asia, and parts of India, leading to the spread of the Greek language and culture across these regions. Greek became the lingua franca of scholarship throughout the Hellenistic world, and the mathematics of the Classical period merged with Egyptian and Babylonian mathematics to give rise to Hellenistic mathematics.Greek mathematics and astronomy reached its acme during the Hellenistic and early Roman periods, and much of the work represented by authors such as Euclid (fl. 300 BC), Archimedes (c. 287–212 BC), Apollonius (c. 240–190 BC), Hipparchus (c. 190–120 BC), and Ptolemy (c. 100–170 AD) was of a very advanced level and rarely mastered outside a small circle. There is also evidence of combining mathematical knowledge with technical or practical applications, as found for instance in the work of Menelaus of Alexandria (c. 70–130 AD), who wrote a work dealing with the geometry of the sphere and its application to astronomical measurements and calculations (Spherica). Similar examples of applied mathematics include the construction of analogue computers like the Antikythera mechanism, the accurate measurement of the circumference of the Earth by Eratosthenes (276–194 BC), and the mechanical works of Hero (c. 10–70 AD).Several centers of learning appeared during the Hellenistic period, of which the most important one was the Mouseion in Alexandria, Egypt, which attracted scholars from across the Hellenistic world (mostly Greek, but also Egyptian, Jewish, Persian, among others). Although few in number, Hellenistic mathematicians actively communicated with each other; publication consisted of passing and copying someone's work among colleagues.Later mathematicians in the Roman era include Diophantus (c. 214–298 AD), who wrote on polygonal numbers and a work in pre-modern algebra (Arithmetica), Pappus of Alexandria (c. 290–350 AD), who compiled many important results in the Collection, Theon of Alexandria (c. 335–405 AD) and his daughter Hypatia (c. 370–415 AD), who edited Ptolemy's Almagest and other works, and Eutocius of Ascalon (c. 480–540 AD), who wrote commentaries on treatises by Archimedes and Apollonius. Although none of these mathematicians, save perhaps Diophantus, had notable original works, they are distinguished for their commentaries and expositions. These commentaries have preserved valuable extracts from works which have perished, or historical allusions which, in the absence of original documents, are precious because of their rarity.Most of the mathematical texts written in Greek survived through the copying of manuscripts over the centuries, though some fragments dating from antiquity have been found in Greece, Egypt, Asia Minor, Mesopotamia, and Sicily.


== Achievements ==
Greek mathematics constitutes an important period in the history of mathematics: fundamental in respect of geometry and for the idea of formal proof. Greek mathematicians also contributed to number theory, mathematical astronomy, combinatorics, mathematical physics, and, at times, approached ideas close to the integral calculus.Eudoxus of Cnidus developed a theory of proportion that bears resemblance to the modern theory of real numbers using the Dedekind cut, developed by Richard Dedekind, who acknowledged Eudoxus as inspiration.Euclid collected many previous results and theorems in the Elements, a canon of geometry and elementary number theory for many centuries.Archimedes made use of a technique dependent on a form of proof by contradiction to reach answers to problems with an arbitrary degree of accuracy, while specifying the limits within which the answers lay. Known as the method of exhaustion, Archimedes employed it in several of his works, including to approximate the value of π (Measurement of the Circle), and to prove that the area enclosed by a parabola and a straight line is 4/3 times the area of a triangle with equal base and height (Quadrature of the Parabola). Archimedes also showed that the number of grains of sand filling the universe was not uncountable, devising his own counting scheme based on the myriad, which denoted 10,000 (The Sand-Reckoner).The most characteristic product of Greek mathematics may be the theory of conic sections, which was largely developed in the Hellenistic period, starting with the work of Menaechmus and perfected primarily under Apollonius. The methods employed in these works made no explicit use of algebra, nor trigonometry, the latter appearing around the time of Hipparchus.Ancient Greek mathematics was not limited to theoretical works but was also used in other activities, such as business transactions and in land mensuration, as evidenced by extant texts where computational procedures and practical considerations took more of a central role.


== Transmission and the manuscript tradition ==

Although the earliest Greek language texts on mathematics that have been found were written after the Hellenistic period, many of these are considered to be copies of works written during and before the Hellenistic period. The two major sources are

Byzantine codices, written some 500 to 1500 years after their originals, and
Syriac or Arabic translations of Greek works and Latin translations of the Arabic versions.Nevertheless, despite the lack of original manuscripts, the dates of Greek mathematics are more certain than the dates of surviving Babylonian or Egyptian sources because a large number of overlapping chronologies exist. Even so, many dates are uncertain; but the doubt is a matter of decades rather than centuries.
Netz has counted 144 ancient authors in the mathematical or exact sciences, from whom only 29 works are extant in Greek: Aristarchus, Autolycus, Philo of Byzantium, Biton, Apollonius, Archimedes, Euclid, Theodosius, Hypsicles, Athenaeus, Geminus, Hero, Apollodorus, Theon of Smyrna, Cleomedes, Nicomachus, Ptolemy, Gaudentius, Anatolius, Aristides Quintilian, Porphyry, Diophantus, Alypius, Damianus, Pappus, Serenus, Theon of Alexandria, Anthemius, and Eutocius.The following works are extant only in Arabic translations:
Apollonius, Conics books V to VII
Apollonius, De Rationis Sectione
Archimedes, Book of Lemmas
Archimedes, Construction of the Regular Heptagon
Diocles, On Burning Mirrors
Diophantus, Arithmetica books IV to VII
Euclid, On Divisions of Figures
Euclid, On Weights
Hero, Catoptrica
Hero, Mechanica
Menelaus, Sphaerica
Pappus, Commentary on Euclid's Elements book X
Ptolemy, Optics (extant in Latin from an Arabic translation of the Greek)
Ptolemy, Planisphaerium


== See also ==
Al-Mansur – 2nd Abbasid caliph (r. 754–775)
Chronology of ancient Greek mathematicians
Greek numerals – System of writing numbers
History of geometry – Historical development of geometry
History of mathematics – Historical development of mathematics
Timeline of ancient Greek mathematicians – Timeline and summary of ancient Greek mathematicians and their discoveries


== Notes ==


== References ==
Boyer, Carl B. (1985), A History of Mathematics, Princeton University Press, ISBN 978-0-691-02391-5
Boyer, Carl B.; Merzbach, Uta C. (2011), A History of Mathematics (3rd ed.), John Wiley & Sons, Inc., ISBN 978-0-471-54397-8
Jean Christianidis, ed. (2004), Classics in the History of Greek Mathematics, Kluwer Academic Publishers, ISBN 978-1-4020-0081-2
Cooke, Roger (1997), The History of Mathematics: A Brief Course, Wiley-Interscience, ISBN 978-0-471-18082-1
Derbyshire, John (2006), Unknown Quantity: A Real And Imaginary History of Algebra, Joseph Henry Press, ISBN 978-0-309-09657-7
Stillwell, John (2004), Mathematics and its History (2nd ed.), Springer Science + Business Media Inc., ISBN 978-0-387-95336-6
Burton, David M. (1997), The History of Mathematics: An Introduction (3rd ed.), The McGraw-Hill Companies, Inc., ISBN 978-0-07-009465-9
Heath, Thomas Little (1981) [First published 1921], A History of Greek Mathematics, Dover publications, ISBN 978-0-486-24073-2
Heath, Thomas Little (2003) [First published 1931], A Manual of Greek Mathematics, Dover publications, ISBN 978-0-486-43231-1
Szabo, Arpad (1978) [First published 1978], The Beginnings of Greek Mathematics, Reidel & Akademiai Kiado, ISBN 978-963-05-1416-3


== External links ==

Vatican Exhibit
Famous Greek Mathematicians"
faec6a0bb0,Operation (mathematics),"In mathematics, an operation is a function which takes zero or more input values (also called ""operands"" or ""arguments"") to a well-defined output value. The number of operands is the arity of the operation.
The most commonly studied operations are binary operations (i.e., operations of arity 2), such as addition and multiplication, and unary operations (i.e., operations of arity 1), such as additive inverse and multiplicative inverse. An operation of arity zero, or nullary operation, is a constant. The mixed product is an example of an operation of arity 3, also called ternary operation.
Generally, the arity is taken to be finite. However, infinitary operations are sometimes considered, in which case the ""usual"" operations of finite arity are called finitary operations.
A partial operation is defined similarly to an operation, but with a partial function in place of a function.


== Types of operation ==

There are two common types of operations: unary and binary. Unary operations involve only one value, such as negation and trigonometric functions. Binary operations, on the other hand, take two values, and include addition, subtraction, multiplication, division, and exponentiation.Operations can involve mathematical objects other than numbers. The logical values true and false can be combined using logic operations, such as and, or, and not. Vectors can be added and subtracted. Rotations can be combined using the function composition operation, performing the first rotation and then the second. Operations on sets include the binary operations union and intersection and the unary operation of complementation. Operations on functions include composition and convolution.Operations may not be defined for every possible value of its domain. For example, in the real numbers one cannot divide by zero or take square roots of negative numbers. The values for which an operation is defined form a set called its domain of definition or active domain. The set which contains the values produced is called the codomain, but the set of actual values attained by the operation is its codomain of definition, active codomain, image or range. For example, in the real numbers, the squaring operation only produces non-negative numbers; the codomain is the set of real numbers, but the range is the non-negative numbers.
Operations can involve dissimilar objects: a vector can be multiplied by a scalar to form another vector (an operation known as scalar multiplication), and the inner product operation on two vectors produces a quantity that is scalar. An operation may or may not have certain properties, for example it may be associative, commutative, anticommutative, idempotent, and so on.
The values combined are called operands, arguments, or inputs, and the value produced is called the value, result, or output. Operations can have fewer or more than two inputs (including the case of zero input and infinitely many inputs).
An operator is similar to an operation in that it refers to the symbol or the process used to denote the operation, hence their point of view is different. For instance, one often speaks of ""the operation of addition"" or ""the addition operation"", when focusing on the operands and result, but one switches to ""addition operator"" (rarely ""operator of addition""), when focusing on the process, or from the more symbolic viewpoint, the function +: X × X → X.


== Definition ==
An n-ary operation ω from X1, …, Xn to Y is a function ω: X1 × … × Xn → Y. The set X1 × … × Xn is called the domain of the operation, the set Y is called the codomain of the operation, and the fixed non-negative integer n (the number of operands) is called the arity of the operation. Thus a unary operation has arity one, and a binary operation has arity two. An operation of arity zero, called a nullary operation, is simply an element of the codomain Y. An n-ary operation can also be viewed as an (n + 1)-ary relation that is total on its n input domains and unique on its output domain.
An n-ary partial operation ω from X1, …, Xn to Y is a partial function ω: X1 × … × Xn → Y. An n-ary partial operation can also be viewed as an (n + 1)-ary relation that is unique on its output domain.
The above describes what is usually called a finitary operation, referring to the finite number of operands (the value n). There are obvious extensions where the arity is taken to be an infinite ordinal or cardinal, or even an arbitrary set indexing the operands.
Often, the use of the term operation implies that the domain of the function includes a power of the codomain (i.e. the Cartesian product of one or more copies of the codomain), although this is by no means universal, as in the case of dot product, where vectors are multiplied and result in a scalar. An n-ary operation ω: Xn → X is called an internal operation. An n-ary operation ω: Xi × S × Xn − i − 1 → X where 0 ≤ i < n is called an external operation by the scalar set or operator set S. In particular for a binary operation, ω: S × X → X is called a left-external operation by S, and ω: X × S → X is called a right-external operation by S. An example of an internal operation is vector addition, where two vectors are added and result in a vector. An example of an external operation is scalar multiplication, where a vector is multiplied by a scalar and result in a vector.
An n-ary multifunction or multioperation ω is a mapping from a Cartesian power of a set into the set of subsets of that set, formally ω: Xn → P(X).


== See also ==
Finitary relation
Hyperoperation
Infix notation
Operator
Order of operations


== References =="
f67d2bf4bb,Mathematical proof,"A mathematical proof is an inferential argument for a mathematical statement, showing that the stated assumptions logically guarantee the conclusion. The argument may use other previously established statements, such as theorems; but every proof can, in principle, be constructed using only certain basic or original assumptions known as axioms, along with the accepted rules of inference. Proofs are examples of exhaustive deductive reasoning which establish logical certainty, to be distinguished from empirical arguments or non-exhaustive inductive reasoning which establish ""reasonable expectation"". Presenting many cases in which the statement holds is not enough for a proof, which must demonstrate that the statement is true in all possible cases. A proposition that has not been proved but is believed to be true is known as a conjecture, or a hypothesis if frequently used as an assumption for further mathematical work.
Proofs employ logic expressed in mathematical symbols, along with natural language which usually admits some ambiguity. In most mathematical literature, proofs are written in terms of rigorous informal logic. Purely formal proofs, written fully in symbolic language without the involvement of natural language, are considered in proof theory. The distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics, oral traditions in the mainstream mathematical community or in other cultures. The philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language.


== History and etymology ==

The word ""proof"" comes from the Latin probare (to test). Related modern words are English ""probe"", ""probation"", and ""probability"", Spanish probar (to smell or taste, or sometimes touch or test), Italian provare (to try), and German probieren (to try).  The legal term ""probity"" means authority or credibility, the power of testimony to prove facts when given by persons of reputation or status.Plausibility arguments using heuristic devices such as pictures and analogies preceded strict mathematical proof. It is likely that the idea of demonstrating a conclusion first arose in connection with geometry, which originated in practical problems of land measurement. The development of mathematical proof is primarily the product of ancient Greek mathematics, and one of its greatest achievements. Thales (624–546 BCE) and Hippocrates of Chios (c. 470–410 BCE) gave some of the first known proofs of theorems in geometry. Eudoxus (408–355 BCE) and Theaetetus (417–369 BCE) formulated theorems but did not prove them. Aristotle (384–322 BCE) said definitions should describe the concept being defined in terms of other concepts already known.
Mathematical proof was revolutionized by Euclid (300 BCE), who introduced the axiomatic method still in use today. It starts with undefined terms and axioms, propositions concerning the undefined terms which are assumed to be self-evidently true (from Greek ""axios"", something worthy). From this basis, the method proves theorems using deductive logic. Euclid's book, the Elements, was read by anyone who was considered educated in the West until the middle of the 20th century. In addition to theorems of geometry, such as the Pythagorean theorem, the Elements also covers number theory, including a proof that the square root of two is irrational and a proof that there are infinitely many prime numbers.
Further advances also took place in medieval Islamic mathematics. While earlier Greek proofs were largely geometric demonstrations, the development of arithmetic and algebra by Islamic mathematicians allowed more general proofs with no dependence on geometric intuition. In the 10th century CE, the Iraqi mathematician Al-Hashimi worked with numbers as such, called ""lines"" but not necessarily considered as measurements of geometric objects, to prove algebraic propositions concerning multiplication, division, etc., including the existence of irrational numbers. An inductive proof for arithmetic sequences was introduced in the Al-Fakhri (1000) by Al-Karaji, who used it to prove the binomial theorem and properties of Pascal's triangle. Alhazen also developed the method of proof by contradiction, as the first attempt at proving the Euclidean parallel postulate.Modern proof theory treats proofs as inductively defined data structures, not requiring an assumption that axioms are ""true"" in any sense. This allows parallel mathematical theories as formal models of a given intuitive concept, based on alternate sets of axioms, for example Axiomatic set theory and Non-Euclidean geometry.


== Nature and purpose ==
As practiced, a proof is expressed in natural language and is a rigorous argument intended to convince the audience of the truth of a statement. The standard of rigor is not absolute and has varied throughout history. A proof can be presented differently depending on the intended audience. In order to gain acceptance, a proof has to meet communal standards of rigor; an argument considered vague or incomplete may be rejected.
The concept of proof is formalized in the field of mathematical logic. A formal proof is written in a formal language instead of natural language. A formal proof is a sequence of formulas in a formal language, starting with an assumption, and with each subsequent formula a logical consequence of the preceding ones. This definition makes the concept of proof amenable to study. Indeed, the field of proof theory studies formal proofs and their properties, the most famous and surprising being that almost all axiomatic systems can generate certain undecidable statements not provable within the system.
The definition of a formal proof is intended to capture the concept of proofs as written in the practice of mathematics. The soundness of this definition amounts to the belief that a published proof can, in principle, be converted into a formal proof. However, outside the field of automated proof assistants, this is rarely done in practice. A classic question in philosophy asks whether mathematical proofs are analytic or synthetic. Kant, who introduced the analytic–synthetic distinction, believed mathematical proofs are synthetic, whereas Quine argued in his 1951 ""Two Dogmas of Empiricism"" that such a distinction is untenable.Proofs may be admired for their mathematical beauty. The mathematician Paul Erdős was known for describing proofs which he found to be particularly elegant as coming from ""The Book"", a hypothetical tome containing the most beautiful method(s) of proving each theorem. The book Proofs from THE BOOK, published in 2003, is devoted to presenting 32 proofs its editors find particularly pleasing.


== Methods of proof ==


=== Direct proof ===

In direct proof, the conclusion is established by logically combining the axioms, definitions, and earlier theorems. For example, direct proof can be used to prove that the sum of two even integers is always even:

Consider two even integers x and y. Since they are even, they can be written as x = 2a and y = 2b, respectively, for some integers a and b. Then the sum is x + y = 2a + 2b = 2(a+b). Therefore x+y has 2 as a factor and, by definition, is even. Hence, the sum of any two even integers is even.This proof uses the definition of even integers, the integer properties of closure under addition and multiplication, and the distributive property.


=== Proof by mathematical induction ===

Despite its name, mathematical induction is a method of deduction, not a form of inductive reasoning. In proof by mathematical induction, a single ""base case"" is proved, and an ""induction rule"" is proved that establishes that any arbitrary case implies the next case. Since in principle the induction rule can be applied repeatedly (starting from the proved base case), it follows that all (usually infinitely many) cases are provable. This avoids having to prove each case individually. A variant of mathematical induction is proof by infinite descent, which can be used, for example, to prove the irrationality of the square root of two.
A common application of proof by mathematical induction is to prove that a property known to hold for one number holds for all natural numbers:
Let N = {1, 2, 3, 4, ...} be the set of natural numbers, and let P(n) be a mathematical statement involving the natural number n belonging to N such that

(i) P(1) is true, i.e., P(n) is true for n = 1.
(ii) P(n+1) is true whenever P(n) is true, i.e., P(n) is true implies that P(n+1) is true.
Then P(n) is true for all natural numbers n.For example, we can prove by induction that all positive integers of the form 2n − 1 are odd.  Let P(n) represent ""2n − 1 is odd"":

(i) For n = 1, 2n − 1 = 2(1) − 1 = 1, and 1 is odd, since it leaves a remainder of 1 when divided by 2. Thus P(1) is true.
(ii) For any n, if 2n − 1 is odd (P(n)), then (2n − 1) + 2 must also be odd, because adding 2 to an odd number results in an odd number.  But (2n − 1) + 2 = 2n + 1 = 2(n+1) − 1, so 2(n+1) − 1 is odd (P(n+1)).  So P(n) implies P(n+1).
Thus 2n − 1 is odd, for all positive integers n.The shorter phrase ""proof by induction"" is often used instead of ""proof by mathematical induction"".


=== Proof by contraposition ===

Proof by contraposition infers the statement ""if p then q"" by establishing the logically equivalent contrapositive statement: ""if not q then not p"".
For example, contraposition can be used to establish that, given an integer 
  
    
      
        x
      
    
    {\displaystyle x}
  , if 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x^{2}}
   is even, then 
  
    
      
        x
      
    
    {\displaystyle x}
   is even:

Suppose 
  
    
      
        x
      
    
    {\displaystyle x}
   is not even. Then 
  
    
      
        x
      
    
    {\displaystyle x}
   is odd. The product of two odd numbers is odd, hence 
  
    
      
        
          x
          
            2
          
        
        =
        x
        ⋅
        x
      
    
    {\displaystyle x^{2}=x\cdot x}
   is odd. Thus 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x^{2}}
   is not even. Thus, if 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x^{2}}
   is even, the supposition must be false, so 
  
    
      
        x
      
    
    {\displaystyle x}
   has to be even.


=== Proof by contradiction ===

In proof by contradiction, also known by the Latin phrase reductio ad absurdum (by reduction to the absurd), it is shown that if some statement is assumed true, a logical contradiction occurs, hence the statement must be false.  A famous example involves the proof that 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   is an irrational number:

Suppose that 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   were a rational number. Then it could be written in lowest terms as 
  
    
      
        
          
            2
          
        
        =
        
          
            a
            b
          
        
      
    
    {\displaystyle {\sqrt {2}}={a \over b}}
   where a and b are non-zero integers with no common factor. Thus, 
  
    
      
        b
        
          
            2
          
        
        =
        a
      
    
    {\displaystyle b{\sqrt {2}}=a}
  . Squaring both sides yields 2b2 = a2. Since the expression on the left is an integer multiple of 2, the right expression is by definition divisible by 2. That is, a2 is even, which implies that a must also be even, as seen in the proposition above (in #Proof by contraposition). So we can write a = 2c, where c is also an integer. Substitution into the original equation yields 2b2 = (2c)2 = 4c2. Dividing both sides by 2 yields b2 = 2c2. But then, by the same argument as before, 2 divides b2, so b must be even. However, if a and b are both even, they have 2 as a common factor. This contradicts our previous statement that a and b have no common factor, so we must conclude that 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   is an irrational number.To paraphrase: if one could write 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   as a fraction, this fraction could never be written in lowest terms, since 2 could always be factored from numerator and denominator.


=== Proof by construction ===

Proof by construction, or proof by example, is the construction of a concrete example with a property to show that something having that property exists. Joseph Liouville, for instance, proved the existence of transcendental numbers by constructing an explicit example.  It can also be used to construct a counterexample to disprove a proposition that all elements have a certain property.


=== Proof by exhaustion ===

In proof by exhaustion, the conclusion is established by dividing it into a finite number of cases and proving each one separately. The number of cases sometimes can become very large. For example, the first proof of the four color theorem was a proof by exhaustion with 1,936 cases. This proof was controversial because the majority of the cases were checked by a computer program, not by hand. The shortest known proof of the four color theorem as of 2011 still has over 600 cases.


=== Probabilistic proof ===

A probabilistic proof is one in which an example is shown to exist, with certainty, by using methods of probability theory. Probabilistic proof, like proof by construction, is one of many ways to prove existence theorems.
In the probabilistic method, one seeks an object having a given property, starting with a large set of candidates. One assigns a certain probability for each candidate to be chosen, and then proves that there is a non-zero probability that a chosen candidate will have the desired property. This does not specify which candidates have the property, but the probability could not be positive without at least one.
A probabilistic proof is not to be confused with an argument that a theorem is 'probably' true, a 'plausibility argument'. The work on the Collatz conjecture shows how far plausibility is from genuine proof. While most mathematicians do not think that probabilistic evidence for the properties of a given object counts as a genuine mathematical proof, a few mathematicians and philosophers have argued that at least some types of probabilistic evidence (such as Rabin's probabilistic algorithm for testing primality) are as good as genuine mathematical proofs.


=== Combinatorial proof ===

A combinatorial proof establishes the equivalence of different expressions by showing that they count the same object in different ways. Often a bijection between two sets is used to show that the expressions for their two sizes are equal. Alternatively, a double counting argument provides two different expressions for the size of a single set, again showing that the two expressions are equal.


=== Nonconstructive proof ===

A nonconstructive proof establishes that a mathematical object with a certain property exists—without explaining how such an object can be found. Often, this takes the form of a proof by contradiction in which the nonexistence of the object is proved to be impossible. In contrast, a constructive proof establishes that a particular object exists by providing a method of finding it. The following famous example of a nonconstructive proof shows that there exist two irrational numbers a and b such that 
  
    
      
        
          a
          
            b
          
        
      
    
    {\displaystyle a^{b}}
   is a rational number. This proof uses that 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   is irrational (an easy proof is known since Euclid), but not that 
  
    
      
        
          
            
              2
            
          
          
            
              2
            
          
        
      
    
    {\displaystyle {\sqrt {2}}^{\sqrt {2}}}
   is irrational (this is true, but the proof is not elementary).

Either 
  
    
      
        
          
            
              2
            
          
          
            
              2
            
          
        
      
    
    {\displaystyle {\sqrt {2}}^{\sqrt {2}}}
   is a rational number and we are done (take 
  
    
      
        a
        =
        b
        =
        
          
            2
          
        
      
    
    {\displaystyle a=b={\sqrt {2}}}
  ), or 
  
    
      
        
          
            
              2
            
          
          
            
              2
            
          
        
      
    
    {\displaystyle {\sqrt {2}}^{\sqrt {2}}}
   is irrational so we can write 
  
    
      
        a
        =
        
          
            
              2
            
          
          
            
              2
            
          
        
      
    
    {\displaystyle a={\sqrt {2}}^{\sqrt {2}}}
   and 
  
    
      
        b
        =
        
          
            2
          
        
      
    
    {\displaystyle b={\sqrt {2}}}
  . This then gives 
  
    
      
        
          
            (
            
              
                
                  2
                
              
              
                
                  2
                
              
            
            )
          
          
            
              2
            
          
        
        =
        
          
            
              2
            
          
          
            2
          
        
        =
        2
      
    
    {\displaystyle \left({\sqrt {2}}^{\sqrt {2}}\right)^{\sqrt {2}}={\sqrt {2}}^{2}=2}
  , which is thus a rational number of the form 
  
    
      
        
          a
          
            b
          
        
        .
      
    
    {\displaystyle a^{b}.}
  


=== Statistical proofs in pure mathematics ===

The expression ""statistical proof"" may be used technically or colloquially in areas of pure mathematics, such as involving cryptography, chaotic series, and probabilistic number theory or analytic number theory. It is less commonly used to refer to a mathematical proof in the branch of mathematics known as mathematical statistics.  See also the ""Statistical proof using data"" section below.


=== Computer-assisted proofs ===

Until the twentieth century it was assumed that any proof could, in principle, be checked by a competent mathematician to confirm its validity. However, computers are now used both to prove theorems and to carry out calculations that are too long for any human or team of humans to check; the first proof of the four color theorem is an example of a computer-assisted proof. Some mathematicians are concerned that the possibility of an error in a computer program or a run-time error in its calculations calls the validity of such computer-assisted proofs into question. In practice, the chances of an error invalidating a computer-assisted proof can be reduced by incorporating redundancy and self-checks into calculations, and by developing multiple independent approaches and programs. Errors can never be completely ruled out in case of verification of a proof by humans either, especially if the proof contains natural language and requires deep mathematical insight to uncover the potential hidden assumptions and fallacies involved.


== Undecidable statements ==
A statement that is neither provable nor disprovable from a set of axioms is called undecidable (from those axioms). One example is the parallel postulate, which is neither provable nor refutable from the remaining axioms of Euclidean geometry.
Mathematicians have shown there are many statements that are neither provable nor disprovable in Zermelo–Fraenkel set theory with the axiom of choice (ZFC), the standard system of set theory in mathematics (assuming that ZFC is consistent); see List of statements undecidable in ZFC.
Gödel's (first) incompleteness theorem shows that many axiom systems of mathematical interest will have undecidable statements.


== Heuristic mathematics and experimental mathematics ==

While early mathematicians such as Eudoxus of Cnidus did not use proofs, from Euclid to the foundational mathematics developments of the late 19th and 20th centuries, proofs were an essential part of mathematics. With the increase in computing power in the 1960s, significant work began to be done investigating mathematical objects outside of the proof-theorem framework, in experimental mathematics. Early pioneers of these methods intended the work ultimately to be embedded in a classical proof-theorem framework, e.g. the early development of fractal geometry, which was ultimately so embedded.


== Related concepts ==


=== Visual proof ===
Although not a formal proof, a visual demonstration of a mathematical theorem is sometimes called a ""proof without words"". The left-hand picture below is an example of a historic visual proof of the Pythagorean theorem in the case of the (3,4,5) triangle.

		
		
Some illusory visual proofs, such as the missing square puzzle, can be constructed in a way which appear to prove a supposed mathematical fact but only do so under the presence of tiny errors (for example, supposedly straight lines which actually bend slightly) which are unnoticeable until the entire picture is closely examined, with lengths and angles precisely measured or calculated.


=== Elementary proof ===

An elementary proof is a proof which only uses basic techniques. More specifically, the term is used in number theory to refer to proofs that make no use of complex analysis. For some time it was thought that certain theorems, like the prime number theorem, could only be proved using ""higher"" mathematics. However, over time, many of these results have been reproved using only elementary techniques.


=== Two-column proof ===

A particular way of organising a proof using two parallel columns is often used as a mathematical exercise in elementary geometry classes in the United States. The proof is written as a series of lines in two columns. In each line, the left-hand column contains a proposition, while the right-hand column contains a brief explanation of how the corresponding proposition in the left-hand column is either an axiom, a hypothesis, or can be logically derived from previous propositions. The left-hand column is typically headed ""Statements"" and the right-hand column is typically headed ""Reasons"".


=== Colloquial use of ""mathematical proof"" ===
The expression ""mathematical proof"" is used by lay people to refer to using mathematical methods or arguing with mathematical objects, such as numbers, to demonstrate something about everyday life, or when data used in an argument is numerical. It is sometimes also used to mean a ""statistical proof"" (below), especially when used to argue from data.


=== Statistical proof using data ===

""Statistical proof"" from data refers to the application of statistics, data analysis, or Bayesian analysis to infer propositions regarding the probability of data. While using mathematical proof to establish theorems in statistics, it is usually not a mathematical proof in that the assumptions from which probability statements are derived require empirical evidence from outside mathematics to verify. In physics, in addition to statistical methods, ""statistical proof"" can refer to the specialized mathematical methods of physics applied to analyze data in a particle physics experiment or observational study in physical cosmology. ""Statistical proof"" may also refer to raw data or a convincing diagram involving data, such as scatter plots, when the data or diagram is adequately convincing without further analysis.


=== Inductive logic proofs and Bayesian analysis ===

Proofs using inductive logic, while considered mathematical in nature, seek to establish propositions with a degree of certainty, which acts in a similar manner to probability, and may be less than full certainty. Inductive logic should not be confused with mathematical induction.
Bayesian analysis uses Bayes' theorem to update a person's assessment of likelihoods of hypotheses when new evidence or information is acquired.


=== Proofs as mental objects ===

Psychologism views mathematical proofs as psychological or mental objects.  Mathematician philosophers, such as Leibniz, Frege, and Carnap have variously criticized this view and attempted to develop a semantics for what they considered to be the language of thought, whereby standards of mathematical proof might be applied to empirical science.


=== Influence of mathematical proof methods outside mathematics ===
Philosopher-mathematicians such as Spinoza have attempted to formulate philosophical arguments in an axiomatic manner, whereby mathematical proof standards could be applied to argumentation in general philosophy. Other mathematician-philosophers have tried to use standards of mathematical proof and reason, without empiricism, to arrive at statements outside of mathematics, but having the certainty of propositions deduced in a mathematical proof, such as Descartes' cogito argument.


== Ending a proof ==

Sometimes, the abbreviation ""Q.E.D."" is written to indicate the end of a proof. This abbreviation stands for ""quod erat demonstrandum"", which is Latin for ""that which was to be demonstrated"". A more common alternative is to use a square or a rectangle, such as □ or ∎, known as a ""tombstone"" or ""halmos"" after its eponym Paul Halmos. Often, ""which was to be shown"" is verbally stated when writing ""QED"", ""□"", or ""∎"" during an oral presentation. Unicode explicitly provides the ""end of proof"" character, U+220E (∎) (220E(hex) = 8718(dec)).


== See also ==


== References ==


== Further reading ==
Pólya, G. (1954), Mathematics and Plausible Reasoning, Princeton University Press, hdl:2027/mdp.39015008206248, ISBN 9780691080055.
Fallis, Don (2002), ""What Do Mathematicians Want? Probabilistic Proofs and the Epistemic Goals of Mathematicians"", Logique et Analyse, 45: 373–88.
Franklin, J.; Daoud, A. (2011), Proof in Mathematics: An Introduction, Kew Books, ISBN 978-0-646-54509-7.
Gold, Bonnie; Simons, Rogers A. (2008). Proof and Other Dilemmas: Mathematics and Philosophy. MAA.
Solow, D. (2004), How to Read and Do Proofs: An Introduction to Mathematical Thought Processes, Wiley, ISBN 978-0-471-68058-1.
Velleman, D. (2006), How to Prove It: A Structured Approach, Cambridge University Press, ISBN 978-0-521-67599-4.


== External links ==

 Media related to Mathematical proof at Wikimedia Commons
Proofs in Mathematics: Simple, Charming and Fallacious
A lesson about proofs, in a course from Wikiversity"
0593e464a8,Closure (mathematics),"In mathematics, a subset of a given set is closed under an operation of the larger set if performing that operation on members of the subset always produces a member of that subset. For example, the natural numbers are closed under addition, but not under subtraction: 1 − 2 is not a natural number, although both 1 and 2 are. 
Similarly, a subset is said to be closed under a collection of operations if it is closed under each of the operations individually.
The closure of a subset is the result of a closure operator applied to the subset. The closure of a subset under some operations is the smallest superset that is closed under these operations. It is often called the span (for example linear span) or the generated set.


== Definitions ==
Let S be a set equipped with one or several methods for producing elements of S from other elements of S.
A subset X of S is said to be closed under these methods, if, when all input elements are in X, then all possible results are also in X. Sometimes, one say also that X has the closure property.
The main property of closed sets, which results immediately from the definition, is that every intersection of closed sets is a closed set. It follows that for every subset Y of S, there is a smallest closed subset X of S such that 
  
    
      
        Y
        ⊆
        X
      
    
    {\displaystyle Y\subseteq X}
   (it is the intersection of all closed subsets that contain Y). Depending on the context, X is called the closure of Y or the set generated or spanned by Y.
The concepts of closed sets and closure are often extended to any property of subsets that are stable under intersection; that is, every intersection of subsets that have the property has also the property. For example, in 
  
    
      
        
          
            C
          
          
            n
          
        
        ,
      
    
    {\displaystyle \mathbb {C} ^{n},}
   a Zariski-closed set, also known as an algebraic set, is the set of the common zeros of a family of polynomials, and the Zariski closure of a set V of points is the smallest algebraic set that contains V.


=== In algebraic structures ===
An algebraic structure is a set equipped with operations that satisfy some axioms. These axioms may be identities. Some axioms may contain existential quantifiers 
  
    
      
        ∃
        ;
      
    
    {\displaystyle \exists ;}
   in this case it is worth to add some auxiliary operations in order that all axioms become identities or purely universally quantified formulas. See Algebraic structure for details.
In this context, given an algebraic structure S, a substructure of S is a subset that is closed under all operations of S, including the auxiliary operations that are needed for avoiding existential quantifiers. A substructure is an algebraic structure of the same type as S. It follows that, in a specific example, when closeness is proved, there is no need to check the axioms for proving that a substructure is a structure of the same type.
Given a subset X of an algebraic structure S, the closure of X is the smallest substructure of S that is closed under all operations of S. In the context of algebraic structures, this closure is generally called the substructure generated or spanned by X, and one says that X is a generating set of the substructure. 
For example, a group is a set with an associative operation, often called multiplication, with an identity element, such that every element has an inverse element. Here, the auxiliary operations are the nullary operation that results in the identity element and the unary operation of inversion. A subset of a group that is closed under multiplication and inversion is also closed under the nullary operation (that is, it contains the identity) if and only if it is non-empty. So, a non-empty subset of a group that is closed under multiplication and inversion is a group that is called a subgroup. The subgroup generated by a single element, that is, the closure of this element, is called a cyclic group.
In linear algebra, the closure of a non-empty subset of a vector space (under vector-space operations, that is, addition and scalar multiplication) is the linear span of this subset. It is a vector space by the preceding general result, and it can be proved easily that is the set of linear combinations of elements of the subset.
Similar examples can be given for almost every algebraic structures, with, sometimes some specific terminology. For example, in a commutative ring, the closure of a single element under ideal operations is called a principal ideal.


== In topology ==
In topology and related branches, the relevant operation is taking limits.  The topological closure of a set is the corresponding closure operator.  The Kuratowski closure axioms characterize this operator.


== Binary relations ==
A binary relation on a set A can be defined as a subset R of 
  
    
      
        A
        ×
        A
        ,
      
    
    {\displaystyle A\times A,}
   the set of the ordered pairs of elements of A. The notation 
  
    
      
        x
        R
        y
      
    
    {\displaystyle xRy}
   is commonly used for 
  
    
      
        (
        x
        ,
        y
        )
        ∈
        R
        .
      
    
    {\displaystyle (x,y)\in R.}
   Many properties or operations on relations can be used to define closures. Some of the most common ones follow:

Reflexivity
A relation R on the set A is reflexive if 
  
    
      
        (
        x
        ,
        x
        )
        ∈
        R
      
    
    {\displaystyle (x,x)\in R}
   for every 
  
    
      
        x
        ∈
        A
        .
      
    
    {\displaystyle x\in A.}
   As every intersection of reflexive relations is reflexive, this defines a closure. The reflexive closure of a relation R is thus 
Symmetry
Symmetry is the unary operation on 
  
    
      
        A
        ×
        A
      
    
    {\displaystyle A\times A}
   that maps 
  
    
      
        (
        x
        ,
        y
        )
      
    
    {\displaystyle (x,y)}
   to 
  
    
      
        (
        y
        ,
        x
        )
        .
      
    
    {\displaystyle (y,x).}
   A relation is symmetric if it is closed under this operation, and the symmetric closure of a relation R is its closure under this relation.
Transitivity
Transitivity is defined by the partial binary operation on 
  
    
      
        A
        ×
        A
      
    
    {\displaystyle A\times A}
   that maps 
  
    
      
        (
        x
        ,
        y
        )
      
    
    {\displaystyle (x,y)}
   and 
  
    
      
        (
        y
        ,
        z
        )
      
    
    {\displaystyle (y,z)}
   to 
  
    
      
        (
        x
        ,
        z
        )
        .
      
    
    {\displaystyle (x,z).}
   A relation is transitive if it is closed under this operation, and the transitive closure of a relation is its closure under this operation.A preorder is a relation that is reflective and transitive. It follows that the reflexive transitive closure of a relation is the smallest preorder containing it. Similarly, the reflexive transitive symmetric closure or equivalence closure of a relation is the smallest equivalence relation that contains it.


== Other examples ==
In matroid theory, the closure of X is the largest superset of X that has the same rank as X.
The transitive closure of a set.
The algebraic closure of a field.
The integral closure of an integral domain in a field that contains it.
The radical of an ideal in a commutative ring.
In geometry, the convex hull of a set S of points is the smallest convex set of which S is a subset.
In formal languages, the Kleene closure of a language can be described as the set of strings that can be made by concatenating zero or more strings from that language.
In group theory, the conjugate closure or normal closure of a set of group elements is the smallest normal subgroup containing the set.
In mathematical analysis and in probability theory, the closure of a collection of subsets of X under countably many set operations is called the σ-algebra generated by the collection.


== Closure operator ==

In the preceding sections, closures are considered for subsets of a given set. The subsets of a set form a partially ordered set (poset) for inclusion. Closure operators allow generalizing the concept of closure to any partially ordered set.
Given a poset S whose partial order is denoted with ≤, a closure operator on S is a function 
  
    
      
        C
        :
        S
        →
        S
      
    
    {\displaystyle C:S\to S}
   that is increasing (
  
    
      
        x
        ≤
        C
        (
        x
        )
      
    
    {\displaystyle x\leq C(x)}
   for all 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  ), idempotent (
  
    
      
        C
        (
        C
        (
        x
        )
        )
        =
        C
        (
        x
        )
      
    
    {\displaystyle C(C(x))=C(x)}
  ), and monotonic (
  
    
      
        x
        ≤
        y
        
        ⟹
        
        C
        (
        x
        )
        ≤
        C
        (
        y
        )
      
    
    {\displaystyle x\leq y\implies C(x)\leq C(y)}
  ).Equivalently, a function from S to S is a closure operator if 
  
    
      
        x
        ≤
        C
        (
        y
        )
        
        ⟺
        
        C
        (
        x
        )
        ≤
        C
        (
        y
        )
      
    
    {\displaystyle x\leq C(y)\iff C(x)\leq C(y)}
   for all 
  
    
      
        x
        ,
        y
        ∈
        S
        .
      
    
    {\displaystyle x,y\in S.}
  
An element of S is closed if it is its own closure, that is, if 
  
    
      
        x
        =
        C
        (
        x
        )
        .
      
    
    {\displaystyle x=C(x).}
   By idempotency, an element is closed if and only if it is the closure of some element of S.
An example of a closure operator that does not operate on subsets is given by the ceiling function, which maps every real number x to the smallest integer that is not smaller than x.


=== Closure operator vs. closed sets ===
A closure on the subsets of a given set may be defined either by a closure operator or by a set of closed sets that is stable under intersection and includes the given set. These two definitions are equivalent. 
Indeed, the defining properties of a closure operator C implies that an intersection of closed sets is closed: if 
  
    
      
        X
        =
        ⋂
        
          X
          
            i
          
        
      
    
    {\textstyle X=\bigcap X_{i}}
   is an intersection of closed sets, then 
  
    
      
        C
        (
        X
        )
      
    
    {\displaystyle C(X)}
   must contain X and be contained in every 
  
    
      
        
          X
          
            i
          
        
        .
      
    
    {\displaystyle X_{i}.}
   This implies 
  
    
      
        C
        (
        X
        )
        =
        X
      
    
    {\displaystyle C(X)=X}
   by definition of the intersection.
Conversely, if closed sets are given and every intersection of closed sets is closed, then one can define a closure operator C such that 
  
    
      
        C
        (
        X
        )
      
    
    {\displaystyle C(X)}
   is the intersection of the closed sets containing X.
This equivalence remains true for partially ordered sets with the greatest-lower-bound property, if one replace ""closed sets"" by ""closed elements"" and ""intersection"" by ""greatest lower bound"".


== Notes ==


== References ==

Weisstein, Eric W. ""Algebraic Closure"". MathWorld."
e137aef299,Music and mathematics,"Music theory analyzes the pitch, timing, and structure of music. It uses mathematics to study elements of music such as tempo, chord progression, form, and meter. The attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory.
While music theory has no axiomatic foundation in modern mathematics, the basis of musical sound can be described mathematically (using acoustics) and exhibits ""a remarkable array of number properties"".


== History ==
Though ancient Chinese, Indians, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound, the Pythagoreans (in particular Philolaus and Archytas) of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios, particularly the ratios of small integers. Their central doctrine was that ""all nature consists of harmony arising out of numbers"".From the time of Plato, harmony was considered a fundamental branch of physics, now known as musical acoustics. Early Indian and Chinese theorists show similar approaches: all sought to show that the mathematical laws of harmonics and rhythms were fundamental not only to our understanding of the world but to human well-being. Confucius, like Pythagoras, regarded the small numbers 1,2,3,4 as the source of all perfection.


== Time, rhythm, and meter ==
Without the boundaries of rhythmic structure – a fundamental equal and regular arrangement of pulse repetition, accent, phrase and duration – music would not be possible. Modern musical use of terms like meter and measure also reflects the historical importance of music, along with astronomy, in the development of counting, arithmetic and the exact measurement of time and periodicity that is fundamental to physics.The elements of musical form often build strict proportions or hypermetric structures (powers of the numbers 2 and 3).


== Musical form ==

Musical form is the plan by which a short piece of music is extended. The term ""plan"" is also used in architecture, to which musical form is often compared. Like the architect, the composer must take into account the function for which the work is intended and the means available, practicing economy and making use of repetition and order. The common types of form known as binary and ternary (""twofold"" and ""threefold"") once again demonstrate the importance of small integral values to the intelligibility and appeal of music.


== Frequency and harmony ==

A musical scale is a discrete set of pitches used in making or describing music. The most important scale in the Western tradition is the diatonic scale but many others have been used and proposed in various historical eras and parts of the world. Each pitch corresponds to a particular frequency, expressed in hertz (Hz), sometimes referred to as cycles per second (c.p.s.). A scale has an interval of repetition, normally the octave. The octave of any pitch refers to a frequency exactly twice that of the given pitch.
Succeeding superoctaves are pitches found at frequencies four, eight, sixteen times, and so on, of the fundamental frequency. Pitches at frequencies of half, a quarter, an eighth and so on of the fundamental are called suboctaves. There is no case in musical harmony where, if a given pitch be considered accordant, that its octaves are considered otherwise. Therefore, any note and its octaves will generally be found similarly named in musical systems (e.g. all will be called doh or A or Sa, as the case may be).
When expressed as a frequency bandwidth an octave A2–A3 spans from 110 Hz to 220 Hz (span=110 Hz). The next octave will span from 220 Hz to 440 Hz (span=220 Hz). The third octave spans from 440 Hz to 880 Hz (span=440 Hz) and so on. Each successive octave spans twice the frequency range of the previous octave.

Because we are often interested in the relations or ratios between the pitches (known as intervals) rather than the precise pitches themselves in describing a scale, it is usual to refer to all the scale pitches in terms of their ratio from a particular pitch, which is given the value of one (often written 1/1), generally a note which functions as the tonic of the scale. For interval size comparison, cents are often used.


== Tuning systems ==

There are two main families of tuning systems: equal temperament and just tuning. Equal temperament scales are built by dividing an octave into intervals which are equal on a logarithmic scale, which results in perfectly evenly divided scales, but with ratios of frequencies which are irrational numbers. Just scales are built by multiplying frequencies by rational numbers, which results in simple ratios between frequencies, but with scale divisions that are uneven.
One major difference between equal temperament tunings and just tunings is differences in acoustical beat when two notes are sounded together, which affects the subjective experience of consonance and dissonance. Both of these systems, and the vast majority of music in general, have scales that repeat on the interval of every octave, which is defined as frequency ratio of 2:1. In other words, every time the frequency is doubled, the given scale repeats.
Below are Ogg Vorbis files demonstrating the difference between just intonation and equal temperament. You might need to play the samples several times before you can detect the difference.

Two sine waves played consecutively – this sample has half-step at 550 Hz (C♯ in the just intonation scale), followed by a half-step at 554.37 Hz (C♯ in the equal temperament scale).
Same two notes, set against an A440 pedal – this sample consists of a ""dyad"". The lower note is a constant A (440 Hz in either scale), the upper note is a C♯ in the equal-tempered scale for the first 1"", and a C♯ in the just intonation scale for the last 1"". Phase differences make it easier to detect the transition than in the previous sample.


=== Just tunings ===

5-limit tuning, the most common form of just intonation, is a system of tuning using tones that are regular number harmonics of a single fundamental frequency. This was one of the scales Johannes Kepler presented in his Harmonices Mundi (1619) in connection with planetary motion. The same scale was given in transposed form by Scottish mathematician and musical theorist, Alexander Malcolm, in 1721 in his 'Treatise of Musick: Speculative, Practical and Historical', and by theorist Jose Wuerschmidt in the 20th century. A form of it is used in the music of northern India.
American composer Terry Riley also made use of the inverted form of it in his ""Harp of New Albion"". Just intonation gives superior results when there is little or no chord progression: voices and other instruments gravitate to just intonation whenever possible. However, it gives two different whole tone intervals (9:8 and 10:9) because a fixed tuned instrument, such as a piano, cannot change key. To calculate the frequency of a note in a scale given in terms of ratios, the frequency ratio is multiplied by the tonic frequency. For instance, with a tonic of A4 (A natural above middle C), the frequency is 440 Hz, and a justly tuned fifth above it (E5) is simply 440×(3:2) = 660 Hz.

Pythagorean tuning is tuning based only on the perfect consonances, the (perfect) octave, perfect fifth, and perfect fourth. Thus the major third is considered not a third but a ditone, literally ""two tones"", and is (9:8)2 = 81:64, rather than the independent and harmonic just 5:4 = 80:64 directly below. A whole tone is a secondary interval, being derived from two perfect fifths minus an octave, (3:2)2/2 = 9:8.
The just major third, 5:4 and minor third, 6:5, are a syntonic comma, 81:80, apart from their Pythagorean equivalents 81:64 and 32:27 respectively. According to Carl Dahlhaus (1990, p. 187), ""the dependent third conforms to the Pythagorean, the independent third to the harmonic tuning of intervals.""
Western common practice music usually cannot be played in just intonation but requires a systematically tempered scale. The tempering can involve either the irregularities of well temperament or be constructed as a regular temperament, either some form of equal temperament or some other regular meantone, but in all cases will involve the fundamental features of meantone temperament. For example, the root of chord ii, if tuned to a fifth above the dominant, would be a major whole tone (9:8) above the tonic. If tuned a just minor third (6:5) below a just subdominant degree of 4:3, however, the interval from the tonic would equal a minor whole tone (10:9). Meantone temperament reduces the difference between 9:8 and 10:9. Their ratio, (9:8)/(10:9) = 81:80, is treated as a unison. The interval 81:80, called the syntonic comma or comma of Didymus, is the key comma of meantone temperament.


=== Equal temperament tunings ===
In equal temperament, the octave is divided into equal parts on the logarithmic scale. While it is possible to construct equal temperament scale with any number of notes (for example, the 24-tone Arab tone system), the most common number is 12, which makes up the equal-temperament chromatic scale. In western music, a division into twelve intervals is commonly assumed unless it is specified otherwise.
For the chromatic scale, the octave is divided into twelve equal parts, each semitone (half-step) is an interval of the twelfth root of two so that twelve of these equal half steps add up to exactly an octave. With fretted instruments it is very useful to use equal temperament so that the frets align evenly across the strings. In the European music tradition, equal temperament was used for lute and guitar music far earlier than for other instruments, such as musical keyboards. Because of this historical force, twelve-tone equal temperament is now the dominant intonation system in the Western, and much of the non-Western, world.
Equally tempered scales have been used and instruments built using various other numbers of equal intervals. The 19 equal temperament, first proposed and used by Guillaume Costeley in the 16th century, uses 19 equally spaced tones, offering better major thirds and far better minor thirds than normal 12-semitone equal temperament at the cost of a flatter fifth. The overall effect is one of greater consonance. Twenty-four equal temperament, with twenty-four equally spaced tones, is widespread in the pedagogy and notation of Arabic music. However, in theory and practice, the intonation of Arabic music conforms to rational ratios, as opposed to the irrational ratios of equally tempered systems.While any analog to the equally tempered quarter tone is entirely absent from Arabic intonation systems, analogs to a three-quarter tone, or neutral second, frequently occur. These neutral seconds, however, vary slightly in their ratios dependent on maqam, as well as geography. Indeed, Arabic music historian Habib Hassan Touma has written that ""the breadth of deviation of this musical step is a crucial ingredient in the peculiar flavor of Arabian music. To temper the scale by dividing the octave into twenty-four quarter-tones of equal size would be to surrender one of the most characteristic elements of this musical culture.""53 equal temperament arises  from the near equality of 53 perfect fifths with 31 octaves, and was noted by Jing Fang and Nicholas Mercator.


== Connections to mathematics ==


=== Set theory ===

Musical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set.


=== Abstract algebra ===

Expanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.Transformational theory is a branch of music theory developed by David Lewin. The theory allows for great generality because it emphasizes transformations between musical objects, rather than the musical objects themselves.
Theorists have also proposed musical applications of more sophisticated algebraic concepts. The theory of regular temperaments has been extensively developed with a wide range of sophisticated mathematics, for example by associating each regular temperament with a rational point on a Grassmannian.
The chromatic scale has a free and transitive action of the cyclic group 
  
    
      
        
          Z
        
        
          /
        
        12
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} /12\mathbb {Z} }
  , with the action being defined via transposition of notes. So the chromatic scale can be thought of as a torsor for the group.


=== Numbers and series ===
Some composers have incorporated the golden ratio and Fibonacci numbers into their work.


=== Category theory ===

The mathematician and musicologist Guerino Mazzola has used category theory (topos theory) for a basis of music theory, which includes using topology as a basis for a theory of rhythm and motives, and differential geometry as a basis for a theory of musical phrasing, tempo, and intonation.


== Musicians who were or are also mathematicians ==
Albert Einstein - Accomplished pianist and violinist.
Art Garfunkel (Simon & Garfunkel) – Masters in Mathematics Education, Columbia University
Brian May (Queen) - BSc (Hons) in Mathematics and Physics, PhD in Astrophysics, both from Imperial College London.
Dan Snaith – PhD Mathematics, Imperial College London
Delia Derbyshire - BA in mathematics and music from Cambridge.
Jonny Buckland (Coldplay) - Studied astronomy and mathematics at University College London.
Kit Armstrong - Degree in music and MSc in mathematics.
Manjul Bhargava - Plays the tabla, won the Fields Medal in 2014.
Phil Alvin (The Blasters) – Mathematics, University of California, Los Angeles
Philip Glass - Studied mathematics and philosophy at the University of Chicago.
Tom Lehrer - BA mathematics from Harvard University.
William Herschel - Astronomer and played the oboe, violin, harpsichord and organ. He composed 24 symphonies and many concertos, as well as some church music.
Jerome Hines - Five articles published in Mathematics Magazine 1951-6.
Donald Knuth - Knuth is an organist and a composer. In 2016 he completed a musical piece for organ titled Fantasia Apocalyptica. It was premièred in Sweden on January 10, 2018


== See also ==

 Music portal


== References ==

Dahlhaus, Carl. 1990. Wagners Konzeption des musikalischen Dramas. Deutscher Taschenbuch Verlag. Kassel: Bärenreiter. ISBN 9783423045384; ISBN 9783761845387.
Ivor Grattan-Guinness (1995) ""Mozart 18, Beethoven 32: Hidden shadows of integers in classical music"", pages 29 to 47 in History of Mathematics: States of the Art, Joseph W. Dauben, Menso Folkerts, Eberhard Knobloch and Hans Wussing editors, Academic Press ISBN 0-12-204055-4


== External links ==
Axiomatic Music Theory by S.M. Nemati
Music and Math by Thomas E. Fiore
Twelve-Tone Musical Scale.
Sonantometry or music as math discipline.
Music: A Mathematical Offering by Dave Benson.
Nicolaus Mercator use of Ratio Theory in Music at Convergence
The Glass Bead Game Hermann Hesse gave music and mathematics a crucial role in the development of his Glass Bead Game.
Harmony and Proportion. Pythagoras, Music and Space.
""Linear Algebra and Music""
Notefreqs — A complete table of note frequencies and ratios for midi, piano, guitar, bass, and violin. Includes fret measurements (in cm and inches) for building instruments.
Mathematics & Music, BBC Radio 4 discussion with Marcus du Sautoy, Robin Wilson & Ruth Tatlow (In Our Time, May 25, 2006)
Measuring note similarity with positive definite kernels, Measuring note similarity with positive definite kernels"
95eb94831e,Paraconsistent mathematics,"Paraconsistent mathematics, sometimes called inconsistent mathematics, represents an attempt to develop the classical infrastructure of mathematics (e.g. analysis) based on a foundation of paraconsistent logic instead of classical logic. A number of reformulations of analysis can be developed, for example functions which both do and do not have a given value simultaneously.
Chris Mortensen claims (see references):

One could hardly ignore the examples of analysis and its special case, the calculus. There prove to be many places where there are distinctive inconsistent insights; see Mortensen (1995) for example. (1) Robinson's non-standard analysis was based on infinitesimals, quantities smaller than any real number, as well as their reciprocals, the infinite numbers. This has an inconsistent version, which has some advantages for calculation in being able to discard higher-order infinitesimals. The theory of differentiation turned out to have these advantages, while the theory of integration did not. (2)


== References ==
McKubre-Jordens, M. and Weber, Z. (2012). ""Real analysis in paraconsistent logic"". Journal of Philosophical Logic 41 (5):901–922. doi: 10.1017/S1755020309990281
Mortensen, C. (1995). Inconsistent Mathematics. Dordrecht: Kluwer. ISBN 0-7923-3186-9
Weber, Z. (2010). ""Transfinite numbers in paraconsistent set theory"". Review of Symbolic Logic 3 (1):71–92. doi:10.1017/S1755020309990281


== External links ==
Entry in the Internet Encyclopedia of Philosophy [1]
Entry in the Stanford Encyclopedia of Philosophy [2]
Lectures by Manuel Bremer of the University of Düsseldorf [3]"
bb27933a56,Vedic Mathematics,"Vedic Mathematics is a book written by the Indian monk Bharati Krishna Tirtha, and first published in 1965. It contains a list of mathematical techniques, which were falsely claimed to have been retrieved from the Vedas and containing mathematical knowledge.Krishna Tirtha failed to produce the sources, and scholars unanimously note it to be a mere compendium of tricks for increasing the speed of elementary mathematical calculations sharing no overlap with historical mathematical developments during the Vedic period. However, there has been a proliferation of publications in this area and multiple attempts to integrate the subject into mainstream education by right-wing Hindu nationalist governments.


== Contents ==
The book contains metaphorical aphorisms in the form of sixteen sutras and thirteen sub-sutras, which Krishna Tirtha states allude to significant mathematical tools. The range of their asserted applications spans from topic as diverse as statics and pneumatics to astronomy and financial domains. Tirtha stated that no part of advanced mathematics lay beyond the realms of his book and propounded that studying it for a couple of hours every day for a year equated to spending about two decades in any standardized education system to become professionally trained in the discipline of mathematics.STS scholar S. G. Dani in 'Vedic Mathematics': Myth and Reality states that the book is primarily a compendium of tricks that can be applied in elementary, middle and high school arithmetic and algebra, to gain faster results. The sutras and sub-sutras are abstract literary expressions (for example, ""as much less"" or ""one less than previous one"") prone to creative interpretations; Krishna Tirtha exploited this to the extent of manipulating the same shloka to generate widely different mathematical equivalencies across a multitude of contexts.


=== Source and relation with The Vedas ===
According to Krishna Tirtha, the sutras and other accessory content were found after years of solitary study of the Vedas—a set of sacred ancient Hindu scriptures—in a forest. They were supposedly contained in the pariśiṣṭa—a supplementary text/appendix—of the Atharvaveda. He does not provide any more bibliographic clarification on the sourcing. The book's editor, Professor V. S. Agrawala argues that since the Vedas are defined as the traditional repositories of all knowledge, any knowledge can be de facto assumed to be in the Vedas, irrespective of whether it may be physically located in them; he even went to the extent of deeming Krishna Tirtha's work as a pariśiṣṭa in itself.However, numerous mathematicians and STS scholars (Dani, Kim Plofker, K.S. Shukla, Jan Hogendijk et al) note that the Vedas do not contain any of those sutras and sub-sutras. When challenged by Shukla, a mathematician and a historiographer of ancient Indian mathematics, to locate the sutras in the Parishishta of a standard edition of the Atharvaveda, Krishna Tirtha stated that they were not included in the standard editions but only in a hitherto-undiscovered version, chanced upon by him; the foreword and introduction of the book also takes a similar stand. Sanskrit scholars have also confirmed that the linguistic style did not correspond to the time-spans but rather reflected contemporary Sanskrit.Dani points out that the contents of the book have ""practically nothing in common"" with the mathematics of the Vedic period or even with subsequent developments in Indian mathematics. Shukla reiterates the observations, on a per-chapter basis. For example, multiple techniques in the book involve the use of high-precision decimals. These were unknown during the Vedic times and were introduced in India only in the sixteenth century; works of numerous ancient mathematicians such as Aryabhata, Brahmagupta and Bhaskara were entirely based on fractions. Some of the sutras even run parallel to the General Leibniz rule and Taylor's theorem (which, per Krishna Tirtha, were to be yet studied by the western world during the time of his writing) but did ultimately boil down to the sub-elementary operations of basic differentiation on polynomials. From a historiographic perspective, India had no minimal knowledge about the conceptual notions of differentiation and integration. Sutras have been further leveraged that analytic geometry of conics occupied an important tier in Vedic mathematics, which runs contrary to all available evidence.


== Publication history and reprints ==
First published in 1965, five years after Krishna Tirtha death, the work consisted of forty chapters, originally on 367 pages, and covered techniques he had propagated, through his lectures. A foreword by Tirtha's disciple Manjula Trivedi stated that he had originally written 16 volumes—one on each sutra—but the manuscripts were lost before publication, and that this work was penned in 1957.: 10 Reprints were published in 1975 and 1978 to accommodate typographical corrections. Several reprints have been published since the 1990s.: 6 


== Reception ==
S. G. Dani of the Indian Institute of Technology Bombay (IIT Bombay) notes the book to be of dubious quality. He believes it did a disservice both to the pedagogy of mathematical education by presenting the subject as a bunch of tricks without any conceptual rigor, and to science and technology studies in India (STS) by adhering to dubious standards of historiography. He also points out that while Tirtha's system could be used as a teaching aid, there was a need to prevent the use of ""public money and energy on its propagation"" except in a limited way and that authentic Vedic studies were being neglected in India even as Tirtha's system received support from several government and private agencies. Jayant Narlikar has voiced similar concerns.Hartosh Singh Bal notes that whilst Krishna Tirtha's attempts might be somewhat acceptable in light of his nationalistic inclinations during colonial rule — he had left his spiritual endeavors to be appointed as the principal of a college to counter Macaulayism —, it provided a fertile ground for further ethno-nationalistic abuse of historiography by Hindu Nationalist parties; Thomas Trautmann views the development of Vedic Mathematics in a similar manner. Meera Nanda has noted hagiographic descriptions of Indian knowledge systems by various right-wing cultural movements (including the BJP), which deemed Krishna Tirtha to be in the same league as Srinivasa Ramanujan.Some have however praised the methods and commented on its potential to attract school-children to mathematics and increase popular engagement with the subject. Others have viewed the works as an attempt at harmonizing religion with science.


=== Originality of methods ===
Dani believes Krishna Tirtha's methods to be a product of his academic training in mathematics and long recorded habit of experimentation with numbers; nonetheless, he considers the work to be an impressive feat. Similar systems include the Trachtenberg system or the techniques mentioned in Lester Meyers's 1947 book High-speed Mathematics. Alex Bellos points out that several of the calculation tricks can also be found in certain European treatises on calculation from the early Modern period.


=== Computation algorithms ===
Some of the algorithms have been tested for efficiency, with positive results. However, most of the algorithms have higher time complexity than conventional ones, which explains the lack of adoption of Vedic mathematics in real life.


== Integration into mainstream education ==
The book had been included in the school syllabus of Madhya Pradesh and Uttar Pradesh, soon after the Bharatiya Janata Party (BJP), a right-wing Hindu nationalist political party came to power and chose to saffronise the education-system.: 6 Dinanath Batra had conducted a lengthy campaign for the inclusion of Vedic Maths into the National Council of Educational Research and Training (NCERT) curricula. Subsequently, there was a proposal from NCERT to induct Vedic Maths, along with a number of fringe pseudo-scientific subjects (Vedic Astrology et al.), into the standard academic curricula. This was only shelved after a number of academics and mathematicians, led by Dani and sometimes backed by political parties, opposed these attempts based on previously discussed rationales and criticized the move as a politically guided attempt at saffronisation. Concurrent official reports also advocated for its inclusion in the Madrassah education system to modernize it.After the BJP's return to power in 2014, three universities began offering courses on the subject while a television channel, catering to the topic, was also launched; generous education and research grants have also been allotted to the subject.


== Notes ==


== References ==


== External links ==
Full text"
156c192259,Lottery mathematics,"Lottery mathematics is used to calculate probabilities of winning or losing a lottery game. It is based primarily on combinatorics, particularly the twelvefold way and  combinations without replacement.


== Choosing 6 from 49 ==
In a typical 6/49 game, each player chooses six distinct numbers from a range of 1-49. If the six numbers on a ticket match the numbers drawn by the lottery, the ticket holder is a jackpot winner—regardless of the order of the numbers. The probability of this happening is 1 in 13,983,816.
The chance of winning can be demonstrated as follows: The first number drawn has a 1 in 49 chance of matching. When the draw comes to the second number, there are now only 48 balls left in the bag, because the balls are drawn without replacement. So there is now a 1 in 48 chance of predicting this number.
Thus for each of the 49 ways of choosing the first number there are 48 different ways of choosing the second. This means that the probability of correctly predicting 2 numbers drawn from 49 in the correct order is calculated as 1 in 49 × 48. On drawing the third number there are only 47 ways of choosing the number; but of course we could have arrived at this point in any of 49 × 48 ways, so the chances of correctly predicting 3 numbers drawn from 49, again in the correct order, is 1 in 49 × 48 × 47. This continues until the sixth number has been drawn, giving the final calculation, 49 × 48 × 47 × 46 × 45 × 44, which can also be written as 
  
    
      
        
          
            
              49
              !
            
            
              (
              49
              −
              6
              )
              !
            
          
        
      
    
    {\displaystyle {49! \over (49-6)!}}
   or 49 factorial divided by 43 factorial or FACT(49)/FACT(43) or simply PERM(49,6) .
608281864034267560872252163321295376887552831379210240000000000 / 60415263063373835637355132068513997507264512000000000 = 10068347520
This works out to 10,068,347,520, which is much bigger than the ~14 million stated above. 
Perm(49,6)=10068347520 and 49 nPr 6 =10068347520. 
However; the order of the 6 numbers is not significant for the payout!  That is, if a ticket has the numbers 1, 2, 3, 4, 5, and 6, it wins as long as all the numbers 1 through 6 are drawn, no matter what order they come out in. Accordingly, given any combination of 6 numbers, there are 6 × 5 × 4 × 3 × 2 × 1 = 6! or 720 orders in which they can be drawn. Dividing 10,068,347,520 by 720 gives 13,983,816, also written as 
  
    
      
        
          
            
              49
              !
            
            
              6
              !
              ∗
              (
              49
              −
              6
              )
              !
            
          
        
      
    
    {\displaystyle {49! \over 6!*(49-6)!}}
  , or COMBIN(49,6) or 49 nCr 6 or more generally as

  
    
      
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        =
        
          
            
              n
              !
            
            
              k
              !
              (
              n
              −
              k
              )
              !
            
          
        
      
    
    {\displaystyle {n \choose k}={n! \over k!(n-k)!}}
  , where n is the number of alternatives and k is the number of choices.  Further information is available at binomial coefficient and multinomial coefficient.This function is called the combination function, COMBIN(n,k). For the rest of this article, we will use the notation 
  
    
      
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
      
    
    {\displaystyle {n \choose k}}
  . ""Combination"" means the group of numbers selected, irrespective of the order in which they are drawn. A combination of numbers is usually presented in ascending order. An eventual 7th drawn number, the reserve or bonus, is presented at the end.
An alternative method of calculating the odds is to note that the probability of the first ball corresponding to one of the six chosen is 6/49; the probability of the second ball corresponding to one of the remaining five chosen is 5/48; and so on.  This yields a final formula of

  
    
      
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        =
        
          
            
              (
            
            
              49
              6
            
            
              )
            
          
        
        =
        
          
            49
            6
          
        
        ∗
        
          
            48
            5
          
        
        ∗
        
          
            47
            4
          
        
        ∗
        
          
            46
            3
          
        
        ∗
        
          
            45
            2
          
        
        ∗
        
          
            44
            1
          
        
      
    
    {\displaystyle {n \choose k}={49 \choose 6}={49 \over 6}*{48 \over 5}*{47 \over 4}*{46 \over 3}*{45 \over 2}*{44 \over 1}}
  A 7th ball often is drawn as reserve  ball, in the past only a second chance to get 5+1 numbers correct with 6 numbers played.


== Odds of getting other possibilities in choosing 6 from 49 ==
One must divide the number of combinations producing the given result by the total number of possible combinations (for example, 
  
    
      
        
          
            
              (
            
            
              49
              6
            
            
              )
            
          
        
        =
        13
        ,
        983
        ,
        816
      
    
    {\displaystyle {49 \choose 6}=13,983,816}
   ). The numerator equates to the number of ways to select the winning numbers multiplied by the number of ways to select the losing numbers.
For a score of n (for example, if 3 choices match three of the 6 balls drawn, then n  = 3), 
  
    
      
        
          
            
              (
            
            
              6
              n
            
            
              )
            
          
        
      
    
    {\displaystyle {6 \choose n}}
   describes the odds of selecting n winning numbers from the 6 winning numbers. This means that there are 6 - n losing numbers, which are chosen from the 43 losing numbers in 
  
    
      
        
          
            
              (
            
            
              43
              
                6
                −
                n
              
            
            
              )
            
          
        
      
    
    {\displaystyle {43 \choose 6-n}}
   ways. The total number of combinations giving that result is, as stated above, the first number multiplied by the second. The expression is therefore 
  
    
      
        
          
            
              
                (
              
              
                6
                n
              
              
                )
              
            
          
          
            
              
                (
              
              
                43
                
                  6
                  −
                  n
                
              
              
                )
              
            
          
        
        
          
            
              (
            
            
              49
              6
            
            
              )
            
          
        
      
    
    {\displaystyle {6 \choose n}{43 \choose 6-n} \over {49 \choose 6}}
  .
This can be written in a general form for all lotteries as:

where 
  
    
      
        N
      
    
    {\displaystyle N}
   is the number of balls in lottery, 
  
    
      
        K
      
    
    {\displaystyle K}
   is the number of balls in a single ticket, and 
  
    
      
        B
      
    
    {\displaystyle B}
   is the number of matching balls for a winning ticket.
The generalisation of this formula is called the hypergeometric distribution.
This gives the following results:

When a 7th number is drawn as bonus number then we have 49!/6!/1!/42!.=combin(49,6)*combin(49-6,1)=601304088 different possible drawing results.

You would expect to score 3 of 6 or better once in around 36.19 drawings. Notice that It takes a 3 if 6 wheel of 163 combinations to be sure of at least one 3/6 score. 
1/p changes when several distinct combinations are played together. It mostly is about winning something, not just the jackpot. 


== Ensuring to win the jackpot ==
There is only one known way to ensure winning the jackpot. That is to buy at least one lottery ticket for every possible number combination. For example, one has to buy 13,983,816 different tickets to ensure to win the jackpot in a 6/49 game.
Lottery organizations have laws, rules and safeguards in place to prevent gamblers from executing such an operation. Further, just winning the jackpot by buying every possible combination does not guarantee to break even or make a profit.
If 
  
    
      
        p
      
    
    {\displaystyle p}
   is the probability to win; 
  
    
      
        
          c
          
            t
          
        
      
    
    {\displaystyle c_{t}}
  the cost of a ticket; 
  
    
      
        
          c
          
            l
          
        
      
    
    {\displaystyle c_{l}}
   the cost for obtaining a ticket (e.g. including the logistics); 
  
    
      
        
          c
          
            f
          
        
      
    
    {\displaystyle c_{f}}
   one time costs for the operation (such as setting up and conducting the operation); then the jackpot 
  
    
      
        
          m
          
            j
          
        
      
    
    {\displaystyle m_{j}}
   should contain at least

  
    
      
        
          m
          
            j
          
        
        ≥
        
          c
          
            f
          
        
        +
        
          
            
              
                c
                
                  t
                
              
              +
              
                c
                
                  l
                
              
            
            p
          
        
      
    
    {\displaystyle m_{j}\geq c_{f}+{\frac {c_{t}+c_{l}}{p}}}
  
to have a chance to at least break even.
The above theoretical ""chance to break-even"" point is slightly offset by the sum 
  
    
      
        
          ∑
          
            i
          
        
        

        
        
          m
          
            i
          
        
      
    
    {\displaystyle \sum _{i}{}m_{i}}
   of the minor wins also included in all the lottery tickets:

  
    
      
        
          m
          
            j
          
        
        ≥
        
          c
          
            f
          
        
        +
        
          
            
              
                c
                
                  t
                
              
              +
              
                c
                
                  l
                
              
            
            p
          
        
        −
        
          ∑
          
            i
          
        
        

        
        
          m
          
            i
          
        
      
    
    {\displaystyle m_{j}\geq c_{f}+{\frac {c_{t}+c_{l}}{p}}-\sum _{i}{}m_{i}}
  
Still, even if the above relation is satisfied, it does not guarantee to break even. The payout depends on the number of winning tickets for all the prizes 
  
    
      
        
          n
          
            x
          
        
      
    
    {\displaystyle n_{x}}
  , resulting in the relation

  
    
      
        
          
            
              m
              
                j
              
            
            
              n
              
                j
              
            
          
        
        ≥
        
          c
          
            f
          
        
        +
        
          
            
              
                c
                
                  t
                
              
              +
              
                c
                
                  l
                
              
            
            p
          
        
        −
        
          ∑
          
            i
          
        
        

        
        
          
            
              m
              
                i
              
            
            
              n
              
                i
              
            
          
        
      
    
    {\displaystyle {\frac {m_{j}}{n_{j}}}\geq c_{f}+{\frac {c_{t}+c_{l}}{p}}-\sum _{i}{}{\frac {m_{i}}{n_{i}}}}
  
In probably the only known successful operations the threshold to execute an operation was set at three times the cost of the tickets alone for unknown reasons

  
    
      
        
          m
          
            j
          
        
        ≥
        3
        ×
        
          
            
              c
              
                t
              
            
            p
          
        
      
    
    {\displaystyle m_{j}\geq 3\times {\frac {c_{t}}{p}}}
  
I.e.

  
    
      
        
          
            
              
                n
                
                  j
                
              
              p
            
            
              c
              
                t
              
            
          
        
        
          (
          
            
              c
              
                f
              
            
            +
            
              
                
                  
                    c
                    
                      t
                    
                  
                  +
                  
                    c
                    
                      l
                    
                  
                
                p
              
            
            −
            
              ∑
              
                i
              
            
            

            
            
              
                
                  m
                  
                    i
                  
                
                
                  n
                  
                    i
                  
                
              
            
          
          )
        
        ≪
        3
      
    
    {\displaystyle {\frac {n_{j}p}{c_{t}}}\left(c_{f}+{\frac {c_{t}+c_{l}}{p}}-\sum _{i}{}{\frac {m_{i}}{n_{i}}}\right)\ll 3}
  
This does, however, not eliminate all risks to make no profit. The success of the operations still depended on a bit of luck. In addition, in one operation the logistics failed and not all combinations could be obtained. This added the risk of not even winning the jackpot at all.


== Powerballs and bonus balls ==

Many lotteries have a Powerball (or ""bonus ball""). If the powerball is drawn from a pool of numbers different from the main lottery, the odds are multiplied by the number of powerballs. For example, in the 6 from 49 lottery, given 10 powerball numbers, then the odds of getting a score of 3 and the powerball would be 1 in 56.66 × 10, or 566.6 (the probability would be divided by 10, to give an exact value of 
  
    
      
        
          
            8815
            4994220
          
        
      
    
    {\textstyle {\frac {8815}{4994220}}}
  ). Another example of such a game is Mega Millions, albeit with different jackpot odds.
Where more than 1 powerball is drawn from a separate pool of balls to the main lottery (for example, in the EuroMillions game), the odds of the different possible powerball matching scores are calculated using the method shown in the ""other scores"" section above (in other words, the powerballs are like a mini-lottery in their own right), and then multiplied by the odds of achieving the required main-lottery score.
If the powerball is drawn from the same pool of numbers as the main lottery, then, for a given target score, the number of winning combinations includes the powerball. For games based on the Canadian lottery (such as the lottery of the United Kingdom), after the 6 main balls are drawn, an extra ball is drawn from the same pool of balls, and this becomes the powerball (or ""bonus ball""). An extra prize is given for matching 5 balls and the bonus ball. As described in the ""other scores"" section above, the number of ways one can obtain a score of 5 from a single ticket is 
  
    
      
        
          
            
              (
            
            
              6
              5
            
            
              )
            
          
        
        
          
            
              (
            
            
              43
              1
            
            
              )
            
          
        
        =
        258
      
    
    {\textstyle {6 \choose 5}{43 \choose 1}=258}
  . Since the number of remaining balls is 43, and the ticket has 1 unmatched number remaining, 1/43 of these 258 combinations will match the next ball drawn (the powerball), leaving 258/43 = 6 ways of achieving it. Therefore, the odds of getting a score of 5 and the powerball are 
  
    
      
        
          
            6
            
              
                
                  (
                
                
                  49
                  6
                
                
                  )
                
              
            
          
        
        =
        
          
            1
            
              2
              ,
              330
              ,
              636
            
          
        
      
    
    {\textstyle {6 \over {49 \choose 6}}={1 \over 2,330,636}}
  .
Of the 258 combinations that match 5 of the main 6 balls, in 42/43 of them the remaining number will not match the powerball, giving odds of 
  
    
      
        
          
            
              258
              ⋅
              
                
                  42
                  43
                
              
            
            
              
                
                  (
                
                
                  49
                  6
                
                
                  )
                
              
            
          
        
        =
        
          
            3
            
              166
              ,
              474
            
          
        
        ≈
        1.802
        ×
        
          10
          
            −
            5
          
        
      
    
    {\textstyle {{258\cdot {\frac {42}{43}}} \over {49 \choose 6}}={\frac {3}{166,474}}\approx 1.802\times 10^{-5}}
   for obtaining a score of 5 without matching the powerball.
Using the same principle, the odds of getting a score of 2 and the powerball are 
  
    
      
        
          
            
              (
            
            
              6
              2
            
            
              )
            
          
        
        
          
            
              (
            
            
              43
              4
            
            
              )
            
          
        
        =
        1
        ,
        
        851
        ,
        
        150
      
    
    {\textstyle {6 \choose 2}{43 \choose 4}=1,\!851,\!150}
   for the score of 2 multiplied by the probability of one of the remaining four numbers matching the bonus ball, which is 4/43. Since 
  
    
      
        1
        ,
        851
        ,
        150
        ⋅
        
          
            4
            43
          
        
        =
        172
        ,
        
        200
      
    
    {\textstyle 1,851,150\cdot {\frac {4}{43}}=172,\!200}
  , the probability of obtaining the score of 2 and the bonus ball is 
  
    
      
        
          
            
              172
              ,
              200
            
            
              
                (
              
              
                49
                6
              
              
                )
              
            
          
        
        =
        
          
            1025
            83237
          
        
        =
        1.231
        %
      
    
    {\textstyle {\frac {172,200}{49 \choose 6}}={\frac {1025}{83237}}=1.231\%}
  , approximate decimal odds of 1 in 81.2.
The general formula for 
  
    
      
        B
      
    
    {\displaystyle B}
   matching balls in a 
  
    
      
        N
      
    
    {\displaystyle N}
   choose 
  
    
      
        K
      
    
    {\displaystyle K}
   lottery with one bonus ball from the 
  
    
      
        N
      
    
    {\displaystyle N}
   pool of balls is: 

The general formula for 
  
    
      
        B
      
    
    {\displaystyle B}
   matching balls in a 
  
    
      
        N
      
    
    {\displaystyle N}
   choose 
  
    
      
        K
      
    
    {\displaystyle K}
   lottery with zero bonus ball from the 
  
    
      
        N
      
    
    {\displaystyle N}
   pool of balls is:

The general formula for 
  
    
      
        B
      
    
    {\displaystyle B}
   matching balls in a 
  
    
      
        N
      
    
    {\displaystyle N}
   choose 
  
    
      
        K
      
    
    {\displaystyle K}
   lottery with one bonus ball from a separate pool of 
  
    
      
        P
      
    
    {\displaystyle P}
   balls is:

The general formula for 
  
    
      
        B
      
    
    {\displaystyle B}
   matching balls in a 
  
    
      
        N
      
    
    {\displaystyle N}
   choose 
  
    
      
        K
      
    
    {\displaystyle K}
   lottery with no bonus ball from a separate pool of 
  
    
      
        P
      
    
    {\displaystyle P}
   balls is: 


== Minimum number of tickets for a match ==
It is a hard (and often open) problem to calculate the minimum number of tickets one needs to purchase to guarantee that at least one of these tickets matches at least 2 numbers. In the 5-from-90 lotto, the minimum number of tickets that can guarantee a ticket with at least 2 matches is 100.


== Information theoretic results ==

As a discrete probability space, the probability of any particular lottery outcome is atomic, meaning it is greater than zero.  Therefore, the probability of any event is the sum of probabilities of the outcomes of the event.  This makes it easy to calculate quantities of interest from information theory. For example, the information content of any event is easy to calculate, by the formula 

In particular, the information content of outcome 
  
    
      
        x
      
    
    {\displaystyle x}
   of discrete random variable 
  
    
      
        X
      
    
    {\displaystyle X}
   is

For example, winning in the example § Choosing 6 from 49 above is a Bernoulli-distributed random variable 
  
    
      
        X
      
    
    {\displaystyle X}
   with a 1/13,983,816 chance of winning (""success"") We write 
  
    
      
        X
        ∼
        
          B
          e
          r
          n
          o
          u
          l
          l
          i
        
        
        
          (
          p
          )
        
        =
        
          B
        
        
        
          (
          
            1
            ,
            p
          
          )
        
      
    
    {\textstyle X\sim \mathrm {Bernoulli} \!\left(p\right)=\mathrm {B} \!\left(1,p\right)}
   with 
  
    
      
        p
        =
        
          
            
              1
              
                13
                ,
                983
                ,
                816
              
            
          
        
      
    
    {\textstyle p={\tfrac {1}{13,983,816}}}
   and 
  
    
      
        q
        =
        
          
            
              
                13
                ,
                983
                ,
                815
              
              
                13
                ,
                983
                ,
                816
              
            
          
        
      
    
    {\textstyle q={\tfrac {13,983,815}{13,983,816}}}
  .  The information content of winning is 
shannons or bits of information. (See units of information for further explanation of terminology.) The information content of losing is 
 
The information entropy of a lottery probability distribution is also easy to calculate as the expected value of the information content.  
 
Oftentimes the random variable of interest in the lottery is a Bernoulli trial. In this case, the Bernoulli entropy function may be used. Using 
  
    
      
        X
      
    
    {\displaystyle X}
   representing winning the 6-of-49 lottery, the Shannon entropy of 6-of-49 above is 

  
    
      
        
          
            
              
                
                  H
                
                (
                X
                )
              
              
                
                =
                −
                p
                log
                ⁡
                (
                p
                )
                −
                q
                log
                ⁡
                (
                q
                )
                =
                −
                
                  
                    
                      1
                      
                        13
                        ,
                        983
                        ,
                        816
                      
                    
                  
                
                log
                
                
                  
                    
                      1
                      
                        13
                        ,
                        983
                        ,
                        816
                      
                    
                  
                
                −
                
                  
                    
                      
                        13
                        ,
                        983
                        ,
                        815
                      
                      
                        13
                        ,
                        983
                        ,
                        816
                      
                    
                  
                
                log
                
                
                  
                    
                      
                        13
                        ,
                        983
                        ,
                        815
                      
                      
                        13
                        ,
                        983
                        ,
                        816
                      
                    
                  
                
              
            
            
              
              
                
                ≈
                1.80065
                ×
                
                  10
                  
                    −
                    6
                  
                
                
                   shannons.
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathrm {H} (X)&=-p\log(p)-q\log(q)=-{\tfrac {1}{13,983,816}}\log \!{\tfrac {1}{13,983,816}}-{\tfrac {13,983,815}{13,983,816}}\log \!{\tfrac {13,983,815}{13,983,816}}\\&\approx 1.80065\times 10^{-6}{\text{ shannons.}}\end{aligned}}}
  


== References ==


== External links ==
Euler's Analysis of the Genoese Lottery – Convergence (August 2010), Mathematical Association of America
Lottery Mathematics – INFAROM Publishing
13,983,816 and the Lottery – YouTube video with James Clewett, Numberphile, March 2012"
79023a7c7f,Mathematical optimization,"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.


== Optimization problems ==

Optimization problems can be divided into two categories, depending on whether the variables are continuous or discrete: 

An optimization problem with discrete variables is known as a discrete optimization, in which an object such as an integer, permutation or graph must be found from a countable set.
A problem with continuous variables is known as a continuous optimization, in which an optimal value from a continuous function must be found. They can include constrained problems and multimodal problems.An optimization problem can be represented in the following way:

Given: a function f : A → ℝ from some set A to the real numbers
Sought: an element x0 ∈ A such that f(x0) ≤ f(x) for all x ∈ A (""minimization"") or such that f(x0) ≥ f(x) for all x ∈ A (""maximization"").Such a formulation is called an optimization problem or a mathematical programming problem (a term not directly related to computer programming, but still in use for example in linear programming – see History below). Many real-world and theoretical problems may be modeled in this general framework.
Since the following is valid

  
    
      
        f
        (
        
          
            x
          
          
            0
          
        
        )
        ≥
        f
        (
        
          x
        
        )
        ⇔
        −
        f
        (
        
          
            x
          
          
            0
          
        
        )
        ≤
        −
        f
        (
        
          x
        
        )
        ,
      
    
    {\displaystyle f(\mathbf {x} _{0})\geq f(\mathbf {x} )\Leftrightarrow -f(\mathbf {x} _{0})\leq -f(\mathbf {x} ),}
  it suffices to solve only minimization problems. However, the opposite perspective of considering only maximization problems would be valid, too.
Problems formulated using this technique in the fields of physics may refer to the technique as energy minimization, speaking of the value of the function f as representing the energy of the system being modeled. In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.
Typically, A is some subset of the Euclidean space ℝn, often specified by a set of constraints, equalities or inequalities that the members of A have to satisfy.  The domain A of f is called the search space or the choice set, while the elements of A are called candidate solutions or feasible solutions.
The function f is called, variously, an objective function, a loss function or cost function (minimization),  a utility function or fitness function (maximization), or, in certain fields, an energy function or energy functional. A feasible solution that minimizes (or maximizes, if that is the goal) the objective function is called an optimal solution.
In mathematics, conventional optimization problems are usually stated in terms of minimization.
A local minimum x* is defined as an element for which there exists some δ > 0 such that

  
    
      
        ∀
        
          x
        
        ∈
        A
        
        
          where
        
        
        
          ‖
          
            
              x
            
            −
            
              
                x
              
              
                ∗
              
            
          
          ‖
        
        ≤
        δ
        ,
        
      
    
    {\displaystyle \forall \mathbf {x} \in A\;{\text{where}}\;\left\Vert \mathbf {x} -\mathbf {x} ^{\ast }\right\Vert \leq \delta ,\,}
  the expression f(x*) ≤ f(x) holds;
that is to say, on some region around x* all of the function values are greater than or equal to the value at that element. 
Local maxima are defined similarly.
While a local minimum is at least as good as any nearby elements, a global minimum is at least as good as every feasible element.
Generally, unless the objective function is convex in a minimization problem, there may be several local minima.
In a convex problem, if there is a local minimum that is interior (not on the edge of the set of feasible elements), it is also the global minimum, but a nonconvex problem may have more than one local minimum not all of which need be global minima.
A large number of algorithms proposed for solving the nonconvex problems – including the majority of commercially available solvers – are  not capable of making a distinction between locally optimal solutions and globally optimal solutions, and will treat the former as actual solutions to the original problem. Global optimization is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a nonconvex problem.


== Notation ==
Optimization problems are often expressed with special notation. Here are some examples:


=== Minimum and maximum value of a function ===
Consider the following notation:

  
    
      
        
          min
          
            x
            ∈
            
              R
            
          
        
        
        
          (
          
            
              x
              
                2
              
            
            +
            1
          
          )
        
      
    
    {\displaystyle \min _{x\in \mathbb {R} }\;\left(x^{2}+1\right)}
  This denotes the minimum value of the objective function x2 + 1, when choosing x from the set of real numbers ℝ. The minimum value in this case is 1, occurring at x = 0.
Similarly, the notation

  
    
      
        
          max
          
            x
            ∈
            
              R
            
          
        
        
        2
        x
      
    
    {\displaystyle \max _{x\in \mathbb {R} }\;2x}
  asks for the maximum value of the objective function 2x, where x may be any real number. In this case, there is no such maximum as the objective function is unbounded, so the answer is ""infinity"" or ""undefined"".


=== Optimal input arguments ===

Consider the following notation:

  
    
      
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            
              x
              ∈
              (
              −
              ∞
              ,
              −
              1
              ]
            
          
        
        
        
          x
          
            2
          
        
        +
        1
        ,
      
    
    {\displaystyle {\underset {x\in (-\infty ,-1]}{\operatorname {arg\,min} }}\;x^{2}+1,}
  or equivalently

  
    
      
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            x
          
        
        
        
          x
          
            2
          
        
        +
        1
        ,
        
        
          subject to:
        
        
        x
        ∈
        (
        −
        ∞
        ,
        −
        1
        ]
        .
      
    
    {\displaystyle {\underset {x}{\operatorname {arg\,min} }}\;x^{2}+1,\;{\text{subject to:}}\;x\in (-\infty ,-1].}
  This represents the value (or values) of the argument x in the interval (−∞,−1] that minimizes (or minimises) the objective function x2 + 1 (the actual minimum value of that function is not what the problem asks for). In this case, the answer is x = −1, since x = 0 is infeasible, that is, it does not belong to the feasible set.
Similarly,

  
    
      
        
          
            
              a
              r
              g
              
              m
              a
              x
            
            
              x
              ∈
              [
              −
              5
              ,
              5
              ]
              ,
              
              y
              ∈
              
                R
              
            
          
        
        
        x
        cos
        ⁡
        y
        ,
      
    
    {\displaystyle {\underset {x\in [-5,5],\;y\in \mathbb {R} }{\operatorname {arg\,max} }}\;x\cos y,}
  or equivalently

  
    
      
        
          
            
              a
              r
              g
              
              m
              a
              x
            
            
              x
              ,
              
              y
            
          
        
        
        x
        cos
        ⁡
        y
        ,
        
        
          subject to:
        
        
        x
        ∈
        [
        −
        5
        ,
        5
        ]
        ,
        
        y
        ∈
        
          R
        
        ,
      
    
    {\displaystyle {\underset {x,\;y}{\operatorname {arg\,max} }}\;x\cos y,\;{\text{subject to:}}\;x\in [-5,5],\;y\in \mathbb {R} ,}
  represents the {x, y} pair (or pairs) that maximizes (or maximize) the value of the objective function x cos y, with the added constraint that x lie in the interval [−5,5] (again, the actual maximum value of the expression does not matter). In this case, the solutions are the pairs of the form {5, 2kπ} and {−5, (2k + 1)π}, where k ranges over all integers.
Operators arg min and arg max are sometimes also written as argmin and argmax, and stand for argument of the minimum and argument of the maximum.


== History ==
Fermat and Lagrange found calculus-based formulae for identifying optima, while Newton and Gauss proposed iterative methods for moving towards an optimum.
The term ""linear programming"" for certain optimization cases was due to George B. Dantzig, although much of the theory had been introduced by Leonid Kantorovich in 1939. (Programming in this context does not refer to computer programming, but comes from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time.) Dantzig published the Simplex algorithm in 1947, and John von Neumann developed the theory of duality in the same year.Other notable researchers in mathematical optimization include the following:


== Major subfields ==
Convex programming studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming.
Linear programming (LP), a type of convex programming, studies the case in which the objective function f is linear and the constraints are specified using only linear equalities and inequalities. Such a constraint set is called a polyhedron or a polytope if it is bounded.
Second-order cone programming (SOCP) is a convex program, and includes certain types of quadratic programs.
Semidefinite programming (SDP) is a subfield of convex optimization where the underlying variables are semidefinite matrices. It is a generalization of linear and convex quadratic programming.
Conic programming is a general form of convex programming.  LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone.
Geometric programming is a technique whereby objective and inequality constraints expressed as posynomials and equality constraints as monomials can be transformed into a convex program.
Integer programming studies linear programs in which some or all variables are constrained to take on integer values.  This is not convex, and in general much more difficult than regular linear programming.
Quadratic programming allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities.  For specific forms of the quadratic term, this is a type of convex programming.
Fractional programming studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem.
Nonlinear programming studies the general case in which the objective function or the constraints or both contain nonlinear parts.  This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it.
Stochastic programming studies the case in which some of the constraints or parameters depend on random variables.
Robust optimization is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization aims to find solutions that are valid under all possible realizations of the uncertainties defined by an uncertainty set.
Combinatorial optimization is concerned with problems where the set of feasible solutions is discrete or can be reduced to a discrete one.
Stochastic optimization is used with random (noisy) function measurements or random inputs in the search process.
Infinite-dimensional optimization studies the case when the set of feasible solutions is a subset of an infinite-dimensional space, such as a space of functions.
Heuristics and metaheuristics make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems.
Constraint satisfaction studies the case in which the objective function f is constant (this is used in artificial intelligence, particularly in automated reasoning).
Constraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints.
Disjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling.
Space mapping is a concept for modeling and optimization of an engineering system to high-fidelity (fine) model accuracy exploiting a suitable physically meaningful coarse or surrogate model.In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time):

Calculus of variations Is concerned with finding the best way to achieve some goal, such as finding a surface whose boundary is a specific curve, but with the least possible area.
Optimal control theory is a generalization of the calculus of variations which introduces control policies.
Dynamic programming is the approach to solve the stochastic optimization problem with stochastic, randomness, and unknown model parameters. It studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the Bellman equation.
Mathematical programming with equilibrium constraints is where the constraints include variational inequalities or  complementarities.


=== Multi-objective optimization ===

Adding more than one objective to an optimization problem adds complexity. For example, to optimize a structural design, one would desire a design that is both light and rigid. When two objectives conflict, a trade-off must be created. There may be one lightest design, one stiffest design, and an infinite number of designs that are some compromise of weight and rigidity. The set of trade-off designs that improve upon one criterion at the expense of another is known as the Pareto set. The curve created plotting weight against stiffness of the best designs is known as the Pareto frontier.
A design is judged to be ""Pareto optimal"" (equivalently, ""Pareto efficient"" or in the Pareto set) if it is not dominated by any other design: If it is worse than another design in some respects and no better in any respect, then it is dominated and is not Pareto optimal.
The choice among ""Pareto optimal"" solutions to determine the ""favorite solution"" is delegated to the decision maker. In other words, defining the problem as multi-objective optimization signals that some information is missing: desirable objectives are given but combinations of them are not rated relative to each other. In some cases, the missing information can be derived by interactive sessions with the decision maker.
Multi-objective optimization problems have been generalized further into vector optimization problems where the (partial) ordering is no longer given by the Pareto ordering.


=== Multi-modal or global optimization ===
Optimization problems are often multi-modal; that is, they possess multiple good solutions. They could all be globally good (same cost function value) or there could be a mix of globally good and locally good solutions. Obtaining all (or at least some of) the multiple solutions is the goal of a multi-modal optimizer.
Classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm.
Common approaches to global optimization problems, where multiple local extrema may be present include evolutionary algorithms, Bayesian optimization and simulated annealing.


== Classification of critical points and extrema ==


=== Feasibility problem ===
The satisfiability problem, also called the feasibility problem, is just the problem of finding any feasible solution at all without regard to objective value. This can be regarded as the special case of mathematical optimization where the objective value is the same for every solution, and thus any solution is optimal.
Many optimization algorithms need to start from a feasible point. One way to obtain such a point is to relax the feasibility conditions using a slack variable; with enough slack, any starting point is feasible. Then, minimize that slack variable until the slack is null or negative.


=== Existence ===
The extreme value theorem of Karl Weierstrass states that a continuous real-valued function on a compact set attains its maximum and minimum value. More generally, a lower semi-continuous function on a compact set attains its minimum; an upper semi-continuous function on a compact set attains its maximum point or view.


=== Necessary conditions for optimality ===
One of Fermat's theorems states that optima of unconstrained problems are found at stationary points, where the first derivative or the gradient of the objective function is zero (see first derivative test). More generally, they may be found at critical points, where the first derivative or gradient of the objective function is zero or is undefined, or on the boundary of the choice set. An equation (or set of equations) stating that the first derivative(s) equal(s) zero at an interior optimum is called a 'first-order condition' or a set of first-order conditions.
Optima of equality-constrained problems can be found by the Lagrange multiplier method. The optima of problems with equality and/or inequality constraints can be found using the 'Karush–Kuhn–Tucker conditions'.


=== Sufficient conditions for optimality ===
While the first derivative test identifies points that might be extrema, this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither. When the objective function is twice differentiable, these cases can be distinguished by checking the second derivative or the matrix of second derivatives (called the Hessian matrix) in unconstrained problems, or the matrix of second derivatives of the objective function and the constraints called the bordered Hessian in constrained problems. The conditions that distinguish maxima, or minima, from other stationary points are called 'second-order conditions' (see 'Second derivative test'). If a candidate solution satisfies the first-order conditions, then the satisfaction of the second-order conditions as well is sufficient to establish at least local optimality.


=== Sensitivity and continuity of optima ===
The envelope theorem describes how the value of an optimal solution changes when an underlying parameter changes. The process of computing this change is called comparative statics.
The maximum theorem of Claude Berge (1963) describes the continuity of an optimal solution as a function of underlying parameters.


=== Calculus of optimization ===

For unconstrained problems with twice-differentiable functions, some critical points can be found by finding the points where the gradient of the objective function is zero (that is, the stationary points). More generally, a zero subgradient certifies that a local minimum has been found for minimization problems with convex functions and other locally Lipschitz functions.
Further, critical points can be classified using the definiteness of the Hessian matrix: If the Hessian is positive definite at a critical point, then the point is a local minimum; if the Hessian matrix is negative definite, then the point is a local maximum; finally, if indefinite, then the point is some kind of saddle point.
Constrained problems can often be transformed into unconstrained problems with the help of Lagrange multipliers. Lagrangian relaxation can also provide approximate solutions to difficult constrained problems.
When the objective function is a convex function, then any local minimum will also be a global minimum. There exist efficient numerical techniques for minimizing convex functions, such as interior-point methods.


=== Global convergence ===
More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. The first and still popular method for ensuring convergence relies on line searches, which optimize a function along one dimension. A second and increasingly popular method for ensuring convergence uses trust regions. Both line searches and trust regions are used in modern methods of non-differentiable optimization. Usually, a global optimizer is much slower than advanced local optimizers (such as BFGS), so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points.


== Computational optimization techniques ==
To solve problems, researchers may use algorithms that terminate in a finite number of steps, or iterative methods that converge to a solution (on some specified class of problems), or heuristics that may provide approximate solutions to some problems (although their iterates need not converge).


=== Optimization algorithms ===

Simplex algorithm of George Dantzig, designed for linear programming
Extensions of the simplex algorithm, designed for quadratic programming and for linear-fractional programming
Variants of the simplex algorithm that are especially suited for network optimization
Combinatorial algorithms
Quantum optimization algorithms


=== Iterative methods ===

The iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values. While evaluating Hessians (H) and gradients (G) improves the rate of convergence, for functions for which these quantities exist and vary sufficiently smoothly, such evaluations increase the computational complexity (or computational cost) of each iteration. In some cases, the computational complexity may be excessively high.
One major criterion for optimizers is just the number of required function evaluations as this often is already a large computational effort, usually much more effort than within the optimizer itself, which mainly has to operate over the N variables. The derivatives provide detailed information for such optimizers, but are even harder to calculate, e.g. approximating the gradient takes at least N+1 function evaluations. For approximations of the 2nd derivatives (collected in the Hessian matrix), the number of function evaluations is in the order of N². Newton's method requires the 2nd-order derivatives, so for each iteration, the number of function calls is in the order of N², but for a simpler pure gradient optimizer it is only N. However, gradient optimizers need usually more iterations than Newton's algorithm. Which one is best with respect to the number of function calls depends on the problem itself.

Methods that evaluate Hessians (or approximate Hessians, using finite differences):
Newton's method
Sequential quadratic programming: A Newton-based method for small-medium scale constrained problems. Some versions can handle large-dimensional problems.
Interior point methods: This is a large class of methods for constrained optimization, some of which use only (sub)gradient information and others of which require the evaluation of Hessians.
Methods that evaluate gradients, or approximate gradients in some way (or even subgradients):
Coordinate descent methods: Algorithms which update a single coordinate in each iteration
Conjugate gradient methods: Iterative methods for large problems. (In theory, these methods terminate in a finite number of steps with quadratic objective functions, but this finite termination is not observed in practice on finite–precision computers.)
Gradient descent (alternatively, ""steepest descent"" or ""steepest ascent""): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems.
Subgradient methods: An iterative method for large locally Lipschitz functions using generalized gradients. Following Boris T. Polyak, subgradient–projection methods are similar to conjugate–gradient methods.
Bundle method of descent: An iterative method for small–medium-sized problems with locally Lipschitz functions, particularly for convex minimization problems (similar to conjugate gradient methods).
Ellipsoid method: An iterative method for small problems with quasiconvex objective functions and of great theoretical interest, particularly in establishing the polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.
Conditional gradient method (Frank–Wolfe) for approximate minimization of specially structured problems with linear constraints, especially with traffic networks. For general unconstrained problems, this method reduces to the gradient method, which is regarded as obsolete (for almost all problems).
Quasi-Newton methods: Iterative methods for medium-large problems (e.g. N<1000).
Simultaneous perturbation stochastic approximation (SPSA) method for stochastic optimization; uses random (efficient) gradient approximation.
Methods that evaluate only function values: If a problem is continuously differentiable, then gradients can be approximated using finite differences, in which case a gradient-based method can be used.
Interpolation methods
Pattern search methods, which have better convergence properties than the Nelder–Mead heuristic (with simplices), which is listed below.
Mirror descent


=== Heuristics ===

Besides (finitely terminating) algorithms and (convergent) iterative methods, there are heuristics. A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations. List of some well-known heuristics:


== Applications ==


=== Mechanics ===
Problems in rigid body dynamics (in particular articulated rigid body dynamics) often require mathematical programming techniques, since you can view rigid body dynamics as attempting to solve an ordinary differential equation on a constraint manifold; the constraints are various nonlinear geometric constraints such as ""these two points must always coincide"", ""this surface must not penetrate any other"", or ""this point must always lie somewhere on this curve"". Also, the problem of computing contact forces can be done by solving a linear complementarity problem, which can also be viewed as a QP (quadratic programming) problem.
Many design problems can also be expressed as optimization programs. This application is called design optimization. One subset is the engineering optimization, and another recent and growing subset of this field is multidisciplinary design optimization, which, while useful in many problems, has in particular been applied to aerospace engineering problems.
This approach may be applied in cosmology and astrophysics.


=== Economics and finance ===
Economics is closely enough linked to optimization of agents that an influential definition relatedly describes economics qua science as the ""study of human behavior as a relationship between ends and scarce means"" with alternative uses.  Modern optimization theory includes traditional optimization theory but also overlaps with game theory and the study of economic equilibria. The Journal of Economic Literature codes classify mathematical programming, optimization techniques, and related topics under JEL:C61-C63.
In microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem, are economic optimization problems. Insofar as they behave consistently, consumers are assumed to maximize their utility, while firms are usually assumed to maximize their profit. Also, agents are often modeled as being risk-averse, thereby preferring to avoid risk. Asset prices are also modeled using optimization theory, though the underlying mathematics relies on optimizing stochastic processes rather than on static optimization. International trade theory also uses optimization to explain trade patterns between nations. The optimization of portfolios is an example of multi-objective optimization in economics.
Since the 1970s, economists have modeled dynamic decisions over time using control theory. For example, dynamic search models are used to study labor-market behavior. A crucial distinction is between deterministic and stochastic models.  Macroeconomists build dynamic stochastic general equilibrium (DSGE) models that describe the dynamics of the whole economy as the result of the interdependent optimizing decisions of workers, consumers, investors, and governments.


=== Electrical engineering ===
Some common applications of optimization techniques in electrical engineering include active filter design, stray field reduction in superconducting magnetic energy storage systems, space mapping design of microwave structures, handset antennas, electromagnetics-based design. Electromagnetically validated design optimization of microwave components and antennas has made extensive use of an appropriate physics-based or empirical surrogate model and space mapping methodologies since the discovery of space mapping in 1993.


=== Civil engineering ===
Optimization has been widely used in civil engineering. Construction management and transportation engineering are among the main branches of civil engineering that heavily rely on optimization. The most common civil engineering problems that are solved by optimization are cut and fill of roads, life-cycle analysis of structures and infrastructures, resource leveling, water resource allocation, traffic management and schedule optimization.


=== Operations research ===
Another field that uses optimization techniques extensively is operations research. Operations research also uses stochastic modeling and simulation to support improved decision-making. Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and stochastic optimization methods.


=== Control engineering ===
Mathematical optimization is used in much modern controller design. High-level controllers such as model predictive control (MPC) or real-time optimization (RTO) employ mathematical optimization. These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical optimization problem including constraints and a model of the system to be controlled.


=== Geophysics ===
Optimization techniques are regularly used in geophysical parameter estimation problems. Given a set of geophysical measurements, e.g. seismic recordings, it is common to solve for the physical properties and geometrical shapes of the underlying rocks and fluids.  The majority of problems in geophysics are nonlinear with both deterministic and stochastic methods being widely used.


=== Molecular modeling ===

Nonlinear optimization methods are widely used in conformational analysis.


=== Computational systems biology ===

Optimization techniques are used in many facets of computational systems biology such as model building, optimal experimental design, metabolic engineering, and synthetic biology. Linear programming has been applied to calculate the maximal possible yields of fermentation products, and to infer gene regulatory networks from multiple microarray datasets as well as transcriptional regulatory networks from high-throughput data. Nonlinear programming has been used to analyze energy metabolism and has been applied to metabolic engineering and parameter estimation in biochemical pathways.


=== Machine learning ===


== Solvers ==


== See also ==


== Notes ==


== Further reading ==
Boyd, Stephen P.; Vandenberghe, Lieven (2004). Convex Optimization. Cambridge: Cambridge University Press. ISBN 0-521-83378-7.
Gill, P. E.; Murray, W.; Wright, M. H. (1982). Practical Optimization. London: Academic Press. ISBN 0-12-283952-8.
Lee, Jon (2004). A First Course in Combinatorial Optimization. Cambridge University Press. ISBN 0-521-01012-8.
Nocedal, Jorge; Wright, Stephen J. (2006). Numerical Optimization (2nd ed.). Berlin: Springer. ISBN 0-387-30303-0.


== External links ==

""Decision Tree for Optimization Software"". Links to optimization source codes
""Global optimization"".
""EE364a: Convex Optimization I"". Course from Stanford University.
Varoquaux, Gaël. ""Mathematical Optimization: Finding Minima of Functions""."
6b7bc9aed2,Curl (mathematics),"In vector calculus, the curl is a vector operator that describes the infinitesimal circulation of a vector field in three-dimensional Euclidean space. The curl at a point in the field is represented by a vector whose length and direction denote the magnitude and axis of the maximum circulation. The curl of a field is formally defined as the circulation density at each point of the field.
A vector field whose curl is zero is called irrotational. The curl is a form of differentiation for vector fields. The corresponding form of the fundamental theorem of calculus is Stokes' theorem, which relates the surface integral of the curl of a vector field to the line integral of the vector field around the boundary curve.
Curl F is a notation common today to the United States and Americas. In many European countries, particularly in classic scientific literature, the alternative notation rot F is traditionally used, which is spelled as ""rotor"", and comes from the ""rate of rotation"", which it represents. To avoid confusion, modern authors tend to use the cross product notation with the del (nabla) operator 
  
    
      
        ∇
        ×
        
          F
        
      
    
    {\displaystyle \nabla \times \mathbf {F} }
    which also reveals the relation between curl (rotor), divergence, and gradient operators.
Unlike the gradient and divergence, curl as formulated in vector calculus does not generalize simply to other dimensions; some generalizations are possible, but only in three dimensions is the geometrically defined curl of a vector field again a vector field. This deficiency is a direct consequence of the limitations of vector calculus; on the other hand, when expressed as an antisymmetric tensor field via the wedge operator of geometric calculus, the curl generalizes to all dimensions. The unfortunate circumstance is similar to that attending the 3-dimensional cross product, and indeed the connection is reflected in the notation 
  
    
      
        ∇
        ×
      
    
    {\displaystyle \nabla \times }
   for the curl. 
The name ""curl"" was first suggested by James Clerk Maxwell in 1871 but the concept was apparently first used in the construction of an optical field theory by James MacCullagh in 1839.


== Definition ==

The curl of a vector field F, denoted by curl F, or 
  
    
      
        ∇
        ×
        
          F
        
      
    
    {\displaystyle \nabla \times \mathbf {F} }
  , or rot F, is an operator that maps Ck functions in R3 to Ck−1 functions in R3, and in particular, it maps continuously differentiable functions R3 → R3 to continuous functions R3 → R3. It can be defined in several ways, to be mentioned below:
One way to define the curl of a vector field at a point is implicitly through its projections onto various axes passing through the point: if 
  
    
      
        
          
            
              u
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {u}} }
   is any unit vector, the projection of the curl of F onto 
  
    
      
        
          
            
              u
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {u}} }
   may be defined to be the limiting value of a closed line integral in a plane orthogonal to 
  
    
      
        
          
            
              u
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {u}} }
   divided by the area enclosed, as the path of integration is contracted indefinitely around the point.
More specifically, the curl is defined at a point p as

  
    
      
        (
        ∇
        ×
        
          F
        
        )
        (
        p
        )
        ⋅
        
          
            
              u
              ^
            
          
        
         
        
          
            
              

              
              =
              

              
            
            
              
              
                d
                e
                f
              
            
          
        
        
          lim
          
            A
            →
            0
          
        
        
          
            1
            
              
                |
              
              A
              
                |
              
            
          
        
        
          ∮
          
            C
          
        
        
          F
        
        ⋅
        
          d
        
        
          r
        
      
    
    {\displaystyle (\nabla \times \mathbf {F} )(p)\cdot \mathbf {\hat {u}} \ {\overset {\underset {\mathrm {def} }{}}{{}={}}}\lim _{A\to 0}{\frac {1}{|A|}}\oint _{C}\mathbf {F} \cdot \mathrm {d} \mathbf {r} }
  where the line integral is calculated along the boundary C of the area A in question,  |A| being the magnitude of the area. This equation defines the projection of the curl of F onto 
  
    
      
        
          
            
              u
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {u}} }
  . The infinitesimal surfaces bounded by C have 
  
    
      
        
          
            
              u
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {u}} }
   as their normal. C is oriented via the right-hand rule.
The above formula means that the projection of the curl of a vector field along a certain axis is the infinitesimal area density of the circulation of the field projected onto a plane perpendicular to that axis. This formula does not a priori define a legitimate vector field, for the individual circulation densities with respect to various axes a priori need not relate to each other in the same way as the components of a vector do; that they do indeed relate to each other in this precise manner must be proven separately.
To this definition fits naturally the Kelvin–Stokes theorem, as a global formula corresponding to the definition. It equates the surface integral of the curl of a vector field to the above line integral taken around the boundary of the surface.
Another way one can define the curl vector of a function F at a point is explicitly as the limiting value of a vector-valued surface integral around a shell enclosing p divided by the volume enclosed, as the shell is contracted indefinitely around p.
More specifically, the curl may be defined by the vector formula

  
    
      
        (
        ∇
        ×
        
          F
        
        )
        (
        p
        )
        
          
            
              

              
              =
              

              
            
            
              
              
                d
                e
                f
              
            
          
        
        
          lim
          
            V
            →
            0
          
        
        
          
            1
            
              
                |
              
              V
              
                |
              
            
          
        
        
          ∮
          
            S
          
        
        
          
            
              n
              ^
            
          
        
        ×
        
          F
        
         
        
          d
        
        S
      
    
    {\displaystyle (\nabla \times \mathbf {F} )(p){\overset {\underset {\mathrm {def} }{}}{{}={}}}\lim _{V\to 0}{\frac {1}{|V|}}\oint _{S}\mathbf {\hat {n}} \times \mathbf {F} \ \mathrm {d} S}
  where the surface integral is calculated along the boundary S of the volume V, |V| being the magnitude of the volume, and 
  
    
      
        
          
            
              n
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {n}} }
   pointing outward from the surface S perpendicularly at every point in S.
In this formula, the cross product in the integrand measures the tangential component of F at each point on the surface S, together with the orientation of these tangential components with respect to the surface S. Thus, the surface integral measures the overall extent to which F circulates around S, together with the net orientation of this circulation in space. The curl of a vector field at a point is then the infinitesimal volume density of the net vector circulation (i.e., both magnitude and spatial orientation) of the field around the point.
To this definition fits naturally another global formula (similar to the Kelvin-Stokes theorem) which equates the volume integral of the curl of a vector field to the above surface integral taken over the boundary of the volume. 
Whereas the above two definitions of the curl are coordinate free, there is another ""easy to memorize"" definition of the curl in curvilinear orthogonal coordinates, e.g. in Cartesian coordinates, spherical, cylindrical, or even elliptical or parabolic coordinates: 
The equation for each component (curl F)k can be obtained by exchanging each occurrence of a subscript 1, 2, 3 in cyclic permutation: 1 → 2, 2 → 3, and 3 → 1 (where the subscripts represent the relevant indices).
If (x1, x2, x3) are the Cartesian coordinates and (u1, u2, u3) are the orthogonal coordinates, then 
 
is the length of the coordinate vector corresponding to ui. The remaining two components of curl result from cyclic permutation of indices: 3,1,2 → 1,2,3 → 2,3,1.


=== Intuitive interpretation ===
Suppose the vector field describes the velocity field of a fluid flow (such as a large tank of liquid or gas) and a small ball is located within the fluid or gas (the centre of the ball being fixed at a certain point). If the ball has a rough surface, the fluid flowing past it will make it rotate. The rotation axis (oriented according to the right hand rule) points in the direction of the curl of the field at the centre of the ball, and the angular speed of the rotation is half the magnitude of the curl at this point.The curl of the vector at any point is given by the rotation of an infinitesimal area in the xy-plane (for z-axis component of the curl), zx-plane (for y-axis component of the curl) and yz-plane (for x-axis component of the curl vector). This can be clearly seen in the examples below.


== Usage ==
In practice, the two coordinate-free definitions described above are rarely used because in virtually all cases, the curl operator can be applied using some set of curvilinear coordinates, for which simpler representations have been derived.
The notation ∇ × F has its origins in the similarities to the 3-dimensional cross product, and it is useful as a mnemonic in Cartesian coordinates if ∇ is taken as a vector differential operator del. Such notation involving operators is common in physics and algebra.
Expanded in 3-dimensional Cartesian coordinates (see Del in cylindrical and spherical coordinates for spherical and cylindrical coordinate representations),∇ × F is, for F composed of [Fx, Fy, Fz] (where the subscripts indicate the components of the vector, not partial derivatives):

  
    
      
        ∇
        ×
        
          F
        
        =
        
          
            |
            
              
                
                  
                    
                      
                        ı
                        ^
                      
                    
                  
                
                
                  
                    
                      
                        ȷ
                        ^
                      
                    
                  
                
                
                  
                    
                      
                        k
                        ^
                      
                    
                  
                
              
              
                
                  
                    
                      
                        ∂
                        
                          ∂
                          x
                        
                      
                    
                  
                
                
                  
                    
                      
                        ∂
                        
                          ∂
                          y
                        
                      
                    
                  
                
                
                  
                    
                      
                        ∂
                        
                          ∂
                          z
                        
                      
                    
                  
                
              
              
                
                  
                    F
                    
                      x
                    
                  
                
                
                  
                    F
                    
                      y
                    
                  
                
                
                  
                    F
                    
                      z
                    
                  
                
              
            
            |
          
        
      
    
    {\displaystyle \nabla \times \mathbf {F} ={\begin{vmatrix}{\boldsymbol {\hat {\imath }}}&{\boldsymbol {\hat {\jmath }}}&{\boldsymbol {\hat {k}}}\\[5pt]{\dfrac {\partial }{\partial x}}&{\dfrac {\partial }{\partial y}}&{\dfrac {\partial }{\partial z}}\\[10pt]F_{x}&F_{y}&F_{z}\end{vmatrix}}}
  where i, j, and k are the unit vectors for the x-, y-, and z-axes, respectively. This expands as follows:: 43 

  
    
      
        ∇
        ×
        
          F
        
        =
        
          (
          
            
              
                
                  ∂
                  
                    F
                    
                      z
                    
                  
                
                
                  ∂
                  y
                
              
            
            −
            
              
                
                  ∂
                  
                    F
                    
                      y
                    
                  
                
                
                  ∂
                  z
                
              
            
          
          )
        
        
          
            
              ı
              ^
            
          
        
        +
        
          (
          
            
              
                
                  ∂
                  
                    F
                    
                      x
                    
                  
                
                
                  ∂
                  z
                
              
            
            −
            
              
                
                  ∂
                  
                    F
                    
                      z
                    
                  
                
                
                  ∂
                  x
                
              
            
          
          )
        
        
          
            
              ȷ
              ^
            
          
        
        +
        
          (
          
            
              
                
                  ∂
                  
                    F
                    
                      y
                    
                  
                
                
                  ∂
                  x
                
              
            
            −
            
              
                
                  ∂
                  
                    F
                    
                      x
                    
                  
                
                
                  ∂
                  y
                
              
            
          
          )
        
        
          
            
              k
              ^
            
          
        
        =
        
          
            [
            
              
                
                  
                    
                      
                        ∂
                        
                          F
                          
                            z
                          
                        
                      
                      
                        ∂
                        y
                      
                    
                  
                  −
                  
                    
                      
                        ∂
                        
                          F
                          
                            y
                          
                        
                      
                      
                        ∂
                        z
                      
                    
                  
                
              
              
                
                  
                    
                      
                        ∂
                        
                          F
                          
                            x
                          
                        
                      
                      
                        ∂
                        z
                      
                    
                  
                  −
                  
                    
                      
                        ∂
                        
                          F
                          
                            z
                          
                        
                      
                      
                        ∂
                        x
                      
                    
                  
                
              
              
                
                  
                    
                      
                        ∂
                        
                          F
                          
                            y
                          
                        
                      
                      
                        ∂
                        x
                      
                    
                  
                  −
                  
                    
                      
                        ∂
                        
                          F
                          
                            x
                          
                        
                      
                      
                        ∂
                        y
                      
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle \nabla \times \mathbf {F} =\left({\frac {\partial F_{z}}{\partial y}}-{\frac {\partial F_{y}}{\partial z}}\right){\boldsymbol {\hat {\imath }}}+\left({\frac {\partial F_{x}}{\partial z}}-{\frac {\partial F_{z}}{\partial x}}\right){\boldsymbol {\hat {\jmath }}}+\left({\frac {\partial F_{y}}{\partial x}}-{\frac {\partial F_{x}}{\partial y}}\right){\boldsymbol {\hat {k}}}={\begin{bmatrix}{\frac {\partial F_{z}}{\partial y}}-{\frac {\partial F_{y}}{\partial z}}\\{\frac {\partial F_{x}}{\partial z}}-{\frac {\partial F_{z}}{\partial x}}\\{\frac {\partial F_{y}}{\partial x}}-{\frac {\partial F_{x}}{\partial y}}\end{bmatrix}}}
  Although expressed in terms of coordinates, the result is invariant under proper rotations of the coordinate axes but the result inverts under reflection.
In a general coordinate system, the curl is given by

  
    
      
        (
        ∇
        ×
        
          F
        
        
          )
          
            k
          
        
        =
        
          
            1
            
              g
            
          
        
        
          ε
          
            k
            ℓ
            m
          
        
        
          ∇
          
            ℓ
          
        
        
          F
          
            m
          
        
      
    
    {\displaystyle (\nabla \times \mathbf {F} )^{k}={\frac {1}{\sqrt {g}}}\varepsilon ^{k\ell m}\nabla _{\ell }F_{m}}
  where ε denotes the Levi-Civita tensor, ∇ the covariant derivative, 
  
    
      
        g
      
    
    {\displaystyle g}
   is the determinant of the metric tensor and the Einstein summation convention implies that repeated indices are summed over. Due to the symmetry of the Christoffel symbols participating in the covariant derivative, this expression reduces to the partial derivative:

  
    
      
        (
        ∇
        ×
        
          F
        
        )
        =
        
          
            1
            
              g
            
          
        
        
          
            R
          
          
            k
          
        
        
          ε
          
            k
            ℓ
            m
          
        
        
          ∂
          
            ℓ
          
        
        
          F
          
            m
          
        
      
    
    {\displaystyle (\nabla \times \mathbf {F} )={\frac {1}{\sqrt {g}}}\mathbf {R} _{k}\varepsilon ^{k\ell m}\partial _{\ell }F_{m}}
  where Rk are the local basis vectors. Equivalently, using the exterior derivative, the curl can be expressed as:

  
    
      
        ∇
        ×
        
          F
        
        =
        
          
            (
            
              ⋆
              
                
                  (
                
              
              
                
                  d
                
              
              
                
                  F
                
                
                  ♭
                
              
              
                
                  )
                
              
            
            )
          
          
            ♯
          
        
      
    
    {\displaystyle \nabla \times \mathbf {F} =\left(\star {\big (}{\mathrm {d} }\mathbf {F} ^{\flat }{\big )}\right)^{\sharp }}
  Here ♭ and ♯ are the musical isomorphisms, and ★ is the Hodge star operator. This formula shows how to calculate the curl of F in any coordinate system, and how to extend the curl to any oriented three-dimensional Riemannian manifold. Since this depends on a choice of orientation, curl is a chiral operation. In other words, if the orientation is reversed, then the direction of the curl is also reversed.


== Examples ==


=== Example 1 ===
The vector field

  
    
      
        
          F
        
        (
        x
        ,
        y
        ,
        z
        )
        =
        y
        
          
            
              ı
              ^
            
          
        
        −
        x
        
          
            
              ȷ
              ^
            
          
        
      
    
    {\displaystyle \mathbf {F} (x,y,z)=y{\boldsymbol {\hat {\imath }}}-x{\boldsymbol {\hat {\jmath }}}}
  can be decomposed as

  
    
      
        
          F
          
            x
          
        
        =
        y
        ,
        
          F
          
            y
          
        
        =
        −
        x
        ,
        
          F
          
            z
          
        
        =
        0.
      
    
    {\displaystyle F_{x}=y,F_{y}=-x,F_{z}=0.}
  Upon visual inspection, the field can be described as ""rotating"". If the vectors of the field were to represent a linear force acting on objects present at that point, and an object were to be placed inside the field, the object would start to rotate clockwise around itself. This is true regardless of where the object is placed.
Calculating the curl:

  
    
      
        ∇
        ×
        
          F
        
        =
        0
        
          
            
              ı
              ^
            
          
        
        +
        0
        
          
            
              ȷ
              ^
            
          
        
        +
        
          (
          
            
              
                ∂
                
                  ∂
                  x
                
              
            
            (
            −
            x
            )
            −
            
              
                ∂
                
                  ∂
                  y
                
              
            
            y
          
          )
        
        
          
            
              k
              ^
            
          
        
        =
        −
        2
        
          
            
              k
              ^
            
          
        
      
    
    {\displaystyle \nabla \times \mathbf {F} =0{\boldsymbol {\hat {\imath }}}+0{\boldsymbol {\hat {\jmath }}}+\left({\frac {\partial }{\partial x}}(-x)-{\frac {\partial }{\partial y}}y\right){\boldsymbol {\hat {k}}}=-2{\boldsymbol {\hat {k}}}}
  The resulting vector field describing the curl would at all points be pointing in the negative z direction. The results of this equation align with what could have been predicted using the right-hand rule using a right-handed coordinate system. Being a uniform vector field, the object described before would have the same rotational intensity regardless of where it was placed.


=== Example 2 ===
For the vector field

  
    
      
        
          F
        
        (
        x
        ,
        y
        ,
        z
        )
        =
        −
        
          x
          
            2
          
        
        
          
            
              ȷ
              ^
            
          
        
      
    
    {\displaystyle \mathbf {F} (x,y,z)=-x^{2}{\boldsymbol {\hat {\jmath }}}}
  the curl is not as obvious from the graph. However, taking the object in the previous example, and placing it anywhere on the line x = 3, the force exerted on the right side would be slightly greater than the force exerted on the left, causing it to rotate clockwise. Using the right-hand rule, it can be predicted that the resulting curl would be straight in the negative z direction. Inversely, if placed on x = −3, the object would rotate counterclockwise and the right-hand rule would result in a positive z direction.
Calculating the curl:

  
    
      
        
          ∇
        
        ×
        
          F
        
        =
        0
        
          
            
              ı
              ^
            
          
        
        +
        0
        
          
            
              ȷ
              ^
            
          
        
        +
        
          
            ∂
            
              ∂
              x
            
          
        
        
          (
          
            −
            
              x
              
                2
              
            
          
          )
        
        
          
            
              k
              ^
            
          
        
        =
        −
        2
        x
        
          
            
              k
              ^
            
          
        
        .
      
    
    {\displaystyle {\nabla }\times \mathbf {F} =0{\boldsymbol {\hat {\imath }}}+0{\boldsymbol {\hat {\jmath }}}+{\frac {\partial }{\partial x}}\left(-x^{2}\right){\boldsymbol {\hat {k}}}=-2x{\boldsymbol {\hat {k}}}.}
  The curl points in the negative z direction when x is positive and vice versa. In this field, the intensity of rotation would be greater as the object moves away from the plane x = 0.


=== Descriptive examples ===
In a vector field describing the linear velocities of each part of a rotating disk, the curl has the same value at all points, and this value turns out to be exactly two times the vectorial angular velocity of the disk (oriented as usual by the right-hand rule). More generally, for any flowing mass, the linear velocity vector field at each point of the mass flow has a curl (the vorticity of the flow at that point) equal to exactly two times the local vectorial angular velocity of the mass about the point.
For any solid object subject to an external physical force (such as gravity or the electromagnetic force), one may consider the vector field representing the infinitesimal force-per-unit-volume contributions acting at each of the points of the object. This force field may create a net torque on the object about its center of mass, and this torque turns out to be directly proportional and vectorially parallel to the (vector-valued) integral of the curl of the force field over the whole volume.
Of the four Maxwell's equations, two—Faraday's law and Ampère's law—can be compactly expressed using curl. Faraday's law states that the curl of an electric field is equal to the opposite of the time rate of change of the magnetic field, while Ampère's law relates the curl of the magnetic field to the current and the time rate of change of the electric field.


== Identities ==

In general curvilinear coordinates (not only in Cartesian coordinates), the curl of a cross product of vector fields v and F can be shown to be

  
    
      
        ∇
        ×
        
          (
          
            v
            ×
            F
          
          )
        
        =
        
          
            (
          
        
        
          (
          
            ∇
            ⋅
            F
          
          )
        
        +
        
          F
          ⋅
          ∇
        
        
          
            )
          
        
        
          v
        
        −
        
          
            (
          
        
        
          (
          
            ∇
            ⋅
            v
          
          )
        
        +
        
          v
          ⋅
          ∇
        
        
          
            )
          
        
        
          F
        
         
        .
      
    
    {\displaystyle \nabla \times \left(\mathbf {v\times F} \right)={\Big (}\left(\mathbf {\nabla \cdot F} \right)+\mathbf {F\cdot \nabla } {\Big )}\mathbf {v} -{\Big (}\left(\mathbf {\nabla \cdot v} \right)+\mathbf {v\cdot \nabla } {\Big )}\mathbf {F} \ .}
  Interchanging the vector field v and ∇ operator, we arrive at the cross product of a vector field with curl of a vector field:

  
    
      
        
          v
           
          ×
        
        
          (
          
            ∇
            ×
            F
          
          )
        
        =
        
          ∇
          
            
              F
            
          
        
        
          (
          
            v
            ⋅
            F
          
          )
        
        −
        
          (
          
            v
            ⋅
            ∇
          
          )
        
        
          F
        
         
        ,
      
    
    {\displaystyle \mathbf {v\ \times } \left(\mathbf {\nabla \times F} \right)=\nabla _{\mathbf {F} }\left(\mathbf {v\cdot F} \right)-\left(\mathbf {v\cdot \nabla } \right)\mathbf {F} \ ,}
  where ∇F is the Feynman subscript notation, which considers only the variation due to the vector field F (i.e., in this case, v is treated as being constant in space).
Another example is the curl of a curl of a vector field. It can be shown that in general coordinates

  
    
      
        ∇
        ×
        
          (
          
            ∇
            ×
            F
          
          )
        
        =
        
          ∇
        
        (
        
          ∇
          ⋅
          F
        
        )
        −
        
          ∇
          
            2
          
        
        
          F
        
         
        ,
      
    
    {\displaystyle \nabla \times \left(\mathbf {\nabla \times F} \right)=\mathbf {\nabla } (\mathbf {\nabla \cdot F} )-\nabla ^{2}\mathbf {F} \ ,}
  and this identity defines the vector Laplacian of F, symbolized as ∇2F.
The curl of the gradient of any scalar field φ is always the zero vector field

  
    
      
        ∇
        ×
        (
        ∇
        φ
        )
        =
        
          0
        
      
    
    {\displaystyle \nabla \times (\nabla \varphi )={\boldsymbol {0}}}
  which follows from the antisymmetry in the definition of the curl, and the symmetry of second derivatives.
The divergence of the curl of any vector field is equal to zero: 

  
    
      
        ∇
        ⋅
        (
        ∇
        ×
        
          F
        
        )
        =
        0.
      
    
    {\displaystyle \nabla \cdot (\nabla \times \mathbf {F} )=0.}
  If φ is a scalar valued function and F is a vector field, then

  
    
      
        ∇
        ×
        (
        φ
        
          F
        
        )
        =
        ∇
        φ
        ×
        
          F
        
        +
        φ
        ∇
        ×
        
          F
        
      
    
    {\displaystyle \nabla \times (\varphi \mathbf {F} )=\nabla \varphi \times \mathbf {F} +\varphi \nabla \times \mathbf {F} }
  


== Generalizations ==
The vector calculus operations of grad, curl, and div are most easily generalized in the context of differential forms, which involves a number of steps. In short, they correspond to the derivatives of 0-forms, 1-forms, and 2-forms, respectively. The geometric interpretation of curl as rotation corresponds to identifying bivectors (2-vectors) in 3 dimensions with the special orthogonal Lie algebra 
  
    
      
        
          
            s
            o
          
        
      
    
    {\displaystyle {\mathfrak {so}}}
  (3) of infinitesimal rotations (in coordinates, skew-symmetric 3 × 3 matrices), while representing rotations by vectors corresponds to identifying 1-vectors (equivalently, 2-vectors) and 
  
    
      
        
          
            s
            o
          
        
      
    
    {\displaystyle {\mathfrak {so}}}
  (3), these all being 3-dimensional spaces.


=== Differential forms ===

In 3 dimensions, a differential 0-form is simply a function f(x, y, z); a differential 1-form is the following expression, where the coefficients are functions:

  
    
      
        
          a
          
            1
          
        
        
        d
        x
        +
        
          a
          
            2
          
        
        
        d
        y
        +
        
          a
          
            3
          
        
        
        d
        z
        ;
      
    
    {\displaystyle a_{1}\,dx+a_{2}\,dy+a_{3}\,dz;}
  a differential 2-form is the formal sum, again with function coefficients:

  
    
      
        
          a
          
            12
          
        
        
        d
        x
        ∧
        d
        y
        +
        
          a
          
            13
          
        
        
        d
        x
        ∧
        d
        z
        +
        
          a
          
            23
          
        
        
        d
        y
        ∧
        d
        z
        ;
      
    
    {\displaystyle a_{12}\,dx\wedge dy+a_{13}\,dx\wedge dz+a_{23}\,dy\wedge dz;}
  and a differential 3-form is defined by a single term with one function as coefficient:

  
    
      
        
          a
          
            123
          
        
        
        d
        x
        ∧
        d
        y
        ∧
        d
        z
        .
      
    
    {\displaystyle a_{123}\,dx\wedge dy\wedge dz.}
  (Here the a-coefficients are real functions of three variables; the ""wedge products"", e.g. dx ∧ dy, can be interpreted as some kind of oriented area elements, dx ∧ dy = −dy ∧ dx, etc.)
The exterior derivative of a k-form in R3 is defined as the (k + 1)-form from above—and in Rn if, e.g.,

  
    
      
        
          ω
          
            (
            k
            )
          
        
        =
        
          ∑
          
            
              
                
                  
                    i
                    
                      1
                    
                  
                  <
                  
                    i
                    
                      2
                    
                  
                  <
                  ⋯
                  <
                  
                    i
                    
                      k
                    
                  
                
              
              
                ∀
                
                  
                    
                      i
                      
                        ν
                      
                    
                    ∈
                    1
                    ,
                    …
                    ,
                    n
                  
                
              
            
          
        
        
          a
          
            
              i
              
                1
              
            
            ,
            …
            ,
            
              i
              
                k
              
            
          
        
        
        d
        
          x
          
            
              i
              
                1
              
            
          
        
        ∧
        ⋯
        ∧
        d
        
          x
          
            
              i
              
                k
              
            
          
        
        ,
      
    
    {\displaystyle \omega ^{(k)}=\sum _{\scriptstyle {i_{1}<i_{2}<\cdots <i_{k}} \atop \forall \scriptstyle {i_{\nu }\in 1,\ldots ,n}}a_{i_{1},\ldots ,i_{k}}\,dx_{i_{1}}\wedge \cdots \wedge dx_{i_{k}},}
  then the exterior derivative d leads to

  
    
      
        d
        
          ω
          
            (
            k
            )
          
        
        =
        
          ∑
          
            
              
                
                  j
                  =
                  1
                
              
              
                
                  
                    i
                    
                      1
                    
                  
                  <
                  ⋯
                  <
                  
                    i
                    
                      k
                    
                  
                
              
            
          
          
            n
          
        
        
          
            
              ∂
              
                a
                
                  
                    i
                    
                      1
                    
                  
                  ,
                  …
                  ,
                  
                    i
                    
                      k
                    
                  
                
              
            
            
              ∂
              
                x
                
                  j
                
              
            
          
        
        
        d
        
          x
          
            j
          
        
        ∧
        d
        
          x
          
            
              i
              
                1
              
            
          
        
        ∧
        ⋯
        ∧
        d
        
          x
          
            
              i
              
                k
              
            
          
        
        .
      
    
    {\displaystyle d\omega ^{(k)}=\sum _{\scriptstyle {j=1} \atop \scriptstyle {i_{1}<\cdots <i_{k}}}^{n}{\frac {\partial a_{i_{1},\ldots ,i_{k}}}{\partial x_{j}}}\,dx_{j}\wedge dx_{i_{1}}\wedge \cdots \wedge dx_{i_{k}}.}
  The exterior derivative of a 1-form is therefore a 2-form, and that of a 2-form is a 3-form. On the other hand, because of the interchangeability of mixed derivatives, e.g. because of

  
    
      
        
          
            
              ∂
              
                2
              
            
            
              ∂
              x
              
              ∂
              y
            
          
        
        =
        
          
            
              ∂
              
                2
              
            
            
              ∂
              y
              
              ∂
              x
            
          
        
        ,
      
    
    {\displaystyle {\frac {\partial ^{2}}{\partial x\,\partial y}}={\frac {\partial ^{2}}{\partial y\,\partial x}},}
  the twofold application of the exterior derivative leads to 0.
Thus, denoting the space of k-forms by Ωk(R3) and the exterior derivative by d one gets a sequence:

  
    
      
        0
        
        
          
            ⟶
            d
          
        
        
        
          Ω
          
            0
          
        
        
          (
          
            
              R
            
            
              3
            
          
          )
        
        
        
          
            ⟶
            d
          
        
        
        
          Ω
          
            1
          
        
        
          (
          
            
              R
            
            
              3
            
          
          )
        
        
        
          
            ⟶
            d
          
        
        
        
          Ω
          
            2
          
        
        
          (
          
            
              R
            
            
              3
            
          
          )
        
        
        
          
            ⟶
            d
          
        
        
        
          Ω
          
            3
          
        
        
          (
          
            
              R
            
            
              3
            
          
          )
        
        
        
          
            ⟶
            d
          
        
        
        0.
      
    
    {\displaystyle 0\,{\overset {d}{\longrightarrow }}\;\Omega ^{0}\left(\mathbb {R} ^{3}\right)\,{\overset {d}{\longrightarrow }}\;\Omega ^{1}\left(\mathbb {R} ^{3}\right)\,{\overset {d}{\longrightarrow }}\;\Omega ^{2}\left(\mathbb {R} ^{3}\right)\,{\overset {d}{\longrightarrow }}\;\Omega ^{3}\left(\mathbb {R} ^{3}\right)\,{\overset {d}{\longrightarrow }}\,0.}
  Here Ωk(Rn) is the space of sections of the exterior algebra Λk(Rn) vector bundle over Rn, whose dimension is the binomial coefficient (nk); note that Ωk(R3) = 0 for k > 3 or k < 0. Writing only dimensions, one obtains a row of Pascal's triangle:

0 → 1 → 3 → 3 → 1 → 0;the 1-dimensional fibers correspond to scalar fields, and the 3-dimensional fibers to vector fields, as described below. Modulo suitable identifications, the three nontrivial occurrences of the exterior derivative correspond to grad, curl, and div.
Differential forms and the differential can be defined on any Euclidean space, or indeed any manifold, without any notion of a Riemannian metric. On a Riemannian manifold, or more generally pseudo-Riemannian manifold, k-forms can be identified with k-vector fields (k-forms are k-covector fields, and a pseudo-Riemannian metric gives an isomorphism between vectors and covectors), and on an oriented vector space with a nondegenerate form (an isomorphism between vectors and covectors), there is an isomorphism between k-vectors and (n − k)-vectors; in particular on (the tangent space of) an oriented pseudo-Riemannian manifold. Thus on an oriented pseudo-Riemannian manifold, one can interchange k-forms, k-vector fields, (n − k)-forms, and (n − k)-vector fields; this is known as Hodge duality. Concretely, on R3 this is given by:

1-forms and 1-vector fields: the 1-form ax dx + ay dy + az dz corresponds to the vector field (ax, ay, az).
1-forms and 2-forms: one replaces dx by the dual quantity dy ∧ dz (i.e., omit dx), and likewise, taking care of orientation: dy corresponds to dz ∧ dx = −dx ∧ dz, and dz corresponds to dx ∧ dy. Thus the form ax dx + ay dy + az dz corresponds to the ""dual form"" az dx ∧ dy + ay dz ∧ dx + ax dy ∧ dz.Thus, identifying 0-forms and 3-forms with scalar fields, and 1-forms and 2-forms with vector fields:

grad takes a scalar field (0-form) to a vector field (1-form);
curl takes a vector field (1-form) to a pseudovector field (2-form);
div takes a pseudovector field (2-form) to a pseudoscalar field (3-form)On the other hand, the fact that d2 = 0 corresponds to the identities

  
    
      
        ∇
        ×
        (
        ∇
        f
        )
        =
        
          0
        
      
    
    {\displaystyle \nabla \times (\nabla f)=\mathbf {0} }
  for any scalar field f, and

  
    
      
        ∇
        ⋅
        (
        ∇
        ×
        
          v
        
        )
        =
        0
      
    
    {\displaystyle \nabla \cdot (\nabla \times \mathbf {v} )=0}
  for any vector field v.
Grad and div generalize to all oriented pseudo-Riemannian manifolds, with the same geometric interpretation, because the spaces of 0-forms and n-forms at each point are always 1-dimensional and can be identified with scalar fields, while the spaces of 1-forms and (n − 1)-forms are always fiberwise n-dimensional and can be identified with vector fields.
Curl does not generalize in this way to 4 or more dimensions (or down to 2 or fewer dimensions); in 4 dimensions the dimensions are

0 → 1 → 4 → 6 → 4 → 1 → 0;so the curl of a 1-vector field (fiberwise 4-dimensional) is a 2-vector field, which at each point belongs to 6-dimensional vector space, and so one has

  
    
      
        
          ω
          
            (
            2
            )
          
        
        =
        
          ∑
          
            i
            <
            k
            =
            1
            ,
            2
            ,
            3
            ,
            4
          
        
        
          a
          
            i
            ,
            k
          
        
        
        d
        
          x
          
            i
          
        
        ∧
        d
        
          x
          
            k
          
        
        ,
      
    
    {\displaystyle \omega ^{(2)}=\sum _{i<k=1,2,3,4}a_{i,k}\,dx_{i}\wedge dx_{k},}
  which yields a sum of six independent terms, and cannot be identified with a 1-vector field. Nor can one meaningfully go from a 1-vector field to a 2-vector field to a 3-vector field (4 → 6 → 4), as taking the differential twice yields zero (d2 = 0). Thus there is no curl function from vector fields to vector fields in other dimensions arising in this way.
However, one can define a curl of a vector field as a 2-vector field in general, as described below.


=== Curl geometrically ===
2-vectors correspond to the exterior power Λ2V; in the presence of an inner product, in coordinates these are the skew-symmetric matrices, which are geometrically considered as the special orthogonal Lie algebra 
  
    
      
        
          
            s
            o
          
        
      
    
    {\displaystyle {\mathfrak {so}}}
  (V) of infinitesimal rotations. This has (n2) = 1/2n(n − 1) dimensions, and allows one to interpret the differential of a 1-vector field as its infinitesimal rotations. Only in 3 dimensions (or trivially in 0 dimensions) we have n = 1/2n(n − 1), which is the most elegant and common case. In 2 dimensions the curl of a vector field is not a vector field but a function, as 2-dimensional rotations are given by an angle (a scalar – an orientation is required to choose whether one counts clockwise or counterclockwise rotations as positive); this is not the div, but is rather perpendicular to it. In 3 dimensions the curl of a vector field is a vector field as is familiar (in 1 and 0 dimensions the curl of a vector field is 0, because there are no non-trivial 2-vectors), while in 4 dimensions the curl of a vector field is, geometrically, at each point an element of the 6-dimensional Lie algebra 
  
    
      
        
          
            s
            o
          
        
        (
        4
        )
      
    
    {\displaystyle {\mathfrak {so}}(4)}
  .
The curl of a 3-dimensional vector field which only depends on 2 coordinates (say x and y) is simply a vertical vector field (in the z direction) whose magnitude is the curl of the 2-dimensional vector field, as in the examples on this page.
Considering curl as a 2-vector field (an antisymmetric 2-tensor) has been used to generalize vector calculus and associated physics to higher dimensions.


== Inverse ==

In the case where the divergence of a vector field V is zero, a vector field W exists such that V = curl(W). This is why the magnetic field, characterized by zero divergence, can be expressed as the curl of a magnetic vector potential.
If W is a vector field with curl(W) = V, then adding any gradient vector field grad(f) to W will result in another vector field W + grad(f) such that curl(W + grad(f)) = V as well. This can be summarized by saying that the inverse curl of a three-dimensional vector field can be obtained up to an unknown irrotational field with the Biot–Savart law.


== See also ==
Helmholtz decomposition
Del in cylindrical and spherical coordinates
Vorticity


== References ==


== Further reading ==
Korn, Granino Arthur and Theresa M. Korn (January 2000). Mathematical Handbook for Scientists and Engineers: Definitions, Theorems, and Formulas for Reference and Review. New York: Dover Publications. pp. 157–160. ISBN 0-486-41147-8.
Schey, H. M. (1997). Div, Grad, Curl, and All That: An Informal Text on Vector Calculus. New York: Norton. ISBN 0-393-96997-5.


== External links ==
""Curl"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
""Multivariable calculus"". mathinsight.org. Retrieved February 12, 2022.
""Divergence and Curl: The Language of Maxwell's Equations, Fluid Flow, and More"". June 21, 2018. Archived from the original on 2021-11-24 – via YouTube."
e5819a0abf,Locus (mathematics),"In geometry, a locus (plural: loci) (Latin word for ""place"", ""location"") is a set of all points (commonly, a line, a line segment, a curve or a surface), whose location satisfies or is determined by one or more specified conditions.The set of the points that satisfy some property is often called the locus of a point satisfying this property. The use of the singular in this formulation is a witness that, until the end of the 19th century, mathematicians did not consider infinite sets. Instead of viewing lines and curves as sets of points, they viewed them as places where a point may be located or may move.


== History and philosophy ==
Until the beginning of the 20th century, a geometrical shape (for example a curve) was not considered as an infinite set of points; rather, it was considered as an entity on which a point may be located or on which it moves. Thus a circle in the Euclidean plane was defined as the locus of a point that is at a given distance of a fixed point, the center of the circle. In modern mathematics, similar concepts are more frequently reformulated by describing shapes as sets; for instance, one says that the circle is the set of points that are at a given distance from the center.In contrast to the set-theoretic view, the old formulation avoids considering infinite collections, as avoiding the actual infinite was an important philosophical position of earlier mathematicians.Once set theory became the universal basis over which the whole mathematics is built, the term of locus became rather old-fashioned. Nevertheless, the word is still widely used, mainly for a concise formulation, for example:

Critical locus, the set of the critical points of a differentiable function.
Zero locus or vanishing locus, the set of points where a function vanishes, in that it takes the value zero.
Singular locus, the set of the singular points of an algebraic variety.
Connectedness locus, the subset of the parameter set of a family of rational functions for which the Julia set of the function is connected.More recently, techniques such as the theory of schemes, and the use of category theory instead of set theory to give a foundation to mathematics, have returned to notions more like the original definition of a locus as an object in itself rather than as a set of points.


== Examples in plane geometry ==
Examples from plane geometry include:

The set of points equidistant from two points is a perpendicular bisector to the line segment connecting the two points.
The set of points equidistant from two intersecting lines is the union of their two  angle bisectors.
All conic sections are loci:Circle: the set of points for which the distance from a fixed point is constant (the radius).
Parabola: the set of points equidistant from a fixed point (the focus) and a line (the directrix).
Hyperbola: the set of points for each of which the absolute value of the difference between the distances to two given foci is a constant.
Ellipse: the set of points for each of which the sum of the distances to two given foci is a constantOther examples of loci appear in various areas of mathematics. For example, in complex dynamics, the Mandelbrot set is a subset of the complex plane that may be characterized as the connectedness locus of a family of polynomial maps.


== Proof of a locus ==
To prove a geometric shape is the correct locus for a given set of conditions, 
one generally divides the proof into two stages: the proof that all the points that satisfy the conditions are on the given shape, and the proof that all the points on the given shape satisfy the conditions.


== Examples ==


=== First example ===
Find the locus of a point P that has a given ratio of distances k = d1/d2 to two given points.
In this example k = 3, A(−1, 0) and B(0, 2) are chosen as the fixed points.

P(x, y) is a point of the locus

  
    
      
        ⇔
        
          |
        
        P
        A
        
          |
        
        =
        3
        
          |
        
        P
        B
        
          |
        
      
    
    {\displaystyle \Leftrightarrow |PA|=3|PB|}
  

  
    
      
        ⇔
        
          |
        
        P
        A
        
          
            |
          
          
            2
          
        
        =
        9
        
          |
        
        P
        B
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle \Leftrightarrow |PA|^{2}=9|PB|^{2}}
  

  
    
      
        ⇔
        (
        x
        +
        1
        
          )
          
            2
          
        
        +
        (
        y
        −
        0
        
          )
          
            2
          
        
        =
        9
        (
        x
        −
        0
        
          )
          
            2
          
        
        +
        9
        (
        y
        −
        2
        
          )
          
            2
          
        
      
    
    {\displaystyle \Leftrightarrow (x+1)^{2}+(y-0)^{2}=9(x-0)^{2}+9(y-2)^{2}}
  

  
    
      
        ⇔
        8
        (
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        )
        −
        2
        x
        −
        36
        y
        +
        35
        =
        0
      
    
    {\displaystyle \Leftrightarrow 8(x^{2}+y^{2})-2x-36y+35=0}
  

  
    
      
        ⇔
        
          
            (
            
              x
              −
              
                
                  1
                  8
                
              
            
            )
          
          
            2
          
        
        +
        
          
            (
            
              y
              −
              
                
                  9
                  4
                
              
            
            )
          
          
            2
          
        
        =
        
          
            45
            64
          
        
        .
      
    
    {\displaystyle \Leftrightarrow \left(x-{\frac {1}{8}}\right)^{2}+\left(y-{\frac {9}{4}}\right)^{2}={\frac {45}{64}}.}
  This equation represents a circle with center (1/8, 9/4) and radius 
  
    
      
        
          
            
              3
              8
            
          
        
        
          
            5
          
        
      
    
    {\displaystyle {\tfrac {3}{8}}{\sqrt {5}}}
  . It is the circle of Apollonius defined by these values of k, A, and B.


=== Second example ===

A triangle ABC has a fixed side [AB] with length c. 
Determine the locus of the third vertex C such that
the medians from A and C are orthogonal.
Choose an orthonormal coordinate system such that A(−c/2, 0), B(c/2, 0). 
C(x, y) is the variable third vertex. The center of [BC] is M((2x + c)/4, y/2). The median from C has a slope  y/x. The median AM has slope  2y/(2x + 3c).

C(x, y) is a point of the locus

  
    
      
        ⇔
      
    
    {\displaystyle \Leftrightarrow }
   the medians from A and C are orthogonal

  
    
      
        ⇔
        
          
            y
            x
          
        
        ⋅
        
          
            
              2
              y
            
            
              2
              x
              +
              3
              c
            
          
        
        =
        −
        1
      
    
    {\displaystyle \Leftrightarrow {\frac {y}{x}}\cdot {\frac {2y}{2x+3c}}=-1}
  

  
    
      
        ⇔
        2
        
          y
          
            2
          
        
        +
        2
        
          x
          
            2
          
        
        +
        3
        c
        x
        =
        0
      
    
    {\displaystyle \Leftrightarrow 2y^{2}+2x^{2}+3cx=0}
  

  
    
      
        ⇔
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        +
        (
        3
        c
        
          /
        
        2
        )
        x
        =
        0
      
    
    {\displaystyle \Leftrightarrow x^{2}+y^{2}+(3c/2)x=0}
  

  
    
      
        ⇔
        (
        x
        +
        3
        c
        
          /
        
        4
        
          )
          
            2
          
        
        +
        
          y
          
            2
          
        
        =
        9
        
          c
          
            2
          
        
        
          /
        
        16.
      
    
    {\displaystyle \Leftrightarrow (x+3c/4)^{2}+y^{2}=9c^{2}/16.}
  The locus of the vertex C is a circle with center (−3c/4, 0) and radius 3c/4.


=== Third example ===

A locus can also be defined by two associated curves depending on one common parameter. If the parameter varies, the intersection points of the associated curves describe the locus.
In the figure, the points K and L are fixed points on a given line m. The line k is a variable line through K. The line l through L is perpendicular to k. The angle 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   between k and m is the parameter.
k and  l are  associated lines depending on the common parameter. The variable intersection point S of k and l describes a circle. This circle is the locus of the intersection point of the two associated lines.


=== Fourth example ===
A locus of points need not be one-dimensional (as a circle, line, etc.). For example, the locus of the inequality 2x + 3y – 6 < 0 is the portion of the plane that is below the line of equation 2x + 3y – 6 = 0.


== See also ==
Algebraic variety
Curve
Line (geometry)
Set-builder notation
Shape (geometry)


== References =="
de280b4eb6,Language of mathematics,"The language of mathematics or mathematical language is an extension of the natural language (for example English) that is used in mathematics and in science for expressing results (scientific laws, theorems, proofs, logical deductions, etc) with concision, precision and unambiguity.


== Features ==
The main features of the mathematical language are the following.

Use of common words with a derived meaning, generally more specific and more precise. For example, ""or"" means ""one, the other or both"", while, in common language, ""both"" is sometimes included and sometimes not. Also, a ""line"" is straight and has zero width.
Use of common words with a meaning that is completely different from their common meaning. For example, a mathematical ring is not related to any other meaning of ""ring"". Real numbers and imaginary numbers are two sorts of numbers, none being more real or more imaginary than the others.
Use of neologisms. For example polynomial, homomorphism.
Use of symbols as words or phrases. For example, 
  
    
      
        A
        =
        B
      
    
    {\displaystyle A=B}
   and 
  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   are respectively read as ""
  
    
      
        A
      
    
    {\displaystyle A}
   equals 
  
    
      
        B
      
    
    {\displaystyle B}
  "" and ""for all 
  
    
      
        x
      
    
    {\displaystyle x}
  "".
Use of formulas as part of sentences. For example: ""
  
    
      
        e
        =
        m
        
          c
          
            2
          
        
      
    
    {\displaystyle e=mc^{2}}
   represents quantitatively the mass–energy equivalence."" A formula that is not included in a sentence is generally meaningless, since the meaning of the symbols may depend on the context: in "" 
  
    
      
        e
        =
        m
        
          c
          
            2
          
        
      
    
    {\displaystyle e=mc^{2}}
   "", this is the context that specifies that e is the energy of a physical body, m is its mass, and c is the speed of light.
Use of mathematical jargon that consists of phrases that are used for informal explanations or shorthands. For example, ""killing"" is often used in place of ""replacing with zero"", and this led to the use of assassinator and annihilator as technical words.


== Understanding mathematical text ==
The consequence of these features is that a mathematical text is generally not understandable without some prerequisite knowledge. For example the sentence ""a free module is a module that has a basis"" is perfectly correct, although it appears only as a grammatically correct nonsense, when one does not know the definitions of basis, module, and free module.
H. B. Williams, an electrophysiologist, wrote in 1927:

Now mathematics is both a body of truth and a special language, a language more carefully defined and more highly abstracted than our ordinary medium of thought and expression. Also it differs from ordinary languages in this important particular: it is subject to rules of manipulation. Once a statement is cast into mathematical form it may be manipulated in accordance with these rules and every configuration of the symbols will represent facts in harmony with and dependent on those contained in the original statement. Now this comes very close to what we conceive the action of the brain structures to be in performing intellectual acts with the symbols of ordinary language. In a sense, therefore, the mathematician has been able to perfect a device through which a part of the labor of logical thought is carried on outside the central nervous system with only that supervision which is requisite to manipulate the symbols in accordance with the rules.: 291 


== See also ==
Formulario mathematico
Formal language
History of mathematical notation
Mathematical notation
List of mathematical jargon


== References ==


== Further reading ==


=== Linguistic point of view ===
Keith Devlin (2000) The Language of Mathematics: Making the Invisible Visible, Holt Publishing.
Kay O'Halloran (2004) Mathematical Discourse: Language, Symbolism and Visual Images, Continuum.
R. L. E. Schwarzenberger (2000), ""The Language of Geometry"", in A Mathematical Spectrum Miscellany, Applied Probability Trust.


=== In education ===
F. Bruun, J. M. Diaz, & V. J. Dykes (2015) The Language of Mathematics. Teaching Children Mathematics, 21(9), 530–536.
J. O. Bullock (1994) Literacy in the Language of Mathematics. The American Mathematical Monthly, 101(8), 735–743.
L. Buschman (1995) Communicating in the Language of Mathematics. Teaching Children Mathematics, 1(6), 324–329.
B. R. Jones, P. F. Hopper, D. P. Franz, L. Knott, & T. A. Evitts (2008) Mathematics: A Second Language. The Mathematics Teacher, 102(4), 307–312. JSTOR.
C. Morgan (1996) “The Language of Mathematics”: Towards a Critical Analysis of Mathematics Texts. For the Learning of Mathematics, 16(3), 2–10.
J. K. Moulton (1946) The Language of Mathematics. The Mathematics Teacher, 39(3), 131–133."
26c1c69ddf,Regularization (mathematics),"In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be ""simpler"". It is often used to obtain results for ill-posed problems or to prevent overfitting.Although regularization procedures can be divided in many ways, the following delineation is particularly helpful:

Explicit regularization is regularization whenever one explicitly adds a term to the optimization problem.  These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.
Implicit regularization is all other forms of regularization.  This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees).In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one chooses to be more addictive to the data or to enforce generalization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. In practice, one usually tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition.
In machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set and not the training data.One of the earliest uses of regularization is Tikhonov regularization, related to the method of least squares.


== Classification ==
Empirical learning of classifiers (from a finite data set) is always an underdetermined problem, because it attempts to infer a function of any 
  
    
      
        x
      
    
    {\displaystyle x}
   given only examples 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        .
        .
        .
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},x_{2},...x_{n}}
  .
A regularization term (or regularizer) 
  
    
      
        R
        (
        f
        )
      
    
    {\displaystyle R(f)}
   is added to a loss function:

  
    
      
        
          min
          
            f
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        V
        (
        f
        (
        
          x
          
            i
          
        
        )
        ,
        
          y
          
            i
          
        
        )
        +
        λ
        R
        (
        f
        )
      
    
    {\displaystyle \min _{f}\sum _{i=1}^{n}V(f(x_{i}),y_{i})+\lambda R(f)}
  where 
  
    
      
        V
      
    
    {\displaystyle V}
   is an underlying loss function that describes the cost of predicting 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   when the label is 
  
    
      
        y
      
    
    {\displaystyle y}
  , such as the square loss or hinge loss; and 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is a parameter which controls the importance of the regularization term. 
  
    
      
        R
        (
        f
        )
      
    
    {\displaystyle R(f)}
   is typically chosen to impose a penalty on the complexity of 
  
    
      
        f
      
    
    {\displaystyle f}
  . Concrete notions of complexity used include restrictions for smoothness and bounds on the vector space norm.A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution (as depicted in the figure above, where the green function, the simpler one, may be preferred). From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.Regularization can serve multiple purposes, including learning simpler models, inducing models to be sparse and introducing group structure into the learning problem.
The same idea arose in many fields of science. A simple form of regularization applied to integral equations (Tikhonov regularization)  is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, non-linear regularization methods, including total variation regularization, have become popular.


=== Generalization ===

Regularization can be motivated as a technique to improve the generalizability of a learned model.
The goal of this learning problem is to find a function that fits or predicts the outcome (label) that minimizes the expected error over all possible inputs and labels. The expected error of a function 
  
    
      
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{n}}
   is:

  
    
      
        I
        [
        
          f
          
            n
          
        
        ]
        =
        
          ∫
          
            X
            ×
            Y
          
        
        V
        (
        
          f
          
            n
          
        
        (
        x
        )
        ,
        y
        )
        ρ
        (
        x
        ,
        y
        )
        
        d
        x
        
        d
        y
      
    
    {\displaystyle I[f_{n}]=\int _{X\times Y}V(f_{n}(x),y)\rho (x,y)\,dx\,dy}
  where 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are the domains of input data 
  
    
      
        x
      
    
    {\displaystyle x}
   and their labels 
  
    
      
        y
      
    
    {\displaystyle y}
   respectively.
Typically in learning problems, only a subset of input data and labels are available, measured with some noise. Therefore, the expected error is unmeasurable, and the best surrogate available is the empirical error over the 
  
    
      
        N
      
    
    {\displaystyle N}
   available samples:

  
    
      
        
          I
          
            S
          
        
        [
        
          f
          
            n
          
        
        ]
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        V
        (
        
          f
          
            n
          
        
        (
        
          
            
              
                x
                ^
              
            
          
          
            i
          
        
        )
        ,
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
        )
      
    
    {\displaystyle I_{S}[f_{n}]={\frac {1}{n}}\sum _{i=1}^{N}V(f_{n}({\hat {x}}_{i}),{\hat {y}}_{i})}
  Without bounds on the complexity of the function space (formally, the reproducing kernel Hilbert space) available, a model will be learned that incurs zero loss on the surrogate empirical error. If measurements (e.g. of 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  ) were made with noise, this model may suffer from overfitting and display poor expected error. Regularization introduces a penalty for exploring certain regions of the function space used to build the model, which can improve generalization.


== Tikhonov regularization ==

These techniques are named for Andrey Nikolayevich Tikhonov, who applied regularization to integral equations and made important contributions in many other areas.
When learning a linear function 
  
    
      
        f
      
    
    {\displaystyle f}
  , characterized by an unknown vector 
  
    
      
        w
      
    
    {\displaystyle w}
   such that 
  
    
      
        f
        (
        x
        )
        =
        w
        ⋅
        x
      
    
    {\displaystyle f(x)=w\cdot x}
  , one can add the 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
  -norm of the vector 
  
    
      
        w
      
    
    {\displaystyle w}
   to the loss expression in order to prefer solutions with smaller norms. Tikhonov regularization is one of the most common forms. It is also known as ridge regression. It is expressed as:

  
    
      
        
          min
          
            w
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        V
        (
        
          
            
              
                x
                ^
              
            
          
          
            i
          
        
        ⋅
        w
        ,
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
        )
        +
        λ
        ‖
        w
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \min _{w}\sum _{i=1}^{n}V({\hat {x}}_{i}\cdot w,{\hat {y}}_{i})+\lambda \|w\|_{2}^{2}}
  ,where 
  
    
      
        (
        
          
            
              
                x
                ^
              
            
          
          
            i
          
        
        ,
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
        )
        ,
        
        1
        ≤
        i
        ≤
        n
        ,
      
    
    {\displaystyle ({\hat {x}}_{i},{\hat {y}}_{i}),\,1\leq i\leq n,}
   would represent samples used for training.
In the case of a general function, the norm of the function in its reproducing kernel Hilbert space is:

  
    
      
        
          min
          
            f
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        V
        (
        f
        (
        
          
            
              
                x
                ^
              
            
          
          
            i
          
        
        )
        ,
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
        )
        +
        λ
        ‖
        f
        
          ‖
          
            
              H
            
          
          
            2
          
        
      
    
    {\displaystyle \min _{f}\sum _{i=1}^{n}V(f({\hat {x}}_{i}),{\hat {y}}_{i})+\lambda \|f\|_{\mathcal {H}}^{2}}
  As the 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
   norm is differentiable, learning can be advanced by gradient descent.


=== Tikhonov-regularized least squares ===
The learning problem with the least squares loss function and Tikhonov regularization can be solved analytically. Written in matrix form, the optimal 
  
    
      
        w
      
    
    {\displaystyle w}
   is the one for which the gradient of the loss function with respect to 
  
    
      
        w
      
    
    {\displaystyle w}
   is 0.

  
    
      
        
          min
          
            w
          
        
        
          
            1
            n
          
        
        (
        
          
            
              X
              ^
            
          
        
        w
        −
        Y
        
          )
          
            T
          
        
        (
        
          
            
              X
              ^
            
          
        
        w
        −
        Y
        )
        +
        λ
        ‖
        w
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \min _{w}{\frac {1}{n}}({\hat {X}}w-Y)^{T}({\hat {X}}w-Y)+\lambda \|w\|_{2}^{2}}
  
  
    
      
        
          ∇
          
            w
          
        
        =
        
          
            2
            n
          
        
        
          
            
              
                X
                ^
              
            
          
          
            T
          
        
        (
        
          
            
              X
              ^
            
          
        
        w
        −
        Y
        )
        +
        2
        λ
        w
      
    
    {\displaystyle \nabla _{w}={\frac {2}{n}}{\hat {X}}^{T}({\hat {X}}w-Y)+2\lambda w}
  
  
    
      
        0
        =
        
          
            
              
                X
                ^
              
            
          
          
            T
          
        
        (
        
          
            
              X
              ^
            
          
        
        w
        −
        Y
        )
        +
        n
        λ
        w
      
    
    {\displaystyle 0={\hat {X}}^{T}({\hat {X}}w-Y)+n\lambda w}
      (first-order condition)
  
    
      
        w
        =
        (
        
          
            
              
                X
                ^
              
            
          
          
            T
          
        
        
          
            
              X
              ^
            
          
        
        +
        λ
        n
        I
        
          )
          
            −
            1
          
        
        (
        
          
            
              
                X
                ^
              
            
          
          
            T
          
        
        Y
        )
      
    
    {\displaystyle w=({\hat {X}}^{T}{\hat {X}}+\lambda nI)^{-1}({\hat {X}}^{T}Y)}
  By construction of the optimization problem, other values of 
  
    
      
        w
      
    
    {\displaystyle w}
   give larger values for the loss function. This can be verified by examining the second derivative 
  
    
      
        
          ∇
          
            w
            w
          
        
      
    
    {\displaystyle \nabla _{ww}}
  .
During training, this algorithm takes 
  
    
      
        O
        (
        
          d
          
            3
          
        
        +
        n
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{3}+nd^{2})}
   time. The terms correspond to the matrix inversion and calculating 
  
    
      
        
          X
          
            T
          
        
        X
      
    
    {\displaystyle X^{T}X}
  , respectively. Testing takes 
  
    
      
        O
        (
        n
        d
        )
      
    
    {\displaystyle O(nd)}
   time.


== Early stopping ==

Early stopping can be viewed as regularization in time. Intuitively, a training procedure such as gradient descent tends to learn more and more complex functions with increasing iterations. By regularizing for time, model complexity can be controlled, improving generalization.
Early stopping is implemented using one data set for training, one statistically independent data set for validation and another for testing. The model is trained until performance on the validation set no longer improves and then applied to the test set.


=== Theoretical motivation in least squares ===
Consider the finite approximation of Neumann series for an invertible matrix A where 
  
    
      
        ‖
        I
        −
        A
        ‖
        <
        1
      
    
    {\displaystyle \|I-A\|<1}
  :

  
    
      
        
          ∑
          
            i
            =
            0
          
          
            T
            −
            1
          
        
        (
        I
        −
        A
        
          )
          
            i
          
        
        ≈
        
          A
          
            −
            1
          
        
      
    
    {\displaystyle \sum _{i=0}^{T-1}(I-A)^{i}\approx A^{-1}}
  This can be used to approximate the analytical solution of unregularized least squares, if γ is introduced to ensure the norm is less than one.

  
    
      
        
          w
          
            T
          
        
        =
        
          
            γ
            n
          
        
        
          ∑
          
            i
            =
            0
          
          
            T
            −
            1
          
        
        (
        I
        −
        
          
            γ
            n
          
        
        
          
            
              
                X
                ^
              
            
          
          
            T
          
        
        
          
            
              X
              ^
            
          
        
        
          )
          
            i
          
        
        
          
            
              
                X
                ^
              
            
          
          
            T
          
        
        
          
            
              Y
              ^
            
          
        
      
    
    {\displaystyle w_{T}={\frac {\gamma }{n}}\sum _{i=0}^{T-1}(I-{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {X}})^{i}{\hat {X}}^{T}{\hat {Y}}}
  The exact solution to the unregularized least squares learning problem minimizes the empirical error, but may fail. By limiting T, the only free parameter in the algorithm above, the problem is regularized for time, which may improve its generalization.
The algorithm above is equivalent to restricting the number of gradient descent iterations for the empirical risk

  
    
      
        
          I
          
            s
          
        
        [
        w
        ]
        =
        
          
            1
            
              2
              n
            
          
        
        ‖
        
          
            
              X
              ^
            
          
        
        w
        −
        
          
            
              Y
              ^
            
          
        
        
          ‖
          
            
              
                R
              
              
                n
              
            
          
          
            2
          
        
      
    
    {\displaystyle I_{s}[w]={\frac {1}{2n}}\|{\hat {X}}w-{\hat {Y}}\|_{\mathbb {R} ^{n}}^{2}}
  with the gradient descent update:

  
    
      
        
          
            
              
                
                  w
                  
                    0
                  
                
              
              
                
                =
                0
              
            
            
              
                
                  w
                  
                    t
                    +
                    1
                  
                
              
              
                
                =
                (
                I
                −
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      X
                      ^
                    
                  
                
                )
                
                  w
                  
                    t
                  
                
                +
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      Y
                      ^
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}w_{0}&=0\\w_{t+1}&=(I-{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {X}})w_{t}+{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {Y}}\end{aligned}}}
  The base case is trivial. The inductive case is proved as follows:

  
    
      
        
          
            
              
                
                  w
                  
                    T
                  
                
              
              
                
                =
                (
                I
                −
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      X
                      ^
                    
                  
                
                )
                
                  
                    γ
                    n
                  
                
                
                  ∑
                  
                    i
                    =
                    0
                  
                  
                    T
                    −
                    2
                  
                
                (
                I
                −
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      X
                      ^
                    
                  
                
                
                  )
                  
                    i
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      Y
                      ^
                    
                  
                
                +
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      Y
                      ^
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    γ
                    n
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    T
                    −
                    1
                  
                
                (
                I
                −
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      X
                      ^
                    
                  
                
                
                  )
                  
                    i
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      Y
                      ^
                    
                  
                
                +
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      Y
                      ^
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    γ
                    n
                  
                
                
                  ∑
                  
                    i
                    =
                    0
                  
                  
                    T
                    −
                    1
                  
                
                (
                I
                −
                
                  
                    γ
                    n
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      X
                      ^
                    
                  
                
                
                  )
                  
                    i
                  
                
                
                  
                    
                      
                        X
                        ^
                      
                    
                  
                  
                    T
                  
                
                
                  
                    
                      Y
                      ^
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}w_{T}&=(I-{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {X}}){\frac {\gamma }{n}}\sum _{i=0}^{T-2}(I-{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {X}})^{i}{\hat {X}}^{T}{\hat {Y}}+{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {Y}}\\&={\frac {\gamma }{n}}\sum _{i=1}^{T-1}(I-{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {X}})^{i}{\hat {X}}^{T}{\hat {Y}}+{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {Y}}\\&={\frac {\gamma }{n}}\sum _{i=0}^{T-1}(I-{\frac {\gamma }{n}}{\hat {X}}^{T}{\hat {X}})^{i}{\hat {X}}^{T}{\hat {Y}}\end{aligned}}}
  


== Regularizers for sparsity ==
Assume that a dictionary 
  
    
      
        
          ϕ
          
            j
          
        
      
    
    {\displaystyle \phi _{j}}
   with dimension 
  
    
      
        p
      
    
    {\displaystyle p}
   is given such that a function in the function space can be expressed as:

  
    
      
        f
        (
        x
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            p
          
        
        
          ϕ
          
            j
          
        
        (
        x
        )
        
          w
          
            j
          
        
      
    
    {\displaystyle f(x)=\sum _{j=1}^{p}\phi _{j}(x)w_{j}}
  
Enforcing a sparsity constraint on 
  
    
      
        w
      
    
    {\displaystyle w}
   can lead to simpler and more interpretable models. This is useful in many real-life applications such as computational biology. An example is developing a simple predictive test for a disease in order to minimize the cost of performing medical tests while maximizing predictive power.
A sensible sparsity constraint is the 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L_{0}}
   norm 
  
    
      
        ‖
        w
        
          ‖
          
            0
          
        
      
    
    {\displaystyle \|w\|_{0}}
  , defined as the number of non-zero elements in 
  
    
      
        w
      
    
    {\displaystyle w}
  . Solving a 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L_{0}}
   regularized learning problem, however, has been demonstrated to be NP-hard.The 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm (see also Norms) can be used to approximate the optimal 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L_{0}}
   norm via convex relaxation. It can be shown that the 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm induces sparsity. In the case of least squares, this problem is known as LASSO in statistics and basis pursuit in signal processing.

  
    
      
        
          min
          
            w
            ∈
            
              
                R
              
              
                p
              
            
          
        
        
          
            1
            n
          
        
        ‖
        
          
            
              X
              ^
            
          
        
        w
        −
        
          
            
              Y
              ^
            
          
        
        
          ‖
          
            2
          
        
        +
        λ
        ‖
        w
        
          ‖
          
            1
          
        
      
    
    {\displaystyle \min _{w\in \mathbb {R} ^{p}}{\frac {1}{n}}\|{\hat {X}}w-{\hat {Y}}\|^{2}+\lambda \|w\|_{1}}
  

  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   regularization can occasionally produce non-unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. This can be problematic for certain applications, and is overcome by combining 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
     with 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
   regularization in elastic net regularization, which takes the following form:

  
    
      
        
          min
          
            w
            ∈
            
              
                R
              
              
                p
              
            
          
        
        
          
            1
            n
          
        
        ‖
        
          
            
              X
              ^
            
          
        
        w
        −
        
          
            
              Y
              ^
            
          
        
        
          ‖
          
            2
          
        
        +
        λ
        (
        α
        ‖
        w
        
          ‖
          
            1
          
        
        +
        (
        1
        −
        α
        )
        ‖
        w
        
          ‖
          
            2
          
          
            2
          
        
        )
        ,
        α
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \min _{w\in \mathbb {R} ^{p}}{\frac {1}{n}}\|{\hat {X}}w-{\hat {Y}}\|^{2}+\lambda (\alpha \|w\|_{1}+(1-\alpha )\|w\|_{2}^{2}),\alpha \in [0,1]}
  Elastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights.
Elastic net regularization is commonly used in practice and is implemented in many machine learning libraries.


=== Proximal methods ===
While the 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm does not result in an NP-hard problem, the 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm is convex but is not strictly differentiable due to the kink at x = 0. Subgradient methods which rely on the subderivative can be used to solve 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   regularized learning problems. However, faster convergence can be achieved through proximal methods.
For a problem 
  
    
      
        
          min
          
            w
            ∈
            H
          
        
        F
        (
        w
        )
        +
        R
        (
        w
        )
      
    
    {\displaystyle \min _{w\in H}F(w)+R(w)}
   such that 
  
    
      
        F
      
    
    {\displaystyle F}
   is convex, continuous, differentiable, with Lipschitz continuous gradient (such as the least squares loss function), and 
  
    
      
        R
      
    
    {\displaystyle R}
   is convex, continuous, and proper, then the proximal method to solve the problem is as follows. First define the proximal operator

  
    
      
        
          prox
          
            R
          
        
        ⁡
        (
        v
        )
        =
        
          argmin
          
            w
            ∈
            
              
                R
              
              
                D
              
            
          
        
        ⁡
        {
        R
        (
        w
        )
        +
        
          
            1
            2
          
        
        ‖
        w
        −
        v
        
          ‖
          
            2
          
        
        }
        ,
      
    
    {\displaystyle \operatorname {prox} _{R}(v)=\operatorname {argmin} \limits _{w\in \mathbb {R} ^{D}}\{R(w)+{\frac {1}{2}}\|w-v\|^{2}\},}
  and then iterate

  
    
      
        
          w
          
            k
            +
            1
          
        
        =
        
          prox
          
            γ
            ,
            R
          
        
        ⁡
        (
        
          w
          
            k
          
        
        −
        γ
        ∇
        F
        (
        
          w
          
            k
          
        
        )
        )
      
    
    {\displaystyle w_{k+1}=\operatorname {prox} \limits _{\gamma ,R}(w_{k}-\gamma \nabla F(w_{k}))}
  The proximal method iteratively performs gradient descent and then projects the result back into the space permitted by 
  
    
      
        R
      
    
    {\displaystyle R}
  .
When 
  
    
      
        R
      
    
    {\displaystyle R}
   is the 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   regularizer, the proximal operator is equivalent to the soft-thresholding operator,

  
    
      
        
          S
          
            λ
          
        
        (
        v
        )
        f
        (
        n
        )
        =
        
          
            {
            
              
                
                  
                    v
                    
                      i
                    
                  
                  −
                  λ
                  ,
                
                
                  
                    if 
                  
                  
                    v
                    
                      i
                    
                  
                  >
                  λ
                
              
              
                
                  0
                  ,
                
                
                  
                    if 
                  
                  
                    v
                    
                      i
                    
                  
                  ∈
                  [
                  −
                  λ
                  ,
                  λ
                  ]
                
              
              
                
                  
                    v
                    
                      i
                    
                  
                  +
                  λ
                  ,
                
                
                  
                    if 
                  
                  
                    v
                    
                      i
                    
                  
                  <
                  −
                  λ
                
              
            
            
          
        
      
    
    {\displaystyle S_{\lambda }(v)f(n)={\begin{cases}v_{i}-\lambda ,&{\text{if }}v_{i}>\lambda \\0,&{\text{if }}v_{i}\in [-\lambda ,\lambda ]\\v_{i}+\lambda ,&{\text{if }}v_{i}<-\lambda \end{cases}}}
  This allows for efficient computation.


=== Group sparsity without overlaps ===
Groups of features can be regularized by a sparsity constraint, which can be useful for expressing certain prior knowledge into an optimization problem.
In the case of a linear model with non-overlapping known groups, a regularizer can be defined:

  
    
      
        R
        (
        w
        )
        =
        
          ∑
          
            g
            =
            1
          
          
            G
          
        
        ‖
        
          w
          
            g
          
        
        
          ‖
          
            2
          
        
        ,
      
    
    {\displaystyle R(w)=\sum _{g=1}^{G}\|w_{g}\|_{2},}
   where 
  
    
      
        ‖
        
          w
          
            g
          
        
        
          ‖
          
            2
          
        
        =
        
          
            
              ∑
              
                j
                =
                1
              
              
                
                  |
                
                
                  G
                  
                    g
                  
                
                
                  |
                
              
            
            (
            
              w
              
                g
              
              
                j
              
            
            
              )
              
                2
              
            
          
        
      
    
    {\displaystyle \|w_{g}\|_{2}={\sqrt {\sum _{j=1}^{|G_{g}|}(w_{g}^{j})^{2}}}}
  This can be viewed as inducing a regularizer over the 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
   norm over members of each group followed by an 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm over groups.
This can be solved by the proximal method, where the proximal operator is a block-wise soft-thresholding function:

  
    
      
        
          prox
          
            λ
            ,
            R
            ,
            g
          
        
        ⁡
        (
        
          w
          
            g
          
        
        )
        =
        
          
            {
            
              
                
                  (
                  1
                  −
                  
                    
                      λ
                      
                        ‖
                        
                          w
                          
                            g
                          
                        
                        
                          ‖
                          
                            2
                          
                        
                      
                    
                  
                  )
                  
                    w
                    
                      g
                    
                  
                  ,
                
                
                  
                    if 
                  
                  ‖
                  
                    w
                    
                      g
                    
                  
                  
                    ‖
                    
                      2
                    
                  
                  >
                  λ
                
              
              
                
                  0
                  ,
                
                
                  
                    if 
                  
                  ‖
                  
                    w
                    
                      g
                    
                  
                  
                    ‖
                    
                      2
                    
                  
                  ≤
                  λ
                
              
            
            
          
        
      
    
    {\displaystyle \operatorname {prox} \limits _{\lambda ,R,g}(w_{g})={\begin{cases}(1-{\frac {\lambda }{\|w_{g}\|_{2}}})w_{g},&{\text{if }}\|w_{g}\|_{2}>\lambda \\0,&{\text{if }}\|w_{g}\|_{2}\leq \lambda \end{cases}}}
  


=== Group sparsity with overlaps ===
The algorithm described for group sparsity without overlaps can be applied to the case where groups do overlap, in certain situations. This will likely result in some groups with all zero elements, and other groups with some non-zero and some zero elements.
If it is desired to preserve the group structure, a new regularizer can be defined:

  
    
      
        R
        (
        w
        )
        =
        inf
        
          {
          
            
              ∑
              
                g
                =
                1
              
              
                G
              
            
            ‖
            
              w
              
                g
              
            
            
              ‖
              
                2
              
            
            :
            w
            =
            
              ∑
              
                g
                =
                1
              
              
                G
              
            
            
              
                
                  
                    w
                    ¯
                  
                
              
              
                g
              
            
          
          }
        
      
    
    {\displaystyle R(w)=\inf \left\{\sum _{g=1}^{G}\|w_{g}\|_{2}:w=\sum _{g=1}^{G}{\bar {w}}_{g}\right\}}
  For each 
  
    
      
        
          w
          
            g
          
        
      
    
    {\displaystyle w_{g}}
  , 
  
    
      
        
          
            
              
                w
                ¯
              
            
          
          
            g
          
        
      
    
    {\displaystyle {\bar {w}}_{g}}
   is defined as the vector such that the restriction of 
  
    
      
        
          
            
              
                w
                ¯
              
            
          
          
            g
          
        
      
    
    {\displaystyle {\bar {w}}_{g}}
   to the group 
  
    
      
        g
      
    
    {\displaystyle g}
   equals 
  
    
      
        
          w
          
            g
          
        
      
    
    {\displaystyle w_{g}}
   and all other entries of 
  
    
      
        
          
            
              
                w
                ¯
              
            
          
          
            g
          
        
      
    
    {\displaystyle {\bar {w}}_{g}}
   are zero. The regularizer finds the optimal disintegration of 
  
    
      
        w
      
    
    {\displaystyle w}
   into parts. It can be viewed as duplicating all elements that exist in multiple groups. Learning problems with this regularizer can also be solved with the proximal method with a complication. The proximal operator cannot be computed in closed form, but can be effectively solved iteratively, inducing an inner iteration within the proximal method iteration.


== Regularizers for semi-supervised learning ==

When labels are more expensive to gather than input examples, semi-supervised learning can be useful. Regularizers have been designed to guide learning algorithms to learn models that respect the structure of unsupervised training samples. If a symmetric weight matrix 
  
    
      
        W
      
    
    {\displaystyle W}
   is given, a regularizer can be defined:

  
    
      
        R
        (
        f
        )
        =
        
          ∑
          
            i
            ,
            j
          
        
        
          w
          
            i
            j
          
        
        (
        f
        (
        
          x
          
            i
          
        
        )
        −
        f
        (
        
          x
          
            j
          
        
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle R(f)=\sum _{i,j}w_{ij}(f(x_{i})-f(x_{j}))^{2}}
  If 
  
    
      
        
          W
          
            i
            j
          
        
      
    
    {\displaystyle W_{ij}}
   encodes the result of some distance metric for points 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   and 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
  , it is desirable that 
  
    
      
        f
        (
        
          x
          
            i
          
        
        )
        ≈
        f
        (
        
          x
          
            j
          
        
        )
      
    
    {\displaystyle f(x_{i})\approx f(x_{j})}
  . This regularizer captures this intuition, and is equivalent to:

  
    
      
        R
        (
        f
        )
        =
        
          
            
              
                f
                ¯
              
            
          
          
            T
          
        
        L
        
          
            
              f
              ¯
            
          
        
      
    
    {\displaystyle R(f)={\bar {f}}^{T}L{\bar {f}}}
   where 
  
    
      
        L
        =
        D
        −
        W
      
    
    {\displaystyle L=D-W}
   is the Laplacian matrix of the graph induced by 
  
    
      
        W
      
    
    {\displaystyle W}
  .The optimization problem 
  
    
      
        
          min
          
            f
            ∈
            
              
                R
              
              
                m
              
            
          
        
        R
        (
        f
        )
        ,
        m
        =
        u
        +
        l
      
    
    {\displaystyle \min _{f\in \mathbb {R} ^{m}}R(f),m=u+l}
   can be solved analytically if the constraint 
  
    
      
        f
        (
        
          x
          
            i
          
        
        )
        =
        
          y
          
            i
          
        
      
    
    {\displaystyle f(x_{i})=y_{i}}
   is applied for all supervised samples. The labeled part of the vector 
  
    
      
        f
      
    
    {\displaystyle f}
   is therefore obvious. The unlabeled part of 
  
    
      
        f
      
    
    {\displaystyle f}
   is solved for by:

  
    
      
        
          min
          
            
              f
              
                u
              
            
            ∈
            
              
                R
              
              
                u
              
            
          
        
        
          f
          
            T
          
        
        L
        f
        =
        
          min
          
            
              f
              
                u
              
            
            ∈
            
              
                R
              
              
                u
              
            
          
        
        {
        
          f
          
            u
          
          
            T
          
        
        
          L
          
            u
            u
          
        
        
          f
          
            u
          
        
        +
        
          f
          
            l
          
          
            T
          
        
        
          L
          
            l
            u
          
        
        
          f
          
            u
          
        
        +
        
          f
          
            u
          
          
            T
          
        
        
          L
          
            u
            l
          
        
        
          f
          
            l
          
        
        }
      
    
    {\displaystyle \min _{f_{u}\in \mathbb {R} ^{u}}f^{T}Lf=\min _{f_{u}\in \mathbb {R} ^{u}}\{f_{u}^{T}L_{uu}f_{u}+f_{l}^{T}L_{lu}f_{u}+f_{u}^{T}L_{ul}f_{l}\}}
  
  
    
      
        
          ∇
          
            
              f
              
                u
              
            
          
        
        =
        2
        
          L
          
            u
            u
          
        
        
          f
          
            u
          
        
        +
        2
        
          L
          
            u
            l
          
        
        Y
      
    
    {\displaystyle \nabla _{f_{u}}=2L_{uu}f_{u}+2L_{ul}Y}
  
  
    
      
        
          f
          
            u
          
        
        =
        
          L
          
            u
            u
          
          
            †
          
        
        (
        
          L
          
            u
            l
          
        
        Y
        )
      
    
    {\displaystyle f_{u}=L_{uu}^{\dagger }(L_{ul}Y)}
  Note that the pseudo-inverse can be taken because 
  
    
      
        
          L
          
            u
            l
          
        
      
    
    {\displaystyle L_{ul}}
   has the same range as 
  
    
      
        
          L
          
            u
            u
          
        
      
    
    {\displaystyle L_{uu}}
  .


== Regularizers for multitask learning ==
In the case of multitask learning, 
  
    
      
        T
      
    
    {\displaystyle T}
   problems are considered simultaneously, each related in some way. The goal is to learn 
  
    
      
        T
      
    
    {\displaystyle T}
   functions, ideally borrowing strength from the relatedness of tasks, that have predictive power. This is equivalent to learning the matrix 
  
    
      
        W
        :
        T
        ×
        D
      
    
    {\displaystyle W:T\times D}
   .


=== Sparse regularizer on columns ===

  
    
      
        R
        (
        w
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            D
          
        
        ‖
        W
        
          ‖
          
            2
            ,
            1
          
        
      
    
    {\displaystyle R(w)=\sum _{i=1}^{D}\|W\|_{2,1}}
  This regularizer defines an L2 norm on each column and an L1 norm over all columns. It can be solved by proximal methods.


=== Nuclear norm regularization ===

  
    
      
        R
        (
        w
        )
        =
        ‖
        σ
        (
        W
        )
        
          ‖
          
            1
          
        
      
    
    {\displaystyle R(w)=\|\sigma (W)\|_{1}}
   where 
  
    
      
        σ
        (
        W
        )
      
    
    {\displaystyle \sigma (W)}
   is the eigenvalues in the singular value decomposition of 
  
    
      
        W
      
    
    {\displaystyle W}
  .


=== Mean-constrained regularization ===

  
    
      
        R
        (
        
          f
          
            1
          
        
        ⋯
        
          f
          
            T
          
        
        )
        =
        
          ∑
          
            t
            =
            1
          
          
            T
          
        
        ‖
        
          f
          
            t
          
        
        −
        
          
            1
            T
          
        
        
          ∑
          
            s
            =
            1
          
          
            T
          
        
        
          f
          
            s
          
        
        
          ‖
          
            
              H
              
                k
              
            
          
          
            2
          
        
      
    
    {\displaystyle R(f_{1}\cdots f_{T})=\sum _{t=1}^{T}\|f_{t}-{\frac {1}{T}}\sum _{s=1}^{T}f_{s}\|_{H_{k}}^{2}}
  This regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents an individual.


=== Clustered mean-constrained regularization ===

  
    
      
        R
        (
        
          f
          
            1
          
        
        ⋯
        
          f
          
            T
          
        
        )
        =
        
          ∑
          
            r
            =
            1
          
          
            C
          
        
        
          ∑
          
            t
            ∈
            I
            (
            r
            )
          
        
        ‖
        
          f
          
            t
          
        
        −
        
          
            1
            
              I
              (
              r
              )
            
          
        
        
          ∑
          
            s
            ∈
            I
            (
            r
            )
          
        
        
          f
          
            s
          
        
        
          ‖
          
            
              H
              
                k
              
            
          
          
            2
          
        
      
    
    {\displaystyle R(f_{1}\cdots f_{T})=\sum _{r=1}^{C}\sum _{t\in I(r)}\|f_{t}-{\frac {1}{I(r)}}\sum _{s\in I(r)}f_{s}\|_{H_{k}}^{2}}
   where 
  
    
      
        I
        (
        r
        )
      
    
    {\displaystyle I(r)}
   is a cluster of tasks.This regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict Netflix recommendations. A cluster would correspond to a group of people who share similar preferences.


=== Graph-based similarity ===
More generally than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks.

  
    
      
        R
        (
        
          f
          
            1
          
        
        ⋯
        
          f
          
            T
          
        
        )
        =
        
          ∑
          
            t
            ,
            s
            =
            1
            ,
            t
            ≠
            s
          
          
            T
          
        
        ‖
        
          f
          
            t
          
        
        −
        
          f
          
            s
          
        
        
          ‖
          
            2
          
        
        
          M
          
            t
            s
          
        
      
    
    {\displaystyle R(f_{1}\cdots f_{T})=\sum _{t,s=1,t\neq s}^{T}\|f_{t}-f_{s}\|^{2}M_{ts}}
   for a given symmetric similarity matrix 
  
    
      
        M
      
    
    {\displaystyle M}
  .


== Other uses of regularization in statistics and machine learning ==
Bayesian learning methods make use of a prior probability that (usually) gives lower probability to more complex models. Well-known model selection techniques include the Akaike information criterion (AIC), minimum description length (MDL), and the Bayesian information criterion (BIC). Alternative methods of controlling overfitting not involving regularization include cross-validation.
Examples of applications of different methods of regularization to the linear model are:


== See also ==
Bayesian interpretation of regularization
Bias–variance tradeoff
Matrix regularization
Regularization by spectral filtering
Regularized least squares
Lagrange multiplier


== Notes ==


== References ==
Neumaier, A. (1998). ""Solving ill-conditioned and singular linear systems: A tutorial on regularization"" (PDF). SIAM Review. 40 (3): 636–666. Bibcode:1998SIAMR..40..636N. doi:10.1137/S0036144597321909."
0f6b01a801,Fuzzy mathematics,"Fuzzy mathematics  is the branch of mathematics including fuzzy set theory and fuzzy logic that deals with partial inclusion of elements in a set on a spectrum, as opposed to simple binary ""yes"" or ""no"" (0 or 1) inclusion.  It started in 1965 after the publication of Lotfi Asker Zadeh's seminal work Fuzzy sets. Linguistics is an example of a field that utilizes fuzzy set theory.


== Definition ==
A fuzzy subset A of a set X is a function A: X → L, where L is the interval [0, 1]. This function is also called a membership function. A membership function is a generalization of an indicator function (also called a characteristic function) of a subset defined for L = {0, 1}.  More generally, one can use any complete lattice L in a definition of a fuzzy subset A.


== Fuzzification ==
The evolution of the fuzzification of mathematical concepts can be broken down into three stages:
straightforward fuzzification during the sixties and seventies,
the explosion of the possible choices in the generalization process during the eighties,
the standardization, axiomatization, and L-fuzzification in the nineties.Usually, a fuzzification of mathematical concepts is based on a generalization of these concepts from characteristic functions to membership functions. Let A and B be two fuzzy subsets of X. 
The intersection A ∩ B and union A ∪ B are defined as follows: (A ∩ B)(x) = min(A(x), B(x)), (A ∪ B)(x) = max(A(x), B(x)) for all x in X. Instead of min and max one can use t-norm and t-conorm, respectively; for example, min(a, b) can be replaced by multiplication ab. A straightforward fuzzification is usually based on min and max operations because in this case more properties of traditional mathematics can be extended to the fuzzy case. 
An important generalization principle used in fuzzification of algebraic operations is a closure property. Let * be a binary operation on X. The closure property for a fuzzy subset A of X is that for all x, y in X, A(x*y) ≥ min(A(x), A(y)). Let (G, *) be a group and A a fuzzy subset of G. Then A is a fuzzy subgroup of G if for all x, y in G, A(x*y−1) ≥ min(A(x), A(y−1)). 
A similar generalization principle is used, for example, for fuzzification of the transitivity property. Let R be a fuzzy relation on X, i.e. R is a fuzzy subset of X × X. Then R is (fuzzy-)transitive if for all x, y, z in X, R(x, z) ≥ min(R(x, y), R(y, z)).


== Fuzzy analogues ==
Fuzzy subgroupoids and fuzzy subgroups were introduced in 1971 by A. Rosenfeld.Analogues of other mathematical subjects have been translated to fuzzy mathematics, such as fuzzy field theory and fuzzy Galois theory, fuzzy topology, fuzzy geometry, fuzzy orderings, and fuzzy graphs.


== See also ==
Fuzzy measure theory
Fuzzy subalgebra
Monoidal t-norm logic
Possibility theory
T-norm


== References ==


== External links ==
Zadeh, L.A. Fuzzy Logic - article at Scholarpedia
Hajek, P. Fuzzy Logic - article at Stanford Encyclopedia of Philosophy
Navara, M. Triangular Norms and Conorms - article at Scholarpedia
Dubois, D., Prade H. Possibility Theory - article at Scholarpedia
Center for Mathematics of Uncertainty Fuzzy Math Research - Web site hosted at Creighton University
Seising, R. [1] Book on the history of the mathematical theory of Fuzzy Sets: The Fuzzification of Systems. The Genesis of Fuzzy Set Theory and Its Initial Applications -- Developments up to the 1970s (Studies in Fuzziness and Soft Computing, Vol. 216) Berlin, New York, [et al.]: Springer 2007."
20e1985cfb,"Science, technology, engineering, and mathematics","Science, technology, engineering, and mathematics (STEM) is an umbrella term used to group together the distinct but related technical disciplines of science, technology, engineering, and mathematics. The term is typically used in the context of education policy or curriculum choices in schools. It has implications for workforce development, national security concerns (as a shortage of STEM-educated citizens can reduce effectiveness in this area) and immigration policy, with regards to admitting foreign students and tech workers.There is no universal agreement on which disciplines are included in STEM; in particular whether or not the science in STEM includes social sciences, such as psychology, sociology, economics, and political science. In the United States, these are typically included by organizations such as the National Science Foundation (NSF), the Department of Labor's O*Net online database for job seekers, and the Department of Homeland Security. In the United Kingdom, the social sciences are categorized separately and are instead grouped together with humanities and arts to form another counterpart acronym HASS (Humanities, Arts, and Social Sciences), rebranded in 2020 as SHAPE (Social Sciences, Humanities and the Arts for People and the Economy). Some sources also use HEAL (health, education, administration, and literacy) as the counterpart of STEM.


== Terminology ==


=== History ===
In the early 1990s, the acronym STEM was used by a variety of educators in preference to SMET, including Charles E. Vela, the founder and director of the Center for the Advancement of Hispanics in Science and Engineering Education (CAHSEE). Moreover, the CAHSEE started a summer program for talented under-represented students in the Washington, D.C., area called the STEM Institute. Based on the program's recognized success and his expertise in STEM education, Charles Vela was asked to serve on numerous NSF and Congressional panels in science, mathematics and engineering education; it is through this manner that NSF was first introduced to the acronym STEM. One of the first NSF projects to use the acronym was STEMTEC, the Science, Technology, Engineering and Math Teacher Education Collaborative at the University of Massachusetts Amherst, which was founded in 1998.
In 2001, at the urging of Dr. Peter Faletra, the Director of Workforce Development for Teachers and Scientists at the Office of Science, the acronym was adopted by Rita Colwell and other science administrators in the National Science Foundation (NSF). The Office of Science was also an early adopter of the STEM acronym.


=== Other variations ===
A-STEM (arts, science, technology, engineering, and mathematics); more focus and based on humanism and arts.
eSTEM (environmental STEM)
GEMS (girls in engineering, math, and science); used for programs to encourage women to enter these fields.
MINT (mathematics, informatics, natural sciences, and technology)
SHTEAM (science, humanities, technology, engineering, arts, and mathematics)
SMET (science, mathematics, engineering, and technology); previous name
STEAM (science, technology, engineering, arts, and mathematics)STEAM (science, technology, engineering, agriculture, and mathematics); add agriculture
STEAM (science, technology, engineering, and applied mathematics); more focus on applied mathematics
STEEM (science, technology, engineering, economics, and mathematics); adds economics as a field
STEMIE (science, technology, engineering, mathematics, invention and entrepreneurship); adds Inventing and Entrepreneurship as means to apply STEM to real world problem solving and markets.
STEMM (science, technology, engineering, mathematics, and medicine)
STM (scientific, technical, and mathematics or science, technology, and medicine)
STREAM (science, technology, robotics, engineering, arts, and mathematics); adds robotics and arts as fields


== Geographic distribution ==


=== Australia ===
The Australian Curriculum, Assessment and Reporting Authority 2015 report entitled, National STEM School Education Strategy, stated that ""A renewed national focus on STEM in school education is critical to ensuring that all young Australians are equipped with the necessary STEM skills and knowledge that they must need to succeed."" Its goals were to:

""Ensure all students finish school with strong foundational knowledge in STEM and related skills""
""Ensure that students are inspired to take on more challenging STEM subjects""Events and programs meant to help develop STEM in Australian schools include the Victorian Model Solar Vehicle Challenge, the Maths Challenge (Australian Mathematics Trust), Go Girl Go Global and the Australian Informatics Olympiad.


=== Canada ===
Canada ranks 12th out of 16 peer countries in the percentage of its graduates who studied in STEM programs, with 21.2%, a number higher than the United States, but lower than France, Germany, and Austria. The peer country with the greatest proportion of STEM graduates, Finland, has over 30% of its university graduates coming from science, mathematics, computer science, and engineering programs.SHAD is an annual Canadian summer enrichment program for high-achieving high school students in July. The program focuses on academic learning particularly in STEAM fields.Scouts Canada has taken similar measures to their American counterpart to promote STEM fields to youth. Their STEM program began in 2015.In 2011 Canadian entrepreneur and philanthropist Seymour Schulich established the Schulich Leader Scholarships, $100 million in $60,000 scholarships for students beginning their university education in a STEM program at 20 institutions across Canada. Each year 40 Canadian students would be selected to receive the award, two at each institution, with the goal of attracting gifted youth into the STEM fields. The program also supplies STEM scholarships to five participating universities in Israel.


=== China ===
To promote STEM in China, the Chinese government issued a guideline in 2016 on national innovation-driven development strategy, instructing that by 2020, China should become an innovative country; by 2030, it should be at the forefront of innovative countries; and by 2050, it should become a technology innovation power.In February 2017, the Ministry of Education in China announced they would officially add STEM education to the primary school curriculum, which is the first official government recognition of STEM education. And later, in May 2018, the launching ceremony and press conference for the 2029 Action Plan for China's STEM Education was held in Beijing, China. This plan aims to allow as many students to benefit from STEM education as possible and equip all students with scientific thinking and the ability to innovate. In response to encouraging policies by the government, schools in both public and private sectors around the country have begun to carry out STEM education programs.However, to effectively implement STEM curricula, full-time teachers specializing in STEM education and relevant content to be taught are needed. Currently, China lacks qualified STEM teachers, and a training system is yet to be established.Several Chinese cities have taken bold measures to add programming as a compulsory course for elementary and middle school students. This is the case of the city of Chongqing.


=== Europe ===

Several European projects have promoted STEM education and careers in Europe. For instance, Scientix is a European cooperation of STEM teachers, education scientists, and policymakers. The SciChallenge project used a social media contest and the student-generated content to increase motivation of pre- university students for STEM education and careers. The Erasmus programme project AutoSTEM used automata to introduce STEM subjects to very young children.


==== Finland ====
In Finland LUMA Center is the leading advocate for STEM oriented education. In the native tongue luma stands for ""luonnontieteellis-matemaattinen"" (lit. adj. ""scientific-mathematical""). The short is more or less a direct translation of STEM, with engineering fields included by association. However unlike STEM, the term is also a portmanteau from lu and ma.


==== France ====
The name of STEM in France is industrial engineering sciences (sciences industrielles or sciences de l'ingénieur). The STEM organization in France is the association UPSTI.


=== Hong Kong ===
STEM education has not been promoted among the local schools in Hong Kong until recent years. In November 2015, the Education Bureau of Hong Kong released a document titled Promotion of STEM Education, which proposes strategies and recommendations on promoting STEM education.


=== India ===
India is next only to China with STEM graduates per population of 1 to 52. The total fresh STEM graduates were 2.6 million in 2016. STEM graduates have been contributing to the Indian economy with well paid salaries locally and abroad since last two decades. The turnaround of Indian economy with comfortable foreign exchange reserves is mainly attributed to the skills of its STEM graduates.


=== Nigeria ===
In Nigeria, The Association of Professional Women Engineers Of Nigeria (APWEN) has involved girls between the ages of 12 to 19 in science based courses in other for them to pursue science based courses in the higher institutions of learning. National Science Foundation (NSF) In Nigeria has made conscious efforts to encourage girls to innovate, invent and build it through the 'invent it, build it challenge' program sponsored by NNPC.


=== Pakistan ===
STEM subjects are taught in Pakistan as part of electives taken in the 9th and 10th grade, culminating in Matriculation exams. These electives are: pure sciences (Physics, Chemistry, Biology), mathematics (Physics, Chemistry, Maths) and computer science (Physics, Chemistry, Computer Science). STEM subjects are also offered as electives taken in the 11th and 12th grade, more commonly referred to as first and second year, culminating in Intermediate exams. These electives are: FSc pre-medical (Physics, Chemistry, Biology), FSc pre-engineering (Physics, Chemistry, Maths) and ICS (Physics/Statistics, Computer Science, Maths). These electives are intended to aid students in pursuing STEM-related careers in the future by preparing them for the study of these courses at university.
A STEM education project has been approved by the government to establish STEM labs in public schools. The Ministry of Information Technology and Telecommunication has collaborated with Google to launch Pakistan's first grassroots level Coding Skills Development Program, based on Google's CS First Program, a global initiative aimed at developing coding skills in children. The aim of the program is to develop applied coding skills using gamification techniques for children between the ages of 9 and 14.
The KPITBs Early Age Programming initiative, established in the province of Khyber Pakhtunkhwa, has been successfully introduced in 225 Elementary and Secondary Schools. There are many private organizations working in Pakistan to introduce STEM education in schools.


=== Philippines ===
In the Philippines, STEM is a two-year program and strand that is used for Senior High School (Grade 11 and 12), as signed by the Department of Education or DepEd. The STEM strand is under the Academic Track, which also include other strands like ABM, HUMSS, and GAS. The purpose of STEM strand is to educate students in the field of science, technology, engineering, and mathematics, in an interdisciplinary and applied approach, and to give students advance knowledge and application in the field. After completing the program, the students will earn a Diploma in Science, Technology, Engineering, and Mathematics. In some colleges and universities, they require students applying for STEM degrees (like medicine, engineering, computer studies, etc.) to be a graduate of STEM, if not, they will need to enter a bridging program.


=== Qatar ===
In Qatar, AL-Bairaq is an outreach program to high-school students with a curriculum that focuses on STEM, run by the Center for Advanced Materials (CAM) at Qatar University. Each year around 946 students, from about 40 high schools, participate in AL-Bairaq competitions. AL-Bairaq make use of project-based learning, encourages students to solve authentic problems, and inquires them to work with each other as a team to build real solutions. Research has so far shown positive results for the program.


=== Singapore ===
STEM is part of the Applied Learning Programme (ALP) that the Singapore Ministry of Education (MOE) has been promoting since 2013, and currently, all secondary schools have such a programme. It is expected that by 2023, all primary schools in Singapore will have an ALP. There are no tests or exams for ALPs. The emphasis is for students to learn through experimentation – they try, fail, try, learn from it and try again. The MOE actively supports schools with ALPs to further enhance and strengthen their capabilities and programmes that nurtures innovation and creativity.
The Singapore Science Centre established a STEM unit in January 2014, dedicated to igniting students’ passion for STEM. To further enrich students’ learning experiences, their Industrial Partnership Programme (IPP) creates opportunities for students to get early exposure to the real-world STEM industries and careers. Curriculum specialists and STEM educators from the Science Centre will work hand-in-hand with teachers to co-develop STEM lessons, provide training to teachers and co-teach such lessons to provide students with an early exposure and develop their interest in STEM.


=== Thailand ===
In 2017, Thai Education Minister Dr Teerakiat Jareonsettasin said after the 49th Southeast Asia Ministers of Education Organisation (SEAMEO) Council Conference in Jakarta that the meeting approved the establishment of two new SEAMEO regional centres in Thailand. One would be the STEM Education Centre, while the other would be a Sufficient Economy Learning Centre.
Teerakiat said that the Thai government had already allocated Bt250 million over five years for the new STEM centre. The centre will be the regional institution responsible for STEM education promotion. It will not only set up policies to improve STEM education, but it will also be the centre for information and experience sharing among the member countries and education experts. According to him, “This is the first SEAMEO regional centre for STEM education, as the existing science education centre in Malaysia only focuses on the academic perspective. Our STEM education centre will also prioritise the implementation and adaptation of science and technology.”The Institute for the Promotion of Teaching Science and Technology has initiated a STEM Education Network. Its goals are to promote integrated learning activities and improve student creativity and application of knowledge, and to establish a network of organisations and personnel for the promotion of STEM education in the country.


=== Turkey ===
Turkish STEM Education Task Force (or FeTeMM—Fen Bilimleri, Teknoloji, Mühendislik ve Matematik) is a coalition of academicians and teachers who show an effort to increase the quality of education in STEM fields rather than focussing on increasing the number of STEM graduates.


=== United States ===
In the United States, the acronym began to be used in education and immigration debates in initiatives to begin to address the perceived lack of qualified candidates for high-tech jobs. It also addresses concern that the subjects are often taught in isolation, instead of as an integrated curriculum. Maintaining a citizenry that is well versed in the STEM fields is a key portion of the public education agenda of the United States. The acronym has been widely used in the immigration debate regarding access to United States work visas for immigrants who are skilled in these fields. It has also become commonplace in education discussions as a reference to the shortage of skilled workers and inadequate education in these areas. The term tends not to refer to the non-professional and less visible sectors of the fields, such as electronics assembly line work.


==== National Science Foundation ====
Many organizations in the United States follow the guidelines of the National Science Foundation on what constitutes a STEM field. The NSF uses a broader definition of STEM subjects that includes subjects in the fields of chemistry, computer and information technology science, engineering, geosciences, life sciences, mathematical sciences, physics and astronomy, social sciences (anthropology, economics, psychology and sociology), and STEM education and learning research.
The NSF is the only American federal agency whose mission includes support for all fields of fundamental science and engineering, except for medical sciences. Its disciplinary program areas include scholarships, grants, fellowships in fields such as biological sciences, computer and information science and engineering, education and human resources, engineering, environmental research and education, geosciences, international science and engineering, mathematical and physical sciences, social, behavioral and economic sciences, cyberinfrastructure, and polar programs.


==== Immigration policy ====
Although many organizations in the United States follow the guidelines of the National Science Foundation on what constitutes a STEM field, the United States Department of Homeland Security (DHS) has its own functional definition used for immigration policy. In 2012, DHS or ICE announced an expanded list of STEM designated-degree programs that qualify eligible graduates on student visas for an optional practical training (OPT) extension. Under the OPT program, international students who graduate from colleges and universities in the United States can stay in the country and receive up to twelve months of training through work experience. Students who graduate from a designated STEM degree program can stay for an additional seventeen months on an OPT STEM extension.


==== STEM-eligible degrees in US immigration ====

An exhaustive list of STEM disciplines does not exist because the definition varies by organization. The U.S. Immigration and Customs Enforcement lists disciplines including architecture, physics, actuarial science, chemistry, biology, mathematics, applied mathematics, statistics, computer science, computational science, psychology, biochemistry, robotics, computer engineering, electrical engineering, electronics, mechanical engineering, industrial engineering, information science, information technology, civil engineering, aerospace engineering, chemical engineering, astrophysics, astronomy, optics, nanotechnology, nuclear physics, mathematical biology, operations research, neurobiology, biomechanics, bioinformatics, acoustical engineering, geographic information systems, atmospheric sciences, educational/instructional technology, software engineering, and educational research.


==== Education ====
By cultivating an interest in the natural and social sciences in preschool or immediately following school entry, the chances of STEM success in high school can be greatly improved.STEM supports broadening the study of engineering within each of the other subjects, and beginning engineering at younger grades, even elementary school. It also brings STEM education to all students rather than only the gifted programs. In his 2012 budget, President Barack Obama renamed and broadened the ""Mathematics and Science Partnership (MSP)"" to award block grants to states for improving teacher education in those subjects.In the 2015 run of the international assessment test the Program for International Student Assessment (PISA), American students came out 35th in mathematics, 24th in reading and 25th in science, out of 109 countries. The United States also ranked 29th in the percentage of 24-year-olds with science or mathematics degrees.STEM education often uses new technologies such as RepRap 3D printers to encourage interest in STEM fields.In 2006 the United States National Academies expressed their concern about the declining state of STEM education in the United States. Its Committee on Science, Engineering, and Public Policy developed a list of 10 actions. Their top three recommendations were to:

Increase America's talent pool by improving K–12 science and mathematics education
Strengthen the skills of teachers through additional training in science, mathematics and technology
Enlarge the pipeline of students prepared to enter college and graduate with STEM degreesThe National Aeronautics and Space Administration also has implemented programs and curricula to advance STEM education in order to replenish the pool of scientists, engineers and mathematicians who will lead space exploration in the 21st century.Individual states, such as California, have run pilot after-school STEM programs to learn what the most promising practices are and how to implement them to increase the chance of student success. Another state to invest in STEM education is Florida, where Florida Polytechnic University, Florida's first public university for engineering and technology dedicated to science, technology, engineering and mathematics (STEM), was established. During school, STEM programs have been established for many districts throughout the U.S. Some states include New Jersey, Arizona, Virginia, North Carolina, Texas, and Ohio.Continuing STEM education has expanded to the post-secondary level through masters programs such as the University of Maryland's STEM Program as well as the University of Cincinnati.


==== Racial gap in STEM fields ====

In the United States, the National Science Foundation found that the average science score on the 2011 National Assessment of Educational Progress was lower for black and Hispanic students than white, Asian, and Pacific Islanders. In 2011, eleven percent of the U.S. workforce was black, while only six percent of STEM workers were black. Though STEM in the U.S. has typically been dominated by white males, there have been considerable efforts to create initiatives to make STEM a more racially and gender diverse field. Some evidence suggests that all students, including black and Hispanic students, have a better chance of earning a STEM degree if they attend a college or university at which their entering academic credentials are at least as high as the average student's.


==== Gender gaps in STEM ====
Although women make up 47% of the workforce in the U.S., they hold only 24% of STEM jobs. Research suggests that exposing girls to female inventors at a young age has the potential to reduce the gender gap in technical STEM fields by half. Campaigns from organizations like the National Inventors Hall of Fame aimed to achieve a 50/50 gender balance in their youth STEM programs by 2020.


==== American Competitiveness Initiative ====
In the State of the Union Address on January 31, 2006, President George W. Bush announced the American Competitiveness Initiative. Bush proposed the initiative to address shortfalls in federal government support of educational development and progress at all academic levels in the STEM fields. In detail, the initiative called for significant increases in federal funding for advanced R&D programs (including a doubling of federal funding support for advanced research in the physical sciences through DOE) and an increase in U.S. higher education graduates within STEM disciplines.
The NASA Means Business competition, sponsored by the Texas Space Grant Consortium, furthers that goal. College students compete to develop promotional plans to encourage students in middle and high school to study STEM subjects and to inspire professors in STEM fields to involve their students in outreach activities that support STEM education.
The National Science Foundation has numerous programs in STEM education, including some for K–12 students such as the ITEST Program that supports The Global Challenge Award ITEST Program. STEM programs have been implemented in some Arizona schools. They implement higher cognitive skills for students and enable them to inquire and use techniques used by professionals in the STEM fields.
Project Lead The Way (PLTW) is a provider of STEM education curricular programs to middle and high schools in the United States. Programs include a high school engineering curriculum called Pathway To Engineering, a high school biomedical sciences program, and a middle school engineering and technology program called Gateway To Technology. PLTW programs have been endorsed by President Barack Obama and United States Secretary of Education Arne Duncan as well as various state, national, and business leaders.


==== STEM Education Coalition ====
The Science, Technology, Engineering, and Mathematics (STEM) Education Coalition works to support STEM programs for teachers and students at the U. S. Department of Education, the National Science Foundation, and other agencies that offer STEM-related programs. Activity of the STEM Coalition seems to have slowed since September 2008.


==== Scouting ====
In 2012, the Boy Scouts of America began handing out awards, titled NOVA and SUPERNOVA, for completing specific requirements appropriate to scouts' program level in each of the four main STEM areas. The Girl Scouts of the USA has similarly incorporated STEM into their program through the introduction of merit badges such as ""Naturalist"" and ""Digital Art"".SAE is an international organization, solutions'provider specialized on supporting education, award and scholarship programs for STEM matters, from pre-K to the college degree. It also promotes scientific and technologic innovation.


==== Department of Defense programs ====

The eCybermission is a free, web-based science, mathematics and technology competition for students in grades six through nine sponsored by the U.S. Army. Each webinar is focused on a different step of the scientific method and is presented by an experienced eCybermission CyberGuide. CyberGuides are military and civilian volunteers with a strong background in STEM and STEM education, who are able to provide insight into science, technology, engineering, and mathematics to students and team advisers.
STARBASE is an educational program, sponsored by the Office of the Assistant Secretary of Defense for Reserve Affairs. Students interact with military personnel to explore careers and make connections with the ""real world."" The program provides students with 20–25 hours of experience at National Guard, Navy, Marines, Air Force Reserve and Air Force bases across the nation.
SeaPerch is an underwater robotics program that trains teachers to teach their students how to build an underwater remotely operated vehicle (ROV) in an in-school or out-of-school setting. Students build the ROV from a kit composed of low-cost, easily accessible parts, following a curriculum that teaches basic engineering and science concepts with a marine engineering theme.


==== NASA ====
NASAStem is a program of the U.S. space agency NASA to increase diversity within its ranks, including age, disability, and gender as well as race/ethnicity.


==== Legislation ====
The America COMPETES Act (P.L. 110–69) became law on August 9, 2007. It is intended to increase the nation's investment in science and engineering research and in STEM education from kindergarten to graduate school and postdoctoral education. The act authorizes funding increases for the National Science Foundation, National Institute of Standards and Technology laboratories, and the Department of Energy (DOE) Office of Science over FY2008–FY2010. Robert Gabrys, Director of Education at NASA's Goddard Space Flight Center, articulated success as increased student achievement, early expression of student interest in STEM subjects, and student preparedness to enter the workforce.


==== Jobs ====
In November 2012 the White House announcement before congressional vote on the STEM Jobs Act put President Obama in opposition to many of the Silicon Valley firms and executives who bankrolled his re-election campaign. The Department of Labor identified 14 sectors that are ""projected to add substantial numbers of new jobs to the economy or affect the growth of other industries or are being transformed by technology and innovation requiring new sets of skills for workers."" The identified sectors were as follows: advanced manufacturing, Automotive, construction, financial services, geospatial technology, homeland security, information technology, Transportation, Aerospace, Biotechnology, energy, healthcare, hospitality, and retail.
The Department of Commerce notes STEM fields careers are some of the best-paying and have the greatest potential for job growth in the early 21st century. The report also notes that STEM workers play a key role in the sustained growth and stability of the U.S. economy, and training in STEM fields generally results in higher wages, whether or not they work in a STEM field.In 2015, there were around 9.0 million STEM jobs in the United States, representing 6.1% of American employment. STEM jobs were increasing around 9% percent per year. Brookings Institution found that the demand for competent technology graduates will surpass the number of capable applicants by at least one million individuals.
According to Pew Research Center, a typical STEM worker earns two-thirds more than those employed in other fields.


==== Trajectories of STEM graduates in STEM and non-STEM jobs ====
According to the 2014 US Census ""74 percent of those who have a bachelor's degree in science, technology, engineering and math — commonly referred to as STEM — are not employed in STEM occupations.""


==== Updates ====
In September 2017, a number of large American technology firms collectively pledged to donate $300 million for computer science education in the U.S.PEW findings revealed in 2018 that Americans identified several issues that hound STEM education which included unconcerned parents, disinterested students, obsolete curriculum materials, and too much focus on state parameters. 57 percent of survey respondents pointed out that one main problem of STEM is lack of students' concentration in learning.The recent National Assessment of Educational Progress (NAEP) report card made public technology as well as engineering literacy scores which determines whether students have the capability to apply technology and engineering proficiency to real-life scenarios. The report showed a gap of 28 points between low-income students and their high-income counterparts. The same report also indicated a 38-point difference between white and black students.The Smithsonian Science Education Center (SSEC) announced the release of a five-year strategic plan by the Committee on STEM Education of the National Science and Technology Council on December 4, 2018. The plan is entitled ""Charting a Course for Success: America's Strategy for STEM Education."" The objective is to propose a federal strategy anchored on a vision for the future so that all Americans are given permanent access to premium-quality education in Science, Technology, Engineering, and Mathematics. In the end, the United States can emerge as world leader in STEM mastery, employment, and innovation. The goals of this plan are building foundations for STEM literacy; enhancing diversity, equality, and inclusion in STEM; and preparing the STEM workforce for the future.The 2019 fiscal budget proposal of the White House supported the funding plan in President Donald Trump's Memorandum on STEM Education which allocated around $200 million (grant funding) on STEM education every year. This budget also supports STEM through a grant program worth $20 million for career as well as technical education programs.


==== Events and programs to help develop STEM in US schools ====
FIRST Tech Challenge
VEX Robotics Competitions
FIRST Robotics Competition


=== Vietnam ===
In Vietnam, beginning in 2012 many private education organizations have STEM education initiatives.
In 2015, the Ministry of Science and Technology and Liên minh STEM organized the first National STEM day, followed by many similar events across the country.
in 2015, Ministry of Education and Training included STEM as an area needed to be encouraged in national school year program.
In May 2017, Prime Minister signed a Directive no. 16 stating: ""Dramatically change the policies, contents, education and vocational training methods to create a human resource capable of receiving new production technology trends, with a focus on promoting training in science, technology, engineering and mathematics (STEM), foreign languages, information technology in general education; "" and asking ""Ministry of Education and Training (to): Promote the deployment of science, technology, engineering and mathematics (STEM) education in general education program; Pilot organize in some high schools from 2017 to 2018.


== Women ==

Women constitute 47% of the U.S. workforce, and perform 24% of STEM-related jobs. In the UK women perform 13% of STEM-related jobs (2014). In the U.S. women with STEM degrees are more likely to work in education or healthcare rather than STEM fields compared with their male counterparts.

The gender ratio depends on field of study. For example, in the European Union in 2012 women made up 47.3% of the total, 51% of the social sciences, business and law, 42% of the science, mathematics and computing, 28% of engineering, manufacturing and construction, and 59% of PhD graduates in Health and Welfare.In a study from 2019 it was shown that part of the success of women in STEM depends on the way women in STEM are viewed. In a study that researched grants given based primarily on project versus primarily based on the project lead there was almost no difference in the evaluation between projects from men or women when evaluated on project, but those evaluated mainly on the project leader showed that projects headed by women were given grants four percent less often.Improving the experiences of women in STEM is a major component of increasing the number of women in STEM. One part of this includes the need for role models and mentors who are women in STEM. Along with this, having good resources for information and networking opportunities can improve women's ability to flourish in STEM fields.


== LGBTQ+ ==

People identifying within the group LGBTQ+ have faced discrimination in STEM fields throughout history. Few were openly queer in STEM; however, a couple of well-known people are Alan Turing, the father of computer science, and Sara Josephine Baker, American physician and public-health leader.Despite recent changes in attitudes towards LGBTQ+ people, discrimination still permeates throughout STEM fields. A recent study has shown that gay men are less likely to have completed a bachelor's degree in a STEM field and to work in a STEM occupation. Along with this, those of sexual minorities overall have been shown to be less likely to remain in STEM majors throughout college. Another study concluded that queer people are more likely to experience exclusion, harassment and other negative impacts while in a STEM career while also having fewer opportunities and resources available to them.Multiple programs and institutions are working towards increasing the inclusion and acceptance of LGBTQ+ people in STEM. In the US, the National Organization of Gay and Lesbian Scientists and Technical Professionals (NOGLSTP) has organized people to address homophobia since the 1980s and now promotes activism and support for queer scientists. Other programs, including 500 Queer Scientists and Pride in STEM, function as visibility campaigns for LGBTQ+ people in STEM worldwide.


== Criticism ==
The focus on increasing participation in STEM fields has attracted criticism. In the 2014 article ""The Myth of the Science and Engineering Shortage"" in The Atlantic, demographer Michael S. Teitelbaum criticized the efforts of the U.S. government to increase the number of STEM graduates, saying that, among studies on the subject, ""No one has been able to find any evidence indicating current widespread labor market shortages or hiring difficulties in science and engineering occupations that require bachelor's degrees or higher"", and that ""Most studies report that real wages in many—but not all—science and engineering occupations have been flat or slow-growing, and unemployment as high or higher than in many comparably-skilled occupations."" Teitelbaum also wrote that the then-current national fixation on increasing STEM participation paralleled previous U.S. government efforts since World War II to increase the number of scientists and engineers, all of which he stated ultimately ended up in ""mass layoffs, hiring freezes, and funding cuts""; including one driven by the Space Race of the late 1950s and 1960s, which he wrote led to ""a bust of serious magnitude in the 1970s.""IEEE Spectrum contributing editor Robert N. Charette echoed these sentiments in the 2013 article ""The STEM Crisis Is a Myth"", also noting that there was a ""mismatch between earning a STEM degree and having a STEM job"" in the United States, with only around 1⁄4 of STEM graduates working in STEM fields, while less than half of workers in STEM fields have a STEM degree.Economics writer Ben Casselman, in a 2014 study of post-graduation earnings in the United States for FiveThirtyEight, wrote that, based on the data, science should not be grouped with the other three STEM categories, because, while the other three generally result in high-paying jobs, ""many sciences, particularly the life sciences, pay below the overall median for recent college graduates.""


== See also ==


== References ==


=== Citations ===


== Further reading ==
David Beede;  et al. (September 2011). ""Education Supports Racial and Ethnic Equality in STEM"" (PDF). U.S. Department of Commerce. Retrieved 2012-12-21.
David Beede;  et al. (August 2011). ""Women in STEM: An Opportunity and An Imperative"" (PDF). U.S. Department of Commerce. Retrieved 2012-12-21.
Kaye Husbands Fealing, Aubrey Incorvaia, and Richard Utz, ""Humanizing Science and Engineering for the Twenty-First Century."" Issues in Science and Technology, Fall issue, 2022: 54-57.
David Langdon;  et al. (July 2011). ""STEM: Good Jobs Now and For the Future"" (PDF). U.S. Department of Commerce. Retrieved 2012-12-21.
Arden Bement (May 24, 2005). ""Statement To House & Senate Appriopriators In Support Of STEM Education And NSF Education"" (PDF). STEM Coalition. Archived from the original (PDF) on November 20, 2012. Retrieved 2012-12-21.
Carla C. Johnson, et al., eds. (2020) Handbook of research on STEM education (Routledge, 2020).
Mary Kirk (2009). Gender and Information Technology: Moving Beyond Access to Co-Create Global Partnership. IGI Global Snippet. ISBN 978-1-59904-786-7.
Shirley M. Malcom; Daryl E. Chubin; Jolene K. Jesse (2004). Standing Our Ground: A Guidebook for STEM Educators in the Post-Michigan Era. American Association for the Advancement of Science. ISBN 0871686996.
Unesco publication on girls education in STEM – Cracking the code: girls' and women's education in science, technology, engineering and mathematics (STEM) ""http://unesdoc.unesco.org/images/0025/002534/253479E.pdf ""
Dr Wing Lau – Chief Engineer at the Department of Physics, Oxford University (Oct 12, 2017). ""STEM Re-vitalisation, not trivialisation"". OpenSchool. Retrieved 2017-10-12.


== External links ==
 Media related to STEM at Wikimedia Commons"
e49b8a796f,Babylonian mathematics,"Babylonian mathematics (also known as Assyro-Babylonian mathematics) are the mathematics developed or practiced by the people of Mesopotamia, from the days of the early Sumerians to the centuries following the fall of Babylon in 539 BC. Babylonian mathematical texts are plentiful and well edited. With respect to time they fall in two distinct groups: one from the Old Babylonian period (1830–1531 BC), the other mainly Seleucid from the last three or four centuries BC. With respect to content, there is scarcely any difference between the two groups of texts. Babylonian mathematics remained constant, in character and content, for nearly two millennia.In contrast to the scarcity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from some 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed while the clay was moist, and baked hard in an oven or by the heat of the sun. The majority of recovered clay tablets date from 1800 to 1600 BC, and cover topics that include fractions, algebra, quadratic and cubic equations and the Pythagorean theorem. The Babylonian tablet YBC 7289 gives an approximation to 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   accurate to three significant sexagesimal digits (about six significant decimal digits).


== Origins of Babylonian mathematics ==
Babylonian mathematics is a range of numeric and more advanced mathematical practices in the ancient Near East, written in cuneiform script. Study has historically focused on the Old Babylonian period in the early second millennium BC due to the wealth of data available. There has been debate over the earliest appearance of Babylonian mathematics, with historians suggesting a range of dates between the 5th and 3rd millennia BC. Babylonian mathematics was primarily written on clay tablets in cuneiform script in the Akkadian or Sumerian languages.
""Babylonian mathematics"" is perhaps an unhelpful term since the earliest suggested origins date to the use of accounting devices, such as bullae and tokens, in the 5th millennium BC.


== Babylonian numerals ==

The Babylonian system of mathematics was a sexagesimal (base 60) numeral system. From this we derive the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 degrees in a circle. The Babylonians were able to make great advances in mathematics for two reasons. Firstly, the number 60 is a superior highly composite number, having factors of 1, 2, 3, 4, 5, 6, 10, 12, 15, 20, 30, 60 (including those that are themselves composite), facilitating calculations with fractions. Additionally, unlike the Egyptians and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values (much as, in our base ten system, 734 = 7×100 + 3×10 + 4×1).


== Sumerian mathematics ==
The ancient Sumerians of Mesopotamia developed a complex system of metrology from 3000 BC. From 2600 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.


== Old Babylonian mathematics (2000–1600 BC) ==
Most clay tablets that describe Babylonian mathematics belong to the Old Babylonian, which is why the mathematics of Mesopotamia is commonly known as Babylonian mathematics. Some clay tablets contain mathematical lists and tables, others contain problems and worked solutions.


=== Arithmetic ===
The Babylonians used pre-calculated tables to assist with arithmetic. For example, two tablets found at Senkerah on the Euphrates in 1854, dating from 2000 BC, give lists of the squares of numbers up to 59 and the cubes of numbers up to 32. The Babylonians used the lists of squares together with the formulae:

  
    
      
        a
        b
        =
        
          
            
              (
              a
              +
              b
              
                )
                
                  2
                
              
              −
              
                a
                
                  2
                
              
              −
              
                b
                
                  2
                
              
            
            2
          
        
      
    
    {\displaystyle ab={\frac {(a+b)^{2}-a^{2}-b^{2}}{2}}}
  
  
    
      
        a
        b
        =
        
          
            
              (
              a
              +
              b
              
                )
                
                  2
                
              
              −
              (
              a
              −
              b
              
                )
                
                  2
                
              
            
            4
          
        
      
    
    {\displaystyle ab={\frac {(a+b)^{2}-(a-b)^{2}}{4}}}
  to simplify multiplication.
The Babylonians did not have an algorithm for long division. Instead they based their method on the fact that:

  
    
      
        
          
            a
            b
          
        
        =
        a
        ×
        
          
            1
            b
          
        
      
    
    {\displaystyle {\frac {a}{b}}=a\times {\frac {1}{b}}}
  together with a table of reciprocals. Numbers whose only prime factors are 2, 3 or 5 (known as 5-smooth or regular numbers) have finite reciprocals in sexagesimal notation, and tables with extensive lists of these reciprocals have been found.
Reciprocals such as 1/7, 1/11, 1/13, etc. do not have finite representations in sexagesimal notation. To compute 1/13 or to divide a number by 13 the Babylonians would use an approximation such as:

  
    
      
        
          
            1
            13
          
        
        =
        
          
            7
            91
          
        
        =
        7
        ×
        
          
            1
            91
          
        
        ≈
        7
        ×
        
          
            1
            90
          
        
        =
        7
        ×
        
          
            40
            3600
          
        
        =
        
          
            280
            3600
          
        
        =
        
          
            4
            60
          
        
        +
        
          
            40
            3600
          
        
        .
      
    
    {\displaystyle {\frac {1}{13}}={\frac {7}{91}}=7\times {\frac {1}{91}}\approx 7\times {\frac {1}{90}}=7\times {\frac {40}{3600}}={\frac {280}{3600}}={\frac {4}{60}}+{\frac {40}{3600}}.}
  


=== Algebra ===

The Babylonian clay tablet YBC 7289 (c. 1800–1600 BC) gives an approximation of √2 in four sexagesimal figures, 1;24,51,10, which is accurate to about six decimal digits, and is the closest possible three-place sexagesimal representation of √2:

  
    
      
        1
        +
        
          
            24
            60
          
        
        +
        
          
            51
            
              60
              
                2
              
            
          
        
        +
        
          
            10
            
              60
              
                3
              
            
          
        
        =
        
          
            30547
            21600
          
        
        =
        1.41421
        
          
            296
            ¯
          
        
        .
      
    
    {\displaystyle 1+{\frac {24}{60}}+{\frac {51}{60^{2}}}+{\frac {10}{60^{3}}}={\frac {30547}{21600}}=1.41421{\overline {296}}.}
  As well as arithmetical calculations, Babylonian mathematicians also developed algebraic methods of solving equations. Once again, these were based on pre-calculated tables.
To solve a quadratic equation, the Babylonians essentially used the standard quadratic formula. They considered quadratic equations of the form:

  
    
      
         
        
          x
          
            2
          
        
        +
        b
        x
        =
        c
      
    
    {\displaystyle \ x^{2}+bx=c}
  where b and c were not necessarily integers, but c was always positive. They knew that a solution to this form of equation is:

  
    
      
        x
        =
        −
        
          
            b
            2
          
        
        +
        
          
            
              
                (
                
                  
                    b
                    2
                  
                
                )
              
              
                2
              
            
            +
            c
          
        
      
    
    {\displaystyle x=-{\frac {b}{2}}+{\sqrt {\left({\frac {b}{2}}\right)^{2}+c}}}
  and they found square roots efficiently using division and averaging. They always used the positive root because this made sense when solving ""real"" problems. Problems of this type included finding the dimensions of a rectangle given its area and the amount by which the length exceeds the width.
Tables of values of n3 + n2 were used to solve certain cubic equations. For example, consider the equation:

  
    
      
         
        a
        
          x
          
            3
          
        
        +
        b
        
          x
          
            2
          
        
        =
        c
        .
      
    
    {\displaystyle \ ax^{3}+bx^{2}=c.}
  Multiplying the equation by a2 and dividing by b3 gives:

  
    
      
        
          
            (
            
              
                
                  a
                  x
                
                b
              
            
            )
          
          
            3
          
        
        +
        
          
            (
            
              
                
                  a
                  x
                
                b
              
            
            )
          
          
            2
          
        
        =
        
          
            
              c
              
                a
                
                  2
                
              
            
            
              b
              
                3
              
            
          
        
        .
      
    
    {\displaystyle \left({\frac {ax}{b}}\right)^{3}+\left({\frac {ax}{b}}\right)^{2}={\frac {ca^{2}}{b^{3}}}.}
  Substituting y = ax/b gives:

  
    
      
        
          y
          
            3
          
        
        +
        
          y
          
            2
          
        
        =
        
          
            
              c
              
                a
                
                  2
                
              
            
            
              b
              
                3
              
            
          
        
      
    
    {\displaystyle y^{3}+y^{2}={\frac {ca^{2}}{b^{3}}}}
  which could now be solved by looking up the n3 + n2 table to find the value closest to the right-hand side. The Babylonians accomplished this without algebraic notation, showing a remarkable depth of understanding. However, they did not have a method for solving the general cubic equation.


=== Growth ===
Babylonians modeled exponential growth, constrained growth (via a form of sigmoid functions), and doubling time, the latter in the context of interest on loans.
Clay tablets from c. 2000 BC include the exercise ""Given an interest rate of 1/60 per month (no compounding), compute the doubling time."" This yields an annual interest rate of 12/60 = 20%, and hence a doubling time of 100% growth/20% growth per year = 5 years.


=== Plimpton 322 ===

The Plimpton 322 tablet contains a list of ""Pythagorean triples"", i.e., integers 
  
    
      
        (
        a
        ,
        b
        ,
        c
        )
      
    
    {\displaystyle (a,b,c)}
   such that 
  
    
      
        
          a
          
            2
          
        
        +
        
          b
          
            2
          
        
        =
        
          c
          
            2
          
        
      
    
    {\displaystyle a^{2}+b^{2}=c^{2}}
  .
The triples are too many and too large to have been obtained by brute force.
Much has been written on the subject, including some speculation (perhaps anachronistic) as to whether the tablet could have served as an early trigonometrical table. Care must be exercised to see the tablet in terms of methods familiar or accessible to scribes at the time.

[...] the question ""how was the tablet calculated?"" does not have to have the
same answer as the question ""what problems does the tablet set?"" The first can be answered
most satisfactorily by reciprocal pairs, as first suggested half a century ago, and the second
by some sort of right-triangle problems.

(E. Robson, ""Neither Sherlock Holmes nor Babylon: a reassessment of Plimpton 322"", Historia Math. 28 (3), p. 202).


=== Geometry ===
Babylonians knew the common rules for measuring volumes and areas. They measured the circumference of a circle as three times the diameter and the area as one-twelfth the square of the circumference, which would be correct if π is estimated as 3. They were aware that this was an approximation, and one Old Babylonian mathematical tablet excavated near Susa in 1936 (dated to between the 19th and 17th centuries BC) gives a better approximation of π as 25/8 = 3.125, about 0.5 percent below the exact value.
The volume of a cylinder was taken as the product of the base and the height, however, the volume of the frustum of a cone or a square pyramid was incorrectly taken as the product of the height and half the sum of the bases. The Pythagorean rule was also known to the Babylonians.The ""Babylonian mile"" was a measure of distance equal to about 11.3 km (or about seven modern miles).
This measurement for distances eventually was converted to a ""time-mile"" used for measuring the travel of the Sun, therefore, representing time.The ancient Babylonians had known of formulas concerning the ratios of the sides of similar triangles for many centuries, but they lacked the concept of an angle measure and consequently, studied the sides of triangles instead.The Babylonian astronomers kept detailed records of the rising and setting of stars, the motion of the planets, and the solar and lunar eclipses, all of which required familiarity with angular distances measured on the celestial sphere.They also used a form of Fourier analysis to compute an ephemeris (table of astronomical positions), which was discovered in the 1950s by Otto Neugebauer. To make calculations of the movements of celestial bodies, the Babylonians used basic arithmetic and a coordinate system based on the ecliptic, the part of the heavens that the sun and planets travel through.
Tablets kept in the British Museum provide evidence that the Babylonians even went so far as to have a concept of objects in an abstract mathematical space. The tablets date from between 350 and 50 B.C.E., revealing that the Babylonians understood and used geometry even earlier than previously thought. The Babylonians used a method for estimating the area under a curve by drawing a trapezoid underneath, a technique previously believed to have originated in 14th century Europe. This method of estimation allowed them to, for example, find the distance Jupiter had traveled in a certain amount of time.


== See also ==
Babylonia
Babylonian astronomy
History of mathematics
Islamic mathematics for mathematics in Islamic Iraq/Mesopotamia


== Notes ==


== References ==
Berriman, A. E. (1956). The Babylonian quadratic equation.
Boyer, C. B. (1989).  Merzbach, Uta C. (ed.). A History of Mathematics (2nd rev. ed.). New York: Wiley. ISBN 0-471-09763-2. (1991 pbk ed. ISBN 0-471-54397-7).
Høyrup, Jens. ""Pythagorean 'Rule' and 'Theorem' – Mirror of the Relation Between Babylonian and Greek Mathematics"".  In Renger, Johannes (ed.). Babylon: Focus mesopotamischer Geschichte, Wiege früher Gelehrsamkeit, Mythos in der Moderne. 2. Internationales Colloquium der Deutschen Orient-Gesellschaft 24.–26. März 1998 in Berlin (PDF). Berlin: Deutsche Orient-Gesellschaft / Saarbrücken: SDV Saarbrücker Druckerei und Verlag. pp. 393–407.
Joseph, G. G. (2000). The Crest of the Peacock. Princeton University Press. ISBN 0-691-00659-8.
Joyce, David E. (1995). ""Plimpton 322"". {{cite journal}}: Cite journal requires |journal= (help)
Neugebauer, Otto (1969) [1957]. The Exact Sciences in Antiquity. Acta Historica Scientiarum Naturalium et Medicinalium. Vol. 9 (2 ed.). Dover Publications. pp. 1–191. ISBN 978-0-486-22332-2. PMID 14884919.
O'Connor, J. J.; Robertson, E. F. (December 2000). ""An overview of Babylonian mathematics"". MacTutor History of Mathematics.
Robson, Eleanor (2001). ""Neither Sherlock Holmes nor Babylon: a reassessment of Plimpton 322"". Historia Math. 28 (3): 167–206. doi:10.1006/hmat.2001.2317. MR 1849797.
Robson, E. (2002). ""Words and pictures: New light on Plimpton 322"". American Mathematical Monthly. Washington. 109 (2): 105–120. doi:10.1080/00029890.2002.11919845. JSTOR 2695324. S2CID 33907668.
Robson, E. (2008). Mathematics in Ancient Iraq: A Social History. Princeton University Press.
Toomer, G. J. (1981). Hipparchus and Babylonian Astronomy."
3bf1eca830,Similarity,
4b92d9d7bc,Set theory,"Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. Although objects of any kind can be collected into a set, set theory, as a branch of mathematics, is mostly concerned with those that are relevant to mathematics as a whole.
The modern study of set theory was initiated by the German mathematicians Richard Dedekind and Georg Cantor in the 1870s. In particular, Georg Cantor is commonly considered the founder of set theory. The non-formalized systems investigated during this early stage go under the name of naive set theory. After the discovery of paradoxes within naive set theory (such as Russell's paradox, Cantor's paradox and the Burali-Forti paradox) various axiomatic systems were proposed in the early twentieth century, of which Zermelo–Fraenkel set theory (with or without the axiom of choice) is still the best-known and most studied.
Set theory is commonly employed as a foundational system for the whole of mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice. Besides its foundational role, set theory also provides the framework to develop a mathematical theory of infinity, and has various applications in computer science (such as in the theory of relational algebra), philosophy and formal semantics. Its foundational appeal, together with its paradoxes, its implications for the concept of infinity and its multiple applications, have made set theory an area of major interest for logicians and philosophers of mathematics. Contemporary research into set theory covers a vast array of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.


== History ==

Mathematical topics typically emerge and evolve through interactions among many researchers. Set theory, however, was founded by a single paper in 1874 by Georg Cantor: ""On a Property of the Collection of All Real Algebraic Numbers"".Since the 5th century BC, beginning with Greek mathematician Zeno of Elea in the West and early Indian mathematicians in the East, mathematicians had struggled with the concept of infinity. Especially notable is the work of Bernard Bolzano in the first half of the 19th century. Modern understanding of infinity began in 1870–1874, and was motivated by Cantor's work in real analysis. An 1872 meeting between Cantor and Richard Dedekind influenced Cantor's thinking, and culminated in Cantor's 1874 paper.
Cantor's work initially polarized the mathematicians of his day. While Karl Weierstrass and Dedekind supported Cantor, Leopold Kronecker, now seen as a founder of mathematical constructivism, did not. Cantorian set theory eventually became widespread, due to the utility of Cantorian concepts, such as one-to-one correspondence among sets, his proof that there are more real numbers than integers, and the ""infinity of infinities"" (""Cantor's paradise"") resulting from the power set operation. This utility of set theory led to the article ""Mengenlehre"", contributed in 1898 by Arthur Schoenflies to Klein's encyclopedia.
The next wave of excitement in set theory came around 1900, when it was discovered that some interpretations of Cantorian set theory gave rise to several contradictions, called antinomies or paradoxes. Bertrand Russell and Ernst Zermelo independently found the simplest and best known paradox, now called Russell's paradox: consider ""the set of all sets that are not members of themselves"", which leads to a contradiction since it must be a member of itself and not a member of itself. In 1899, Cantor had himself posed the question ""What is the cardinal number of the set of all sets?"", and obtained a related paradox. Russell used his paradox as a theme in his 1903 review of continental mathematics in his The Principles of Mathematics. Rather than the term set, Russell used the term class, which has subsequently been used more technically.
In 1906, the term set appeared in the book Theory of Sets of Points by husband and wife William Henry Young and Grace Chisholm Young, published by Cambridge University Press.
The momentum of set theory was such that debate on the paradoxes did not lead to its abandonment. The work of Zermelo in 1908 and the work of Abraham Fraenkel and Thoralf Skolem in 1922 resulted in the set of axioms ZFC, which became the most commonly used set of axioms for set theory. The work of analysts, such as that of Henri Lebesgue, demonstrated the great mathematical utility of set theory, which has since become woven into the fabric of modern mathematics. Set theory is commonly used as a foundational system, although in some areas—such as algebraic geometry and algebraic topology—category theory is thought to be a preferred foundation.


== Basic concepts and notation ==

Set theory begins with a fundamental binary relation between an object o and a set A. If o is a member (or element) of A, the notation o ∈ A is used. A set is described by listing elements separated by commas, or by a characterizing property of its elements, within braces { }. Since sets are objects, the membership relation can relate sets as well.
A derived binary relation between two sets is the subset relation, also called set inclusion. If all the members of set A are also members of set B, then A is a subset of B, denoted A ⊆ B. For example, {1, 2} is a subset of {1, 2, 3}, and so is {2} but {1, 4} is not. As implied by this definition, a set is a subset of itself. For cases where this possibility is unsuitable or would make sense to be rejected, the term proper subset is defined. A is called a proper subset of B if and only if A is a subset of B, but A is not equal to B. Also, 1, 2, and 3 are members (elements) of the set {1, 2, 3}, but are not subsets of it; and in turn, the subsets, such as {1}, are not members of the set {1, 2, 3}.
Just as arithmetic features binary operations on numbers, set theory features binary operations on sets. The following is a partial list of them:

Union of the sets A and B, denoted A ∪ B, is the set of all objects that are a member of A, or B, or both. For example, the union of {1, 2, 3} and {2, 3, 4} is the set {1, 2, 3, 4}.
Intersection of the sets A and B, denoted A ∩ B, is the set of all objects that are members of both A and B. For example, the intersection of {1, 2, 3} and {2, 3, 4} is the set {2, 3}.
Set difference of U and A, denoted U \ A, is the set of all members of U that are not members of A. The set difference {1, 2, 3} \ {2, 3, 4}  is {1}, while conversely, the set difference {2, 3, 4} \ {1, 2, 3} is {4}. When A is a subset of U, the set difference U \ A is also called the complement of A in U. In this case, if the choice of U is clear from the context, the notation Ac is sometimes used instead of U \ A, particularly if U is a universal set as in the study of Venn diagrams.
Symmetric difference of sets A and B, denoted A △ B or A ⊖ B, is the set of all objects that are a member of exactly one of A and B (elements which are in one of the sets, but not in both). For instance, for the sets {1, 2, 3} and {2, 3, 4}, the symmetric difference set is {1, 4}. It is the set difference of the union and the intersection, (A ∪ B) \ (A ∩ B) or (A \ B) ∪ (B \ A).
Cartesian product of A and B, denoted A × B, is the set whose members are all possible ordered pairs (a, b), where a is a member of A and b is a member of B. For example, the Cartesian product of {1, 2} and {red, white} is {(1, red), (1, white), (2, red), (2, white)}.
Power set of a set A, denoted 
  
    
      
        
          
            P
          
        
        (
        A
        )
      
    
    {\displaystyle {\mathcal {P}}(A)}
  , is the set whose members are all of the possible subsets of A. For example, the power set of {1, 2} is { {}, {1}, {2}, {1, 2} }.Some basic sets of central importance are the set of natural numbers, the set of real numbers and the empty set—the unique set containing no elements. The empty set is also occasionally called the null set, though this name is ambiguous and can lead to several interpretations.


== Ontology ==

A set is pure if all of its members are sets, all members of its members are sets, and so on. For example, the set containing only the empty set is a nonempty pure set. In modern set theory, it is common to restrict attention to the von Neumann universe of pure sets, and many systems of axiomatic set theory are designed to axiomatize the pure sets only. There are many technical advantages to this restriction, and little generality is lost, because essentially all mathematical concepts can be modeled by pure sets. Sets in the von Neumann universe are organized into a cumulative hierarchy, based on how deeply their members, members of members, etc. are nested. Each set in this hierarchy is assigned (by transfinite recursion) an ordinal number 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  , known as its rank. The rank of a pure set 
  
    
      
        X
      
    
    {\displaystyle X}
   is defined to be the least ordinal that is strictly greater than the rank of any of its elements. For example, the empty set is assigned rank 0, while the set  {{}}  containing only the empty set is assigned rank 1. For each ordinal 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  , the set 
  
    
      
        
          V
          
            α
          
        
      
    
    {\displaystyle V_{\alpha }}
   is defined to consist of all pure sets with rank less than 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  . The entire von Neumann universe is denoted 
  
    
      
        V
      
    
    {\displaystyle V}
  .


== Formalized set theory ==
Elementary set theory can be studied informally and intuitively, and so can be taught in primary schools using Venn diagrams. The intuitive approach tacitly assumes that a set may be formed from the class of all objects satisfying any particular defining condition. This assumption gives rise to paradoxes, the simplest and best known of which are Russell's paradox and the Burali-Forti paradox. Axiomatic set theory was originally devised to rid set theory of such paradoxes.The most widely studied systems of axiomatic set theory imply that all sets form a cumulative hierarchy. Such systems come in two flavors, those whose ontology consists of:

Sets alone. This includes the most common axiomatic set theory, Zermelo–Fraenkel set theory with the axiom of choice (ZFC). Fragments of ZFC include:
Zermelo set theory, which replaces the axiom schema of replacement with that of separation;
General set theory, a small fragment of Zermelo set theory sufficient for the Peano axioms and finite sets;
Kripke–Platek set theory, which omits the axioms of infinity, powerset, and choice, and weakens the axiom schemata of separation and replacement.
Sets and proper classes. These include Von Neumann–Bernays–Gödel set theory, which has the same strength as ZFC for theorems about sets alone, and Morse–Kelley set theory and Tarski–Grothendieck set theory, both of which are stronger than ZFC.The above systems can be modified to allow urelements, objects that can be members of sets but that are not themselves sets and do not have any members.
The New Foundations systems of NFU (allowing urelements) and NF (lacking them), associate with Willard Van Orman Quine, are not based on a cumulative hierarchy. NF and NFU include a ""set of everything"", relative to which every set has a complement. In these systems urelements matter, because NF, but not NFU, produces sets for which the axiom of choice does not hold. Despite NF's ontology not reflecting the traditional cumulative hierarchy and violating well-foundedness, Thomas Forster has argued that it does reflect an iterative conception of set.Systems of constructive set theory, such as CST, CZF, and IZF, embed their set axioms in intuitionistic instead of classical logic. Yet other systems accept classical logic but feature a nonstandard membership relation. These include rough set theory and fuzzy set theory, in which the value of an atomic formula embodying the membership relation is not simply True or False. The Boolean-valued models of ZFC are a related subject.
An enrichment of ZFC called internal set theory was proposed by Edward Nelson in 1977.


== Applications ==
Many mathematical concepts can be defined precisely using only set theoretic concepts. For example, mathematical structures as diverse as graphs, manifolds, rings, vector spaces, and relational algebras can all be defined as sets satisfying various (axiomatic) properties. Equivalence and order relations are ubiquitous in mathematics, and the theory of mathematical relations can be described in set theory.Set theory is also a promising foundational system for much of mathematics. Since the publication of the first volume of Principia Mathematica, it has been claimed that most (or even all) mathematical theorems can be derived using an aptly designed set of axioms for set theory, augmented with many definitions, using first or second-order logic. For example, properties of the natural and real numbers can be derived within set theory, as each number system can be identified with a set of equivalence classes under a suitable equivalence relation whose field is some infinite set.Set theory as a foundation for mathematical analysis, topology, abstract algebra, and discrete mathematics is likewise uncontroversial; mathematicians accept (in principle) that theorems in these areas can be derived from the relevant definitions and the axioms of set theory. However, it remains that few full derivations of complex mathematical theorems from set theory have been formally verified, since such formal derivations are often much longer than the natural language proofs mathematicians commonly present. One verification project, Metamath, includes human-written, computer-verified derivations of more than 12,000 theorems starting from ZFC set theory, first-order logic and propositional logic.


== Areas of study ==
Set theory is a major area of research in mathematics, with many interrelated subfields.


=== Combinatorial set theory ===

Combinatorial set theory concerns extensions of finite combinatorics to infinite sets. This includes the study of cardinal arithmetic and the study of extensions of Ramsey's theorem such as the Erdős–Rado theorem.


=== Descriptive set theory ===

Descriptive set theory is the study of subsets of the real line and, more generally, subsets of Polish spaces. It begins with the study of pointclasses in the Borel hierarchy and extends to the study of more complex hierarchies such as the projective hierarchy and the Wadge hierarchy. Many properties of Borel sets can be established in ZFC, but proving these properties hold for more complicated sets requires additional axioms related to determinacy and large cardinals.
The field of effective descriptive set theory is between set theory and recursion theory. It includes the study of lightface pointclasses, and is closely related to hyperarithmetical theory. In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending (""relativizing"") it to make it more broadly applicable.
A recent area of research concerns Borel equivalence relations and more complicated definable equivalence relations. This has important applications to the study of invariants in many fields of mathematics.


=== Fuzzy set theory ===

In set theory as Cantor defined and Zermelo and Fraenkel axiomatized, an object is either a member of a set or not. In fuzzy set theory this condition was relaxed by Lotfi A. Zadeh so an object has a degree of membership in a set, a number between 0 and 1. For example, the degree of membership of a person in the set of ""tall people"" is more flexible than a simple yes or no answer and can be a real number such as 0.75.


=== Inner model theory ===

An inner model of Zermelo–Fraenkel set theory (ZF) is a transitive class that includes all the ordinals and satisfies all the axioms of ZF. The canonical example is the constructible universe L developed by Gödel.
One reason that the study of inner models is of interest is that it can be used to prove consistency results. For example, it can be shown that regardless of whether a model V of ZF satisfies the continuum hypothesis or the axiom of choice, the inner model L constructed inside the original model will satisfy both the generalized continuum hypothesis and the axiom of choice. Thus the assumption that ZF is consistent (has at least one model) implies that ZF together with these two principles is consistent.
The study of inner models is common in the study of determinacy and large cardinals, especially when considering axioms such as the axiom of determinacy that contradict the axiom of choice. Even if a fixed model of set theory satisfies the axiom of choice, it is possible for an inner model to fail to satisfy the axiom of choice. For example, the existence of sufficiently large cardinals implies that there is an inner model satisfying the axiom of determinacy (and thus not satisfying the axiom of choice).


=== Large cardinals ===

A large cardinal is a cardinal number with an extra property. Many such properties are studied, including inaccessible cardinals, measurable cardinals, and many more. These properties typically imply the cardinal number must be very large, with the existence of a cardinal with the specified property unprovable in Zermelo–Fraenkel set theory.


=== Determinacy ===

Determinacy refers to the fact that, under appropriate assumptions, certain two-player games of perfect information are determined from the start in the sense that one player must have a winning strategy. The existence of these strategies has important consequences in descriptive set theory, as the assumption that a broader class of games is determined often implies that a broader class of sets will have a topological property. The axiom of determinacy (AD) is an important object of study; although incompatible with the axiom of choice, AD implies that all subsets of the real line are well behaved (in particular, measurable and with the perfect set property). AD can be used to prove that the Wadge degrees have an elegant structure.


=== Forcing ===

Paul Cohen invented the method of forcing while searching for a model of ZFC in which the continuum hypothesis fails, or a model of ZF in which the axiom of choice fails. Forcing adjoins to some given model of set theory additional sets in order to create a larger model with properties determined (i.e. ""forced"") by the construction and the original model. For example, Cohen's construction adjoins additional subsets of the natural numbers without changing any of the cardinal numbers of the original model. Forcing is also one of two methods for proving relative consistency by finitistic methods, the other method being Boolean-valued models.


=== Cardinal invariants ===

A cardinal invariant is a property of the real line measured by a cardinal number. For example, a well-studied invariant is the smallest cardinality of a collection of meagre sets of reals whose union is the entire real line. These are invariants in the sense that any two isomorphic models of set theory must give the same cardinal for each invariant. Many cardinal invariants have been studied, and the relationships between them are often complex and related to axioms of set theory.


=== Set-theoretic topology ===

Set-theoretic topology studies questions of general topology that are set-theoretic in nature or that require advanced methods of set theory for their solution. Many of these theorems are independent of ZFC, requiring stronger axioms for their proof. A famous problem is the normal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.


== Objections to set theory ==
From set theory's inception, some mathematicians have objected to it as a foundation for mathematics: see Controversy over Cantor's theory. The most common objection to set theory, one Kronecker voiced in set theory's earliest years, starts from the constructivist view that mathematics is loosely related to computation. If this view is granted, then the treatment of infinite sets, both in naive and in axiomatic set theory, introduces into mathematics methods and objects that are not computable even in principle. The feasibility of constructivism as a substitute foundation for mathematics was greatly increased by Errett Bishop's influential book Foundations of Constructive Analysis.A different objection put forth by Henri Poincaré is that defining sets using the axiom schemas of specification and replacement, as well as the axiom of power set, introduces impredicativity, a type of circularity, into the definitions of mathematical objects. The scope of predicatively founded mathematics, while less than that of the commonly accepted Zermelo–Fraenkel theory, is much greater than that of constructive mathematics, to the point that Solomon Feferman has said that ""all of scientifically applicable analysis can be developed [using predicative methods]"".Ludwig Wittgenstein condemned set theory philosophically for its connotations of mathematical platonism.  He wrote that ""set theory is wrong"", since it builds on the ""nonsense"" of fictitious symbolism, has ""pernicious idioms"", and that it is nonsensical to talk about ""all numbers"".  Wittgenstein identified mathematics with algorithmic human deduction; the need for a secure foundation for mathematics seemed, to him, nonsensical.  Moreover, since human effort is necessarily finite, Wittgenstein's philosophy required an ontological commitment to radical constructivism and finitism.  Meta-mathematical statements — which, for Wittgenstein, included any statement quantifying over infinite domains, and thus almost all modern set theory — are not mathematics.  Few modern philosophers have adopted Wittgenstein's views after a spectacular blunder in Remarks on the Foundations of Mathematics: Wittgenstein attempted to refute Gödel's incompleteness theorems after having only read the abstract.  As reviewers Kreisel, Bernays, Dummett, and Goodstein all pointed out, many of his critiques did not apply to the paper in full.  Only recently have philosophers such as Crispin Wright begun to rehabilitate Wittgenstein's arguments.Category theorists have proposed topos theory as an alternative to traditional axiomatic set theory. Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory. Topoi also give a natural setting for forcing and discussions of the independence of choice from ZF, as well as providing the framework for pointless topology and Stone spaces.An active area of research is the univalent foundations and related to it homotopy type theory. Within homotopy type theory, a set may be regarded as a homotopy 0-type, with universal properties of sets arising from the inductive and recursive properties of higher inductive types. Principles such as the axiom of choice and the law of the excluded middle can be formulated in a manner corresponding to the classical formulation in set theory or perhaps in a spectrum of distinct ways unique to type theory. Some of these principles may be proven to be a consequence of other principles. The variety of formulations of these axiomatic principles allows for a detailed analysis of the formulations required in order to derive various mathematical results.


== Set theory in mathematical education ==
As set theory gained popularity as a foundation for modern mathematics, there has been support for the idea of introducing the basics of naive set theory early in mathematics education.
In the US in the 1960s, the New Math experiment aimed to teach basic set theory, among other abstract concepts, to primary school students, but was met with much criticism. The math syllabus in European schools followed this trend, and currently includes the subject at different levels in all grades. Venn diagrams are widely employed to explain basic set-theoretic relationships to primary school students (even though John Venn originally devised them as part of a procedure to assess the validity of inferences in term logic).
Set theory is used to introduce students to logical operators (NOT, AND, OR), and semantic or rule description (technically intensional definition) of sets (e.g. ""months starting with the letter A""), which may be useful when learning computer programming, since boolean logic is used in various programming languages. Likewise, sets and other collection-like objects, such as multisets and lists, are common datatypes in computer science and programming.
In addition to that, sets are commonly referred to in mathematical teaching when talking about different types of numbers (the sets 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   of natural numbers, 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   of integers, 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   of real numbers, etc.), and when defining a mathematical function as a relation from one set (the domain) to another set (the range).


== See also ==
Glossary of set theory
Class (set theory)
List of set theory topics
Relational model – borrows from set theory
Venn diagram


== Notes ==


== References ==

Kunen, Kenneth (1980), Set Theory: An Introduction to Independence Proofs, North-Holland, ISBN 0-444-85401-0
Johnson, Philip (1972), A History of Set Theory, Prindle, Weber & Schmidt, ISBN 0-87150-154-6


== Further reading ==
Devlin, Keith (1993), The Joy of Sets (2nd ed.), Springer Verlag, ISBN 0-387-94094-4
Ferreirós, Jose (2001), Labyrinth of Thought: A History of Set Theory and Its Role in Modern Mathematics, Berlin: Springer, ISBN 978-3-7643-5749-8
Monk, J. Donald (1969), Introduction to Set Theory, McGraw-Hill Book Company, ISBN 978-0-898-74006-6
Potter, Michael (2004), Set Theory and Its Philosophy: A Critical Introduction, Oxford University Press, ISBN 978-0-191-55643-2
Smullyan, Raymond M.; Fitting, Melvin (2010), Set Theory and the Continuum Problem, Dover Publications, ISBN 978-0-486-47484-7
Tiles, Mary (2004), The Philosophy of Set Theory: An Historical Introduction to Cantor's Paradise, Dover Publications, ISBN 978-0-486-43520-6


== External links ==

Daniel Cunningham, Set Theory article in the Internet Encyclopedia of Philosophy.
Jose Ferreiros, ""The Early Development of Set Theory"" article in the [Stanford Encyclopedia of Philosophy].
Foreman, Matthew, Akihiro Kanamori, eds. Handbook of Set Theory. 3 vols., 2010. Each chapter surveys some aspect of contemporary research in set theory. Does not cover established elementary set theory, on which see Devlin (1993).
""Axiomatic set theory"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
""Set theory"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Schoenflies, Arthur (1898). Mengenlehre in Klein's encyclopedia.
Online books, and library resources in your library and in other libraries about set theory
Rudin, Walter B. (April 6, 1990). ""Set Theory: An Offspring of Analysis"". Marden Lecture in Mathematics. University of Wisconsin-Milwaukee. Archived from the original on 2021-10-31 – via YouTube."
c683b0e60a,Mathematical economics,"Mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics.  Often, these applied methods are beyond simple geometry, and may include differential and integral calculus, difference and differential equations, matrix algebra, mathematical programming, or other computational methods. Proponents of this approach claim that it allows the formulation of theoretical relationships with rigor, generality, and simplicity.Mathematics allows economists to form meaningful, testable propositions about  wide-ranging and complex subjects which could less easily  be expressed informally. Further, the language of mathematics allows economists to make specific, positive claims about controversial or contentious subjects that would be impossible without mathematics. Much of economic theory is currently presented in terms of mathematical economic models, a set of stylized and simplified mathematical relationships asserted to clarify assumptions and implications.Broad applications include:

optimization problems as to goal equilibrium, whether of a household, business firm, or policy maker
static (or equilibrium) analysis in which the economic unit (such as a household) or economic system (such as a market or the economy) is modeled as not changing
comparative statics as to a change from one equilibrium to another induced by a change in one or more factors
dynamic analysis, tracing changes in an economic system over time, for example from economic growth.Formal economic modeling began in the 19th century with the use of differential calculus to represent and explain economic behavior, such as utility maximization, an early economic application of mathematical optimization. Economics became more mathematical as a discipline throughout the first half of the 20th century, but introduction of new and generalized techniques in the period around the Second World War, as in game theory, would greatly broaden the use of mathematical formulations in economics.This rapid systematizing of economics alarmed critics of the discipline as well as some noted economists.  John Maynard Keynes, Robert Heilbroner, Friedrich Hayek and others have criticized the broad use of mathematical models for human behavior, arguing that some human choices are irreducible to mathematics.


== History ==

The use of mathematics in the service of social and economic analysis dates back to the 17th century.  Then, mainly in German universities, a style of instruction emerged which dealt specifically with detailed presentation of data as it related to public administration.  Gottfried Achenwall lectured in this fashion, coining the term statistics.  At the same time, a small group of professors in England established a method of ""reasoning by figures upon things relating to government"" and referred to this practice as Political Arithmetick.  Sir William Petty wrote at length on issues that would later concern economists, such as taxation, Velocity of money and national income, but while his analysis was numerical, he rejected abstract mathematical methodology.  Petty's use of detailed numerical data (along with John Graunt) would influence statisticians and economists for some time, even though Petty's works were largely ignored by English scholars.The mathematization of economics began in earnest in the 19th century.  Most of the economic analysis of the time was what would later be called classical economics.  Subjects were discussed and dispensed with through algebraic means, but calculus was not used.  More importantly, until Johann Heinrich von Thünen's The Isolated State in 1826, economists did not develop explicit and abstract models for behavior in order to apply the tools of mathematics.  Thünen's model of farmland use represents the first example of marginal analysis.  Thünen's work was largely theoretical, but he also mined empirical data in order to attempt to support his generalizations.  In comparison to his contemporaries, Thünen built economic models and tools, rather than applying previous tools to new problems.Meanwhile, a new cohort of scholars trained in the mathematical methods of the physical sciences  gravitated to economics, advocating and applying those methods to their subject, and described today as moving from geometry to mechanics.
These included W.S. Jevons who presented paper on a ""general mathematical theory of political economy"" in 1862, providing an outline for use of the theory of marginal utility in political economy.  In 1871, he published The Principles of Political Economy, declaring that  the subject as science ""must be mathematical simply because it deals with quantities"". Jevons expected that only collection of statistics for price and quantities would permit the subject as presented to become an exact science.  Others preceded and followed in expanding mathematical representations of economic problems.


=== Marginalists and the roots of neoclassical economics ===

Augustin Cournot and Léon Walras built the tools of the discipline axiomatically around utility, arguing that individuals sought to maximize their utility across choices in a way that could be described mathematically.  At the time, it was thought that utility was quantifiable, in units known as utils.  Cournot, Walras and Francis Ysidro Edgeworth are considered the precursors to modern mathematical economics.


==== Augustin Cournot ====
Cournot, a professor of mathematics, developed a mathematical treatment in 1838 for duopoly—a market condition defined by competition between two sellers.  This treatment of competition, first published in Researches into the Mathematical Principles of Wealth, is referred to as Cournot duopoly. It is assumed that both sellers had equal access to the market and could produce their goods without cost.  Further, it assumed that both goods were homogeneous.  Each seller would vary her output based on the output of the other and the market price would be determined by the total quantity supplied.  The profit for each firm would be determined by multiplying their output by the per unit market price.  Differentiating the profit function with respect to quantity supplied for each firm left a system of linear equations, the simultaneous solution of which gave the equilibrium quantity, price and profits. Cournot's contributions to the mathematization of economics would be neglected for decades, but eventually influenced many of the marginalists.  Cournot's models of duopoly and oligopoly also represent one of the first formulations of non-cooperative games.  Today the solution can be given as a Nash equilibrium but Cournot's work preceded modern game theory by over 100 years.


==== Léon Walras ====
While Cournot provided a solution for what would later be called partial equilibrium, Léon Walras attempted to formalize discussion of the economy as a whole through a theory of general competitive equilibrium.  The behavior of every economic actor would be considered on both the production and consumption side.  Walras originally presented four separate models of exchange, each recursively included in the next.  The solution of the resulting system of equations (both linear and non-linear) is the general equilibrium.  At the time, no general solution could be expressed for a system of arbitrarily many equations, but Walras's attempts produced two famous results in economics.  The first is Walras' law and the second is the principle of tâtonnement.  Walras' method was considered highly mathematical for the time and Edgeworth commented at length about this fact in his review of Éléments d'économie politique pure (Elements of Pure Economics).Walras' law was introduced as a theoretical answer to the problem of determining the solutions in general equilibrium.  His notation is different from modern notation but can be constructed using more modern summation notation.  Walras assumed that in equilibrium, all money would be spent on all goods: every good would be sold at the market price for that good and every buyer would expend their last dollar on a basket of goods.  Starting from this assumption, Walras could then show that if there were n markets and n-1 markets cleared (reached equilibrium conditions) that the nth market would clear as well.  This is easiest to visualize with two markets (considered in most texts as a market for goods and a market for money).  If one of two markets has reached an equilibrium state, no additional goods (or conversely, money) can enter or exit the second market, so it must be in a state of equilibrium as well.  Walras used this statement to move toward a proof of existence of solutions to general equilibrium but it is commonly used today to illustrate market clearing in money markets at the undergraduate level.Tâtonnement (roughly, French for groping toward) was meant to serve as the practical expression of Walrasian general equilibrium.  Walras abstracted the marketplace as an auction of goods where the auctioneer would call out prices and market participants would wait until they could each satisfy their personal reservation prices for the quantity desired (remembering here that this is an auction on all goods, so everyone has a reservation price for their desired basket of goods).Only when all buyers are satisfied with the given market price would transactions occur.  The market would ""clear"" at that price—no surplus or shortage would exist.  The word tâtonnement is used to describe the directions the market takes in groping toward equilibrium, settling high or low prices on different goods until a price is agreed upon for all goods.  While the process appears dynamic, Walras only presented a static model, as no transactions would occur until all markets were in equilibrium.  In practice, very few markets operate in this manner.


==== Francis Ysidro Edgeworth ====
Edgeworth introduced mathematical elements to Economics explicitly in Mathematical Psychics: An Essay on the Application of Mathematics to the Moral Sciences, published in 1881.  He adopted Jeremy Bentham's felicific calculus to economic behavior, allowing the outcome of each decision to be converted into a change in utility.  Using this assumption, Edgeworth built a model of exchange on three assumptions: individuals are self-interested, individuals act to maximize utility, and individuals are ""free to recontract with another independently of...any third party"".

Given two individuals, the set of solutions where both individuals can maximize utility is described by the contract curve on what is now known as an Edgeworth Box.  Technically, the construction of the two-person solution to Edgeworth's problem was not developed graphically until 1924 by Arthur Lyon Bowley.  The contract curve of the Edgeworth box (or more generally on any set of solutions to Edgeworth's problem for more actors) is referred to as the core of an economy.Edgeworth devoted considerable effort to insisting that mathematical proofs were appropriate for all schools of thought in economics.  While at the helm of The Economic Journal, he published several articles criticizing the mathematical rigor of rival researchers,  including Edwin Robert Anderson Seligman, a noted skeptic of mathematical economics.  The articles focused on a back and forth over tax incidence and responses by producers.  Edgeworth noticed that a monopoly producing a good that had jointness of supply but not jointness of demand (such as first class and economy on an airplane, if the plane flies, both sets of seats fly with it) might actually lower the price seen by the consumer for one of the two commodities if a tax were applied.  Common sense and more traditional, numerical analysis seemed to indicate that this was preposterous.  Seligman insisted that the results Edgeworth achieved were a quirk of his mathematical formulation.  He suggested that the assumption of a continuous demand function and an infinitesimal change in the tax resulted in the paradoxical predictions.  Harold Hotelling later showed that Edgeworth was correct and that the same result (a ""diminution of price as a result of the tax"") could occur with a discontinuous demand function and large changes in the tax rate.


== Modern mathematical economics ==
From the later-1930s, an array of new mathematical tools from the differential calculus and differential equations, convex sets, and graph theory were deployed to advance  economic theory in a way similar to new mathematical methods earlier applied to physics. The process was later described as moving from mechanics to axiomatics.


=== Differential calculus ===

Vilfredo Pareto analyzed microeconomics by treating decisions by economic actors as attempts to change a given allotment of goods to another, more preferred allotment.  Sets of allocations could then be treated as Pareto efficient (Pareto optimal is an equivalent term) when no exchanges could occur between actors that could make at least one individual better off without making any other individual worse off.  Pareto's proof is commonly conflated with Walrassian equilibrium or informally ascribed to Adam Smith's Invisible hand hypothesis. Rather, Pareto's statement was the first formal assertion of what would be known as the first fundamental theorem of welfare economics.  These models lacked the inequalities of the next generation of mathematical economics.
In the landmark treatise Foundations of Economic Analysis (1947), Paul Samuelson identified a common paradigm and mathematical structure across multiple fields in the subject, building on previous work by Alfred Marshall. Foundations took mathematical concepts from  physics and applied them to economic problems. This broad view (for example, comparing Le Chatelier's principle to tâtonnement) drives the fundamental premise of mathematical economics: systems of economic actors may be modeled and their behavior described much like any other system. This extension followed on the work of the marginalists in the previous century and extended it significantly. Samuelson approached the problems of applying individual utility maximization over aggregate groups with comparative statics, which compares two different equilibrium states after an exogenous change in a variable. This and other methods in the book provided the foundation for mathematical economics in the 20th century.


=== Linear models ===

Restricted models of general equilibrium were formulated  by John von Neumann in 1937. Unlike earlier versions, the models of von Neumann had inequality constraints. For his model of an expanding economy, von Neumann proved the existence and uniqueness of an equilibrium using his generalization of Brouwer's fixed point theorem. Von Neumann's model of an expanding economy considered the matrix pencil  A - λ B  with nonnegative matrices A and B; von Neumann sought probability vectors p and q and a positive number λ that would solve the complementarity equation

 pT (A − λ B) q = 0,along with two inequality systems expressing economic efficiency. In this model, the (transposed) probability vector p represents the prices of the goods while the probability vector q represents the ""intensity"" at which the production process would run.  The unique solution λ represents the rate of growth of the economy, which equals the interest rate. Proving the existence of a positive growth rate and proving that the growth rate equals the interest rate were remarkable achievements, even for von Neumann. Von Neumann's results have been viewed as a special case of linear programming, where von Neumann's model uses only nonnegative matrices. The study of von Neumann's model of an expanding economy continues to interest mathematical economists with interests in computational economics.


==== Input-output economics ====

In 1936, the Russian–born economist Wassily Leontief built his model of input-output analysis from the 'material balance' tables constructed by Soviet economists, which themselves followed earlier work by the physiocrats.  With his model, which described a system of production and demand processes, Leontief described how changes in demand in one economic sector would influence production in another. In practice, Leontief  estimated the coefficients of his simple models, to address economically interesting questions.  In production economics, ""Leontief technologies"" produce outputs using constant proportions of inputs, regardless of the price of inputs, reducing the value of Leontief models for understanding economies but allowing their parameters to be estimated relatively easily. In contrast, the von Neumann model of an expanding economy allows for choice of techniques, but the coefficients must be estimated for each technology.


=== Mathematical optimization ===

In mathematics, mathematical optimization (or optimization or mathematical programming) refers to the selection of a best element from some set of available alternatives. In the simplest case, an optimization problem  involves maximizing or minimizing a real function by selecting input values of the function and computing the corresponding values of the function. The solution process includes satisfying general necessary and sufficient conditions for optimality. For optimization problems, specialized notation may be used as to the function and its input(s). More generally, optimization includes finding the best available element of some function given a defined domain and may use a variety of different  computational optimization techniques.Economics is closely enough linked to optimization by agents in an economy that an influential definition relatedly describes economics qua science as the ""study of human behavior as a relationship between ends and scarce means"" with alternative uses. Optimization problems run through modern economics, many with explicit economic or technical constraints. In microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem for a given level of utility, are economic optimization problems. Theory posits that consumers  maximize their utility, subject to their budget constraints and that firms maximize their profits, subject to their production functions, input costs, and market demand.Economic equilibrium is studied in optimization theory as a key ingredient of economic theorems that in principle could be tested against empirical data.  Newer developments have occurred in dynamic programming and modeling optimization with risk and uncertainty, including applications to portfolio theory, the economics of information, and search theory.Optimality properties for an entire market system may be stated in mathematical terms, as in formulation of the two fundamental theorems of welfare economics and in the Arrow–Debreu model of general equilibrium (also discussed below). More concretely, many problems are amenable to analytical (formulaic) solution. Many others may be sufficiently complex to require numerical methods of solution, aided by software. Still others are complex but tractable enough to allow computable methods of solution, in particular computable general equilibrium models for the entire economy.Linear and nonlinear programming have profoundly affected microeconomics, which had earlier considered only equality constraints. Many of the mathematical economists who received Nobel Prizes in Economics had conducted notable research using linear programming: Leonid Kantorovich, Leonid Hurwicz, Tjalling Koopmans, Kenneth J. Arrow, Robert Dorfman, Paul Samuelson and Robert Solow. Both Kantorovich and Koopmans acknowledged that George B. Dantzig deserved to share their Nobel Prize for linear programming. Economists who conducted research in nonlinear programming also have won the  Nobel prize, notably Ragnar Frisch in addition to Kantorovich, Hurwicz, Koopmans, Arrow, and Samuelson.


==== Linear optimization ====

Linear programming was developed to aid the allocation of resources in firms and in industries during the 1930s in Russia and during the 1940s in the United States. During the Berlin airlift (1948), linear programming was used to plan the shipment of supplies to prevent Berlin from starving after the Soviet  blockade.


==== Nonlinear programming ====

Extensions to nonlinear optimization with inequality constraints were achieved in 1951 by Albert W. Tucker and Harold Kuhn, who considered the nonlinear optimization problem:

Minimize 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   subject to 
  
    
      
        
          g
          
            i
          
        
        (
        x
        )
        ≤
        0
      
    
    {\displaystyle g_{i}(x)\leq 0}
   and 
  
    
      
        
          h
          
            j
          
        
        (
        x
        )
        =
        0
      
    
    {\displaystyle h_{j}(x)=0}
   where

  
    
      
        f
        (
        ⋅
        )
      
    
    {\displaystyle f(\cdot )}
   is the function to be minimized

  
    
      
        
          g
          
            i
          
        
        (
        ⋅
        )
      
    
    {\displaystyle g_{i}(\cdot )}
   are the functions of the 
  
    
      
        m
      
    
    {\displaystyle m}
   inequality constraints where 
  
    
      
        i
        =
        1
        ,
        …
        ,
        m
      
    
    {\displaystyle i=1,\dots ,m}
  

  
    
      
        
          h
          
            j
          
        
        (
        ⋅
        )
      
    
    {\displaystyle h_{j}(\cdot )}
   are the functions of the 
  
    
      
        l
      
    
    {\displaystyle l}
   equality constraints where 
  
    
      
        j
        =
        1
        ,
        …
        ,
        l
      
    
    {\displaystyle j=1,\dots ,l}
  .In allowing inequality constraints, the Kuhn–Tucker approach generalized the classic method of Lagrange multipliers, which (until then) had  allowed only equality constraints.  The Kuhn–Tucker approach inspired further research on Lagrangian duality, including the treatment of inequality constraints. The duality theory of nonlinear programming is particularly satisfactory when applied to convex minimization problems, which enjoy the convex-analytic duality theory of Fenchel and Rockafellar; this convex duality is particularly strong for polyhedral convex functions, such as those arising in linear programming. Lagrangian duality and convex analysis are used daily in operations research, in the scheduling of power plants, the planning of production schedules for factories, and the routing of airlines (routes, flights, planes, crews).


==== Variational calculus and optimal control ====

Economic dynamics allows for changes in economic variables over time, including in dynamic systems.  The problem of finding optimal functions for such changes is studied in variational calculus and in optimal control theory.  Before the Second World War, Frank Ramsey and Harold Hotelling used the calculus of variations to that end.
Following Richard Bellman's work on dynamic programming and the 1962 English translation of L. Pontryagin et al.'s earlier work, optimal control theory was used more extensively in economics in addressing dynamic problems, especially as to economic growth equilibrium and stability of economic systems, of which a textbook example is optimal consumption and saving. A crucial distinction is between deterministic and stochastic control models.  Other applications of optimal control theory include those in finance, inventories, and production for example.


==== Functional analysis ====

It was in the course of proving of the existence of an optimal equilibrium in his 1937 model of economic growth that John von Neumann introduced functional analytic methods to include topology in economic theory, in particular,  fixed-point theory through his generalization of Brouwer's fixed-point theorem. Following von Neumann's program, Kenneth Arrow and Gérard Debreu formulated abstract models of economic equilibria using convex sets  and fixed–point theory.  In introducing the Arrow–Debreu model in 1954, they proved the existence (but not the uniqueness) of an equilibrium and also proved that every Walras equilibrium is Pareto efficient; in general, equilibria need not be unique. In their models, the (""primal"") vector space represented quantities while the ""dual"" vector space represented prices.In Russia, the mathematician Leonid Kantorovich developed economic models in partially ordered vector spaces, that emphasized the duality between quantities and prices. Kantorovich renamed prices as ""objectively determined valuations""  which were abbreviated in Russian as ""o. o. o."", alluding to the difficulty of discussing prices in the Soviet Union.Even in finite dimensions, the concepts of functional analysis have illuminated economic theory, particularly in clarifying the role of prices as normal vectors to a hyperplane supporting a convex set, representing production or consumption possibilities. However, problems of describing optimization over time or under uncertainty require the use of infinite–dimensional function spaces, because agents are choosing among functions or stochastic processes.


=== Differential decline and rise ===

John von Neumann's work on functional analysis and topology broke new ground in mathematics and economic theory. It also left advanced mathematical economics with fewer applications of differential calculus. In particular, general equilibrium theorists used general topology, convex geometry, and optimization theory more than differential calculus, because the approach of differential calculus had failed to establish the existence of an equilibrium.
However, the decline of differential calculus should not be exaggerated, because differential calculus has always been used in graduate training and in applications. Moreover, differential calculus has returned to the highest levels of mathematical economics, general equilibrium theory (GET), as practiced by the ""GET-set"" (the humorous designation due to Jacques H. Drèze). In the 1960s and 1970s, however, Gérard Debreu and Stephen Smale led a revival of the use of differential calculus in mathematical economics. In particular, they were able to prove the existence of a general equilibrium, where earlier writers had failed, because of their novel mathematics: Baire category from general topology and Sard's lemma from differential topology. Other economists associated with the use of differential analysis include Egbert Dierker, Andreu Mas-Colell, and Yves Balasko. These advances have changed the traditional narrative of the history of mathematical economics, following von Neumann, which celebrated the abandonment of differential calculus.


=== Game theory ===

John von Neumann, working with Oskar Morgenstern on the theory of games, broke new mathematical ground in 1944 by extending functional analytic methods related to convex sets and topological fixed-point theory to economic analysis. Their work thereby avoided the traditional differential calculus, for which the maximum–operator did not apply to non-differentiable functions. Continuing von Neumann's work in cooperative game theory, game theorists Lloyd S. Shapley, Martin Shubik, Hervé Moulin, Nimrod Megiddo, Bezalel Peleg influenced economic research in politics and economics. For example, research on the fair prices in cooperative games and fair values for voting games led to changed rules for voting in legislatures and for accounting for the costs in public–works projects. For example, cooperative game theory was used in designing the water distribution system of Southern Sweden and for setting rates for dedicated telephone lines in the USA.
Earlier neoclassical theory had bounded only the range of bargaining outcomes and in special cases, for  example bilateral monopoly or along the contract curve of the Edgeworth box.  Von Neumann and Morgenstern's results were similarly weak. Following von Neumann's program, however, John Nash used fixed–point theory to prove conditions under which the bargaining problem and noncooperative games can generate a unique equilibrium solution. Noncooperative game theory has been adopted as a fundamental aspect of  experimental economics, behavioral economics, information economics, industrial organization, and  political economy. It has also given rise to the subject of mechanism design (sometimes called reverse game theory), which has private and public-policy applications as to ways of improving economic efficiency through incentives for information sharing.In 1994, Nash, John Harsanyi, and Reinhard Selten received the Nobel Memorial Prize in Economic Sciences their work on non–cooperative games. Harsanyi and Selten were awarded for their work on repeated games. Later work extended their results to computational methods of modeling.


=== Agent-based computational economics ===

Agent-based computational economics (ACE) as a named field is relatively recent, dating from about the 1990s as to published work. It studies economic processes, including whole economies, as dynamic systems of interacting agents over time. As such, it falls in the paradigm of complex adaptive systems. In corresponding agent-based models, agents are not real people but ""computational objects modeled as interacting according to rules"" ... ""whose micro-level interactions create emergent patterns""   in space and time. The rules are formulated to predict behavior and social interactions based on incentives and information. The theoretical assumption of mathematical optimization by agents markets is replaced by the less restrictive postulate of agents with bounded rationality adapting to market forces.ACE models apply numerical methods of analysis to computer-based simulations of complex dynamic problems for which more conventional methods, such as theorem formulation, may not find ready use. Starting from specified initial conditions, the computational economic system is modeled as evolving over time as its constituent agents repeatedly interact with each other. In these respects, ACE has been characterized as a bottom-up culture-dish approach to the study of the economy. In contrast to other standard modeling methods, ACE events are driven solely by initial conditions, whether or not equilibria exist or are computationally tractable.   ACE modeling, however, includes agent adaptation, autonomy, and learning. It has a similarity to, and overlap with, game theory as an agent-based method for modeling social interactions. Other dimensions of the approach include such standard economic subjects as competition and collaboration, market structure and industrial organization, transaction costs, welfare economics and mechanism design,  information and uncertainty, and macroeconomics.The method is said to benefit from continuing improvements in modeling techniques of computer science and increased computer capabilities.  Issues include those common to experimental economics in general and by comparison and to development of a common framework for empirical validation and resolving open questions in agent-based modeling. The ultimate scientific objective of the method has been described as ""test[ing] theoretical findings against real-world data in ways that permit empirically supported theories to cumulate over time, with each researcher's work building appropriately on the work that has gone before"".


== Mathematicization of economics ==

Over the course of the 20th century, articles in ""core journals"" in economics have been almost exclusively written by economists in academia.  As a result, much of the material transmitted in those journals relates to economic theory, and ""economic theory itself has been continuously more abstract and mathematical."" A subjective assessment of mathematical techniques employed in these core journals showed a decrease in articles that use neither geometric representations nor mathematical notation from 95% in 1892 to 5.3% in 1990.  A 2007 survey of ten of the top economic journals finds that only 5.8% of the articles published in 2003 and 2004 both lacked statistical analysis of data and  lacked displayed mathematical expressions that were indexed with numbers at the margin of the page.


== Econometrics ==

Between the world wars, advances in mathematical statistics and a cadre of mathematically trained economists  led to econometrics, which was the name proposed for the discipline of advancing economics by using mathematics and statistics. Within economics, ""econometrics"" has often been used for statistical methods in economics, rather than mathematical economics. Statistical econometrics features the application of linear regression and time series analysis to economic data.
Ragnar Frisch coined the word ""econometrics"" and helped to found both the Econometric Society in 1930 and the journal Econometrica in 1933.  A student of Frisch's, Trygve Haavelmo published The Probability Approach in Econometrics in 1944, where he asserted that precise statistical analysis could be used as a tool to validate mathematical theories about economic actors with data from complex sources.  This linking of statistical analysis of systems to economic theory was also promulgated by the Cowles Commission (now the Cowles Foundation) throughout the 1930s and 1940s.The roots of modern econometrics can be traced to the American economist Henry L. Moore.  Moore studied agricultural productivity and attempted to fit changing values of productivity for plots of corn and other crops to a curve using different values of elasticity.  Moore made several errors in his work, some from his choice of models and some from limitations in his use of mathematics.  The accuracy of Moore's models also was limited by the poor data for national accounts in the United States at the time.  While his first models of production were static, in 1925 he published a dynamic ""moving equilibrium"" model designed to explain business cycles—this periodic variation from over-correction in supply and demand curves is now known as the cobweb model.  A more formal derivation of this model was made later by Nicholas Kaldor, who is largely credited for its exposition.


== Application ==

Much of classical economics can be presented in simple geometric terms or elementary mathematical notation. Mathematical economics, however, conventionally makes use of calculus and matrix algebra in economic analysis in order to make powerful claims that would be more difficult without such mathematical tools. These tools are prerequisites for formal study, not only in mathematical economics but in contemporary economic theory in general. Economic problems often involve so many variables that mathematics is the only practical way of attacking and solving them.  Alfred Marshall argued that every economic problem which can be quantified, analytically expressed and solved, should be treated by means of mathematical work.Economics has become increasingly dependent upon mathematical methods and the mathematical tools it employs have become more sophisticated.  As a result, mathematics has become considerably more important to professionals in economics and finance. Graduate programs in both economics and finance require strong undergraduate preparation in mathematics for admission and, for this reason, attract an increasingly high number of mathematicians. Applied mathematicians apply mathematical principles to practical problems, such as economic analysis and other economics-related issues, and many economic problems are often defined as integrated into the scope of applied mathematics.This integration results from the formulation of economic problems as stylized models with clear assumptions and falsifiable predictions.  This modeling may be informal or prosaic, as it was in Adam Smith's The Wealth of Nations, or it may be formal, rigorous and mathematical.
Broadly speaking, formal economic models may be classified as stochastic or deterministic and as discrete or continuous.  At a practical level, quantitative modeling is applied to many areas of economics and several methodologies have evolved more or less independently of each other.
Stochastic models are formulated using stochastic processes. They model economically observable values over time. Most of econometrics is based on statistics to formulate and test hypotheses about these processes or estimate parameters for them. Between the World Wars, Herman Wold developed a representation of stationary stochastic processes in terms of autoregressive models and a determinist trend. Wold and Jan Tinbergen applied time-series analysis to economic data. Contemporary research on time series statistics consider additional formulations of stationary processes, such as  autoregressive moving average models. More general models include  autoregressive conditional heteroskedasticity (ARCH) models and generalized ARCH (GARCH) models.
Non-stochastic mathematical models may be purely qualitative (for example, models involved in some aspect of social choice theory) or quantitative (involving rationalization of financial variables, for example with hyperbolic coordinates, and/or specific forms of functional relationships between variables).  In some cases economic predictions of a model merely assert the direction of movement of economic variables, and so the functional relationships are used only in a qualitative sense: for example, if the price of an item increases, then the demand for that item will decrease.  For such models, economists often use two-dimensional graphs instead of functions.
Qualitative models are occasionally used. One example is qualitative scenario planning in which possible future events are played out. Another example is non-numerical decision tree analysis. Qualitative models often suffer from lack of precision.


== Example: The effect of a corporate tax cut on wages ==
The great appeal of mathematical economics is that it brings a degree of rigor to economic thinking, particularly around charged political topics. For example, during the discussion of the efficacy of a corporate tax cut for increasing the wages of workers, a simple mathematical model proved beneficial to understanding the issues at hand.

As an intellectual exercise, the following problem was posed by Prof. Greg Mankiw of Harvard University:An open economy has the production function 
  
    
      
        y
        =
        f
        (
        k
        )
      
    
    {\textstyle y=f(k)}
  , where 
  
    
      
        y
      
    
    {\textstyle y}
   is output per worker and 
  
    
      
        k
      
    
    {\textstyle k}
   is capital per worker. The capital stock adjusts so that the after-tax marginal product of capital equals the exogenously given world interest rate 
  
    
      
        r
      
    
    {\textstyle r}
  ...How much will the tax cut increase wages?To answer this question, we follow John H. Cochrane of the Hoover Institution. Suppose an open economy has the production function:Where the variables in this equation are:

  
    
      
        Y
      
    
    {\textstyle Y}
   is the total output

  
    
      
        F
        (
        K
        ,
        L
        )
      
    
    {\textstyle F(K,L)}
   is the production function

  
    
      
        K
      
    
    {\textstyle K}
   is the total capital stock

  
    
      
        L
      
    
    {\textstyle L}
   is the total labor stockThe standard choice for the production function is the Cobb-Douglas production function:where 
  
    
      
        A
      
    
    {\textstyle A}
   is the factor of productivity - assumed to be a constant. A corporate tax cut in this model is equivalent to a tax on capital. With taxes, firms look to maximize:where 
  
    
      
        τ
      
    
    {\textstyle \tau }
   is the capital tax rate, 
  
    
      
        w
      
    
    {\textstyle w}
   is wages per worker, and 
  
    
      
        r
      
    
    {\textstyle r}
   is the exogenous interest rate. Then the first-order optimality conditions become:Therefore, the optimality conditions imply that:Define total taxes 
  
    
      
        X
        =
        τ
        [
        F
        (
        K
        ,
        L
        )
        −
        w
        L
        ]
      
    
    {\textstyle X=\tau [F(K,L)-wL]}
  . This implies that taxes per worker 
  
    
      
        x
      
    
    {\textstyle x}
   are:Then the change in taxes per worker, given the tax rate, is:To find the change in wages, we differentiate the second optimality condition for the per worker wages to obtain:Assuming that the interest rate is fixed at 
  
    
      
        r
      
    
    {\textstyle r}
  , so that 
  
    
      
        d
        r
        
          /
        
        d
        τ
        =
        0
      
    
    {\textstyle dr/d\tau =0}
  , we may differentiate the first optimality condition for the interest rate to find:For the moment, let's focus only on the static effect of a capital tax cut, so that 
  
    
      
        d
        x
        
          /
        
        d
        τ
        =
        
          f
          ′
        
        (
        k
        )
        k
      
    
    {\textstyle dx/d\tau =f'(k)k}
  . If we substitute this equation into equation for wage changes with respect to the tax rate, then we find that:Therefore, the static effect of a capital tax cut on wages is:Based on the model, it seems possible that we may achieve a rise in the wage of a worker greater than the amount of the tax cut. But that only considers the static effect, and we know that the dynamic effect must be accounted for. In the dynamic model, we may rewrite the equation for changes in taxes per worker with respect to the tax rate as:Recalling that 
  
    
      
        d
        w
        
          /
        
        d
        τ
        =
        −
        
          f
          ′
        
        (
        k
        )
        k
        
          /
        
        (
        1
        −
        τ
        )
      
    
    {\textstyle dw/d\tau =-f'(k)k/(1-\tau )}
  , we have that:Using the Cobb-Douglas production function, we have that:Therefore, the dynamic effect of a capital tax cut on wages is:If we take 
  
    
      
        α
        =
        τ
        =
        1
        
          /
        
        3
      
    
    {\textstyle \alpha =\tau =1/3}
  , then the dynamic effect of lowering capital taxes on wages will be even larger than the static effect. Moreover, if there are positive externalities to capital accumulation, the effect of the tax cut on wages would be larger than in the model we just derived. It is important to note that the result is a combination of:

The standard result that in a small open economy labor bears 100% of a small capital income tax
The fact that, starting at a positive tax rate, the burden of a tax increase exceeds revenue collection due to the first-order deadweight lossThis result showing that, under certain assumptions, a corporate tax cut can boost the wages of workers by more than the lost revenue does not imply that the magnitude is correct.  Rather, it suggests a basis for policy analysis that is not grounded in handwaving.  If the assumptions are reasonable, then the model is an acceptable approximation of reality; if they are not, then better models should be developed.


=== CES production function ===
Now let's assume that instead of the Cobb-Douglas production function we have a more general constant elasticity of substitution (CES) production function:where 
  
    
      
        ρ
        =
        1
        −
        
          σ
          
            −
            1
          
        
      
    
    {\textstyle \rho =1-\sigma ^{-1}}
  ; 
  
    
      
        σ
      
    
    {\textstyle \sigma }
   is the elasticity of substitution between capital and labor. The relevant quantity we want to calculate is 
  
    
      
        
          f
          ′
        
        
          /
        
        k
        
          f
          ″
        
      
    
    {\textstyle f'/kf''}
  , which may be derived as:Therefore, we may use this to find that:Therefore, under a general CES model, the dynamic effect of a capital tax cut on wages is:We recover the Cobb-Douglas solution when 
  
    
      
        ρ
        =
        0
      
    
    {\textstyle \rho =0}
  .  When 
  
    
      
        ρ
        =
        1
      
    
    {\textstyle \rho =1}
  , which is the case when perfect substitutes exist, we find that 
  
    
      
        d
        w
        
          /
        
        d
        x
        =
        0
      
    
    {\textstyle dw/dx=0}
   - there is no effect of changes in capital taxes on wages.  And when 
  
    
      
        ρ
        =
        −
        ∞
      
    
    {\textstyle \rho =-\infty }
  , which is the case when perfect complements exist, we find that 
  
    
      
        d
        w
        
          /
        
        d
        x
        =
        −
        1
      
    
    {\textstyle dw/dx=-1}
   - a cut in capital taxes increases wages by exactly one dollar.


== Criticisms and defences ==


=== Adequacy of mathematics for qualitative and complicated economics ===
Friedrich Hayek contended that the use of formal techniques projects a scientific exactness that does not appropriately account for informational limitations faced by real economic agents. In an interview in 1999, the economic historian  Robert Heilbroner stated:
I guess the scientific approach began to penetrate and soon dominate the profession in the past twenty to thirty years. This came about in part because of the ""invention"" of mathematical analysis of various kinds and, indeed, considerable improvements in it. This is the age in which we have not only more data but more sophisticated use of data. So there is a strong feeling that this is a data-laden science and a data-laden undertaking, which, by virtue of the sheer numerics, the sheer equations, and the sheer look of a journal page, bears a certain resemblance to science . . . That one central activity looks scientific. I understand that. I think that is genuine. It approaches being a universal law. But resembling a science is different from being a science.
Heilbroner stated that ""some/much of economics is not naturally quantitative and therefore does not lend itself to mathematical exposition.""


=== Testing predictions of mathematical economics ===
Philosopher Karl Popper discussed the scientific standing of economics in the 1940s and 1950s. He argued that mathematical economics suffered from being tautological. In other words, insofar as economics became a mathematical theory, mathematical economics ceased to rely on empirical refutation  but rather relied on mathematical proofs and disproof. According to Popper,  falsifiable assumptions can be tested by experiment and observation while unfalsifiable assumptions can be explored mathematically for their consequences and for their consistency with other assumptions.Sharing Popper's concerns about assumptions in economics generally, and not just mathematical economics, Milton Friedman declared that ""all assumptions are unrealistic"".  Friedman proposed judging economic models by their predictive performance rather than by the match between their assumptions and reality.


=== Mathematical economics as a form of pure mathematics ===

Considering mathematical economics, J.M. Keynes wrote in The General Theory:
It is a great fault of symbolic pseudo-mathematical methods of formalising a system of economic analysis ... that they expressly assume strict independence between the factors involved and lose their cogency and authority if this hypothesis is disallowed; whereas, in ordinary discourse, where we are not blindly manipulating and know all the time what we are doing and what the words mean, we can keep ‘at the back of our heads’ the necessary reserves and qualifications and the adjustments which we shall have to make later on, in a way in which we cannot keep complicated partial differentials ‘at the back’ of several pages of algebra which assume they all vanish. Too large a proportion of recent ‘mathematical’ economics are merely concoctions, as imprecise as the initial assumptions they rest on, which allow the author to lose sight of the complexities and interdependencies of the real world in a maze of pretentious and unhelpful symbols.


=== Defense of mathematical economics ===
In response to these criticisms, Paul Samuelson argued that mathematics is a language, repeating a thesis of Josiah Willard Gibbs. In economics, the language of mathematics is sometimes necessary for representing substantive problems. Moreover, mathematical economics has led to conceptual advances in economics.  In particular, Samuelson gave the example of microeconomics, writing that ""few people are ingenious enough to grasp [its] more complex parts... without resorting to the language of mathematics, while most ordinary individuals can do so fairly easily with the aid of mathematics.""Some economists state that mathematical economics deserves support just like other forms of mathematics, particularly its neighbors in mathematical optimization and mathematical statistics and increasingly in theoretical computer science. Mathematical economics and other mathematical sciences have a history in which theoretical advances have regularly contributed to the reform of the more applied branches of economics. In particular, following the program of John von Neumann, game theory now provides the foundations for describing much of applied economics, from  statistical decision theory (as ""games against nature"") and econometrics to general equilibrium theory and industrial organization. In the last decade, with the rise of the internet, mathematical economists and optimization experts and computer scientists have worked on problems of pricing for on-line services --- their contributions using mathematics from cooperative game theory, nondifferentiable optimization, and combinatorial games.
Robert M. Solow concluded that mathematical economics was the core ""infrastructure"" of contemporary economics:

Economics is no longer a fit conversation piece for ladies and gentlemen. It has become a technical subject. Like any technical subject it attracts some people who are more interested in the technique than the subject. That is too bad, but it may be inevitable. In any case, do not kid yourself: the technical core of economics is indispensable infrastructure for the political economy. That is why, if you consult [a reference in contemporary economics]  looking for enlightenment about the world today, you will be led to technical economics, or history, or nothing at all.


== Mathematical economists ==
Prominent mathematical economists include the following.


=== 19th century ===


=== 20th century ===


== See also ==
Econophysics
Mathematical finance


== References ==


== Further reading ==
Alpha C. Chiang and Kevin Wainwright, [1967] 2005. Fundamental Methods of Mathematical Economics, McGraw-Hill Irwin. Contents.
E. Roy Weintraub, 1982. Mathematics for Economists,  Cambridge.  Contents.
Stephen Glaister, 1984. Mathematical Methods for Economists, 3rd ed., Blackwell. Contents.
Akira Takayama, 1985. Mathematical Economics, 2nd ed. Cambridge. Contents.
Nancy L. Stokey and Robert E. Lucas with Edward Prescott, 1989. Recursive Methods in Economic Dynamics, Harvard University Press. Desecription and chapter-preview links.
A. K. Dixit, [1976] 1990.  Optimization in Economic Theory, 2nd ed., Oxford. Description and contents preview.
Kenneth L. Judd, 1998. Numerical Methods in Economics, MIT Press.  Description and chapter-preview links.
Michael Carter, 2001. Foundations of Mathematical Economics, MIT Press. Contents.
Ferenc Szidarovszky and Sándor Molnár, 2002. Introduction to Matrix Theory: With Applications to Business and Economics, World Scientific Publishing. Description and preview.
D. Wade Hands, 2004. Introductory Mathematical Economics, 2nd ed. Oxford. Contents.
Giancarlo Gandolfo, [1997] 2009. Economic Dynamics, 4th ed., Springer.   Description and preview.
John Stachurski, 2009. Economic Dynamics: Theory and Computation, MIT Press. Description and preview.


== External links ==

Journal of Mathematical Economics Aims & Scope
Mathematical Economics and Financial Mathematics at Curlie
Erasmus Mundus Master QEM - Models and Methods of Quantitative Economics, The Models and Methods of Quantitative Economics - QEM"
b6ec51cca4,Generator (mathematics),"In mathematics and physics, the term generator or generating set may refer to any of a number of related concepts.  The underlying concept in each case is that of a smaller set of objects, together with a set of operations that can be applied to it, that result in the creation of a larger collection of objects, called the generated set.  The larger set is then said to be generated by the smaller set.  It is commonly the case that the generating set has a simpler set of properties than the generated set, thus making it easier to discuss and examine.  It is usually the case that properties of the generating set are in some way preserved by the act of generation; likewise, the properties of the generated set are often reflected in the generating set.


== List of generators ==
A list of examples of generating sets follow.

Generating set or spanning set of a vector space: a set that spans the vector space
Generating set of a group: A subset of a group that is not contained in any subgroup of the group other than the entire group
Generating set of a ring: A subset S of a ring A generates A if the only subring of A containing S is A
Generating set of an ideal in a ring
Generating set of a module
A generator, in category theory, is an object that can be used to distinguish morphisms
In topology, a collection of sets that generate the topology is called a subbase
Generating set of a topological algebra: S is a generating set of a topological algebra A if the smallest closed subalgebra of A containing S is A


== Differential equations ==
In the study of differential equations, and commonly those occurring in physics, one has the idea of a set of infinitesimal displacements that can be extended to obtain a manifold, or at least, a local part of it, by means of integration.  The general concept is of using the exponential map to take the vectors in the tangent space and extend them, as geodesics, to an open set surrounding the tangent point. In this case, it is not unusual to call the elements of the tangent space the generators of the manifold. When the manifold possesses some sort of symmetry, there is also the related notion of a charge or current, which is sometimes also called the generator, although, strictly speaking, charges are not elements of the tangent space.

Elements of the Lie algebra to a Lie group are sometimes referred to as ""generators of the group,"" especially by physicists. The Lie algebra can be thought of as the infinitesimal vectors generating the group, at least locally, by means of the exponential map, but the Lie algebra does not form a generating set in the strict sense.
In stochastic analysis, an Itō diffusion or more general Itō process has an infinitesimal generator.
The generator of any continuous symmetry implied by Noether's theorem, the generators of a Lie group being a special case. In this case, a generator is sometimes called a charge or Noether charge, examples include:
angular momentum as the generator of rotations,
linear momentum as the generator of translations,
electric charge being the generator of the U(1) symmetry group of electromagnetism,
the color charges of quarks are the generators of the SU(3) color symmetry in quantum chromodynamics,
More precisely, ""charge"" should apply only to the root system of a Lie group.


== See also ==
Generating function
Lie theory
Symmetry (physics)
Particle physics
Supersymmetry
Gauge theory
Field (physics)


== References ==


== External links ==
Generating Sets, K. Conrad"
3b039d175e,Space (mathematics),"In mathematics, a space is a set (sometimes called a universe) with some added structure.
While modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, it does not define the notion of ""space"" itself.

A space consists of selected mathematical objects that are treated as points, and selected relationships between these points. 
The nature of the points can vary widely: for example, the points can be elements of a set, functions on another space, or subspaces of another space. It is the relationships that define the nature of the space. More precisely, isomorphic spaces are considered identical, where an isomorphism between two spaces is a one-to-one correspondence between their points that preserves the relationships. For example, the relationships between the points of a three-dimensional Euclidean space are uniquely determined by Euclid's axioms, and all three-dimensional Euclidean spaces are considered identical.
Topological notions such as continuity have natural definitions in every Euclidean space. 
However, topology does not distinguish straight lines from curved lines, and the relation between Euclidean and topological spaces is thus ""forgetful"". Relations of this kind are  treated in more detail in the Section ""Types of spaces"". 
It is not always clear whether a given mathematical object should be considered as a geometric ""space"", or an  algebraic ""structure"". A general definition of ""structure"", proposed by Bourbaki, embraces all common types of spaces, provides a general definition of isomorphism, and justifies the transfer of properties between isomorphic structures.


== History ==


=== Before the golden age of geometry ===

In ancient Greek mathematics, ""space"" was a geometric abstraction of the three-dimensional reality observed in everyday life. About 300 BC, Euclid gave axioms for the properties of space. Euclid built all of mathematics on these geometric foundations, going so far as to define numbers by comparing the lengths of line segments to the length of a chosen reference segment.
The method of coordinates (analytic geometry) was adopted by René Descartes in 1637. At that time, geometric theorems were treated as absolute objective truths knowable through intuition and reason, similar to objects of natural science;: 11  and axioms were treated as obvious implications of definitions.: 15 Two equivalence relations between geometric figures were used: congruence and similarity. Translations, rotations and reflections transform a figure into congruent figures; homotheties — into similar figures. For example, all circles are mutually similar, but ellipses are not similar to circles. A third equivalence relation, introduced by Gaspard Monge in 1795, occurs in projective geometry: not only ellipses, but also parabolas and hyperbolas, turn into circles under appropriate projective transformations; they all are projectively equivalent figures.
The relation between the two geometries, Euclidean and projective,: 133  shows that mathematical objects are not given to us with their structure.: 21  Rather, each mathematical theory describes its objects by some of their properties, precisely those that are put as axioms at the foundations of the theory.: 20 Distances and angles cannot appear in theorems of projective geometry, since these notions are neither mentioned in the axioms of projective geometry nor defined from the notions mentioned there. The question ""what is the sum of the three angles of a triangle"" is meaningful in Euclidean geometry but meaningless in projective geometry.
A different situation appeared in the 19th century: in some geometries the sum of the three angles of a triangle is well-defined but different from the classical value (180 degrees). Non-Euclidean hyperbolic geometry, introduced by Nikolai Lobachevsky in 1829 and János Bolyai in 1832 (and Carl Friedrich Gauss in 1816, unpublished): 133  stated that the sum depends on the triangle and is always less than 180 degrees. Eugenio Beltrami in 1868 and Felix Klein in 1871 obtained Euclidean ""models"" of the non-Euclidean hyperbolic geometry, and thereby completely justified this theory as a logical possibility.: 24 This discovery forced the abandonment of the pretensions to the absolute truth of Euclidean geometry. It showed that axioms are not ""obvious"", nor ""implications of definitions"". Rather, they are hypotheses. To what extent do they correspond to an experimental reality? This important physical problem no longer has anything to do with mathematics. Even if a ""geometry"" does not correspond to an experimental reality, its theorems remain no less ""mathematical truths"".: 15 A Euclidean model of a non-Euclidean geometry is a choice of some objects existing in Euclidean space and some relations between these objects that satisfy all axioms (and therefore, all theorems) of the non-Euclidean geometry. These Euclidean objects and relations ""play"" the non-Euclidean geometry like contemporary actors playing an ancient performance. Actors can imitate a situation that never occurred in reality. Relations between the actors on the stage imitate relations between the characters in the play. Likewise, the chosen relations between the chosen objects of the Euclidean model imitate the non-Euclidean relations. It shows that relations between objects are essential in mathematics, while the nature of the objects is not.


=== The golden age of geometry and afterwards ===
The word ""geometry"" (from Ancient Greek: geo- ""earth"", -metron ""measurement"") initially meant a practical way of processing lengths, regions and volumes in the space in which we live, but was then extended widely (as well as the notion of space in question here).
According to Bourbaki,: 131  the period between 1795 (Géométrie descriptive of Monge) and 1872 (the ""Erlangen programme"" of Klein) can be called ""the golden age of geometry"".  The original space investigated by Euclid is now called three-dimensional Euclidean space. Its axiomatization, started by Euclid 23 centuries ago, was reformed with Hilbert's axioms, Tarski's axioms and Birkhoff's axioms. These axiom systems describe the space via primitive notions (such as ""point"", ""between"", ""congruent"") constrained by a number of axioms.
Analytic geometry made great progress and succeeded in replacing theorems of classical geometry with computations via invariants of transformation groups.: 134, 5  Since that time, new theorems of classical geometry have been of more interest to amateurs than to professional mathematicians.: 136  However, the heritage of classical geometry was not lost. According to Bourbaki,: 138  ""passed over in its role as an autonomous and living science, classical geometry is thus transfigured into a universal language of contemporary mathematics"".
Simultaneously, numbers began to displace geometry as the foundation of mathematics.  For instance, in Richard Dedekind's 1872 essay Stetigkeit und irrationale Zahlen (Continuity and irrational numbers), he asserts that points on a line ought to have the properties of Dedekind cuts, and that therefore a line was the same thing as the set of real numbers.  Dedekind is careful to note that this is an assumption that is incapable of being proven.  In modern treatments, Dedekind's assertion is often taken to be the definition of a line, thereby reducing geometry to arithmetic.  Three-dimensional Euclidean space is defined to be an affine space whose associated vector space of differences of its elements is equipped with an inner product. A definition ""from scratch"", as in Euclid, is now not often used, since it does not reveal the relation of this space to other spaces. Also, a three-dimensional projective space is now defined as the space of all one-dimensional subspaces (that is, straight lines through the origin) of a four-dimensional vector space.  This shift in foundations requires a new set of axioms, and if these axioms are adopted, the classical axioms of geometry become theorems.
A space now consists of selected mathematical objects (for instance, functions on another space, or subspaces of another space, or just elements of a set) treated as points, and selected relationships between these points. Therefore, spaces are just mathematical structures of convenience. One may expect that the structures called ""spaces"" are perceived more geometrically than other mathematical objects, but this is not always true.
According to the famous inaugural lecture given by Bernhard Riemann in 1854, every mathematical object parametrized by n real numbers may be treated as a point of the n-dimensional space of all such objects.: 140  Contemporary mathematicians follow this idea routinely and find it extremely suggestive to use the terminology of classical geometry nearly everywhere.: 138 Functions are important mathematical objects. Usually they form infinite-dimensional function spaces, as noted already by Riemann: 141  and elaborated in the 20th century by functional analysis.


== Taxonomy of spaces ==


=== Three taxonomic ranks ===
While each type of space has its own definition, the general idea of ""space"" evades formalization. Some structures are called spaces, other are not, without a formal criterion. Moreover, there is no consensus on the general idea of ""structure"".
According to Pudlák, ""Mathematics [...] cannot be explained completely by a single concept such as the mathematical structure. Nevertheless, Bourbaki's structuralist approach is the best that we have.""
We will return to Bourbaki's structuralist approach in the last section ""Spaces and structures"", while  we now outline a possible classification of spaces (and structures) in the spirit of Bourbaki.
We classify spaces on three levels. Given that each mathematical theory describes its objects by some of their properties, the first question to ask is: which properties? This leads to the first (upper) classification level. On the second level, one takes into account answers to especially important questions (among the questions that make sense according to the first level). On the third level of classification, one takes into account answers to all possible questions.
For example, the upper-level classification distinguishes between Euclidean and projective spaces, since the distance between two points is defined in Euclidean spaces but undefined in projective spaces. 
Another example. The question ""what is the sum of the three angles of a triangle"" makes sense in a Euclidean space but not in a projective space. In a non-Euclidean space the question makes sense but is answered differently, which is not an upper-level distinction.
Also, the distinction between a Euclidean plane and a Euclidean 3-dimensional space is not an upper-level distinction; the question ""what is the dimension"" makes sense in both cases.
The second-level classification distinguishes, for example, between Euclidean and non-Euclidean spaces; between finite-dimensional and infinite-dimensional spaces; between compact and non-compact spaces, etc.
In Bourbaki's terms, the second-level classification is the classification by ""species"". Unlike biological taxonomy, a space may belong to several species.
The third-level classification distinguishes, for example, between spaces of different dimension, but does not distinguish between a plane of a three-dimensional Euclidean space, treated as a two-dimensional Euclidean space, and the set of all pairs of real numbers, also treated as a two-dimensional Euclidean space. Likewise it does not distinguish between different Euclidean models of the same non-Euclidean space.
More formally, the third level classifies spaces up to isomorphism. An isomorphism between two spaces is defined as a one-to-one correspondence between the points of the first space and the points of the second space, that preserves all relations stipulated according to the first level. Mutually isomorphic spaces are thought of as copies of a single space. If one of them belongs to a given species then they all do.
The notion of isomorphism sheds light on the upper-level classification. Given a one-to-one correspondence between two spaces of the same upper-level class, one may ask whether it is an isomorphism or not. This question makes no sense for two spaces of different classes.
An isomorphism to itself is called an automorphism. Automorphisms of a Euclidean space are shifts, rotations, reflections and compositions of these. Euclidean space is homogeneous in the sense that every point can be transformed into every other point by some automorphism.
Euclidean axioms leave no freedom; they determine uniquely all geometric properties of the space. More exactly: all three-dimensional Euclidean spaces are mutually isomorphic. In this sense we have ""the"" three-dimensional Euclidean space. In Bourbaki's terms, the corresponding theory is univalent. In contrast, topological spaces are generally non-isomorphic; their theory is multivalent. A similar idea occurs in mathematical logic: a theory is called categorical if all its models of the same cardinality are mutually isomorphic. According to Bourbaki, the study of multivalent theories is the most striking feature which distinguishes modern mathematics from classical mathematics.


=== Relations between species of spaces ===
Topological notions (continuity, convergence, open sets, closed sets etc.) are defined naturally in every Euclidean space. In other words, every Euclidean space is also a topological space. Every isomorphism between two Euclidean spaces is also an isomorphism between the corresponding topological spaces (called ""homeomorphism""), but the converse is wrong: a homeomorphism may distort distances. In Bourbaki's terms, ""topological space"" is an underlying structure of the ""Euclidean space"" structure. Similar ideas occur in category theory: the category of Euclidean spaces is a concrete category over the category of topological spaces; the forgetful (or ""stripping"") functor maps the former category to the latter category.
A three-dimensional Euclidean space is a special case of a Euclidean space. In Bourbaki's terms, the species of three-dimensional Euclidean space is richer than the species of Euclidean space. Likewise, the species of compact topological space is richer than the species of topological space.

Such relations between species of spaces may be expressed diagrammatically as shown in Fig. 3. An arrow from A to B means that every A-space is also a B-space, or may be treated as a B-space, or provides a B-space, etc. Treating A and B as classes of spaces one may interpret the arrow as a transition from A to B. (In Bourbaki's terms, ""procedure of deduction"" of a B-space from a A-space. Not quite a function unless the classes A,B are sets; this nuance does not invalidate the following.) The two arrows on Fig. 3 are not invertible, but for different reasons. 
The transition from ""Euclidean"" to ""topological"" is forgetful. Topology distinguishes continuous from discontinuous, but does not distinguish rectilinear from curvilinear. Intuition tells us that the Euclidean structure cannot be restored from the topology. A proof uses an automorphism of the topological space (that is, self-homeomorphism) that is not an automorphism of the Euclidean space (that is, not a composition of shifts, rotations and reflections). Such transformation turns the given Euclidean structure into a (isomorphic but) different Euclidean structure; both Euclidean structures correspond to a single topological structure.
In contrast, the transition from ""3-dim Euclidean"" to ""Euclidean"" is not forgetful; a Euclidean space need not be 3-dimensional, but if it happens to be 3-dimensional, it is full-fledged, no structure is lost. In other words, the latter transition is injective (one-to-one), while the former transition is not injective (many-to-one). We denote injective transitions by an arrow with a barbed tail, ""↣"" rather than ""→"".
Both transitions are not surjective, that is, not every B-space results from some A-space. First, a 3-dim Euclidean space is a special (not general) case of a Euclidean space. Second, a topology of a Euclidean space is a special case of topology (for instance, it must be non-compact, and connected, etc). We denote surjective transitions by a two-headed arrow, ""↠"" rather than ""→"". See for example Fig. 4; there, the arrow from ""real linear topological"" to ""real linear"" is two-headed, since every real linear space admits some (at least one) topology compatible with its linear structure.
Such topology is non-unique in general, but unique when the real linear space is finite-dimensional. For these spaces the transition is both injective and surjective, that is, bijective; see the arrow from ""finite-dim real linear topological"" to ""finite-dim real linear"" on Fig. 4. The inverse transition exists (and could be shown by a second, backward arrow). The two species of structures are thus equivalent. In practice, one makes no distinction between equivalent species of structures. Equivalent structures may be treated as a single structure, as shown by a large box on Fig. 4.
The transitions denoted by the arrows obey isomorphisms. That is, two isomorphic A-spaces lead to two isomorphic B-spaces.
The diagram on Fig. 4 is commutative. That is, all directed paths in the diagram with the same start and endpoints lead to the same result. Other diagrams below are also commutative, except for dashed arrows on Fig. 9. The arrow from ""topological"" to ""measurable"" is dashed for the reason explained there: ""In order to turn a topological space into a measurable space one endows it with a σ-algebra. The σ-algebra of Borel sets is the most popular, but not the only choice."" A solid arrow denotes a prevalent, so-called ""canonical"" transition that suggests itself naturally and is widely used, often implicitly, by default. For example, speaking about a continuous function on a Euclidean space, one need not specify its topology explicitly. In fact, alternative topologies exist and are used sometimes, for example, the fine topology; but these are always specified explicitly, since they are much less notable that the prevalent topology. A dashed arrow indicates that several transitions are in use and no one is quite prevalent.


== Types of spaces ==


=== Linear and topological spaces ===

Two basic spaces are linear spaces (also called vector spaces) and topological spaces.
Linear spaces are of algebraic nature; there are real linear spaces (over the field of real numbers),
complex linear spaces (over the field of complex numbers), and more generally, linear spaces over any field. Every complex linear space is also a real linear space (the latter underlies the former), since each complex number can be specified by two real numbers. For example, the complex plane treated as a one-dimensional complex linear space may be downgraded to a two-dimensional real linear space. In contrast, the real line can be treated as a one-dimensional real linear space but not a complex linear space. See also field extensions. More generally, a vector space over a field also has the structure of a vector space over a subfield of that field.
Linear operations, given in a linear space by definition, lead to such notions as straight lines (and planes, and other linear subspaces); parallel lines; ellipses (and ellipsoids). However, it is impossible to define 
orthogonal (perpendicular) lines, or to single out circles among ellipses, because in a linear space 
there is no structure like a scalar product that could be used for measuring angles. The dimension of a linear space is defined as the maximal number of linearly independent vectors or, equivalently, as the minimal number of vectors that span the space; it may be finite or infinite. Two linear spaces over the same field are isomorphic if and only if they are of the same dimension. A n-dimensional complex linear space is also a 2n-dimensional real linear space.
Topological spaces are of analytic nature. Open sets, given in a topological space by definition, lead to such notions as continuous functions, paths, maps; convergent sequences, limits; interior, boundary, exterior. However, uniform continuity, bounded sets, Cauchy sequences, differentiable functions (paths, maps) remain undefined. Isomorphisms between topological spaces are traditionally called homeomorphisms; these are one-to-one correspondences continuous in both directions. The open interval (0,1) is homeomorphic to the whole real line (−∞,∞) but not homeomorphic to the closed interval [0,1], nor to a circle. The surface of a cube is homeomorphic to a sphere (the surface of a ball) but not homeomorphic to a torus. Euclidean spaces of different dimensions are not homeomorphic, which seems evident, but is not easy to prove. The dimension of a topological space is difficult to define; inductive dimension (based on the observation that the dimension of the boundary of a geometric figure is usually one less than the dimension of the figure itself) and Lebesgue covering dimension can be used. In the case of a n-dimensional Euclidean space, both topological dimensions are equal to n.
Every subset of a topological space is itself a topological space (in contrast, only linear subsets of a linear space are linear spaces). Arbitrary topological spaces, investigated by general topology (called also point-set topology) are too diverse for a complete classification up to homeomorphism. Compact topological spaces are an important class of topological spaces (""species"" of this ""type""). Every continuous function is bounded on such space. The closed interval [0,1] and the extended real line [−∞,∞] are compact; the open interval (0,1) and the line (−∞,∞) are not. Geometric topology investigates manifolds (another ""species"" of this ""type""); these are topological spaces locally homeomorphic to Euclidean spaces (and satisfying a few extra conditions). Low-dimensional manifolds are completely classified up to homeomorphism.
Both the linear and topological structures underlie the linear topological space (in other words, topological vector space) structure. A linear topological space is both a real or complex linear space and a topological space, such that the linear operations are continuous. So a linear space that is also topological is not in general a linear topological space. 
Every finite-dimensional real or complex linear space is a linear topological space in the sense that it carries one and only one topology that makes it a linear topological space. The two structures, ""finite-dimensional real or complex linear space"" and ""finite-dimensional linear topological space"", are thus equivalent, that is, mutually underlying. Accordingly, every invertible linear transformation of a finite-dimensional linear topological space is a homeomorphism. The three notions of dimension (one algebraic and two topological) agree for finite-dimensional real linear spaces. In infinite-dimensional spaces, however, different topologies can conform to a given linear structure, and invertible linear transformations are generally not homeomorphisms.


=== Affine and projective spaces ===

It is convenient to introduce affine and projective spaces by means of linear spaces, as follows. A n-dimensional linear subspace of a (n+1)-dimensional linear space, being itself a n-dimensional linear space, is not homogeneous; it contains a special point, the origin. Shifting it by a vector external to it, one obtains a n-dimensional affine subspace. It is homogeneous. An affine space need not be included into a linear space, but is isomorphic to an affine subspace of a linear space. All n-dimensional affine spaces over a given field are mutually isomorphic. In the words of John Baez, ""an affine space is a vector space that's forgotten its origin"". In particular, every linear space is also an affine space.
Given an n-dimensional affine subspace A in a (n+1)-dimensional linear space L, a straight line in A may be defined as the intersection of A with a two-dimensional linear subspace  of L that intersects A: in other words, with a plane through the origin that is not parallel to A. More generally, a k-dimensional affine subspace of A is the intersection of A with a (k+1)-dimensional linear subspace of L that intersects A.
Every point of the affine subspace A is the intersection of A with a one-dimensional linear subspace of L. However, some one-dimensional subspaces of L are parallel to A; in some sense, they intersect A at infinity. The set of all one-dimensional linear subspaces of a (n+1)-dimensional linear space is, by definition, a n-dimensional projective space. And the affine subspace A is embedded into the projective space as a proper subset. However, the projective space itself is homogeneous. A straight line in the projective space corresponds to a two-dimensional linear subspace of the (n+1)-dimensional linear space. More generally, a k-dimensional projective subspace of the projective space corresponds to a (k+1)-dimensional linear subspace of the (n+1)-dimensional linear space, and is isomorphic to the k-dimensional projective space.
Defined this way, affine and projective spaces are of algebraic nature; they can be real, complex, and more generally, over any field.
Every real or complex affine or projective space is also a topological space. An affine space is a non-compact manifold; a projective space is a compact manifold. In a real projective space a straight line is homeomorphic to a circle, therefore compact, in contrast to a straight line in a linear of affine space.


=== Metric and uniform spaces ===

Distances between points are defined in a metric space. Isomorphisms between metric spaces are called isometries. Every metric space is also a topological space. A topological space is called metrizable, if it underlies a metric space. All manifolds are metrizable.
In a metric space, we can define 
bounded sets and Cauchy sequences. A metric space is called complete if all Cauchy sequences converge. Every incomplete space is isometrically embedded, as a dense subset, into a complete space (the completion). Every compact metric space is complete; the real line is non-compact but complete; the open interval (0,1) is incomplete. 
Every Euclidean space is also a complete metric space. Moreover, all geometric notions immanent to a Euclidean space can be characterized in terms of its metric. For example, the straight segment connecting two given points A and C consists of all points B such that the distance between A and C is equal to the sum of two distances, between A and B and between B and C.
The Hausdorff dimension (related to the number of small balls that cover the given set) applies to metric spaces, and can be non-integer (especially for fractals). For a n-dimensional Euclidean space, the Hausdorff dimension is equal to n.
Uniform spaces do not introduce distances, but still allow one to use uniform continuity, Cauchy sequences (or filters or nets), completeness and completion. Every uniform space is also a topological space. Every linear topological space (metrizable or not) is also a uniform space, and is complete in finite dimension but generally incomplete in infinite dimension. More generally, every commutative topological group is also a uniform space. A non-commutative topological group, however, carries two uniform structures, one left-invariant, the other right-invariant.


=== Normed, Banach, inner product, and Hilbert spaces ===

Vectors in a Euclidean space form a linear space, but each vector 
  
    
      
        x
      
    
    {\displaystyle x}
   has also a length, in other words, norm, 
  
    
      
        ‖
        x
        ‖
      
    
    {\displaystyle \lVert x\rVert }
  . A real or complex linear space endowed with a norm is a normed space. Every normed space is both a linear topological space and a metric space. A Banach space is a complete normed space. Many spaces of sequences or functions are infinite-dimensional Banach spaces.
The set of all vectors of norm less than one is called the unit ball of a normed space. It is a convex, centrally symmetric set, generally not an ellipsoid; for example, it may be a polygon (in the plane) or, more generally, a polytope (in arbitrary finite dimension). The parallelogram law (called also parallelogram identity) 

  
    
      
        ‖
        x
        −
        y
        
          ‖
          
            2
          
        
        +
        ‖
        x
        +
        y
        
          ‖
          
            2
          
        
        =
        2
        ‖
        x
        
          ‖
          
            2
          
        
        +
        2
        ‖
        y
        
          ‖
          
            2
          
        
         
        ,
      
    
    {\displaystyle \lVert x-y\rVert ^{2}+\lVert x+y\rVert ^{2}=2\lVert x\rVert ^{2}+2\lVert y\rVert ^{2}\ ,}
  generally fails in normed spaces, but holds for vectors in Euclidean spaces, which follows from the fact that the squared Euclidean norm of a vector is its inner product with itself, 
  
    
      
        ‖
        x
        
          ‖
          
            2
          
        
        =
        (
        x
        ,
        x
        )
      
    
    {\displaystyle \lVert x\rVert ^{2}=(x,x)}
  .
An inner product space is a real or complex linear space, endowed with a bilinear or respectively sesquilinear form, satisfying some conditions and called an inner product. Every inner product space is also a normed space. A normed space underlies an inner product space if and only if it satisfies the parallelogram law, or equivalently, if its unit ball is an ellipsoid. Angles between vectors are defined in inner product spaces. A Hilbert space is defined as a complete inner product space. (Some authors insist that it must be complex, others admit also real Hilbert spaces.) Many spaces of sequences or functions are infinite-dimensional Hilbert spaces. Hilbert spaces are very important for quantum theory.All n-dimensional real inner product spaces are mutually isomorphic. One may say that the n-dimensional Euclidean space is the n-dimensional real inner product space that forgot its origin.


=== Smooth and Riemannian manifolds ===

Smooth manifolds are not called ""spaces"", but could be. Every smooth manifold is a topological manifold, and can be embedded into a finite-dimensional linear space. Smooth surfaces in a finite-dimensional linear space  are smooth manifolds: for example, the surface of an ellipsoid is a smooth manifold, a polytope is not. Real or complex finite-dimensional linear, affine and projective spaces are also smooth manifolds.
At each one of its points, a smooth path in a smooth manifold has a tangent vector that belongs to the manifold's tangent space at this point. Tangent spaces to an n-dimensional smooth manifold are n-dimensional linear spaces. The differential of a smooth function on a smooth manifold provides a linear functional on the tangent space at each point. 
A Riemannian manifold, or Riemann space, is a smooth manifold whose tangent spaces are endowed with inner products satisfying some conditions. Euclidean spaces are also Riemann spaces. Smooth surfaces in Euclidean spaces are Riemann spaces. A hyperbolic non-Euclidean space is also a Riemann space. A curve in a Riemann space has a length, and the length of the shortest curve between two points defines a distance, such that the Riemann space is a metric space. The angle between two curves intersecting at a point is the angle between their tangent lines.
Waiving positivity of inner products on tangent spaces, one obtains pseudo-Riemann spaces, including the Lorentzian spaces that are very important for general relativity.


=== Measurable, measure, and probability spaces ===

Waiving distances and angles while retaining volumes (of geometric bodies) one reaches measure theory. Besides the volume, a measure generalizes the notions of area, length, mass (or charge) distribution, and also probability distribution, according to Andrey Kolmogorov's approach to probability theory.
A ""geometric body"" of classical mathematics is much more regular than just a set of points. The boundary of the body is of zero volume. Thus, the volume of the body is the volume of its interior, and the interior can be exhausted by an infinite sequence of cubes. In contrast, the boundary of an arbitrary set of points can be of non-zero volume (an example: the set of all rational points inside a given cube). Measure theory succeeded in extending the notion of volume to a vast class of sets, the so-called measurable sets. Indeed, non-measurable sets almost never occur in applications.
Measurable sets, given in a measurable space by definition, lead to measurable functions and maps. In order to turn a topological space into a measurable space one endows it with a σ-algebra. The σ-algebra of Borel sets is the most popular, but not the only choice. (Baire sets, universally measurable sets, etc, are also used sometimes.) 
The topology is not uniquely determined by the Borel σ-algebra; for example, the norm topology and the weak topology on a separable Hilbert space lead to the same Borel σ-algebra.
Not every σ-algebra is the Borel σ-algebra of some topology.
Actually, a σ-algebra can be generated by a given collection of sets (or functions) irrespective of any topology.  Every subset of a measurable space is itself a measurable space.
Standard measurable spaces (also called standard Borel spaces) are especially useful due to some similarity to compact spaces (see EoM). Every bijective measurable mapping between standard measurable spaces is an isomorphism; that is, the inverse mapping is also measurable. And a mapping between such spaces is measurable if and only if its graph is measurable in the product space. Similarly, every bijective continuous mapping between compact metric spaces is a homeomorphism; that is, the inverse mapping is also continuous. And a mapping between such spaces is continuous if and only if its graph is closed in the product space.
Every Borel set in a Euclidean space (and more generally, in a complete separable metric space), endowed with the Borel σ-algebra, is a standard measurable space. All uncountable standard measurable spaces are mutually isomorphic.
A measure space is a measurable space endowed with a measure. A Euclidean space with the Lebesgue measure is a measure space. Integration theory defines integrability and integrals of measurable functions on a measure space.
Sets of measure 0, called null sets, are negligible. Accordingly, a ""mod 0 isomorphism"" is defined as isomorphism between subsets of full measure (that is, with negligible complement).
A probability space is a measure space such that the measure of the whole space is equal to 1. The product of any family (finite or not) of probability spaces is a probability space. In contrast, for measure spaces in general, only the product of finitely many spaces is defined. Accordingly, there are many infinite-dimensional probability measures (especially, Gaussian measures), but no infinite-dimensional Lebesgue measures.
Standard probability spaces are especially useful. On a standard probability space a conditional expectation may be treated as the integral over the conditional measure (regular conditional probabilities, see also disintegration of measure). Given two standard probability spaces, every homomorphism of their measure algebras is induced by some measure preserving map. Every probability measure on a standard measurable space leads to a standard probability space. The product of a sequence (finite or not) of standard probability spaces is a standard probability space. All non-atomic standard probability spaces are mutually isomorphic mod 0; one of them is the interval (0,1) with the Lebesgue measure.
These spaces are less geometric. In particular, the idea of dimension, applicable (in one form or another) to all other spaces, does not apply to measurable, measure and probability spaces.


=== Non-commutative geometry ===
The theoretical study of calculus, known as mathematical analysis, led in the early 20th century to the consideration of linear spaces of real-valued or complex-valued functions.  The earliest examples of these were function spaces, each one adapted to its own class of problems.  These examples shared many common features, and these features were soon abstracted into Hilbert spaces, Banach spaces, and more general topological vector spaces.  These were a powerful toolkit for the solution of a wide range of mathematical problems.
The most detailed information was carried by a class of spaces called Banach algebras.  These are Banach spaces together with a continuous multiplication operation.  An important early example was the Banach algebra of essentially bounded measurable functions on a measure space X.  This set of functions is a Banach space under pointwise addition and scalar multiplication.  With the operation of pointwise multiplication, it becomes a special type of Banach space, one now called a commutative von Neumann algebra.  Pointwise multiplication determines a representation of this algebra on the Hilbert space of square integrable functions on X.  An early observation of John von Neumann was that this correspondence also worked in reverse: Given some mild technical hypotheses, a commutative von Neumann algebra together with a representation on a Hilbert space determines a measure space, and these two constructions (of a von Neumann algebra plus a representation and of a measure space) are mutually inverse.
Von Neumann then proposed that non-commutative von Neumann algebras should have geometric meaning, just as commutative von Neumann algebras do.  Together with Francis Murray, he produced a classification of von Neumann algebras.  The direct integral construction shows how to break any von Neumann algebra into a collection of simpler algebras called factors.  Von Neumann and Murray classified factors into three types.  Type I was nearly identical to the commutative case.  Types II and III exhibited new phenomena.  A type II von Neumann algebra determined a geometry with the peculiar feature that the dimension could be any non-negative real number, not just an integer.  Type III algebras were those that were neither types I nor II, and after several decades of effort, these were proven to be closely related to type II factors.
A slightly different approach to the geometry of function spaces developed at the same time as von Neumann and Murray's work on the classification of factors.  This approach is the theory of C*-algebras.  Here, the motivating example is the C*-algebra 
  
    
      
        
          C
          
            0
          
        
        (
        X
        )
      
    
    {\displaystyle C_{0}(X)}
  , where X is a locally compact Hausdorff topological space.  By definition, this is the algebra of continuous complex-valued functions on X that vanish at infinity (which loosely means that the farther you go from a chosen point, the closer the function gets to zero) with the operations of pointwise addition and multiplication.  The Gelfand–Naimark theorem implied that there is a correspondence between commutative C*-algebras and geometric objects: Every commutative C*-algebra is of the form 
  
    
      
        
          C
          
            0
          
        
        (
        X
        )
      
    
    {\displaystyle C_{0}(X)}
   for some locally compact Hausdorff space X.  Consequently it is possible to study locally compact Hausdorff spaces purely in terms of commutative C*-algebras.  Non-commutative geometry takes this as inspiration for the study of non-commutative C*-algebras: If there were such a thing as a ""non-commutative space X,"" then its 
  
    
      
        
          C
          
            0
          
        
        (
        X
        )
      
    
    {\displaystyle C_{0}(X)}
   would be a non-commutative C*-algebra; if in addition the Gelfand–Naimark theorem applied to these non-existent objects, then spaces (commutative or not) would be the same as C*-algebras; so, for lack of a direct approach to the definition of a non-commutative space, a non-commutative space is defined to be a non-commutative C*-algebra.  Many standard geometric tools can be restated in terms of C*-algebras, and this gives geometrically-inspired techniques for studying non-commutative C*-algebras.
Both of these examples are now cases of a field called non-commutative geometry.  The specific examples of von Neumann algebras and C*-algebras are known as non-commutative measure theory and non-commutative topology, respectively.  Non-commutative geometry is not merely a pursuit of generality for its own sake and is not just a curiosity.  Non-commutative spaces arise naturally, even inevitably, from some constructions.  For example, consider the non-periodic Penrose tilings of the plane by kites and darts.  It is a theorem that, in such a tiling, every finite patch of kites and darts appears infinitely often.  As a consequence, there is no way to distinguish two Penrose tilings by looking at a finite portion.  This makes it impossible to assign the set of all tilings a topology in the traditional sense.  Despite this, the Penrose tilings determine a non-commutative C*-algebra, and consequently they can be studied by the techniques of non-commutative geometry.  Another example, and one of great interest within differential geometry, comes from foliations of manifolds.  These are ways of splitting the manifold up into smaller-dimensional submanifolds called leaves, each of which is locally parallel to others nearby.  The set of all leaves can be made into a topological space.  However, the example of an irrational rotation shows that this topological space can be inaccessible to the techniques of classical measure theory.  However, there is a non-commutative von Neumann algebra associated to the leaf space of a foliation, and once again, this gives an otherwise unintelligible space a good geometric structure.


=== Schemes ===

Algebraic geometry studies the geometric properties of polynomial equations.  Polynomials are a type of function defined from the basic arithmetic operations of addition and multiplication.  Because of this, they are closely tied to algebra. Algebraic geometry offers a way to apply geometric techniques to questions of pure algebra, and vice versa.
Prior to the 1940s, algebraic geometry worked exclusively over the complex numbers, and the most fundamental variety was projective space.  The geometry of projective space is closely related to the theory of perspective, and its algebra is described by homogeneous polynomials.  All other varieties were defined as subsets of projective space.  Projective varieties were subsets defined by a set of homogeneous polynomials.  At each point of the projective variety, all the polynomials in the set were required to equal zero.  The complement of the zero set of a linear polynomial is an affine space, and an affine variety was the intersection of a projective variety with an affine space.
André Weil saw that geometric reasoning could sometimes be applied in number-theoretic situations where the spaces in question might be discrete or even finite.  In pursuit of this idea, Weil rewrote the foundations of algebraic geometry, both freeing algebraic geometry from its reliance on complex numbers and introducing abstract algebraic varieties which were not embedded in projective space.  These are now simply called varieties.
The type of space that underlies most modern algebraic geometry is even more general than Weil's abstract algebraic varieties.  It was introduced by Alexander Grothendieck and is called a scheme.  One of the motivations for scheme theory is that polynomials are unusually structured among functions, and algebraic varieties are consequently rigid.  This presents problems when attempting to study degenerate situations.  For example, almost any pair of points on a circle determines a unique line called the secant line, and as the two points move around the circle, the secant line varies continuously.  However, when the two points collide, the secant line degenerates to a tangent line.  The tangent line is unique, but the geometry of this configuration—a single point on a circle—is not expressive enough to determine a unique line.  Studying situations like this requires a theory capable of assigning extra data to degenerate situations.
One of the building blocks of a scheme is a topological space.  Topological spaces have continuous functions, but continuous functions are too general to reflect the underlying algebraic structure of interest. The other ingredient in a scheme, therefore, is a sheaf on the topological space, called the ""structure sheaf"".  On each open subset of the topological space, the sheaf specifies a collection of functions, called ""regular functions"".  The topological space and the structure sheaf together are required to satisfy conditions that mean the functions come from algebraic operations.
Like manifolds, schemes are defined as spaces that are locally modeled on a familiar space.  In the case of manifolds, the familiar space is Euclidean space.  For a scheme, the local models are called affine schemes.  Affine schemes provide a direct link between algebraic geometry and commutative algebra.  The fundamental objects of study in commutative algebra are commutative rings.  If 
  
    
      
        R
      
    
    {\displaystyle R}
   is a commutative ring, then there is a corresponding affine scheme 
  
    
      
        Spec
        ⁡
        R
      
    
    {\displaystyle \operatorname {Spec} R}
   which translates the algebraic structure of 
  
    
      
        R
      
    
    {\displaystyle R}
   into geometry.  Conversely, every affine scheme determines a commutative ring, namely, the ring of global sections of its structure sheaf.  These two operations are mutually inverse, so affine schemes provide a new language with which to study questions in commutative algebra.  By definition, every point in a scheme has an open neighborhood which is an affine scheme.
There are many schemes that are not affine.  In particular,  projective spaces satisfy a condition called properness which is analogous to compactness.  Affine schemes cannot be proper (except in trivial situations like when the scheme has only a single point), and hence no projective space is an affine scheme (except for zero-dimensional projective spaces).  Projective schemes, meaning those that arise as closed subschemes of a projective space, are the single most important family of schemes.Several generalizations of schemes have been introduced.  Michael Artin defined an algebraic space as the quotient of a scheme by the equivalence relations that define étale morphisms.  Algebraic spaces retain many of the useful properties of schemes while simultaneously being more flexible.  For instance, the Keel–Mori theorem can be used to show that many moduli spaces are algebraic spaces.
More general than an algebraic space is a Deligne–Mumford stack.  DM stacks are similar to schemes, but they permit singularities that cannot be described solely in terms of polynomials.  They play the same role for schemes that orbifolds do for manifolds.  For example, the quotient of the affine plane by a finite group of rotations around the origin yields a Deligne–Mumford stack that is not a scheme or an algebraic space.  Away from the origin, the quotient by the group action identifies finite sets of equally spaced points on a circle.  But at the origin, the circle consists of only a single point, the origin itself, and the group action fixes this point.  In the quotient DM stack, however, this point comes with the extra data of being a quotient.  This kind of refined structure is useful in the theory of moduli spaces, and in fact, it was originally introduced to describe moduli of algebraic curves.
A further generalization are the algebraic stacks, also called Artin stacks.  DM stacks are limited to quotients by finite group actions.  While this suffices for many problems in moduli theory, it is too restrictive for others, and Artin stacks permit more general quotients.


=== Topoi ===

In Grothendieck's work on the Weil conjectures, he introduced a new type of topology now called a Grothendieck topology.  A topological space (in the ordinary sense) axiomatizes the notion of ""nearness,"" making two points be nearby if and only if they lie in many of the same open sets.  By contrast, a Grothendieck topology axiomatizes the notion of ""covering"".  A covering of a space is a collection of subspaces that jointly contain all the information of the ambient space.  Since sheaves are defined in terms of coverings, a Grothendieck topology can also be seen as an axiomatization of the theory of sheaves.
Grothendieck's work on his topologies led him to the theory of topoi.  In his memoir Récoltes et Semailles, he called them his ""most vast conception"". A sheaf (either on a topological space or with respect to a Grothendieck topology) is used to express local data.  The category of all sheaves carries all possible ways of expressing local data.  Since topological spaces are constructed from points, which are themselves a kind of local data, the category of sheaves can therefore be used as a replacement for the original space.  Grothendieck consequently defined a topos to be a category of sheaves and studied topoi as objects of interest in their own right.  These are now called Grothendieck topoi.
Every topological space determines a topos, and vice versa.  There are topological spaces where taking the associated topos loses information, but these are generally considered pathological.  (A necessary and sufficient condition is that the topological space be a sober space.)  Conversely, there are topoi whose associated topological spaces do not capture the original topos.  But, far from being pathological, these topoi can be of great mathematical interest.  For instance, Grothendieck's theory of étale cohomology (which eventually led to the proof of the Weil conjectures) can be phrased as cohomology in the étale topos of a scheme, and this topos does not come from a topological space.
Topological spaces in fact lead to very special topoi called locales.  The set of open subsets of a topological space determines a lattice.  The axioms for a topological space cause these lattices to be complete Heyting algebras.  The theory of locales takes this as its starting point.  A locale is defined to be a complete Heyting algebra, and the elementary properties of topological spaces are re-expressed and reproved in these terms.  The concept of a locale turns out to be more general than a topological space, in that every sober topological space determines a unique locale, but many interesting locales do not come from topological spaces.  Because locales need not have points, the study of locales is somewhat jokingly called pointless topology.
Topoi also display deep connections to mathematical logic.  Every Grothendieck topos has a special sheaf called a subobject classifier.  This subobject classifier functions like the set of all possible truth values.  In the topos of sets, the subobject classifier is the set 
  
    
      
        {
        0
        ,
        1
        }
      
    
    {\displaystyle \{0,1\}}
  , corresponding to ""False"" and ""True"".  But in other topoi, the subobject classifier can be much more complicated.  Lawvere and Tierney recognized that axiomatizing the subobject classifier yielded a more general kind of topos, now known as an elementary topos, and that elementary topoi were models of intuitionistic logic.  In addition to providing a powerful way to apply tools from logic to geometry, this made possible the use of geometric methods in logic.


== Spaces and structure ==
According to Kevin Carlson,

Neither of these words [""space"" and ""structure""] have a single mathematical definition. The English words can be used in essentially all the same situations, but you often think of a ""space"" as more geometric and a ""structure"" as more algebraic. [...] So you could think of ""structures"" as places we do algebra, and ""spaces"" as places we do geometry. Then a lot of great mathematics has come from passing from structures to spaces and vice versa, as when we look at the fundamental group of a topological space or the spectrum of a ring. But in the end, the distinction is neither hard nor fast and only goes so far: many things are obviously both structures and spaces, some things are not obviously either, and some people might well disagree with everything I've said here.Nevertheless, a general definition of ""structure"" was proposed by Bourbaki; it embraces all types of spaces mentioned above, (nearly?) all types of mathematical structures used till now, and more. It provides a general definition of isomorphism, and justifies transfer of properties between isomorphic structures. However, it was never used actively in mathematical practice (not even in the mathematical treatises written by Bourbaki himself). Here are the last phrases from a review by Robert Reed of a book by Leo Corry:

Corry does not seem to feel that any formal definition of structure could do justice to the use of the concept in actual mathematical practice [...] Corry's view could be summarized as the belief that 'structure' refers essentially to a way of doing mathematics, and is therefore a concept probably just as far from being precisely definable as the cultural artifact of mathematics itself.For more information on mathematical structures see Wikipedia: mathematical structure, equivalent definitions of mathematical structures, and transport of structure.
The distinction between geometric ""spaces"" and algebraic ""structures"" is sometimes clear, sometimes elusive. Clearly, groups are algebraic, while Euclidean spaces are geometric. Modules over rings are as algebraic as groups. In particular, when the ring appears to be a field, the module appears to be a linear space; is it algebraic or geometric? In particular, when it is finite-dimensional, over real numbers, and endowed with inner product, it becomes Euclidean space; now geometric. The (algebraic?) field of real numbers is the same as the (geometric?) real line. Its algebraic closure, the (algebraic?) field of complex numbers, is the same as the (geometric?) complex plane. It is first of all ""a place we do analysis"" (rather than algebra or geometry).
Every space treated in Section ""Types of spaces"" above, except for ""Non-commutative geometry"", ""Schemes"" and ""Topoi"" subsections, is a set (the ""principal base set"" of the structure, according to Bourbaki) endowed with some additional structure; elements of the base set are usually called ""points"" of this space. In contrast, elements of (the base set of) an algebraic structure usually are not called ""points"".
However, sometimes one uses more than one principal base set. For example, two-dimensional projective geometry may be formalized via two base sets, the set of points and the set of lines. Moreover, a striking feature of projective planes is the symmetry of the roles played by points and lines. A less geometric example: a graph may be formalized via two base sets, the set of vertices (called also nodes or points) and the set of edges (called also arcs or lines). Generally, finitely many principal base sets and finitely many auxiliary base sets are stipulated by Bourbaki.
Many mathematical structures of geometric flavor treated in the ""Non-commutative geometry"", ""Schemes"" and ""Topoi"" subsections above do not stipulate a base set of points. For example, ""pointless topology"" (in other words, point-free topology, or locale theory) starts with a single base set whose elements imitate open sets in a topological space (but are not sets of points); see also mereotopology and point-free geometry.


== List of mathematical spaces ==


== See also ==
Mathematical structure
Transport of structure
Set (mathematics)
Dimension


== Notes ==


== Footnotes ==


== References ==
 This article was submitted to WikiJournal of Science for external academic peer review in 2017 (reviewer reports). The updated content was reintegrated into the Wikipedia page under a CC-BY-SA-3.0 license (2018). The version of record as reviewed is: 
Boris Tsirelson;  et al. (1 June 2018). ""Spaces in mathematics"" (PDF). WikiJournal of Science. 1 (1): 2. doi:10.15347/WJS/2018.002. ISSN 2470-6345. Wikidata Q55120290.

Bourbaki, Nicolas, Elements of mathematics, Hermann (original), Addison-Wesley (translation).
Bourbaki, Nicolas (1968), Elements of mathematics: Theory of sets, Hermann (original), Addison-Wesley (translation).
Eisenbud, David; Harris, Joe (2000), The Geometry of Schemes, Springer-Verlag, doi:10.1007/b97680, ISBN 978-0-387-98638-8.
Gowers, Timothy; Barrow-Green, June; Leader, Imre, eds. (2008), The Princeton Companion to Mathematics, Princeton University Press, ISBN 978-0-691-11880-2.
Itô, Kiyosi, ed. (1993), Encyclopedic dictionary of mathematics (second ed.), Mathematical society of Japan (original), MIT press (translation).


== External links ==
 Media related to Space (mathematics) at Wikimedia Commons
Matilde Marcolli (2009) The notion of space in mathematics, from Caltech."
366fc4ceca,Further Mathematics,"Further Mathematics is the title given to a number of advanced secondary mathematics courses. The term ""Higher and Further Mathematics"", and the term ""Advanced Level Mathematics"", may also refer to any of several advanced mathematics courses at many institutions.
In the United Kingdom, Further Mathematics describes a course studied in addition to the standard mathematics AS-Level and A-Level courses. In the state of Victoria in Australia, it describes a course delivered as part of the Victorian Certificate of Education (see § Australia (Victoria) for a more detailed explanation). Globally, it describes a course studied in addition to GCE AS-Level and A-Level Mathematics, or one which is delivered as part of the International Baccalaureate Diploma.


== United Kingdom ==


=== Background ===
A qualification in Further Mathematics involves studying both pure and applied modules. Whilst the pure modules (formerly known as Pure 4–6 or Core 4–6, now known as Further Pure 1–3, where 4 exists for the AQA board) build on knowledge from the core mathematics modules, the applied modules may start from first principles.
The structure of the qualification varies between exam boards.
With regard to Mathematics degrees, most universities do not require Further Mathematics, and may incorporate foundation math modules or offer ""catch-up"" classes covering any additional content. Exceptions are the University of Warwick, the University of Cambridge which requires Further Mathematics to at least AS level; University College London requires or recommends an A2 in Further Maths for its maths courses; Imperial College requires an A in A level Further Maths, while other universities may recommend it or may promise lower offers in return. Some schools and colleges may not offer Further mathematics, but online resources are available 
Although the subject has about 60% of its cohort obtaining ""A"" grades, students choosing the subject are assumed to be more proficient in mathematics, and there is much more overlap of topics compared to base mathematics courses at A level.
Some medicine courses do not count maths and further maths as separate subjects for the purposes of making offers. This is due to the overlap in content, and the potentially narrow education a candidate with maths, further maths and just one other subject may have.
The IGCSE equivalent of GCSE Further Maths is IGCSE Additional Mathematics.


=== Support ===
There are numerous sources of support for both teachers and students. The AMSP (formerly FMSP) is a government-funded organisation that offers professional development, enrichment activities and is a source of additional materials via its website. Registering with AMSP gives access to Integral, another source of both teaching and learning materials hosted by Mathematics Education Innovation (MEI). Underground Mathematics is another resource in active development which reflects the emphasis on problem solving and reasoning in the UK curriculum. A collection of tasks for post-16 mathematics can be also found on the NRICH site.


== Australia (Victoria) ==
In contrast with other Further Mathematics courses, Further Maths as part of the VCE is the easiest level of mathematics. Any student wishing to undertake tertiary studies in areas such as Science, Engineering, Commerce, Economics and some Information Technology courses must undertake one or both of the other two VCE maths subjects— Mathematical Methods or Specialist Mathematics. The Further Mathematics syllabus in VCE consists of three core modules, which all students undertake, plus two modules chosen by the student (or usually by the school or teacher) from a list of four. The core modules are Univariate Data, Bivariate Data, Time Series, Number Patterns and Business-Related Mathematics. The optional modules are Geometry and Trigonometry, Graphs and Relations, Networks and Decision Mathematics, or Matrices.


== Singapore ==
Further Mathematics is available as a second and higher mathematics course at A Level (now H2), in addition to the Mathematics course at A Level. Students can pursue this subject if they have A2 and better in 'O' Level Mathematics and Additional Mathematics, depending on the school. Some topics covered in this course include mathematical induction, complex number, polar curve and conic sections, differential equations, recurrence relations, matrices and linear spaces, numerical methods, random variables and hypothesis testing and confidence intervals.


== International Baccalaureate Diploma ==
Further Mathematics, as studied within the International Baccalaureate Diploma Programme, was a Higher Level (HL) course that could be taken in conjunction with Mathematics HL or on its own. It consisted of studying all four of the options in Mathematics HL, plus two additional topics.
Topics studied in Further Mathematics included:
Topic 1 - Linear algebra - studies on matrices, vector spaces, linear and geometric transformations
Topic 2 - Geometry - closer look on triangles, circles and conic sections
Topic 3 - Statistics and probability - the geometric and negative binomial distributions, unbiased estimators, statistical hypothesis testing and an introduction to bivariate distributions
Topic 4 - Sets, relations and groups - algebra of sets, ordered pairs, binary operations and group homomorphism
Topic 5 - Calculus - infinite sequences and series, limits, improper integrals and various first-order ordinary differential equations
Topic 6 - Discrete mathematics - complete mathematical induction, linear Diophantine equations, Fermat's little theorem, route inspection problem and recurrence relationsFrom 2019, the course has been discontinued and transited info the followings modules:
Mathematics: analysis and approaches SL
Mathematics: analysis and approaches HL
Mathematics: applications and interpretation SL
Mathematics: applications and interpretation HL


== India ==
CBSE does not include any 'further' mathematics courses. While the JEE Advanced syllabus is a slightly extended variant of the CBSE Class 11 and 12 syllabus, that is not a school-leaving examination.


== See also ==
Additional Mathematics
Advanced level mathematics


== References ==


== External links ==
The Further Mathematics Support Programme
Mechanics M1 Material
AMSP (Advanced Math Support Program)
Integral (High level support for AS/A level Maths & Further Maths)
Underground Mathematics (Resources on A level mathematics)"
935446639c,Graph (discrete mathematics),"In discrete mathematics, and more specifically in graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense ""related"". The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called link or line). Typically, a graph is depicted in diagrammatic form as a set of dots or circles for the vertices, joined by lines or curves for the edges. Graphs are one of the objects of study in discrete mathematics.
The edges may be directed or undirected. For example, if the vertices represent people at a party, and there is an edge between two people if they shake hands, then this graph is undirected because any person A can shake hands with a person B only if B also shakes hands with A. In contrast, if an edge from a person A to a person B means that A owes money to B, then this graph is directed, because owing money is not necessarily reciprocated.
Graphs are the basic subject studied by graph theory. The word ""graph"" was first used in this sense by J. J. Sylvester in 1878 due to a direct relation between mathematics and chemical structure (what he called a chemico-graphical image).


== Definitions ==
Definitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.


=== Graph ===

A graph (sometimes called an undirected graph to distinguish it from a directed graph, or a simple graph to distinguish it from a multigraph) is a pair G = (V, E), where V is a set whose elements are called vertices (singular: vertex), and E is a set of paired vertices, whose elements are called edges (sometimes links or lines).
The vertices x and y of an edge {x, y}  are called the endpoints of the edge. The edge is said to join x and y and to be incident on x and y. A vertex may belong to no edge, in which case it is not joined to any other vertex.
A multigraph is a generalization that allows multiple edges to have the same pair of endpoints. In some texts, multigraphs are simply called graphs.Sometimes, graphs are allowed to contain loops, which are edges that join a vertex to itself. To allow loops, the pairs of vertices in E must be allowed to have the same node twice.  Such generalized graphs are called graphs with loops or simply graphs when it is clear from the context that loops are allowed.
Generally, the set of vertices V is supposed to be finite; this implies that the set of edges is also finite. Infinite graphs are sometimes considered, but are more often viewed as a special kind of binary relation, as most results on finite graphs do not extend to the infinite case, or need a rather different proof.
An empty graph is a graph that has an empty set of vertices (and thus an empty set of edges). The order of a graph is its number of vertices |V|. The size of a graph is its number of edges  |E|. However, in some contexts, such as for expressing the computational complexity of algorithms, the size is |V| + |E| (otherwise, a non-empty graph could have size 0). The degree or valency of a vertex is the number of edges that are incident to it; for graphs with loops, a loop is counted twice.
In a graph of order n, the maximum degree of each vertex is n − 1 (or n + 1 if loops are allowed, because a loop contributes 2 to the degree), and the maximum number of edges is n(n − 1)/2 (or n(n + 1)/2 if loops are allowed).
The edges of a graph define a symmetric relation on the vertices, called the adjacency relation. Specifically, two vertices x and y are adjacent if {x, y}  is an edge. A graph may be fully specified by its adjacency matrix A, which is an n × n square matrix, with Aij specifying the number of connections from vertex i to vertex j. For a simple graph, Aij is either 0, indicating disconnection, or 1, indicating connection; moreover Aii = 0 because an edge in a simple graph cannot start and end at the same vertex. Graphs with self-loops will be characterized by some or all Aii being equal to a positive integer, and multigraphs (with multiple edges between vertices) will be characterized by some or all Aij being equal to a positive integer. Undirected graphs will have a symmetric adjacency matrix (meaning Aij = Aji).


=== Directed graph ===

A directed graph or digraph is a graph in which edges have orientations.
In one restricted but very common sense of the term, a directed graph is a pair G = (V, E) comprising:

V, a set of vertices (also called nodes or points);
E, a set of edges (also called directed edges, directed links, directed lines, arrows, or arcs), which are ordered pairs of distinct vertices: 
  
    
      
        E
        ⊆
        {
        (
        x
        ,
        y
        )
        ∣
        (
        x
        ,
        y
        )
        ∈
        
          V
          
            2
          
        
        
        
          
            and
          
        
        
        x
        ≠
        y
        }
      
    
    {\displaystyle E\subseteq \{(x,y)\mid (x,y)\in V^{2}\;{\textrm {and}}\;x\neq y\}}
  .To avoid ambiguity, this type of object may be called precisely a directed simple graph.
In the edge (x, y) directed from x to y, the vertices x and y are called the endpoints of the edge, x the tail of the edge and y the head of the edge. The edge is said to join x and y and to be incident on x and on y. A vertex may exist in a graph and not belong to an edge. The edge (y, x) is called the inverted edge of (x, y). Multiple edges, not allowed under the definition above, are two or more edges with both the same tail and the same head.
In one more general sense of the term allowing multiple edges, a directed graph is an ordered triple G = (V, E, ϕ) comprising:

V, a set of vertices (also called nodes or points);
E, a set of edges (also called directed edges, directed links, directed lines, arrows or arcs);
ϕ, an incidence function mapping every edge to an ordered pair of vertices (that is, an edge is associated with two distinct vertices): 
  
    
      
        ϕ
        :
        E
        →
        {
        (
        x
        ,
        y
        )
        ∣
        (
        x
        ,
        y
        )
        ∈
        
          V
          
            2
          
        
        
        
          
            and
          
        
        
        x
        ≠
        y
        }
      
    
    {\displaystyle \phi :E\to \{(x,y)\mid (x,y)\in V^{2}\;{\textrm {and}}\;x\neq y\}}
  .To avoid ambiguity, this type of object may be called precisely a directed multigraph.
A loop is an edge that joins a vertex to itself. Directed graphs as defined in the two definitions above cannot have loops, because a loop joining a vertex 
  
    
      
        x
      
    
    {\displaystyle x}
   to itself is the edge (for a directed simple graph) or is incident on (for a directed multigraph) 
  
    
      
        (
        x
        ,
        x
        )
      
    
    {\displaystyle (x,x)}
   which is not in 
  
    
      
        {
        (
        x
        ,
        y
        )
        ∣
        (
        x
        ,
        y
        )
        ∈
        
          V
          
            2
          
        
        
        
          
            and
          
        
        
        x
        ≠
        y
        }
      
    
    {\displaystyle \{(x,y)\mid (x,y)\in V^{2}\;{\textrm {and}}\;x\neq y\}}
  . So to allow loops the definitions must be expanded. For directed simple graphs, the definition of 
  
    
      
        E
      
    
    {\displaystyle E}
   should be modified to 
  
    
      
        E
        ⊆
        {
        (
        x
        ,
        y
        )
        ∣
        (
        x
        ,
        y
        )
        ∈
        
          V
          
            2
          
        
        }
      
    
    {\displaystyle E\subseteq \{(x,y)\mid (x,y)\in V^{2}\}}
  . For directed multigraphs, the definition of 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   should be modified to 
  
    
      
        ϕ
        :
        E
        →
        {
        (
        x
        ,
        y
        )
        ∣
        (
        x
        ,
        y
        )
        ∈
        
          V
          
            2
          
        
        }
      
    
    {\displaystyle \phi :E\to \{(x,y)\mid (x,y)\in V^{2}\}}
  . To avoid ambiguity, these types of objects may be called precisely a directed simple graph permitting loops and a directed multigraph permitting loops (or a quiver) respectively.
The edges of a directed simple graph permitting loops G is a homogeneous relation ~ on the vertices of G that is called the adjacency relation of G. Specifically, for each edge (x, y), its endpoints x and y are said to be adjacent to one another, which is denoted x ~ y.


=== Mixed graph ===

A mixed graph is a graph in which some edges may be directed and some may be undirected. It is an ordered triple G = (V, E, A) for a mixed simple graph and G = (V, E, A, ϕE, ϕA) for a mixed multigraph with V, E (the undirected edges), A (the directed edges), ϕE and ϕA defined as above. Directed and undirected graphs are special cases.


=== Weighted graph ===

A weighted graph or a network is a graph in which a number (the weight) is assigned to each edge. Such weights might represent for example costs, lengths or capacities, depending on the problem at hand. Such graphs arise in many contexts, for example in shortest path problems such as the traveling salesman problem.


== Types of graphs ==


=== Oriented graph ===
One definition of an oriented graph is that it is a directed graph in which at most one of (x, y) and (y, x) may be edges of the graph. That is, it is a directed graph that can be formed as an orientation of an undirected (simple) graph. 
Some authors use ""oriented graph"" to mean the same as ""directed graph"".  Some authors use ""oriented graph"" to mean any orientation of a given undirected graph or multigraph.


=== Regular graph ===

A regular graph is a graph in which each vertex has the same number of neighbours, i.e., every vertex has the same degree. A regular graph with vertices of degree k is called a k‑regular graph or regular graph of degree k.


=== Complete graph ===

A complete graph is a graph in which each pair of vertices is joined by an edge. A complete graph contains all possible edges.


=== Finite graph ===
A finite graph is a graph in which the vertex set and the edge set are finite sets. Otherwise, it is called an infinite graph.
Most commonly in graph theory it is implied that the graphs discussed are finite. If the graphs are infinite, that is usually specifically stated.


=== Connected graph ===

In an undirected graph, an unordered pair of vertices {x, y} is called connected if a path leads from x to y. Otherwise, the unordered pair is called disconnected.
A connected graph is an undirected graph in which every unordered pair of vertices in the graph is connected. Otherwise, it is called a disconnected graph.
In a directed graph, an ordered pair of vertices (x, y) is called strongly connected if a directed path leads from x to y. Otherwise, the ordered pair is called weakly connected if an undirected path leads from x to y after replacing all of its directed edges with undirected edges. Otherwise, the ordered pair is called disconnected.
A strongly connected graph is a directed graph in which every ordered pair of vertices in the graph is strongly connected. Otherwise, it is called a weakly connected graph if every ordered pair of vertices in the graph is weakly connected. Otherwise it is called a disconnected graph.
A k-vertex-connected graph or k-edge-connected graph is a graph in which no set of k − 1 vertices (respectively, edges) exists that, when removed, disconnects the graph. A k-vertex-connected graph is often called simply a k-connected graph.


=== Bipartite graph ===

A bipartite graph is a simple graph in which the vertex set can be partitioned into two sets, W and X, so that no two vertices in W share a common edge and no two vertices in X share a common edge. Alternatively, it is a graph with a chromatic number of 2.
In a complete bipartite graph, the vertex set is the union of two disjoint sets, W and X, so that every vertex in W is adjacent to every vertex in X but there are no edges within W or X.


=== Path graph ===

A path graph or linear graph of order n ≥ 2 is a graph in which the vertices can be listed in an order v1, v2, …, vn such that the edges are the {vi, vi+1} where i = 1, 2, …, n − 1. Path graphs can be characterized as connected graphs in which the degree of all but two vertices is 2 and the degree of the two remaining vertices is 1. If a path graph occurs as a subgraph of another graph, it is a path in that graph.


=== Planar graph ===

A planar graph is a graph whose vertices and edges can be drawn in a plane such that no two of the edges intersect.


=== Cycle graph ===

A cycle graph or circular graph of order n ≥ 3 is a graph in which the vertices can be listed in an order v1, v2, …, vn such that the edges are the {vi, vi+1} where i = 1, 2, …, n − 1, plus the edge {vn, v1}. Cycle graphs can be characterized as connected graphs in which the degree of all vertices is 2. If a cycle graph occurs as a subgraph of another graph, it is a cycle or circuit in that graph.


=== Tree ===

A tree is an undirected graph in which any two vertices are connected by exactly one path, or equivalently a connected acyclic undirected graph.
A forest is an undirected graph in which any two vertices are connected by at most one path, or equivalently an acyclic undirected graph, or equivalently a disjoint union of trees.


=== Polytree ===

A polytree (or directed tree or oriented tree or singly connected network) is a directed acyclic graph (DAG) whose underlying undirected graph is a tree.
A polyforest (or directed forest or oriented forest) is a directed acyclic graph whose underlying undirected graph is a forest.


=== Advanced classes ===
More advanced kinds of graphs are:

Petersen graph and its generalizations;
perfect graphs;
cographs;
chordal graphs;
other graphs with large automorphism groups: vertex-transitive, arc-transitive, and distance-transitive graphs;
strongly regular graphs and their generalizations distance-regular graphs.


== Properties of graphs ==

Two edges of a graph are called adjacent if they share a common vertex. Two edges of a directed graph are called consecutive if the head of the first one is the tail of the second one. Similarly, two vertices are called adjacent if they share a common edge (consecutive if the first one is the tail and the second one is the head of an edge), in which case the common edge is said to join the two vertices. An edge and a vertex on that edge are called incident.
The graph with only one vertex and no edges is called the trivial graph. A graph with only vertices and no edges is known as an edgeless graph. The graph with no vertices and no edges is sometimes called the null graph or empty graph, but the terminology is not consistent and not all mathematicians allow this object.
Normally, the vertices of a graph, by their nature as elements of a set, are distinguishable. This kind of graph may be called vertex-labeled. However, for many questions it is better to treat vertices as indistinguishable. (Of course, the vertices may be still distinguishable by the properties of the graph itself, e.g., by the numbers of incident edges.) The same remarks apply to edges, so graphs with labeled edges are called edge-labeled. Graphs with labels attached to edges or vertices are more generally designated as labeled. Consequently, graphs in which vertices are indistinguishable and edges are indistinguishable are called unlabeled. (In the literature, the term labeled may apply to other kinds of labeling, besides that which serves only to distinguish different vertices or edges.)
The category of all graphs is the comma category Set ↓ D where D: Set → Set is the functor taking a set s to s × s.


== Examples ==

The diagram is a schematic representation of the graph with vertices 
  
    
      
        V
        =
        {
        1
        ,
        2
        ,
        3
        ,
        4
        ,
        5
        ,
        6
        }
      
    
    {\displaystyle V=\{1,2,3,4,5,6\}}
   and edges 
  
    
      
        E
        =
        {
        {
        1
        ,
        2
        }
        ,
        {
        1
        ,
        5
        }
        ,
        {
        2
        ,
        3
        }
        ,
        {
        2
        ,
        5
        }
        ,
        {
        3
        ,
        4
        }
        ,
        {
        4
        ,
        5
        }
        ,
        {
        4
        ,
        6
        }
        }
        .
      
    
    {\displaystyle E=\{\{1,2\},\{1,5\},\{2,3\},\{2,5\},\{3,4\},\{4,5\},\{4,6\}\}.}
  
In computer science, directed graphs are used to represent knowledge (e.g., conceptual graph), finite state machines, and many other discrete structures.
A binary relation R on a set X defines a directed graph. An element x of X is a direct predecessor of an element y of X if and only if xRy.
A directed graph can model information networks such as Twitter, with one user following another.
Particularly regular examples of directed graphs are given by the Cayley graphs of finitely-generated groups, as well as Schreier coset graphs
In category theory, every small category has an underlying directed multigraph whose vertices are the objects of the category, and whose edges are the arrows of the category.  In the language of category theory, one says that there is a forgetful functor from the category of small categories to the category of quivers.


== Graph operations ==

There are several operations that produce new graphs from initial ones, which might be classified into the following categories:

unary operations, which create a new graph from an initial one, such as:
edge contraction,
line graph,
dual graph,
complement graph,
graph rewriting;
binary operations, which create a new graph from two initial ones, such as:
disjoint union of graphs,
cartesian product of graphs,
tensor product of graphs,
strong product of graphs,
lexicographic product of graphs,
series–parallel graphs.


== Generalizations ==
In a hypergraph, an edge can join more than two vertices.
An undirected graph can be seen as a simplicial complex consisting of 1-simplices (the edges) and 0-simplices (the vertices). As such, complexes are generalizations of graphs since they allow for higher-dimensional simplices.
Every graph gives rise to a matroid.
In model theory, a graph is just a structure. But in that case, there is no limitation on the number of edges: it can be any cardinal number, see continuous graph.
In computational biology, power graph analysis introduces power graphs as an alternative representation of undirected graphs.
In geographic information systems, geometric networks are closely modeled after graphs, and borrow many concepts from graph theory to perform spatial analysis on road networks or utility grids.


== See also ==
Conceptual graph
Graph (abstract data type)
Graph database
Graph drawing
List of graph theory topics
List of publications in graph theory
Network theory


== Notes ==


== References ==
Balakrishnan, V. K. (1997). Graph Theory (1st ed.). McGraw-Hill. ISBN 978-0-07-005489-9.
Bang-Jensen, J.; Gutin, G. (2000). Digraphs: Theory, Algorithms and Applications. Springer.
Bender, Edward A.; Williamson, S. Gill (2010). Lists, Decisions and Graphs. With an Introduction to Probability.
Berge, Claude (1958). Théorie des graphes et ses applications (in French). Paris: Dunod.
Biggs, Norman (1993). Algebraic Graph Theory (2nd ed.). Cambridge University Press. ISBN 978-0-521-45897-9.
Bollobás, Béla (2002). Modern Graph Theory (1st ed.). Springer. ISBN 978-0-387-98488-9.
Diestel, Reinhard (2005). Graph Theory (3rd ed.). Berlin, New York: Springer-Verlag. ISBN 978-3-540-26183-4.
Graham, R.L.; Grötschel, M.; Lovász, L. (1995). Handbook of Combinatorics. MIT Press. ISBN 978-0-262-07169-7.
Gross, Jonathan L.; Yellen, Jay (1998). Graph Theory and Its Applications. CRC Press. ISBN 978-0-8493-3982-0.
Gross, Jonathan L.; Yellen, Jay (2003). Handbook of Graph Theory. CRC. ISBN 978-1-58488-090-5.
Harary, Frank (1995). Graph Theory. Addison Wesley Publishing Company. ISBN 978-0-201-41033-4.
Iyanaga, Shôkichi; Kawada, Yukiyosi (1977). Encyclopedic Dictionary of Mathematics. MIT Press. ISBN 978-0-262-09016-2.
Zwillinger, Daniel (2002). CRC Standard Mathematical Tables and Formulae (31st ed.). Chapman & Hall/CRC. ISBN 978-1-58488-291-6.


== Further reading ==
Trudeau, Richard J. (1993). Introduction to Graph Theory (Corrected, enlarged republication. ed.). New York: Dover Publications. ISBN 978-0-486-67870-2. Retrieved 8 August 2012.


== External links ==
 Media related to Graph (discrete mathematics) at Wikimedia Commons
Weisstein, Eric W. ""Graph"". MathWorld."
85c1460f75,Equality (mathematics),"In mathematics, equality is a relationship between two quantities or, more generally two mathematical expressions, asserting that the quantities have the same value, or that the expressions represent the same mathematical object. The equality between A and B is written A = B, and pronounced A equals B. The symbol ""="" is called an ""equals sign"". Two objects that are not equal are said to be distinct.
For example:

  
    
      
        x
        =
        y
      
    
    {\displaystyle x=y}
   means that x and y denote the same object.
The identity 
  
    
      
        (
        x
        +
        1
        
          )
          
            2
          
        
        =
        
          x
          
            2
          
        
        +
        2
        x
        +
        1
      
    
    {\displaystyle (x+1)^{2}=x^{2}+2x+1}
   means that if x is any number, then the two expressions have the same value. This may also be interpreted as saying that the two sides of the equals sign represent the same function.

  
    
      
        {
        x
        ∣
        P
        (
        x
        )
        }
        =
        {
        x
        ∣
        Q
        (
        x
        )
        }
      
    
    {\displaystyle \{x\mid P(x)\}=\{x\mid Q(x)\}}
   if and only if 
  
    
      
        P
        (
        x
        )
        ⇔
        Q
        (
        x
        )
        .
      
    
    {\displaystyle P(x)\Leftrightarrow Q(x).}
   This assertion, which uses set-builder notation, means that if the elements satisfying the property 
  
    
      
        P
        (
        x
        )
      
    
    {\displaystyle P(x)}
   are the same as the elements satisfying 
  
    
      
        Q
        (
        x
        )
        ,
      
    
    {\displaystyle Q(x),}
   then the two uses of the set-builder notation define the same set. This property is often expressed as ""two sets that have the same elements are equal."" It is one of the usual axioms of set theory, called axiom of extensionality.


== Etymology ==
The etymology of the word is from the Latin aequālis (“equal”, “like”, “comparable”, “similar”) from aequus (“equal”, “level”, “fair”, “just”).


== Basic properties ==

These last three properties make equality an equivalence relation. They were originally included among the Peano axioms for natural numbers. Although the symmetric and transitive properties are often seen as fundamental, they can be deduced from substitution and reflexive properties.


== Equality as predicate ==
When A and B are not fully specified or depend on some variables, equality is a proposition, which may be true for some values and false for other values. Equality is a binary relation (i.e., a two-argument predicate) which may produce a truth value (false or true) from its arguments. In computer programming, its computation from the two expressions is known as comparison.


== Identities ==

When A and B may be viewed as functions of some variables, then A = B means that A and B define the same function. Such an equality of functions is sometimes called an identity. An example is 
  
    
      
        
          (
          
            x
            +
            1
          
          )
        
        
          (
          
            x
            +
            1
          
          )
        
        =
        
          x
          
            2
          
        
        +
        2
        x
        +
        1.
      
    
    {\displaystyle \left(x+1\right)\left(x+1\right)=x^{2}+2x+1.}
   Sometimes, but not always, an identity is written with a triple bar: 
  
    
      
        
          (
          
            x
            +
            1
          
          )
        
        
          (
          
            x
            +
            1
          
          )
        
        ≡
        
          x
          
            2
          
        
        +
        2
        x
        +
        1.
      
    
    {\displaystyle \left(x+1\right)\left(x+1\right)\equiv x^{2}+2x+1.}
  


== Equations ==
An equation is a problem of finding values of some variables, called unknowns, for which the specified equality is true. The term ""equation"" may also refer to an equality relation that is satisfied only for the values of the variables that one is interested in. For example, 
  
    
      
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        =
        1
      
    
    {\displaystyle x^{2}+y^{2}=1}
   is the equation of the unit circle.
There is no standard notation that distinguishes an equation from an identity, or other use of the equality relation: one has to guess an appropriate interpretation from the semantics of expressions and the context. An identity is asserted to be true for all values of variables in a given domain. An ""equation"" may sometimes mean an identity, but more often than not, it specifies a subset of the variable space to be the subset where the equation is true.


== Approximate equality ==
There are some logic systems that do not have any notion of equality. This reflects the undecidability of the equality of two real numbers, defined by formulas involving the integers, the basic arithmetic operations, the logarithm and the exponential function. In other words, there cannot exist any algorithm for deciding such an equality.
The binary relation ""is approximately equal"" (denoted by the symbol 
  
    
      
        ≈
      
    
    {\displaystyle \approx }
  ) between real numbers or other things, even if more precisely defined, is not transitive (since many small differences can add up to something big). However, equality almost everywhere is transitive.
A questionable equality under test may be denoted using the ≟ symbol.


== Relation with equivalence, congruence, and isomorphism ==

Viewed as a relation, equality is the archetype of the more general concept of an equivalence relation on a set: those binary relations that are reflexive, symmetric and transitive. The identity relation is an equivalence relation. Conversely, let R be an equivalence relation, and let us denote by xR the equivalence class of x, consisting of all elements z such that x R z. Then the relation x R y is equivalent with the equality xR = yR. It follows that equality is the finest equivalence relation on any set S in the sense that it is the relation that has the smallest equivalence classes (every class is reduced to a single element).
In some contexts, equality is sharply distinguished from equivalence or isomorphism. For example, one may distinguish fractions from rational numbers, the latter being equivalence classes of fractions: the fractions 
  
    
      
        1
        
          /
        
        2
      
    
    {\displaystyle 1/2}
   and 
  
    
      
        2
        
          /
        
        4
      
    
    {\displaystyle 2/4}
   are distinct as fractions (as different strings of symbols) but they ""represent"" the same rational number (the same point on a number line). This distinction gives rise to the notion of a quotient set.
Similarly, the sets

  
    
      
        {
        
          A
        
        ,
        
          B
        
        ,
        
          C
        
        }
      
    
    {\displaystyle \{{\text{A}},{\text{B}},{\text{C}}\}}
   and 
  
    
      
        {
        1
        ,
        2
        ,
        3
        }
      
    
    {\displaystyle \{1,2,3\}}
  are not equal sets — the first consists of letters, while the second consists of numbers — but they are both sets of three elements and thus isomorphic, meaning that there is a bijection between them. For example

  
    
      
        
          A
        
        ↦
        1
        ,
        
          B
        
        ↦
        2
        ,
        
          C
        
        ↦
        3.
      
    
    {\displaystyle {\text{A}}\mapsto 1,{\text{B}}\mapsto 2,{\text{C}}\mapsto 3.}
  However, there are other choices of isomorphism, such as

  
    
      
        
          A
        
        ↦
        3
        ,
        
          B
        
        ↦
        2
        ,
        
          C
        
        ↦
        1
        ,
      
    
    {\displaystyle {\text{A}}\mapsto 3,{\text{B}}\mapsto 2,{\text{C}}\mapsto 1,}
  and these sets cannot be identified without making such a choice — any statement that identifies them ""depends on choice of identification"". This distinction, between equality and isomorphism, is of fundamental importance in category theory and is one motivation for the development of category theory.
In some cases, one may consider as equal two mathematical objects that are only equivalent for the properties and structure being considered. The word congruence (and the associated symbol 
  
    
      
        ≅
      
    
    {\displaystyle \cong }
  ) is frequently used for this kind of equality, and is defined as the quotient set of the isomorphism classes between the objects. In geometry for instance, two geometric shapes are said to be equal or congruent when one may be moved to coincide with the other, and the equality/congruence relation is the isomorphism classes of isometries between shapes. Similarly to isomorphisms of sets, the difference between isomorphisms and equality/congruence between such mathematical objects with properties and structure was one motivation for the development of category theory, as well as for homotopy type theory and univalent foundations.


== Logical definitions ==

Leibniz characterized the notion of equality as follows:

Given any x and y, x = y if and only if, given any predicate P, P(x) if and only if P(y).


== Equality in set theory ==

Equality of sets is axiomatized in set theory in two different ways, depending on whether the axioms are based on a first-order language with or without equality.


=== Set equality based on first-order logic with equality ===
In first-order logic with equality, the axiom of extensionality states that two sets which contain the same elements are the same set.
Logic axiom: x = y ⇒ ∀z, (z ∈ x ⇔ z ∈ y)
Logic axiom: x = y ⇒ ∀z, (x ∈ z ⇔ y ∈ z)
Set theory axiom: (∀z, (z ∈ x ⇔ z ∈ y)) ⇒ x = yIncorporating half of the work into the first-order logic may be regarded as a mere matter of convenience, as noted by Lévy.

""The reason why we take up first-order predicate calculus with equality is a matter of convenience; by this we save the labor of defining equality and proving all its properties; this burden is now assumed by the logic.""


=== Set equality based on first-order logic without equality ===
In first-order logic without equality, two sets are defined to be equal if they contain the same elements. Then the axiom of extensionality states that two equal sets are contained in the same sets.
Set theory definition: ""x = y"" means ∀z, (z ∈ x ⇔ z ∈ y)
Set theory axiom: x = y ⇒ ∀z, (x ∈ z ⇔ y ∈ z)


== See also ==
Extensionality
Homotopy type theory
Inequality
List of mathematical symbols
Logical equality
Proportionality (mathematics)


== Notes ==


== References ==


== External links ==
""Equality axioms"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]"
bcf8be03ae,Mathematical induction,"Mathematical induction is a method for proving that a statement 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   is true for every natural number 
  
    
      
        n
      
    
    {\displaystyle n}
  , that is, that the infinitely many cases 
  
    
      
        P
        (
        0
        )
        ,
        P
        (
        1
        )
        ,
        P
        (
        2
        )
        ,
        P
        (
        3
        )
        ,
        …
      
    
    {\displaystyle P(0),P(1),P(2),P(3),\dots }
    all hold. Informal metaphors help to explain this technique, such as falling dominoes or climbing a ladder:

Mathematical induction proves that we can climb as high as we like on a ladder, by proving that we can climb onto the bottom rung (the basis) and that from each rung we can climb up to the next one (the step).
A proof by induction consists of two cases. The first, the base case, proves the statement for 
  
    
      
        n
        =
        0
      
    
    {\displaystyle n=0}
   without assuming any knowledge of other cases. The second case, the induction step, proves that if the statement holds for any given case 
  
    
      
        n
        =
        k
      
    
    {\displaystyle n=k}
  , then it must also hold for the next case 
  
    
      
        n
        =
        k
        +
        1
      
    
    {\displaystyle n=k+1}
  . These two steps establish that the statement holds for every natural number 
  
    
      
        n
      
    
    {\displaystyle n}
  . The base case does not necessarily begin with 
  
    
      
        n
        =
        0
      
    
    {\displaystyle n=0}
  , but often with 
  
    
      
        n
        =
        1
      
    
    {\displaystyle n=1}
  , and possibly with any fixed natural number 
  
    
      
        n
        =
        N
      
    
    {\displaystyle n=N}
  , establishing the truth of the statement for all natural numbers 
  
    
      
        n
        ≥
        N
      
    
    {\displaystyle n\geq N}
  .
The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction is an inference rule used in formal proofs, and is the foundation of most correctness proofs for computer programs.Although its name may suggest otherwise, mathematical induction should not be confused with inductive reasoning as used in philosophy (see Problem of induction). The mathematical method examines infinitely many cases to prove a general statement, but does so by a finite chain of deductive reasoning involving the variable 
  
    
      
        n
      
    
    {\displaystyle n}
  , which can take infinitely many values.


== History ==
In 370 BC, Plato's Parmenides may have contained traces of an early example of an implicit inductive proof.The earliest implicit proof by mathematical induction is in the al-Fakhri written by al-Karaji around 1000 AD, who applied it to arithmetic sequences to prove the binomial theorem and properties of Pascal's triangle.
 As Katz says Another important idea introduced by al-Karaji and continued by al-Samaw'al and others was that of an inductive argument for dealing with certain arithmetic sequences. Thus al-Karaji used such an argument to prove the result on the sums of integral cubes already known to Aryabhata [...] Al-Karaji did not, however, state a general result for arbitrary n. He stated his theorem for the particular integer 10 [...] His proof, nevertheless, was clearly designed to be extendable to any other integer. [...] Al-Karaji's argument includes in essence the two basic components of a modern argument by induction, namely the truth of the statement for n = 1 (1 = 13) and the deriving of the truth for n = k from that of n = k - 1. Of course, this second component is not explicit since, in some sense, al-Karaji's argument is in reverse; this is, he starts from n = 10 and goes down to 1 rather than proceeding upward. Nevertheless, his argument in al-Fakhri is the earliest extant proof of the sum formula for integral cubes.

In India, early implicit proofs by mathematical induction appear in Bhaskara's ""cyclic method"".None of these ancient mathematicians, however, explicitly stated the induction hypothesis. Another similar case (contrary to what Vacca has written, as Freudenthal carefully showed) was that of Francesco Maurolico in his Arithmeticorum libri duo (1575), who used the technique to prove that the sum of the first n odd integers is n2.
The earliest rigorous use of induction was by Gersonides (1288–1344). The first explicit formulation of the principle of induction was given by Pascal in his Traité du triangle arithmétique (1665). Another Frenchman, Fermat, made ample use of a related principle: indirect proof by infinite descent.
The induction hypothesis was also employed by the Swiss Jakob Bernoulli, and from then on it became well known. The modern formal treatment of the principle came only in the 19th century, with George Boole, Augustus De Morgan, Charles Sanders Peirce, Giuseppe Peano, and Richard Dedekind.


== Description ==
The simplest and most common form of mathematical induction infers that a statement involving a natural number n (that is, an integer n ≥ 0 or 1) holds for all values of n. The proof consists of two steps:

The base case (or initial case): prove that the statement holds for 0, or 1.
The induction step (or inductive step, or step case): prove that for every n, if the statement holds for n, then it holds for n + 1. In other words, assume that the statement holds for some arbitrary natural number n, and prove that the statement holds for n + 1.The hypothesis in the induction step, that the statement holds for a particular n, is called the induction hypothesis or inductive hypothesis. To prove the induction step, one assumes the induction hypothesis for n and then uses this assumption to prove that the statement holds for n + 1.
Authors who prefer to define natural numbers to begin at 0 use that value in the base case; those who define natural numbers to begin at 1 use that value.


== Examples ==


=== Sum of consecutive natural numbers ===
Mathematical induction can be used to prove the following statement P(n) for all natural numbers n.

  
    
      
        P
        (
        n
        )
        
        :
         
         
        0
        +
        1
        +
        2
        +
        ⋯
        +
        n
        =
        
          
            
              n
              (
              n
              +
              1
              )
            
            2
          
        
        .
      
    
    {\displaystyle P(n)\!:\ \ 0+1+2+\cdots +n={\frac {n(n+1)}{2}}.}
  This states a general formula for the sum of the natural numbers less than or equal to a given number; in fact an infinite sequence of statements: 
  
    
      
        0
        =
        
          
            
              
                (
                0
                )
                (
                0
                +
                1
                )
              
              2
            
          
        
      
    
    {\displaystyle 0={\tfrac {(0)(0+1)}{2}}}
  , 
  
    
      
        0
        +
        1
        =
        
          
            
              
                (
                1
                )
                (
                1
                +
                1
                )
              
              2
            
          
        
      
    
    {\displaystyle 0+1={\tfrac {(1)(1+1)}{2}}}
  , 
  
    
      
        0
        +
        1
        +
        2
        =
        
          
            
              
                (
                2
                )
                (
                2
                +
                1
                )
              
              2
            
          
        
      
    
    {\displaystyle 0+1+2={\tfrac {(2)(2+1)}{2}}}
  , etc.
Proposition. For every 
  
    
      
        n
        ∈
        
          N
        
      
    
    {\displaystyle n\in \mathbb {N} }
  , 
  
    
      
        0
        +
        1
        +
        2
        +
        ⋯
        +
        n
        =
        
          
            
              
                n
                (
                n
                +
                1
                )
              
              2
            
          
        
        .
      
    
    {\displaystyle 0+1+2+\cdots +n={\tfrac {n(n+1)}{2}}.}
  
Proof. Let P(n) be the statement 
  
    
      
        0
        +
        1
        +
        2
        +
        ⋯
        +
        n
        =
        
          
            
              
                n
                (
                n
                +
                1
                )
              
              2
            
          
        
        .
      
    
    {\displaystyle 0+1+2+\cdots +n={\tfrac {n(n+1)}{2}}.}
   We give a proof by induction on n.
Base case: Show that the statement holds for the smallest natural number n = 0.
P(0) is clearly true: 
  
    
      
        0
        =
        
          
            
              
                0
                (
                0
                +
                1
                )
              
              2
            
          
        
        
        .
      
    
    {\displaystyle 0={\tfrac {0(0+1)}{2}}\,.}
  
Induction step: Show that for every k ≥ 0, if P(k) holds, then P(k + 1) also holds.

Assume the induction hypothesis that for a particular k, the single case n = k holds, meaning P(k) is true:
  
    
      
        0
        +
        1
        +
        ⋯
        +
        k
        =
        
          
            
              k
              (
              k
              +
              1
              )
            
            2
          
        
        .
      
    
    {\displaystyle 0+1+\cdots +k={\frac {k(k+1)}{2}}.}
  It follows that:

  
    
      
        (
        0
        +
        1
        +
        2
        +
        ⋯
        +
        k
        )
        +
        (
        k
        +
        1
        )
        =
        
          
            
              k
              (
              k
              +
              1
              )
            
            2
          
        
        +
        (
        k
        +
        1
        )
        .
      
    
    {\displaystyle (0+1+2+\cdots +k)+(k+1)={\frac {k(k+1)}{2}}+(k+1).}
  Algebraically, the right hand side simplifies as:

  
    
      
        
          
            
              
                
                  
                    
                      k
                      (
                      k
                      +
                      1
                      )
                    
                    2
                  
                
                +
                (
                k
                +
                1
                )
              
              
                
                =
                
                  
                    
                      k
                      (
                      k
                      +
                      1
                      )
                      +
                      2
                      (
                      k
                      +
                      1
                      )
                    
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      (
                      k
                      +
                      1
                      )
                      (
                      k
                      +
                      2
                      )
                    
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      (
                      k
                      +
                      1
                      )
                      (
                      (
                      k
                      +
                      1
                      )
                      +
                      1
                      )
                    
                    2
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {k(k+1)}{2}}+(k+1)&={\frac {k(k+1)+2(k+1)}{2}}\\&={\frac {(k+1)(k+2)}{2}}\\&={\frac {(k+1)((k+1)+1)}{2}}.\end{aligned}}}
  
Equating the extreme left hand and right hand sides, we deduce that:
  
    
      
        0
        +
        1
        +
        2
        +
        ⋯
        +
        k
        +
        (
        k
        +
        1
        )
        =
        
          
            
              (
              k
              +
              1
              )
              (
              (
              k
              +
              1
              )
              +
              1
              )
            
            2
          
        
        .
      
    
    {\displaystyle 0+1+2+\cdots +k+(k+1)={\frac {(k+1)((k+1)+1)}{2}}.}
  That is, the statement P(k + 1) also holds true, establishing the induction step.
Conclusion: Since both the base case and the induction step have been proved as true, by mathematical induction the statement P(n) holds for every natural number n. ∎


=== A trigonometric inequality ===
Induction is often used to prove inequalities. As an example, we prove that 
  
    
      
        
          |
        
        
        sin
        ⁡
        n
        x
        
          |
        
        ≤
        n
        
        
          |
        
        
        sin
        ⁡
        x
        
          |
        
      
    
    {\displaystyle |\!\sin nx|\leq n\,|\!\sin x|}
   for any real number 
  
    
      
        x
      
    
    {\displaystyle x}
   and natural number 
  
    
      
        n
      
    
    {\displaystyle n}
  .
At first glance, it may appear that a more general version, 
  
    
      
        
          |
        
        
        sin
        ⁡
        n
        x
        
          |
        
        ≤
        n
        
        
          |
        
        
        sin
        ⁡
        x
        
          |
        
      
    
    {\displaystyle |\!\sin nx|\leq n\,|\!\sin x|}
   for any real numbers 
  
    
      
        n
        ,
        x
      
    
    {\displaystyle n,x}
  , could be proven without induction; but the case 
  
    
      
        n
        =
        
          
            1
            2
          
        
        ,
        
        x
        =
        π
      
    
    {\textstyle n={\frac {1}{2}},\,x=\pi }
   shows it may be false for non-integer values of 
  
    
      
        n
      
    
    {\displaystyle n}
  . This suggests we examine the statement specifically for natural values of 
  
    
      
        n
      
    
    {\displaystyle n}
  , and induction is the readiest tool.
Proposition. For any 
  
    
      
        x
        ∈
        
          R
        
      
    
    {\displaystyle x\in \mathbb {R} }
   and 
  
    
      
        n
        ∈
        
          N
        
      
    
    {\displaystyle n\in \mathbb {N} }
  , 
  
    
      
        
          |
        
        
        sin
        ⁡
        n
        x
        
          |
        
        ≤
        n
        
        
          |
        
        
        sin
        ⁡
        x
        
          |
        
      
    
    {\displaystyle |\!\sin nx|\leq n\,|\!\sin x|}
  .
Proof. Fix an arbitrary real number 
  
    
      
        x
      
    
    {\displaystyle x}
  , and let 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   be the statement 
  
    
      
        
          |
        
        
        sin
        ⁡
        n
        x
        
          |
        
        ≤
        n
        
        
          |
        
        
        sin
        ⁡
        x
        
          |
        
      
    
    {\displaystyle |\!\sin nx|\leq n\,|\!\sin x|}
  . We induct on 
  
    
      
        n
      
    
    {\displaystyle n}
  .
Base case: The calculation 
  
    
      
        
          |
        
        
        sin
        ⁡
        0
        x
        
          |
        
        =
        0
        ≤
        0
        =
        0
        
        
          |
        
        
        sin
        ⁡
        x
        
          |
        
      
    
    {\displaystyle |\!\sin 0x|=0\leq 0=0\,|\!\sin x|}
   verifies 
  
    
      
        P
        (
        0
        )
      
    
    {\displaystyle P(0)}
  .
Induction step: We show the implication 
  
    
      
        P
        (
        k
        )
        
        ⟹
        
        P
        (
        k
        +
        1
        )
      
    
    {\displaystyle P(k)\implies P(k+1)}
   for any natural number 
  
    
      
        k
      
    
    {\displaystyle k}
  . Assume the induction hypothesis: for a given value 
  
    
      
        n
        =
        k
        ≥
        0
      
    
    {\displaystyle n=k\geq 0}
  , the single case 
  
    
      
        P
        (
        k
        )
      
    
    {\displaystyle P(k)}
   is true. Using the angle addition formula and the triangle inequality, we deduce:

  
    
      
        
          
            
              
                
                  |
                
                
                sin
                ⁡
                (
                k
                +
                1
                )
                x
                
                  |
                
              
              
                =
              
              
                
                  |
                
                
                sin
                ⁡
                k
                x
                
                cos
                ⁡
                x
                +
                sin
                ⁡
                x
                
                cos
                ⁡
                k
                x
                
                
                  |
                
              
              
                
                  (angle addition)
                
              
            
            
              
              
                ≤
              
              
                
                  |
                
                
                sin
                ⁡
                k
                x
                
                cos
                ⁡
                x
                
                  |
                
                +
                
                  |
                
                
                sin
                ⁡
                x
                
                cos
                ⁡
                k
                x
                
                  |
                
              
              
                
                  (triangle inequality)
                
              
            
            
              
              
                =
              
              
                
                  |
                
                
                sin
                ⁡
                k
                x
                
                  |
                
                
                
                  |
                
                
                cos
                ⁡
                x
                
                  |
                
                +
                
                  |
                
                
                sin
                ⁡
                x
                
                  |
                
                
                
                  |
                
                
                cos
                ⁡
                k
                x
                
                  |
                
              
              
            
            
              
              
                ≤
              
              
                
                  |
                
                
                sin
                ⁡
                k
                x
                
                  |
                
                +
                
                  |
                
                
                sin
                ⁡
                x
                
                  |
                
              
              
                (
                
                  |
                
                
                cos
                ⁡
                t
                
                  |
                
                ≤
                1
                )
              
            
            
              
              
                ≤
              
              
                k
                
                
                  |
                
                
                sin
                ⁡
                x
                
                  |
                
                +
                
                  |
                
                
                sin
                ⁡
                x
                
                  |
                
              
              
                
                  (induction hypothesis
                
                )
              
            
            
              
              
                =
              
              
                (
                k
                +
                1
                )
                
                
                  |
                
                
                sin
                ⁡
                x
                
                  |
                
                .
              
              
            
          
        
      
    
    {\displaystyle {\begin{array}{rcll}|\!\sin(k+1)x|&=&|\!\sin kx\,\cos x+\sin x\,\cos kx\,|&{\text{(angle addition)}}\\&\leq &|\!\sin kx\,\cos x|+|\!\sin x\,\cos kx|&{\text{(triangle inequality)}}\\&=&|\!\sin kx|\,|\!\cos x|+|\!\sin x|\,|\!\cos kx|&\\&\leq &|\!\sin kx|+|\!\sin x|&(|\!\cos t|\leq 1)\\&\leq &k\,|\!\sin x|+|\!\sin x|&{\text{(induction hypothesis}})\\&=&(k+1)\,|\!\sin x|.&\end{array}}}
  The inequality between the extreme left-hand and right-hand quantities shows that 
  
    
      
        P
        (
        k
        +
        1
        )
      
    
    {\displaystyle P(k+1)}
   is true, which completes the induction step.
Conclusion: The proposition 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   holds for all natural numbers 
  
    
      
        n
      
    
    {\displaystyle n}
  . ∎


== Variants ==
In practice, proofs by induction are often structured differently, depending on the exact nature of the property to be proven.
All variants of induction are special cases of transfinite induction; see below.


=== Base case other than 0 or 1 ===
If one wishes to prove a statement, not for all natural numbers, but only for all numbers n greater than or equal to a certain number b, then the proof by induction consists of the following:

Showing that the statement holds when n = b.
Showing that if the statement holds for an arbitrary number n ≥ b, then the same statement also holds for n + 1.This can be used, for example, to show that 2n ≥ n + 5 for n ≥ 3.
In this way, one can prove that some statement P(n) holds for all n ≥ 1, or even for all n ≥ −5. This form of mathematical induction is actually a special case of the previous form, because if the statement to be proved is P(n) then proving it with these two rules is equivalent with proving P(n + b) for all natural numbers n with an induction base case 0.


==== Example: forming dollar amounts by coins ====
Assume an infinite supply of 4- and 5-dollar coins. Induction can be used to prove that any whole amount of dollars greater than or equal to 12 can be formed by a combination of such coins. Let S(k) denote the statement ""k dollars can be formed by a combination of 4- and 5-dollar coins"". The proof that S(k) is true for all k ≥ 12 can then be achieved by induction on k as follows:
Base case: Showing that S(k) holds for k = 12 is simple: take three 4-dollar coins.
Induction step: Given that S(k) holds for some value of k ≥ 12 (induction hypothesis), prove that S(k + 1) holds, too. Assume S(k) is true for some arbitrary k ≥ 12. If there is a solution for k dollars that includes at least one 4-dollar coin, replace it by a 5-dollar coin to make k + 1 dollars. Otherwise, if only 5-dollar coins are used, k must be a multiple of 5 and so at least 15; but then we can replace three 5-dollar coins by four 4-dollar coins to make k + 1 dollars. In each case, S(k + 1) is true.
Therefore, by the principle of induction, S(k) holds for all k ≥ 12, and the proof is complete.
In this example, although S(k) also holds for 
  
    
      
        k
        ∈
        {
        4
        ,
        5
        ,
        8
        ,
        9
        ,
        10
        }
      
    
    {\textstyle k\in \{4,5,8,9,10\}}
  , the above proof cannot be modified to replace the minimum amount of 12 dollar to any lower value m. For m = 11, the base case is actually false; for m = 10, the second case in the induction step (replacing three 5- by four 4-dollar coins) will not work; let alone for even lower m.


=== Induction on more than one counter ===
It is sometimes desirable to prove a statement involving two natural numbers, n and m, by iterating the induction process. That is, one proves a base case and an induction step for n, and in each of those proves a base case and an induction step for m. See, for example, the proof of commutativity accompanying addition of natural numbers. More complicated arguments involving three or more counters are also possible.


=== Infinite descent ===

The method of infinite descent is a variation of mathematical induction which was used by Pierre de Fermat. It is used to show that some statement Q(n) is false for all natural numbers n. Its traditional form consists of showing that if Q(n) is true for some natural number n, it also holds for some strictly smaller natural number m. Because there are no infinite decreasing sequences of natural numbers, this situation would be impossible, thereby showing (by contradiction) that Q(n) cannot be true for any n.
The validity of this method can be verified from the usual principle of mathematical induction. Using mathematical induction on the statement P(n) defined as ""Q(m) is false for all natural numbers m less than or equal to n"", it follows that P(n) holds for all n, which means that Q(n) is false for every natural number n.


=== Prefix induction ===
The most common form of proof by mathematical induction requires proving in the induction step that

  
    
      
        ∀
        k
        (
        P
        (
        k
        )
        →
        P
        (
        k
        +
        1
        )
        )
      
    
    {\displaystyle \forall k(P(k)\to P(k+1))}
  whereupon the induction principle ""automates"" n applications of this step in getting from P(0) to P(n). This could be called ""predecessor induction"" because each step proves something about a number from something about that number's predecessor.
A variant of interest in computational complexity is ""prefix induction"", in which one proves the following statement in the induction step:

  
    
      
        ∀
        k
        (
        P
        (
        k
        )
        →
        P
        (
        2
        k
        )
        ∧
        P
        (
        2
        k
        +
        1
        )
        )
      
    
    {\displaystyle \forall k(P(k)\to P(2k)\land P(2k+1))}
  or equivalently

  
    
      
        ∀
        k
        
          (
          
            P
            
            
              (
              
                ⌊
                
                  
                    k
                    2
                  
                
                ⌋
              
              )
            
            →
            P
            (
            k
            )
          
          )
        
      
    
    {\displaystyle \forall k\left(P\!\left(\left\lfloor {\frac {k}{2}}\right\rfloor \right)\to P(k)\right)}
  The induction principle then ""automates"" log2 n applications of this inference in getting from P(0) to P(n). In fact, it is called ""prefix induction"" because each step proves something about a number from something about the ""prefix"" of that number — as formed by truncating the low bit of its binary representation. It can also be viewed as an application of traditional induction on the length of that binary representation.
If traditional predecessor induction is interpreted computationally as an n-step loop, then prefix induction would correspond to a log-n-step loop. Because of that, proofs using prefix induction are ""more feasibly constructive"" than proofs using predecessor induction.
Predecessor induction can trivially simulate prefix induction on the same statement. Prefix induction can simulate predecessor induction, but only at the cost of making the statement more syntactically complex (adding a bounded universal quantifier), so the interesting results relating prefix induction to polynomial-time computation depend on excluding unbounded quantifiers entirely, and limiting the alternation of bounded universal and existential quantifiers allowed in the statement.One can take the idea a step further: one must prove

  
    
      
        ∀
        k
        
          (
          
            P
            
            
              (
              
                ⌊
                
                  
                    k
                  
                
                ⌋
              
              )
            
            →
            P
            (
            k
            )
          
          )
        
      
    
    {\displaystyle \forall k\left(P\!\left(\left\lfloor {\sqrt {k}}\right\rfloor \right)\to P(k)\right)}
  whereupon the induction principle ""automates"" log log n applications of this inference in getting from P(0) to P(n). This form of induction has been used, analogously, to study log-time parallel computation.


=== Complete (strong) induction ===
Another variant, called complete induction, course of values induction or strong induction (in contrast to which the basic form of induction is sometimes known as weak induction), makes the induction step easier to prove by using a stronger hypothesis: one proves the statement 
  
    
      
        P
        (
        m
        +
        1
        )
      
    
    {\displaystyle P(m+1)}
   under the assumption that 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   holds for all natural numbers 
  
    
      
        n
      
    
    {\displaystyle n}
   less than 
  
    
      
        m
        +
        1
      
    
    {\displaystyle m+1}
  ; by contrast, the basic form only assumes 
  
    
      
        P
        (
        m
        )
      
    
    {\displaystyle P(m)}
  . The name ""strong induction"" does not mean that this method can prove more than ""weak induction"", but merely refers to the stronger hypothesis used in the induction step.
In fact, it can be shown that the two methods are actually equivalent, as explained below. In this form of complete induction, one still has to prove the base case, 
  
    
      
        P
        (
        0
        )
      
    
    {\displaystyle P(0)}
  , and it may even be necessary to prove extra-base cases such as 
  
    
      
        P
        (
        1
        )
      
    
    {\displaystyle P(1)}
   before the general argument applies, as in the example below of the Fibonacci number 
  
    
      
        
          F
          
            n
          
        
      
    
    {\displaystyle F_{n}}
  .
Although the form just described requires one to prove the base case, this is unnecessary if one can prove 
  
    
      
        P
        (
        m
        )
      
    
    {\displaystyle P(m)}
   (assuming 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   for all lower 
  
    
      
        n
      
    
    {\displaystyle n}
  ) for all 
  
    
      
        m
        ≥
        0
      
    
    {\displaystyle m\geq 0}
  . This is a special case of transfinite induction as described below, although it is no longer equivalent to ordinary induction. In this form the base case is subsumed by the case 
  
    
      
        m
        =
        0
      
    
    {\displaystyle m=0}
  , where 
  
    
      
        P
        (
        0
        )
      
    
    {\displaystyle P(0)}
   is proved with no other 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   assumed;
this case may need to be handled separately, but sometimes the same argument applies for 
  
    
      
        m
        =
        0
      
    
    {\displaystyle m=0}
   and 
  
    
      
        m
        >
        0
      
    
    {\displaystyle m>0}
  , making the proof simpler and more elegant.
In this method, however, it is vital to ensure that the proof of 
  
    
      
        P
        (
        m
        )
      
    
    {\displaystyle P(m)}
   does not implicitly assume that 
  
    
      
        m
        >
        0
      
    
    {\displaystyle m>0}
  , e.g. by saying ""choose an arbitrary 
  
    
      
        n
        <
        m
      
    
    {\displaystyle n<m}
  "", or by assuming that a set of m elements has an element.
Complete induction is equivalent to ordinary mathematical induction as described above, in the sense that a proof by one method can be transformed into a proof by the other. Suppose there is a proof of 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   by complete induction. Let 
  
    
      
        Q
        (
        n
        )
      
    
    {\displaystyle Q(n)}
   be the statement ""
  
    
      
        P
        (
        m
        )
      
    
    {\displaystyle P(m)}
   holds for all 
  
    
      
        m
      
    
    {\displaystyle m}
   such that 
  
    
      
        0
        ≤
        m
        ≤
        n
      
    
    {\displaystyle 0\leq m\leq n}
  "". Then 
  
    
      
        Q
        (
        n
        )
      
    
    {\displaystyle Q(n)}
   holds for all 
  
    
      
        n
      
    
    {\displaystyle n}
   if and only if 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   holds for all 
  
    
      
        n
      
    
    {\displaystyle n}
  , and our proof of 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   is easily transformed into a proof of 
  
    
      
        Q
        (
        n
        )
      
    
    {\displaystyle Q(n)}
   by (ordinary) induction. If, on the other hand, 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
   had been proven by ordinary induction, the proof would already effectively be one by complete induction: 
  
    
      
        P
        (
        0
        )
      
    
    {\displaystyle P(0)}
   is proved in the base case, using no assumptions, and 
  
    
      
        P
        (
        n
        +
        1
        )
      
    
    {\displaystyle P(n+1)}
   is proved in the induction step, in which one may assume all earlier cases but need only use the case 
  
    
      
        P
        (
        n
        )
      
    
    {\displaystyle P(n)}
  .


==== Example: Fibonacci numbers ====
Complete induction is most useful when several instances of the inductive hypothesis are required for each induction step. For example, complete induction can be used to show that

  
    
      
        
          F
          
            n
          
        
        =
        
          
            
              
                φ
                
                  n
                
              
              −
              
                ψ
                
                  n
                
              
            
            
              φ
              −
              ψ
            
          
        
      
    
    {\displaystyle F_{n}={\frac {\varphi ^{n}-\psi ^{n}}{\varphi -\psi }}}
  where 
  
    
      
        
          F
          
            n
          
        
      
    
    {\displaystyle F_{n}}
   is the nth Fibonacci number, and 
  
    
      
        φ
        =
        
          
            
              1
              +
              
                
                  5
                
              
            
            2
          
        
      
    
    {\textstyle \varphi ={{1+{\sqrt {5}}} \over 2}}
   (the golden ratio) and 
  
    
      
        ψ
        =
        
          
            
              1
              −
              
                
                  5
                
              
            
            2
          
        
      
    
    {\textstyle \psi ={{1-{\sqrt {5}}} \over 2}}
   are the roots of the polynomial 
  
    
      
        
          x
          
            2
          
        
        −
        x
        −
        1
      
    
    {\displaystyle x^{2}-x-1}
  . By using the fact that 
  
    
      
        
          F
          
            n
            +
            2
          
        
        =
        
          F
          
            n
            +
            1
          
        
        +
        
          F
          
            n
          
        
      
    
    {\displaystyle F_{n+2}=F_{n+1}+F_{n}}
   for each 
  
    
      
        n
        ∈
        
          
            N
          
        
      
    
    {\displaystyle n\in {\mathbb {N}}}
  , the identity above can be verified by direct calculation for 
  
    
      
        
          F
          
            n
            +
            2
          
        
      
    
    {\textstyle F_{n+2}}
   if one assumes that it already holds for both 
  
    
      
        
          F
          
            n
            +
            1
          
        
      
    
    {\textstyle F_{n+1}}
   and 
  
    
      
        
          F
          
            n
          
        
      
    
    {\textstyle F_{n}}
  . To complete the proof, the identity must be verified in the two base cases: 
  
    
      
        n
        =
        0
      
    
    {\displaystyle n=0}
   and 
  
    
      
        n
        =
        1
      
    
    {\textstyle n=1}
  .


==== Example: prime factorization ====
Another proof by complete induction uses the hypothesis that the statement holds for all smaller 
  
    
      
        n
      
    
    {\displaystyle n}
   more thoroughly. Consider the statement that ""every natural number greater than 1 is a product of (one or more) prime numbers"", which is the ""existence"" part of the fundamental theorem of arithmetic. For proving the induction step, the induction hypothesis is that for a given 
  
    
      
        n
        >
        1
      
    
    {\displaystyle n>1}
   the statement holds for all smaller 
  
    
      
        n
        >
        1
      
    
    {\displaystyle n>1}
  . If 
  
    
      
        m
      
    
    {\displaystyle m}
   is prime then it is certainly a product of primes, and if not, then by definition it is a product: 
  
    
      
        m
        =
        
          n
          
            1
          
        
        
          n
          
            2
          
        
      
    
    {\displaystyle m=n_{1}n_{2}}
  , where neither of the factors is equal to 1; hence neither is equal to 
  
    
      
        m
      
    
    {\displaystyle m}
  , and so both are greater than 1 and smaller than 
  
    
      
        m
      
    
    {\displaystyle m}
  . The induction hypothesis now applies to 
  
    
      
        
          n
          
            1
          
        
      
    
    {\displaystyle n_{1}}
   and 
  
    
      
        
          n
          
            2
          
        
      
    
    {\displaystyle n_{2}}
  , so each one is a product of primes. Thus 
  
    
      
        m
      
    
    {\displaystyle m}
   is a product of products of primes, and hence by extension a product of primes itself.


==== Example: dollar amounts revisited ====
We shall look to prove the same example as above, this time with strong induction. The statement remains the same:

  
    
      
        S
        (
        n
        )
        :
        
        
        n
        ≥
        12
        
        ⟹
        
        
        ∃
        
        a
        ,
        b
        ∈
        
          N
        
        .
        
        
        n
        =
        4
        a
        +
        5
        b
      
    
    {\displaystyle S(n):\,\,n\geq 12\implies \,\exists \,a,b\in \mathbb {N} .\,\,n=4a+5b}
  However, there will be slight differences in the structure and the assumptions of the proof, starting with the extended base case.
Proof.
Base case: Show that 
  
    
      
        S
        (
        k
        )
      
    
    {\displaystyle S(k)}
   holds for 
  
    
      
        k
        =
        12
        ,
        13
        ,
        14
        ,
        15
      
    
    {\displaystyle k=12,13,14,15}
  .

  
    
      
        
          
            
              
                4
                ⋅
                3
                +
                5
                ⋅
                0
                =
                12
              
            
            
              
                4
                ⋅
                2
                +
                5
                ⋅
                1
                =
                13
              
            
            
              
                4
                ⋅
                1
                +
                5
                ⋅
                2
                =
                14
              
            
            
              
                4
                ⋅
                0
                +
                5
                ⋅
                3
                =
                15
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}4\cdot 3+5\cdot 0=12\\4\cdot 2+5\cdot 1=13\\4\cdot 1+5\cdot 2=14\\4\cdot 0+5\cdot 3=15\end{aligned}}}
  The base case holds.
Induction step: Given some 
  
    
      
        j
        >
        15
      
    
    {\displaystyle j>15}
  , assume 
  
    
      
        S
        (
        m
        )
      
    
    {\displaystyle S(m)}
   holds for all 
  
    
      
        m
      
    
    {\displaystyle m}
   with 
  
    
      
        12
        ≤
        m
        <
        j
      
    
    {\displaystyle 12\leq m<j}
  . Prove that 
  
    
      
        S
        (
        j
        )
      
    
    {\displaystyle S(j)}
   holds.
Choosing 
  
    
      
        m
        =
        j
        −
        4
      
    
    {\displaystyle m=j-4}
  , and observing that 
  
    
      
        15
        <
        j
        
        ⟹
        
        12
        ≤
        j
        −
        4
        <
        j
      
    
    {\displaystyle 15<j\implies 12\leq j-4<j}
   shows that 
  
    
      
        S
        (
        j
        −
        4
        )
      
    
    {\displaystyle S(j-4)}
   holds, by the inductive hypothesis. That is, the sum 
  
    
      
        j
        −
        4
      
    
    {\displaystyle j-4}
   can be formed by some combination of 
  
    
      
        4
      
    
    {\displaystyle 4}
   and 
  
    
      
        5
      
    
    {\displaystyle 5}
   dollar coins. Then, simply adding a 
  
    
      
        4
      
    
    {\displaystyle 4}
   dollar coin to that combination yields the sum 
  
    
      
        j
      
    
    {\displaystyle j}
  . That is, 
  
    
      
        S
        (
        j
        )
      
    
    {\displaystyle S(j)}
   holds. ∎


=== Forward-backward induction ===

Sometimes, it is more convenient to deduce backwards, proving the statement for 
  
    
      
        n
        −
        1
      
    
    {\displaystyle n-1}
  , given its validity for 
  
    
      
        n
      
    
    {\displaystyle n}
  . However, proving the validity of the statement for no single number suffices to establish the base case; instead, one needs to prove the statement for an infinite subset of the natural numbers. For example, Augustin Louis Cauchy first used forward (regular) induction to prove the
inequality of arithmetic and geometric means for all powers of 2, and then used backwards induction to show it for all natural numbers.


== Example of error in the induction step ==

The induction step must be proved for all values of n. To illustrate this, Joel E. Cohen proposed the following argument, which purports to prove by mathematical induction that all horses are of the same color:Base case: in a set of only one horse, there is only one color.
Induction step: assume as induction hypothesis that within any set of 
  
    
      
        n
      
    
    {\displaystyle n}
   horses, there is only one color. Now look at any set of 
  
    
      
        n
        +
        1
      
    
    {\displaystyle n+1}
   horses. Number them: 
  
    
      
        1
        ,
        2
        ,
        3
        ,
        …
        ,
        n
        ,
        n
        +
        1
      
    
    {\displaystyle 1,2,3,\dotsc ,n,n+1}
  . Consider the sets 
  
    
      
        
          {
          
            1
            ,
            2
            ,
            3
            ,
            …
            ,
            n
          
          }
        
      
    
    {\textstyle \left\{1,2,3,\dotsc ,n\right\}}
   and 
  
    
      
        
          {
          
            2
            ,
            3
            ,
            4
            ,
            …
            ,
            n
            +
            1
          
          }
        
      
    
    {\textstyle \left\{2,3,4,\dotsc ,n+1\right\}}
  . Each is a set of only 
  
    
      
        n
      
    
    {\displaystyle n}
   horses, therefore within each there is only one color. But the two sets overlap, so there must be only one color among all 
  
    
      
        n
        +
        1
      
    
    {\displaystyle n+1}
   horses.
The base case 
  
    
      
        n
        =
        1
      
    
    {\displaystyle n=1}
   is trivial, and the induction step is correct in all cases 
  
    
      
        n
        >
        1
      
    
    {\displaystyle n>1}
  . However, the argument used in the induction step is incorrect for 
  
    
      
        n
        +
        1
        =
        2
      
    
    {\displaystyle n+1=2}
  , because the statement that ""the two sets overlap"" is false for 
  
    
      
        
          {
          1
          }
        
      
    
    {\textstyle \left\{1\right\}}
   and 
  
    
      
        
          {
          2
          }
        
      
    
    {\textstyle \left\{2\right\}}
  .


== Formalization ==
In second-order logic, one can write down the ""axiom of induction"" as follows:

  
    
      
        ∀
        P
        
          
            (
          
        
        P
        (
        0
        )
        ∧
        ∀
        k
        
          
            (
          
        
        P
        (
        k
        )
        →
        P
        (
        k
        +
        1
        )
        
          
            )
          
        
        →
        ∀
        n
        
          
            (
          
        
        P
        (
        n
        )
        
          
            )
          
        
        
          
            )
          
        
      
    
    {\displaystyle \forall P{\Bigl (}P(0)\land \forall k{\bigl (}P(k)\to P(k+1){\bigr )}\to \forall n{\bigl (}P(n){\bigr )}{\Bigr )}}
  ,where P(.) is a variable for predicates involving one natural number and k and n are variables for natural numbers.
In words, the base case P(0) and the induction step (namely, that the induction hypothesis P(k) implies P(k + 1)) together imply that P(n) for any natural number n. The axiom of induction asserts the validity of inferring that P(n) holds for any natural number n from the base case and the induction step.
The first quantifier in the axiom ranges over predicates rather than over individual numbers. This is a second-order quantifier, which means that this axiom is stated in second-order logic. Axiomatizing arithmetic induction in first-order logic requires an axiom schema containing a separate axiom for each possible predicate. The article Peano axioms contains further discussion of this issue.
The axiom of structural induction for the natural numbers was first formulated by Peano, who used it to specify the natural numbers together with the following four other axioms:

0 is a natural number.
The successor function s of every natural number yields a natural number (s(x) = x + 1).
The successor function is injective.
0 is not in the range of s.In first-order ZFC set theory, quantification over predicates is not allowed, but one can still express induction by quantification over sets:

  
    
      
        ∀
        A
        
          
            (
          
        
        0
        ∈
        A
        ∧
        ∀
        k
        ∈
        
          N
        
        
          
            (
          
        
        k
        ∈
        A
        →
        (
        k
        +
        1
        )
        ∈
        A
        
          
            )
          
        
        →
        
          N
        
        ⊆
        A
        
          
            )
          
        
      
    
    {\displaystyle \forall A{\Bigl (}0\in A\land \forall k\in \mathbb {N} {\bigl (}k\in A\to (k+1)\in A{\bigr )}\to \mathbb {N} \subseteq A{\Bigr )}}
  A may be read as a set representing a proposition, and containing natural numbers, for which the proposition holds. This is not an axiom, but a theorem, given that natural numbers are defined in the language of ZFC set theory by axioms, analogous to Peano's.


== Transfinite induction ==

One variation of the principle of complete induction can be generalized for statements about elements of any well-founded set, that is, a set with an irreflexive relation < that contains no infinite descending chains. Every set representing an ordinal number is well-founded, the set of natural numbers is one of them.
Applied to a well-founded set, transfinite induction can be formulated as a single step. To prove that a statement P(n) holds for each ordinal number:

Show, for each ordinal number n, that if P(m) holds for all m < n, then P(n) also holds.This form of induction, when applied to a set of ordinal numbers (which form a well-ordered and hence well-founded class), is called transfinite induction. It is an important proof technique in set theory, topology and other fields.
Proofs by transfinite induction typically distinguish three cases:

when n is a minimal element, i.e. there is no element smaller than n;
when n has a direct predecessor, i.e. the set of elements which are smaller than n has a largest element;
when n has no direct predecessor, i.e. n is a so-called limit ordinal.Strictly speaking, it is not necessary in transfinite induction to prove a base case, because it is a vacuous special case of the proposition that if P is true of all n < m, then P is true of m. It is vacuously true precisely because there are no values of n < m that could serve as counterexamples. So the special cases are special cases of the general case.


== Relationship to the well-ordering principle ==
The principle of mathematical induction is usually stated as an axiom of the natural numbers; see Peano axioms. It is strictly stronger than the well-ordering principle in the context of the other Peano axioms. Suppose the following:

The trichotomy axiom: For any natural numbers n and m, n is less than or equal to m if and only if m is not less than n.
For any natural number n, n + 1 is greater than n.
For any natural number n, no natural number is between n and n + 1.
No natural number is less than zero.It can then be proved that induction, given the above-listed axioms, implies the well-ordering principle. The following proof uses complete induction and the first and fourth axioms.
Proof. Suppose there exists a non-empty set, S, of natural numbers that has no least element. Let P(n) be the assertion that n is not in S. Then P(0) is true, for if it were false then 0 is the least element of S. Furthermore, let n be a natural number, and suppose P(m) is true for all natural numbers m less than n + 1. Then if P(n + 1) is false n + 1 is in S, thus being a minimal element in S, a contradiction. Thus P(n + 1) is true. Therefore, by the complete induction principle, P(n) holds for all natural numbers n; so S is empty, a contradiction. ∎

On the other hand, the set 
  
    
      
        {
        (
        0
        ,
        n
        )
        :
        n
        ∈
        
          N
        
        }
        ∪
        {
        (
        1
        ,
        n
        )
        :
        n
        ∈
        
          N
        
        }
      
    
    {\displaystyle \{(0,n):n\in \mathbb {N} \}\cup \{(1,n):n\in \mathbb {N} \}}
  , shown in the picture, is well-ordered: 35lf  by the lexicographic order.
Moreover, except for the induction axiom, it satisfies all Peano axioms, where Peano's constant 0 is interpreted as the pair (0, 0), and Peano's successor function is defined on pairs by succ(x, n) = (x, n + 1) for all 
  
    
      
        x
        ∈
        {
        0
        ,
        1
        }
      
    
    {\displaystyle x\in \{0,1\}}
   and 
  
    
      
        n
        ∈
        
          N
        
      
    
    {\displaystyle n\in \mathbb {N} }
  .
As an example for the violation of the induction axiom, define the predicate P(x, n) as (x, n) = (0, 0) or (x, n) = succ(y, m) for some 
  
    
      
        y
        ∈
        {
        0
        ,
        1
        }
      
    
    {\displaystyle y\in \{0,1\}}
   and 
  
    
      
        m
        ∈
        
          N
        
      
    
    {\displaystyle m\in \mathbb {N} }
  . Then the base case P(0, 0) is trivially true, and so is the induction step: if P(x, n), then P(succ(x, n)). However, P is not true for all pairs in the set.
Peano's axioms with the induction principle uniquely model the natural numbers. Replacing the induction principle with the well-ordering principle allows for more exotic models that fulfill all the axioms.It is mistakenly printed in several books and sources that the well-ordering principle is equivalent to the induction axiom. In the context of the other Peano axioms, this is not the case, but in the context of other axioms, they are equivalent; specifically, the well-ordering principle implies the induction axiom in the context of the first two above listed axioms and

Every natural number is either 0 or n + 1 for some natural number n.A common mistake in many erroneous proofs is to assume that n − 1 is a unique and well-defined natural number, a property which is not implied by the other Peano axioms.


== See also ==
Combinatorial proof
Induction puzzles
Proof by exhaustion
Recursion
Recursion (computer science)
Structural induction
Transfinite induction


== Notes ==


== References =="
2a27518a31,Chinese mathematics,"Mathematics in China emerged independently by the 11th century BCE. The Chinese independently developed a real number system that includes significantly large and negative numbers, more than one numeral system (base 2 and base 10), algebra, geometry, number theory and trigonometry.
Since the Han Dynasty, as diophantine approximation being a prominent numerical method, the Chinese made substantial progress on polynomial evaluation. Algorithms like regula falsi and expressions like continued fractions are widely used and have been well-documented ever-since. They deliberately find the principal nth root of positive numbers and the roots of equations. The major texts from the period, The Nine Chapters on the Mathematical Art and the Book on Numbers and Computation gave detailed processes for solving various mathematical problems in daily life. All procedures were computed using a counting board in both texts, and they included inverse elements as well as Euclidean divisions. The texts provide procedures similar to that of Gaussian elimination and Horner's method for linear algebra. The achievement of Chinese algebra reached a zenith in the 13th century during the Yuan dynasty with the development of tiān yuán shù.

As a result of obvious linguistic and geographic barriers, as well as content, Chinese mathematics and the mathematics of the ancient Mediterranean world are presumed to have developed more or less independently up to the time when The Nine Chapters on the Mathematical Art reached its final form, while the Book on Numbers and Computation and Huainanzi are roughly contemporary with classical Greek mathematics. Some exchange of ideas across Asia through known cultural exchanges from at least Roman times is likely. Frequently, elements of the mathematics of early societies correspond to rudimentary results found later in branches of modern mathematics such as geometry or number theory. The Pythagorean theorem for example, has been attested to the time of the Duke of Zhou. Knowledge of Pascal's triangle has also been shown to have existed in China centuries before Pascal, such as the Song dynasty Chinese polymath Shen Kuo.


== Early Chinese mathematics ==

 Shang Dynasty (1600–1050 BC). One of the oldest surviving mathematical works is the I Ching, which greatly influenced written literature during the Zhou Dynasty (1050–256 BC). For mathematics, the book included a sophisticated use of hexagrams. Leibniz pointed out, the I Ching (Yi Jing) contained elements of binary numbers.
Since the Shang period, the Chinese had already fully developed a decimal system. Since early times, Chinese understood basic arithmetic (which dominated far eastern history), algebra, equations, and negative numbers with counting rods. Although the Chinese were more focused on arithmetic and advanced algebra for astronomical uses, they were also the first to develop negative numbers, algebraic geometry (only Chinese geometry) and the usage of decimals.
Math was one of the Liù Yì (六藝) or Six Arts, students were required to master during the Zhou Dynasty (1122–256 BC). Learning them all perfectly was required to be a perfect gentleman, or in the Chinese sense, a ""Renaissance Man"". Six Arts have their roots in the Confucian philosophy.
The oldest existent work on geometry in China comes from the philosophical Mohist canon of c. 330 BC, compiled by the followers of Mozi (470–390 BC). The Mo Jing described various aspects of many fields associated with physical science, and provided a small wealth of information on mathematics as well. It provided an 'atomic' definition of the geometric point, stating that a line is separated into parts, and the part which has no remaining parts (i.e. cannot be divided into smaller parts) and thus forms the extreme end of a line is a point. Much like Euclid's first and third definitions and Plato's 'beginning of a line', the Mo Jing stated that ""a point may stand at the end (of a line) or at its beginning like a head-presentation in childbirth. (As to its invisibility) there is nothing similar to it."" Similar to the atomists of Democritus, the Mo Jing stated that a point is the smallest unit, and cannot be cut in half, since 'nothing' cannot be halved. It stated that two lines of equal length will always finish at the same place, while providing definitions for the comparison of lengths and for parallels, along with principles of space and bounded space. It also described the fact that planes without the quality of thickness cannot be piled up since they cannot mutually touch. The book provided word recognition for circumference, diameter, and radius, along with the definition of volume.The history of mathematical development lacks some evidence. There are still debates about certain mathematical classics. For example, the Zhoubi Suanjing dates around 1200–1000 BC, yet many scholars believed it was written between 300 and 250 BC. The Zhoubi Suanjing contains an in-depth proof of the Gougu Theorem (a special case of the Pythagorean Theorem) but focuses more on astronomical calculations. However, the recent archaeological discovery of the Tsinghua Bamboo Slips, dated c. 305 BC, has revealed some aspects of pre-Qin mathematics, such as the first known decimal multiplication table.The abacus was first mentioned in the second century BC, alongside 'calculation with rods' (suan zi) in which small bamboo sticks are placed in successive squares of a checkerboard.


== Qin mathematics ==
Not much is known about Qin dynasty mathematics, or before, due to the burning of books and burying of scholars, circa 213–210 BC. Knowledge of this period can be determined from civil projects and historical evidence. The Qin dynasty created a standard system of weights. Civil projects of the Qin dynasty were significant feats of human engineering. Emperor Qin Shihuang (秦始皇) ordered many men to build large, lifesize statues for the palace tomb along with other temples and shrines, and the shape of the tomb was designed with geometric skills of architecture. It is certain that one of the greatest feats of human history, the Great Wall of China, required many mathematical techniques. All Qin dynasty buildings and grand projects used advanced computation formulas for volume, area and proportion.
Qin bamboo cash purchased at the antiquarian market of Hong Kong by the Yuelu Academy, according to the preliminary reports, contains the earliest epigraphic sample of a mathematical treatise.


== Han mathematics ==

In the Han Dynasty, numbers were developed into a place value decimal system and used on a counting board with a set of counting rods called chousuan, consisting of only nine symbols with a blank space on the counting board representing zero. Negative numbers and fractions were also incorporated into solutions of the great mathematical texts of the period. The mathematical texts of the time, the Suàn shù shū and the Jiuzhang suanshu solved basic arithmetic problems such as addition, subtraction, multiplication and division. Furthermore, they gave the processes for square and cubed root extraction, which eventually was applied to solving quadratic equations up to the third order. Both texts also made substantial progress in Linear Algebra, namely solving systems of equations with multiple unknowns. The value of pi is taken to be equal to three in both texts. However, the mathematicians Liu Xin (d. 23) and Zhang Heng (78–139) gave more accurate approximations for pi than Chinese of previous centuries had used. Mathematics was developed to solve practical problems in the time such as division of land or problems related to division of payment. The Chinese did not focus on theoretical proofs based on geometry or algebra in the modern sense of proving equations to find area or volume. The Book of Computations and The Nine Chapters on the Mathematical Art provide numerous practical examples that would be used in daily life.


=== Suan shu shu ===
The Suàn shù shū (Writings on Reckoning or The Book of Computations) is an ancient Chinese text on mathematics approximately seven thousand characters in length, written on 190 bamboo strips. It was discovered together with other writings in 1984 when archaeologists opened a tomb at Zhangjiashan in Hubei province. From documentary evidence this tomb is known to have been closed in 186 BC, early in the Western Han dynasty. While its relationship to the Nine Chapters is still under discussion by scholars, some of its contents are clearly paralleled there. The text of the Suan shu shu is however much less systematic than the Nine Chapters, and appears to consist of a number of more or less independent short sections of text drawn from a number of sources.The Book of Computations contains many perquisites to problems that would be expanded upon in The Nine Chapters on the Mathematical Art. An example of the elementary mathematics in the Suàn shù shū, the square root is approximated by using false position method which says to ""combine the excess and deficiency as the divisor; (taking) the deficiency numerator multiplied by the excess denominator and the excess numerator times the deficiency denominator, combine them as the dividend."" Furthermore, The Book of Computations solves systems of two equations and two unknowns using the same false position method.


=== The Nine Chapters on the Mathematical Art ===
The Nine Chapters on the Mathematical Art is a Chinese mathematics book, its oldest archeological date being 179 AD (traditionally dated 1000 BC), but perhaps as early as 300–200 BC. Although the author(s) are unknown, they made a major contribution in the eastern world. Problems are set up with questions immediately followed by answers and procedure. There are no formal mathematical proofs within the text, just a step-by-step procedure. The commentary of Liu Hui provided geometrical and algebraic proofs to the problems given within the text.The Nine Chapters on the Mathematical Art was one of the most influential of all Chinese mathematical books and it is composed of 246 problems. It was later incorporated into The Ten Computational Canons, which became the core of mathematical education in later centuries. This book includes 246 problems on surveying, agriculture, partnerships, engineering, taxation, calculation, the solution of equations, and the properties of right triangles. The Nine Chapters made significant additions to solving quadratic equations in a way similar to Horner's method. It also made advanced contributions to ""fangcheng"" or what is now known as linear algebra. Chapter seven solves system of linear equations with two unknowns using the false position method, similar to The Book of Computations. Chapter eight deals with solving determinate and indeterminate simultaneous linear equations using positive and negative numbers, with one problem dealing with solving four equations in five unknowns. The Nine Chapters solves systems of equations using methods similar to the modern Gaussian elimination and back substitution.The version of The Nine Chapters that has served as the foundation for modern renditions was a result of the efforts of the scholar Dai Zhen. Transcribing the problems directly from Yongle Encyclopedia, he then proceeded to make revisions to the original text, along with the inclusion his own notes explaining his reasoning behind the alterations. His finished work would be first published in 1774, but a new revision would be published in 1776 to correct various errors as well as include a version of The Nine Chapters from the Southern Song that contained the commentaries of Lui Hui and Li Chunfeng. The final version of Dai Zhen's work would come in 1777, titled Ripple Pavilion, with this final rendition being widely distributed and coming to serve as the standard for modern versions of The Nine Chapters. However, this version has come under scrutiny from Guo Shuchen, alleging that the edited version still contains numerous errors and that not all of the original amendments were done by Dai Zhen himself.


=== Calculation of pi ===
Problems in The Nine Chapters on the Mathematical Art take pi to be equal to three in calculating problems related to circles and spheres, such as spherical surface area. There is no explicit formula given within the text for the calculation of pi to be three, but it is used throughout the problems of both The Nine Chapters on the Mathematical Art and the Artificer's Record, which was produced in the same time period. Historians believe that this figure of pi was calculated using the 3:1 relationship between the circumference and diameter of a circle. Some Han mathematicians attempted to improve this number, such as Liu Xin, who is believed to have estimated pi to be 3.154. Later, Liu Hui attempted to improve the calculation by calculating pi to be 3.141024 (a low estimate of the number). Liu calculated this number by using polygons inside a hexagon as a lower limit compared to a circle. Zu Chongzhi later discovered the calculation of pi to be 3.1415926 < π < 3.1415927 by using polygons with 24,576 sides. This calculation would be discovered in Europe during the 16th century.There is no explicit method or record of how he calculated this estimate.


=== Division and root extraction ===
Basic arithmetic processes such as addition, subtraction, multiplication and division were present before the Han Dynasty. The Nine Chapters on the Mathematical Art take these basic operations for granted and simply instruct the reader to perform them. Han mathematicians calculated square and cube roots in a similar manner as division, and problems on division and root extraction both occur in Chapter Four of The Nine Chapters on the Mathematical Art. Calculating the square and cube roots of numbers is done through successive approximation, the same as division, and often uses similar terms such as dividend (shi) and divisor (fa) throughout the process. This process of successive approximation was then extended to solving quadratics of the second and third order, such as 
  
    
      
        
          x
          
            2
          
        
        +
        a
        =
        b
      
    
    {\displaystyle x^{2}+a=b}
  , using a method similar to Horner's method. The method was not extended to solve quadratics of the nth order during the Han Dynasty; however, this method was eventually used to solve these equations.


=== Linear algebra ===
The Book of Computations is the first known text to solve systems of equations with two unknowns. There are a total of three sets of problems within The Book of Computations involving solving systems of equations with the false position method, which again are put into practical terms. Chapter Seven of The Nine Chapters on the Mathematical Art also deals with solving a system of two equations with two unknowns with the false position method. To solve for the greater of the two unknowns, the false position method instructs the reader to cross-multiply the minor terms or zi (which are the values given for the excess and deficit) with the major terms mu. To solve for the lesser of the two unknowns, simply add the minor terms together.Chapter Eight of The Nine Chapters on the Mathematical Art deals with solving infinite equations with infinite unknowns. This process is referred to as the ""fangcheng procedure"" throughout the chapter. Many historians chose to leave the term fangcheng untranslated due to conflicting evidence of what the term means. Many historians translate the word to linear algebra today. In this chapter, the process of Gaussian elimination and back-substitution are used to solve systems of equations with many unknowns. Problems were done on a counting board and included the use of negative numbers as well as fractions. The counting board was effectively a matrix, where the top line is the first variable of one equation and the bottom was the last.


=== Liu Hui's commentary on The Nine Chapters on the Mathematical Art ===

Liu Hui's commentary on The Nine Chapters on the Mathematical Art is the earliest edition of the original text available. Hui is believed by most to be a mathematician shortly after the Han dynasty. Within his commentary, Hui qualified and proved some of the problems from either an algebraic or geometrical standpoint. For instance, throughout The Nine Chapters on the Mathematical Art, the value of pi is taken to be equal to three in problems regarding circles or spheres. In his commentary, Liu Hui finds a more accurate estimation of pi using the method of exhaustion. The method involves creating successive polynomials within a circle so that eventually the area of a higher-order polygon will be identical to that of the circle. From this method, Liu Hui asserted that the value of pi is about 3.14. Liu Hui also presented a geometric proof of square and cubed root extraction similar to the Greek method, which involved cutting a square or cube in any line or section and determining the square root through symmetry of the remaining rectangles.


== Mathematics in the period of disunity ==

In the third century Liu Hui wrote his commentary on the Nine Chapters and also wrote Haidao Suanjing which dealt with using Pythagorean theorem (already known by the 9 chapters), and triple, quadruple triangulation for surveying; his accomplishment in the mathematical surveying exceeded those accomplished in the west by a millennium. He was the first Chinese mathematician to calculate π=3.1416 with his π algorithm. He discovered the usage of Cavalieri's principle to find an accurate formula for the volume of a cylinder, and also developed elements of the infinitesimal calculus during the 3rd century CE.

In the fourth century, another influential mathematician named Zu Chongzhi, introduced the Da Ming Li. This calendar was specifically calculated to predict many cosmological cycles that will occur in a period of time. Very little is really known about his life. Today, the only sources are found in Book of Sui, we now know that Zu Chongzhi was one of the generations of mathematicians. He used Liu Hui's pi-algorithm applied to a 12288-gon and obtained a value of pi to 7 accurate decimal places (between 3.1415926 and 3.1415927), which would remain the most accurate approximation of π available for the next 900 years. He also applied He Chengtian's interpolation for approximating irrational number with fraction in his astronomy and mathematical works, he obtained 
  
    
      
        
          
            
              355
              113
            
          
        
      
    
    {\displaystyle {\tfrac {355}{113}}}
   as a good fraction approximate for pi; Yoshio Mikami commented that neither the Greeks, nor the Hindus nor Arabs knew about this fraction approximation to pi, not until the Dutch mathematician Adrian Anthoniszoom rediscovered it in 1585, ""the Chinese had therefore been possessed of this the most extraordinary of all fractional values over a whole millennium earlier than Europe""Along with his son, Zu Geng, Zu Chongzhi applied the Cavalieri's principle to find an accurate solution for calculating the volume of the sphere. Besides containing formulas for the volume of the sphere, his book also included formulas of cubic equations and the accurate value of pi. His work, Zhui Shu was discarded out of the syllabus of mathematics during the Song dynasty and lost. Many believed that Zhui Shu contains the formulas and methods for linear, matrix algebra, algorithm for calculating the value of π, formula for the volume of the sphere. The text should also associate with his astronomical methods of interpolation, which would contain knowledge, similar to our modern mathematics.
A mathematical manual called Sunzi mathematical classic dated between 200 and 400 CE contained the most detailed step by step description of multiplication and division algorithm with counting rods. Intriguingly, Sunzi may have influenced the development of place-value systems and place-value systems and the associated Galley division in the West. European sources learned place-value techniques in the 13th century, from a Latin translation an early-9th-century work by Al-Khwarizmi. Khwarizmi's presentation is almost identical to the division algorithm in Sunzi, even regarding stylistic matters (for example, using blank spaces to represent trailing zeros); the similarity suggests that the results may not have been an independent discovery. Islamic commentators on Al-Khwarizmi's work believed that it primarily summarized Hindu knowledge; Al-Khwarizmi's failure to cite his sources makes it difficult to determine whether those sources had in turn learned the procedure from China.In the fifth century the manual called ""Zhang Qiujian suanjing"" discussed linear and quadratic equations. By this point the Chinese had the concept of negative numbers.


== Tang mathematics ==
By the Tang Dynasty study of mathematics was fairly standard in the great schools. The Ten Computational Canons was a collection of ten Chinese mathematical works, compiled by early Tang dynasty mathematician Li Chunfeng (李淳風 602–670), as the official mathematical texts for imperial examinations in mathematics. The Sui dynasty and Tang dynasty ran the ""School of Computations"".Wang Xiaotong was a great mathematician in the beginning of the Tang Dynasty, and he wrote a book: Jigu Suanjing (Continuation of Ancient Mathematics), where numerical solutions which general cubic equations appear for the first timeThe Tibetans obtained their first knowledge of mathematics (arithmetic) from China during the reign of Nam-ri srong btsan, who died in 630.The table of sines by the Indian mathematician, Aryabhata, were translated into the Chinese mathematical book of the Kaiyuan Zhanjing, compiled in 718 AD during the Tang Dynasty. Although the Chinese excelled in other fields of mathematics such as solid geometry, binomial theorem, and complex algebraic formulas, early forms of trigonometry were not as widely appreciated as in the contemporary Indian and Islamic mathematics.Yi Xing, the mathematician and Buddhist monk was credited for calculating the tangent table. Instead, the early Chinese used an empirical substitute known as chong cha, while practical use of plane trigonometry in using the sine, the tangent, and the secant were known. Yi Xing was famed for his genius, and was known to have calculated the number of possible positions on a go board game (though without a symbol for zero he had difficulties expressing the number).


== Song and Yuan mathematics ==
Northern Song Dynasty mathematician Jia Xian developed an additive multiplicative method for extraction of square root and cubic root which implemented the ""Horner"" rule.

Four outstanding mathematicians arose during the Song Dynasty and Yuan Dynasty, particularly in the twelfth and thirteenth centuries: Yang Hui, Qin Jiushao, Li Zhi (Li Ye), and Zhu Shijie. Yang Hui, Qin Jiushao, Zhu Shijie all used the Horner-Ruffini method six hundred years earlier to solve certain types of simultaneous equations, roots, quadratic, cubic, and quartic equations. Yang Hui was also the first person in history to discover and prove ""Pascal's Triangle"", along with its binomial proof (although the earliest mention of the Pascal's triangle in China exists before the eleventh century AD). Li Zhi on the other hand, investigated on a form of algebraic geometry based on tiān yuán shù. His book; Ceyuan haijing revolutionized the idea of inscribing a circle into triangles, by turning this geometry problem by algebra instead of the traditional method of using Pythagorean theorem. Guo Shoujing of this era also worked on spherical trigonometry for precise astronomical calculations. At this point of mathematical history, a lot of modern western mathematics were already discovered by Chinese mathematicians. Things grew quiet for a time until the thirteenth century Renaissance of Chinese math. This saw Chinese mathematicians solving equations with methods Europe would not know until the eighteenth century. The high point of this era came with Zhu Shijie's two books Suanxue qimeng and the Siyuan yujian. In one case he reportedly gave a method equivalent to Gauss's pivotal condensation.
Qin Jiushao (c. 1202–1261) was the first to introduce the zero symbol into Chinese mathematics. Before this innovation, blank spaces were used instead of zeros in the system of counting rods. One of the most important contribution of Qin Jiushao was his method of solving high order numerical equations. Referring to Qin's solution of a 4th order equation, Yoshio Mikami put it: ""Who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe?"" Qin also solved a 10th order equation.Pascal's triangle was first illustrated in China by Yang Hui in his book Xiangjie Jiuzhang Suanfa (詳解九章算法), although it was described earlier around 1100 by Jia Xian. Although the Introduction to Computational Studies (算學啓蒙) written by Zhu Shijie (fl. 13th century) in 1299 contained nothing new in Chinese algebra, it had a great impact on the development of Japanese mathematics.


=== Algebra ===


==== Ceyuan haijing ====

Ceyuan haijing (Chinese: 測圓海鏡; pinyin: Cèyuán Hǎijìng), or Sea-Mirror of the Circle Measurements, is a collection of 692 formula and 170 problems related to inscribed circle in a triangle, written by Li Zhi (or Li Ye) (1192–1272 AD). He used Tian yuan shu to convert intricated geometry problems into pure algebra problems. He then used fan fa, or Horner's method, to solve equations of degree as high as six, although he did not describe his method of solving equations. ""Li Chih (or Li Yeh, 1192–1279), a mathematician of Peking who was offered a government post by Khublai Khan in 1206, but politely found an excuse to decline it. His Ts'e-yuan hai-ching (Sea-Mirror of the Circle Measurements) includes 170 problems dealing with[...]some of the problems leading to polynomial equations of sixth degree. Although he did not describe his method of solution of equations, it appears that it was not very different from that used by Chu Shih-chieh and Horner. Others who used the Horner method were Ch'in Chiu-shao (ca. 1202 – ca.1261) and Yang Hui (fl. ca. 1261–1275).


==== Jade Mirror of the Four Unknowns ====

Si-yüan yü-jian (四元玉鑒), or Jade Mirror of the Four Unknowns, was written by Zhu Shijie in 1303 AD and marks the peak in the development of Chinese algebra. The four elements, called heaven, earth, man and matter, represented the four unknown quantities in his algebraic equations. It deals with simultaneous equations and with equations of degrees as high as fourteen. The author uses the method of fan fa, today called Horner's method, to solve these equations.There are many summation series equations given without proof in the Mirror. A few of the summation series are:

  
    
      
        
          1
          
            2
          
        
        +
        
          2
          
            2
          
        
        +
        
          3
          
            2
          
        
        +
        ⋯
        +
        
          n
          
            2
          
        
        =
        
          
            
              n
              (
              n
              +
              1
              )
              (
              2
              n
              +
              1
              )
            
            
              3
              !
            
          
        
      
    
    {\displaystyle 1^{2}+2^{2}+3^{2}+\cdots +n^{2}={n(n+1)(2n+1) \over 3!}}
  

  
    
      
        1
        +
        8
        +
        30
        +
        80
        +
        ⋯
        +
        
          
            
              
                n
                
                  2
                
              
              (
              n
              +
              1
              )
              (
              n
              +
              2
              )
            
            
              3
              !
            
          
        
        =
        
          
            
              n
              (
              n
              +
              1
              )
              (
              n
              +
              2
              )
              (
              n
              +
              3
              )
              (
              4
              n
              +
              1
              )
            
            
              5
              !
            
          
        
      
    
    {\displaystyle 1+8+30+80+\cdots +{n^{2}(n+1)(n+2) \over 3!}={n(n+1)(n+2)(n+3)(4n+1) \over 5!}}
  


==== Mathematical Treatise in Nine Sections ====
Shu-shu chiu-chang, or Mathematical Treatise in Nine Sections, was written by the wealthy governor and minister Ch'in Chiu-shao (ca. 1202 – ca. 1261 AD) and with the invention of a method of solving simultaneous congruences, it marks the high point in Chinese indeterminate analysis.


==== Magic squares and magic circles ====
The earliest known magic squares of order greater than three are attributed to Yang Hui (fl. ca. 1261–1275), who worked with magic squares of order as high as ten. He also worked with magic circle.


=== Trigonometry ===
The embryonic state of trigonometry in China slowly began to change and advance during the Song Dynasty (960–1279), where Chinese mathematicians began to express greater emphasis for the need of spherical trigonometry in calendarical science and astronomical calculations. The polymath Chinese scientist, mathematician and official Shen Kuo (1031–1095) used trigonometric functions to solve mathematical problems of chords and arcs. Victor J. Katz writes that in Shen's formula ""technique of intersecting circles"", he created an approximation of the arc of a circle s by s = c + 2v2/d, where d is the diameter, v is the versine, c is the length of the chord c subtending the arc. Sal Restivo writes that Shen's work in the lengths of arcs of circles provided the basis for spherical trigonometry developed in the 13th century by the mathematician and astronomer Guo Shoujing (1231–1316). As the historians L. Gauchet and Joseph Needham state, Guo Shoujing used spherical trigonometry in his calculations to improve the calendar system and Chinese astronomy. Along with a later 17th-century Chinese illustration of Guo's mathematical proofs, Needham states that:

Guo used a quadrangular spherical pyramid, the basal quadrilateral of which consisted of one equatorial and one ecliptic arc, together with two meridian arcs, one of which passed through the summer solstice point...By such methods he was able to obtain the du lü (degrees of equator corresponding to degrees of ecliptic), the ji cha (values of chords for given ecliptic arcs), and the cha lü (difference between chords of arcs differing by 1 degree).Despite the achievements of Shen and Guo's work in trigonometry, another substantial work in Chinese trigonometry would not be published again until 1607, with the dual publication of Euclid's Elements by Chinese official and astronomer Xu Guangqi (1562–1633) and the Italian Jesuit Matteo Ricci (1552–1610).


== Ming mathematics ==
After the overthrow of the Yuan Dynasty, China became suspicious of Mongol-favored knowledge. The court turned away from math and physics in favor of botany and pharmacology. Imperial examinations included little mathematics, and what little they included ignored recent developments. Martzloff writes:At the end of the 16th century, Chinese autochthonous mathematics known by the Chinese themselves amounted to almost nothing, little more than calculation on the abacus, whilst in the 17th and 18th centuries nothing could be paralleled with the revolutionary progress in the theatre of European science. Moreover, at this same period, no one could report what had taken place in the more distant past, since the Chinese themselves only had a fragmentary knowledge of that. One should not forget that, in China itself, autochthonous mathematics was not rediscovered on a large scale prior to the last quarter of the 18th century.Correspondingly, scholars paid less attention to mathematics; pre-eminent mathematicians such as Gu Yingxiang and Tang Shunzhi appear to have been ignorant of the Tian yuan shu (Increase multiply) method. Without oral interlocutors to explicate them, the texts rapidly became incomprehensible; worse yet, most problems could be solved with more elementary methods. To the average scholar, then, tianyuan seemed numerology. When Wu Jing collated all the mathematical works of previous dynasties into The Annotations of Calculations in the Nine Chapters on the Mathematical Art, he omitted Tian yuan shu and the increase multiply method.
Instead, mathematical progress became focused on computational tools. In 15 century, abacus came into its suan pan form. Easy to use and carry, both fast and accurate, it rapidly overtook rod calculus as the preferred form of computation. Zhusuan, the arithmetic calculation through abacus, inspired multiple new works. Suanfa Tongzong (General Source of Computational Methods), a 17-volume work published in 1592 by Cheng Dawei, remained in use for over 300 years. Zhu Zaiyu, Prince of Zheng used 81 position abacus to calculate the square root and cubic root of 2 to 25 figure accuracy, a precision that enabled his development of the equal-temperament system.
Although this switch from counting rods to the abacus allowed for reduced computation times, it may have also led to the stagnation and decline of Chinese mathematics. The pattern rich layout of counting rod numerals on counting boards inspired many Chinese inventions in mathematics, such as the cross multiplication principle of fractions and methods for solving linear equations. Similarly, Japanese mathematicians were influenced by the counting rod numeral layout in their definition of the concept of a matrix. Algorithms for the abacus did not lead to similar conceptual advances. (This distinction, of course, is a modern one: until the 20th century, Chinese mathematics was exclusively a computational science.)
In the late 16th century, Matteo Ricci decided to published Western scientific works in order to establish a position at the Imperial Court. With the assistance of Xu Guangqi, he was able to translate Euclid's Elements using the same techniques used to teach classical Buddhist texts. Other missionaries followed in his example, translating Western works on special functions (trigonometry and logarithms) that were neglected in the Chinese tradition. However, contemporary scholars found the emphasis on proofs — as opposed to solved problems — baffling, and most continued to work from classical texts alone.


== Qing dynasty ==
Under the Kangxi Emperor, who learned Western mathematics from the Jesuits and was open to outside knowledge and ideas, Chinese mathematics enjoyed a brief period of official support. At Kangxi's direction, Mei Goucheng and three other outstanding mathematicians compiled a 53-volume Shuli Jingyun [The Essence of Mathematical Study] (printed 1723) which gave a systematic introduction to western mathematical knowledge. At the same time, Mei Goucheng also developed to Meishi Congshu Jiyang [The Compiled works of Mei]. Meishi Congshu Jiyang was an encyclopedic summary of nearly all schools of Chinese mathematics at that time, but it also included the cross-cultural works of Mei Wending (1633-1721), Goucheng's grandfather. The enterprise sought to alleviate the difficulties for Chinese mathematicians working on Western mathematics in tracking down citations.However, no sooner were the encyclopedias published than the Yongzheng Emperor acceded to the throne. Yongzheng introduced a sharply anti-Western turn to Chinese policy, and banished most missionaries from the Court. With access to neither Western texts nor intelligible Chinese ones, Chinese mathematics stagnated.
In 1773, the Qianlong Emperor decided to compile Siku Quanshu (The Complete Library of the Four Treasuries). Dai Zhen (1724-1777) selected and proofread The Nine Chapters on the Mathematical Art from Yongle Encyclopedia and several other mathematical works from Han and Tang dynasties. The long-missing mathematical works from Song and Yuan dynasties such as Si-yüan yü-jian and Ceyuan haijing were also found and printed, which directly led to a wave of new research. The most annotated work were Jiuzhang suanshu xicaotushuo (The Illustrations of Calculation Process for The Nine Chapters on the Mathematical Art ) contributed by Li Huang and Siyuan yujian xicao (The Detailed Explanation of Si-yuan yu-jian) by Luo Shilin.


== Western influences ==
In 1840, the First Opium War forced China to open its door and look at the outside world, which also led to an influx of western mathematical studies at a rate unrivaled in the previous centuries. In 1852, the Chinese mathematician Li Shanlan and the British missionary Alexander Wylie co-translated the later nine volumes of Elements and 13 volumes on Algebra. With the assistance of Joseph Edkins, more works on astronomy and calculus soon followed. Chinese scholars were initially unsure whether to approach the new works: was study of Western knowledge a form of submission to foreign invaders? But by the end of the century, it became clear that China could only begin to recover its sovereignty by incorporating Western works. Chinese scholars, taught in Western missionary schools, from (translated) Western texts, rapidly lost touch with the indigenous tradition. Those who were self-trained or in traditionalist circles nevertheless continued to work within the traditional framework of algorithmic mathematics without resorting to Western symbolism. Yet, as Martzloff notes, ""from 1911 onwards, solely Western mathematics has been practised in China.""


=== Western mathematics in modern China ===
Chinese mathematics experienced a great surge of revival following the establishment of a modern Chinese republic in 1912. Ever since then, modern Chinese mathematicians have made numerous achievements in various mathematical fields.
Some famous modern ethnic Chinese mathematicians include:

Shiing-Shen Chern was widely regarded as a leader in geometry and one of the greatest mathematicians of the twentieth century and was awarded the Wolf prize for his immense number of mathematical contributions.
Ky Fan, made a tremendous number of fundamental contributions to many different fields of mathematics. His work in fixed point theory, in addition to influencing nonlinear functional analysis, has found wide application in mathematical economics and game theory, potential theory, calculus of variations, and differential equations.
Shing-Tung Yau, his contributions have influenced both physics and mathematics, and he has been active at the interface between geometry and theoretical physics and subsequently awarded the Fields medal for his contributions.
Terence Tao, an ethnic Chinese child prodigy who received his master's degree at age 16, was the youngest participant in the International Mathematical Olympiad's entire history, first competing at the age of ten, winning a bronze, silver, and gold medal. He remains the youngest winner of each of the three medals in the Olympiad's history. He went on to receive the Fields medal.
Yitang Zhang, a number theorist who established the first finite bound on gaps between prime numbers.
Chen Jingrun, a number theorist who proved that every sufficiently large even number can be written as the sum of either two primes, or a prime and a semiprime (the product of two primes) which is now called Chen's theorem . His work was known as a milestone in the research of Goldbach's conjecture.


== Mathematics in the People's Republic of China ==
In 1949, at the beginning of the founding of the People's Republic of China, the government paid great attention to the cause of science although the country was in a predicament of lack of funds. The Chinese Academy of Sciences was established in November 1949. The Institute of Mathematics was formally established in July 1952. Then, the Chinese Mathematical Society and its founding journals restored and added other special journals. In the 18 years after 1949, the number of published papers accounted for more than three times the total number of articles before 1949. Many of them not only filled the gaps in China's past, but also reached the world's advanced level.During the chaos of the Cultural Revolution, the sciences declined. In the field of mathematics, in addition to Chen Jingrun, Hua Luogeng, Zhang Guanghou and other mathematicians struggling to continue their work. After the catastrophe, with the publication of Guo Moruo's literary ""Spring of Science"", Chinese sciences and mathematics experienced a revival. In 1977, a new mathematical development plan was formulated in Beijing, the work of the mathematics society was resumed, the journal was re-published, the academic journal was published, the mathematics education was strengthened, and basic theoretical research was strengthened.An important mathematical achievement of the Chinese mathematician in the direction of the power system is how Xia Zhihong proved the Painleve conjecture in 1988. When there are some initial states of N celestial bodies, one of the celestial bodies ran to infinity or speed in a limited time. Infinity is reached, that is, there are non-collision singularities. The Painleve conjecture is an important conjecture in the field of power systems proposed in 1895. A very important recent development for the 4-body problem is that Xue Jinxin and Dolgopyat proved a non-collision singularity in a simplified version of the 4-body system around 2013.In addition, in 2007, Shen Weixiao and Kozlovski, Van-Strien proved the Real Fatou conjecture: Real hyperbolic polynomials are dense in the space of real polynomials with fixed degree. This conjecture can be traced back to Fatou in the 1920s, and later Smale posed it in the 1960s. The proof of Real Fatou conjecture is one of the most important developments in conformal dynamics in the past decade.


=== Performance at the IMO ===
In comparison to other participating countries at the International Mathematical Olympiad, China has highest team scores and has won the all-members-gold IMO with a full team the most number of times.


== Mathematical texts ==
Zhou Dynasty
Zhoubi Suanjing c. 1000 BCE-100 CE

Astronomical theories, and computation techniques
Proof of the Pythagorean theorem (Shang Gao Theorem)
Fractional computations
Pythagorean theorem for astronomical purposesNine Chapters on the Mathematical Art 1000 BCE? – 50 CE

ch.1, computational algorithm, area of plane figures, GCF, LCD
ch.2, proportions
ch.3, proportions
ch.4, square, cube roots, finding unknowns
ch.5, volume and usage of pi as 3
ch.6, proportions
ch,7, interdeterminate equations
ch.8, Gaussian elimination and matrices
ch.9, Pythagorean theorem (Gougu Theorem)Han Dynasty
Book on Numbers and Computation 202 BC-186 BC

Calculation of the volume of various 3-dimensional shapes
Calculation of unknown side of rectangle, given area and one side
Using the false position method for finding roots and the extraction of approximate square roots
Conversion between different units


== Mathematics in education ==
The first reference to a book being used in learning mathematics in China is dated to the second century CE (Hou Hanshu: 24, 862; 35,1207). We are told that Ma Xu (a youth ca 110) and Zheng Xuan (127-200) both studied the Nine Chapters on Mathematical procedures. C.Cullen claims that mathematics, in a manner akin to medicine, was taught orally. The stylistics of the Suàn shù shū from Zhangjiashan suggest that the text was assembled from various sources and then underwent codification.


== See also ==
Chinese astronomy
History of mathematics
Indian mathematics
Islamic mathematics
Japanese mathematics
List of Chinese discoveries
List of Chinese mathematicians
Numbers in Chinese culture


== References ==


=== Citations ===


=== Sources ===
Boyer, C. B. (1991). A History of Mathematics. rev. by Uta C. Merzbach (paperback ed.). Wiley. ISBN 0-471-54397-7.
Bréard, Andrea (2019). Nine Chapters on Mathematical Modernity. Essays on the Global Historical Entanglements of the Science of Numbers in China (eBook ed.). Springer. ISBN 978-3-319-93695-6.
Dauben, Joseph W. (2007). ""Chinese Mathematics"".  In Victor J. Katz (ed.). The Mathematics of Egypt, Mesopotamia, China, India, and Islam: A Sourcebook. Princeton University Press. ISBN 978-0-691-11485-9.
Lander, Brian. ""State Management of River Dikes in Early China: New Sources on the Environmental History of the Central Yangzi Region."" T'oung Pao 100.4-5 (2014): 325–62.
Martzloff, Jean-Claude (1987). A history of chinese mathematics (PDF). Translated by Wilson, Stephen S. Berlin: Springer. p. 4. doi:10.1007/978-3-540-33783-6. ISBN 9783540337836. OCLC 262687287. Retrieved 1 December 2018.
Needham, Joseph (1986). Science and Civilization in China: Volume 3, Mathematics and the Sciences of the Heavens and the Earth. Taipei: Caves Books, Ltd.Public domain This article incorporates text from The Encyclopædia Britannica: a dictionary of arts, sciences, literature and general information, Volume 26, by Hugh Chisholm, a  publication from 1911, now in the public domain in the United States.
 This article incorporates text from The Life of the Buddha and the early history of his order: derived from Tibetan works in the Bkah-hgyur and Bstan-hgyur followed by notices on the early history of Tibet and Khoten, by Translated by William Woodville Rockhill, Ernst Leumann, Bunyiu Nanjio, a  publication from 1907, now in the public domain in the United States.


== External links ==
Early mathematics texts (Chinese) - Chinese Text Project
Overview of Chinese mathematics
Chinese Mathematics Through the Han Dynasty
Primer of Mathematics by Zhu Shijie"
ba9d988043,Net (mathematics),"In mathematics, more specifically in general topology and related branches, a net or Moore–Smith sequence is a generalization of the notion of a sequence. In essence, a sequence is a function whose domain is the natural numbers. The codomain of this function is usually some topological space.
The motivation for generalizing the notion of a sequence is that, in the context of topology, sequences do not fully encode all information about functions between topological spaces. In particular, the following two conditions are, in general, not equivalent for a map 
  
    
      
        f
      
    
    {\displaystyle f}
   between topological spaces 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  :

The map 
  
    
      
        f
      
    
    {\displaystyle f}
   is continuous in the topological sense;
Given any point 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   and any sequence in 
  
    
      
        X
      
    
    {\displaystyle X}
   converging to 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   the composition of 
  
    
      
        f
      
    
    {\displaystyle f}
   with this sequence converges to 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   (continuous in the sequential sense).While condition 1 always guarantees condition 2, the converse is not necessarily true if the topological spaces are not both first-countable. In particular, the two conditions are equivalent for metric spaces. The spaces for which the converse holds are the sequential spaces.
The concept of a net, first introduced by E. H. Moore and Herman L. Smith in 1922, is to generalize the notion of a sequence so that the above conditions (with ""sequence"" being replaced by ""net"" in condition 2) are in fact equivalent for all maps of topological spaces. In particular, rather than being defined on a countable linearly ordered set, a net is defined on an arbitrary directed set. This allows for theorems similar to the assertion that the conditions 1 and 2 above are equivalent to hold in the context of topological spaces that do not necessarily have a countable or linearly ordered neighbourhood basis around a point. Therefore, while sequences do not encode sufficient information about functions between topological spaces, nets do, because collections of open sets in topological spaces are much like directed sets in behavior. The term ""net"" was coined by John L. Kelley.Nets are one of the many tools used in topology to generalize certain concepts that may not be general enough in the context of metric spaces. A related notion, that of the filter, was developed in 1937 by Henri Cartan.


== Definitions ==
Any function whose domain is a directed set is called a net. If this function takes values in some set 
  
    
      
        X
      
    
    {\displaystyle X}
   then it may also be referred to as a net in 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   Explicitly, a net in 
  
    
      
        X
      
    
    {\displaystyle X}
   is a function of the form 
  
    
      
        f
        :
        A
        →
        X
      
    
    {\displaystyle f:A\to X}
   where 
  
    
      
        A
      
    
    {\displaystyle A}
   is some directed set. Elements of a net's domain are called its indices. 
A directed set is a non-empty set 
  
    
      
        A
      
    
    {\displaystyle A}
   together with a preorder, typically automatically assumed to be denoted by 
  
    
      
        
        ≤
        
      
    
    {\displaystyle \,\leq \,}
   (unless indicated otherwise), with the property that it is also (upward) directed, which means that for any 
  
    
      
        a
        ,
        b
        ∈
        A
        ,
      
    
    {\displaystyle a,b\in A,}
   there exists some 
  
    
      
        c
        ∈
        A
      
    
    {\displaystyle c\in A}
   such that 
  
    
      
        a
        ≤
        c
      
    
    {\displaystyle a\leq c}
   and 
  
    
      
        b
        ≤
        c
        .
      
    
    {\displaystyle b\leq c.}
   
In words, this property means that given any two elements (of 
  
    
      
        A
      
    
    {\displaystyle A}
  ), there is always some element that is ""above"" both of them (that is, that is greater than or equal to each of them); in this way, directed sets generalize the notion of ""a direction"" in a mathematically rigorous way. 
The natural numbers 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   together with the usual integer comparison 
  
    
      
        
        ≤
        
      
    
    {\displaystyle \,\leq \,}
   preorder form the archetypical example of a directed set. Indeed, a net whose domain is the natural numbers is a sequence because by definition, a sequence in 
  
    
      
        X
      
    
    {\displaystyle X}
   is just a function from 
  
    
      
        
          N
        
        =
        {
        1
        ,
        2
        ,
        …
        }
      
    
    {\displaystyle \mathbb {N} =\{1,2,\ldots \}}
   into 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   It is in this way that nets are generalizations of sequences. Importantly though, unlike the natural numbers, directed sets are not required to be total orders or even partial orders. 
Moreover, directed sets are allowed to have greatest elements and/or maximal elements, which is the reason why when using nets, caution is advised when using the induced strict preorder 
  
    
      
        
        <
        
      
    
    {\displaystyle \,<\,}
   instead of the original (non-strict) preorder 
  
    
      
        
        ≤
      
    
    {\displaystyle \,\leq }
  ; in particular, if a directed set 
  
    
      
        (
        A
        ,
        ≤
        )
      
    
    {\displaystyle (A,\leq )}
   has a greatest element 
  
    
      
        a
        ∈
        A
      
    
    {\displaystyle a\in A}
   then there does not exist any 
  
    
      
        b
        ∈
        A
      
    
    {\displaystyle b\in A}
   such that 
  
    
      
        a
        <
        b
      
    
    {\displaystyle a<b}
   (in contrast, there always exists some 
  
    
      
        b
        ∈
        A
      
    
    {\displaystyle b\in A}
   such that 
  
    
      
        a
        ≤
        b
      
    
    {\displaystyle a\leq b}
  ).
Nets are frequently denoted using notation that is similar to (and inspired by) that used with sequences. 
A net in 
  
    
      
        X
      
    
    {\displaystyle X}
   may be denoted by 
  
    
      
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
        ,
      
    
    {\displaystyle \left(x_{a}\right)_{a\in A},}
   where unless there is reason to think otherwise, it should automatically be assumed that the set 
  
    
      
        A
      
    
    {\displaystyle A}
   is directed and that its associated preorder is denoted by 
  
    
      
        
        ≤
        .
      
    
    {\displaystyle \,\leq .}
   
However, notation for nets varies with some authors using, for instance, angled brackets 
  
    
      
        
          
            ⟨
            
              x
              
                a
              
            
            ⟩
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle \left\langle x_{a}\right\rangle _{a\in A}}
   instead of parentheses. 
A net in 
  
    
      
        X
      
    
    {\displaystyle X}
   may also be written as 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
        ,
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A},}
   which expresses the fact that this net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   is a function 
  
    
      
        
          x
          
            ∙
          
        
        :
        A
        →
        X
      
    
    {\displaystyle x_{\bullet }:A\to X}
   whose value at an element 
  
    
      
        a
      
    
    {\displaystyle a}
   in its domain is denoted by 
  
    
      
        
          x
          
            a
          
        
      
    
    {\displaystyle x_{a}}
   instead of the usual parentheses notation 
  
    
      
        
          x
          
            ∙
          
        
        (
        a
        )
      
    
    {\displaystyle x_{\bullet }(a)}
   that is typically used with functions (this subscript notation being taken from sequences). As in the field of algebraic topology, the filled disk or ""bullet"" denotes the location where arguments to the net (that is, elements 
  
    
      
        a
        ∈
        A
      
    
    {\displaystyle a\in A}
   of the net's domain) are placed; it helps emphasize that the net is a function and also reduces the number of indices and other symbols that must be written when referring to it later.
Nets are primarily used in the fields of Analysis and Topology, where they are used to characterize many important topological properties that (in general), sequences are unable to characterize (this shortcoming of sequences motivated the study of sequential spaces and Fréchet–Urysohn spaces). Nets are intimately related to filters, which are also often used in topology. Every net may be associated with a filter and every filter may be associated with a net, where the properties of these associated objects are closely tied together (see the article about Filters in topology for more details). Nets directly generalize sequences and they may often be used very similarly to sequences. Consequently, the learning curve for using nets is typically much less steep than that for filters, which is why many mathematicians, especially analysts, prefer them over filters. However, filters, and especially ultrafilters, have some important technical advantages over nets that ultimately result in nets being encountered much less often than filters outside of the fields of Analysis and Topology.
A subnet is not merely the restriction of a net 
  
    
      
        f
      
    
    {\displaystyle f}
   to a directed subset of 
  
    
      
        A
        ;
      
    
    {\displaystyle A;}
   see the linked page for a definition.


== Examples of nets ==
Every non-empty totally ordered set is directed. Therefore, every function on such a set is a net. In particular, the natural numbers with the usual order form such a set, and a sequence is a function on the natural numbers, so every sequence is a net.
Another important example is as follows. Given a point 
  
    
      
        x
      
    
    {\displaystyle x}
   in a topological space, let 
  
    
      
        
          N
          
            x
          
        
      
    
    {\displaystyle N_{x}}
   denote the set of all neighbourhoods containing 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
   Then 
  
    
      
        
          N
          
            x
          
        
      
    
    {\displaystyle N_{x}}
   is a directed set, where the direction is given by reverse inclusion, so that 
  
    
      
        S
        ≥
        T
      
    
    {\displaystyle S\geq T}
   if and only if 
  
    
      
        S
      
    
    {\displaystyle S}
   is contained in 
  
    
      
        T
        .
      
    
    {\displaystyle T.}
   For 
  
    
      
        S
        ∈
        
          N
          
            x
          
        
        ,
      
    
    {\displaystyle S\in N_{x},}
   let 
  
    
      
        
          x
          
            S
          
        
      
    
    {\displaystyle x_{S}}
   be a point in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   Then 
  
    
      
        
          (
          
            x
            
              S
            
          
          )
        
      
    
    {\displaystyle \left(x_{S}\right)}
   is a net. As 
  
    
      
        S
      
    
    {\displaystyle S}
   increases with respect to 
  
    
      
        
        ≥
        ,
      
    
    {\displaystyle \,\geq ,}
   the points 
  
    
      
        
          x
          
            S
          
        
      
    
    {\displaystyle x_{S}}
   in the net are constrained to lie in decreasing neighbourhoods of 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   so intuitively speaking, we are led to the idea that 
  
    
      
        
          x
          
            S
          
        
      
    
    {\displaystyle x_{S}}
   must tend towards 
  
    
      
        x
      
    
    {\displaystyle x}
   in some sense. We can make this limiting concept precise.
A subnet of a sequence is not necessarily a sequence. 
For an example, let 
  
    
      
        X
        =
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle X=\mathbb {R} ^{n}}
   and let 
  
    
      
        
          x
          
            i
          
        
        =
        0
      
    
    {\displaystyle x_{i}=0}
   for every 
  
    
      
        i
        ∈
        
          N
        
        ,
      
    
    {\displaystyle i\in \mathbb {N} ,}
   so that 
  
    
      
        
          x
          
            ∙
          
        
        =
        (
        0
        
          )
          
            i
            ∈
            
              N
            
          
        
        :
        
          N
        
        →
        X
      
    
    {\displaystyle x_{\bullet }=(0)_{i\in \mathbb {N} }:\mathbb {N} \to X}
   is the constant zero sequence. 
Let 
  
    
      
        I
        =
        {
        r
        ∈
        
          R
        
        :
        r
        >
        0
        }
      
    
    {\displaystyle I=\{r\in \mathbb {R} :r>0\}}
   be directed by the usual order 
  
    
      
        
        ≤
        
      
    
    {\displaystyle \,\leq \,}
   and let 
  
    
      
        
          s
          
            r
          
        
        =
        0
      
    
    {\displaystyle s_{r}=0}
   for each 
  
    
      
        r
        ∈
        R
        .
      
    
    {\displaystyle r\in R.}
   
Define 
  
    
      
        φ
        :
        I
        →
        
          N
        
      
    
    {\displaystyle \varphi :I\to \mathbb {N} }
   by letting 
  
    
      
        φ
        (
        r
        )
        =
        ⌈
        r
        ⌉
      
    
    {\displaystyle \varphi (r)=\lceil r\rceil }
   be the ceiling of 
  
    
      
        r
        .
      
    
    {\displaystyle r.}
   
The map 
  
    
      
        φ
        :
        I
        →
        
          N
        
      
    
    {\displaystyle \varphi :I\to \mathbb {N} }
   is an order morphism whose image is cofinal in its codomain and 
  
    
      
        
          (
          
            
              x
              
                ∙
              
            
            ∘
            φ
          
          )
        
        (
        r
        )
        =
        
          x
          
            φ
            (
            r
            )
          
        
        =
        0
        =
        
          s
          
            r
          
        
      
    
    {\displaystyle \left(x_{\bullet }\circ \varphi \right)(r)=x_{\varphi (r)}=0=s_{r}}
   holds for every 
  
    
      
        r
        ∈
        R
        .
      
    
    {\displaystyle r\in R.}
   This shows that 
  
    
      
        
          
            (
            
              s
              
                r
              
            
            )
          
          
            r
            ∈
            R
          
        
        =
        
          x
          
            ∙
          
        
        ∘
        φ
      
    
    {\displaystyle \left(s_{r}\right)_{r\in R}=x_{\bullet }\circ \varphi }
   is a subnet of the sequence 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   (where this subnet is not a subsequence of 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   because it is not even a sequence since its domain is an uncountable set).


== Limits of nets ==

A net 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   is said to be eventually or residually in a set 
  
    
      
        S
      
    
    {\displaystyle S}
   if there exists some 
  
    
      
        a
        ∈
        A
      
    
    {\displaystyle a\in A}
   such that for every 
  
    
      
        b
        ∈
        A
      
    
    {\displaystyle b\in A}
   with 
  
    
      
        b
        ≥
        a
        ,
      
    
    {\displaystyle b\geq a,}
   the point 
  
    
      
        
          x
          
            b
          
        
        ∈
        S
        .
      
    
    {\displaystyle x_{b}\in S.}
   
And it is said to be frequently or cofinally in 
  
    
      
        S
      
    
    {\displaystyle S}
   if for every 
  
    
      
        a
        ∈
        A
      
    
    {\displaystyle a\in A}
   there exists some 
  
    
      
        b
        ∈
        A
      
    
    {\displaystyle b\in A}
   such that 
  
    
      
        b
        ≥
        a
      
    
    {\displaystyle b\geq a}
   and 
  
    
      
        
          x
          
            b
          
        
        ∈
        S
        .
      
    
    {\displaystyle x_{b}\in S.}
   
A point is called a limit point (respectively, cluster point) of a net if that net is eventually (respectively, cofinally) in every neighborhood of that point.
Explicitly, a point 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   is said to be an accumulation point or cluster point of a net if for every neighborhood 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   the net is frequently in 
  
    
      
        U
        .
      
    
    {\displaystyle U.}
  A point 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   is called a limit point or limit of the net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   if (and only if)

for every open neighborhood 
  
    
      
        U
      
    
    {\displaystyle U}
   of 
  
    
      
        x
        ,
      
    
    {\displaystyle x,}
   the net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   is eventually in 
  
    
      
        U
        ,
      
    
    {\displaystyle U,}
  in which case, this net is then also said to converge to/towards 
  
    
      
        x
      
    
    {\displaystyle x}
   and to have 
  
    
      
        x
      
    
    {\displaystyle x}
   as a limit.
Intuitively, convergence of a net 
  
    
      
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle \left(x_{a}\right)_{a\in A}}
   means that the values 
  
    
      
        
          x
          
            a
          
        
      
    
    {\displaystyle x_{a}}
   come and stay as close as we want to 
  
    
      
        x
      
    
    {\displaystyle x}
   for large enough 
  
    
      
        a
        .
      
    
    {\displaystyle a.}
   
The example net given above on the neighborhood system of a point 
  
    
      
        x
      
    
    {\displaystyle x}
   does indeed converge to 
  
    
      
        x
      
    
    {\displaystyle x}
   according to this definition.
Notation for limits
If the net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   converges in 
  
    
      
        X
      
    
    {\displaystyle X}
   to a point 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   then this fact may be expressed by writing any of the following:

where if the topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   is clear from context then the words ""in 
  
    
      
        X
      
    
    {\displaystyle X}
  "" may be omitted.
If 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }\to x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   and if this limit in 
  
    
      
        X
      
    
    {\displaystyle X}
   is unique (uniqueness in 
  
    
      
        X
      
    
    {\displaystyle X}
   means that if 
  
    
      
        y
        ∈
        X
      
    
    {\displaystyle y\in X}
   is such that 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        →
        y
        ,
      
    
    {\displaystyle \lim _{}x_{\bullet }\to y,}
   then necessarily 
  
    
      
        x
        =
        y
      
    
    {\displaystyle x=y}
  ) then this fact may be indicated by writing

where an equals sign is used in place of the arrow 
  
    
      
        →
        .
      
    
    {\displaystyle \to .}
   In a Hausdorff space, every net has at most one limit so the limit of a convergent net in a Hausdorff space is always unique. 
Some authors instead use the notation ""
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        =
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }=x}
  "" to mean 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }\to x}
   without also requiring that the limit be unique; however, if this notation is defined in this way then the equals sign 
  
    
      
        =
      
    
    {\displaystyle =}
   is no longer guaranteed to denote a transitive relationship and so no longer denotes equality. Specifically, without the uniqueness requirement, if 
  
    
      
        x
        ,
        y
        ∈
        X
      
    
    {\displaystyle x,y\in X}
   are distinct and if each is also a limit of 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   then 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        =
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }=x}
   and 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        =
        y
      
    
    {\displaystyle \lim _{}x_{\bullet }=y}
   could be written (using the equals sign 
  
    
      
        =
      
    
    {\displaystyle =}
  ) despite 
  
    
      
        x
        =
        y
      
    
    {\displaystyle x=y}
   being false.
Bases and subbases
Given a subbase 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\mathcal {B}}}
   for the topology on 
  
    
      
        X
      
    
    {\displaystyle X}
   (where note that every base for a topology is also a subbase) and given a point 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
   a net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   converges to 
  
    
      
        x
      
    
    {\displaystyle x}
   if and only if it is eventually in every neighborhood 
  
    
      
        U
        ∈
        
          
            B
          
        
      
    
    {\displaystyle U\in {\mathcal {B}}}
   of 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
   This characterization extends to neighborhood subbases (and so also neighborhood bases) of the given point 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
  
Convergence in metric spaces
Suppose 
  
    
      
        (
        X
        ,
        d
        )
      
    
    {\displaystyle (X,d)}
   is a metric space (or a pseudometric space) and 
  
    
      
        X
      
    
    {\displaystyle X}
   is endowed with the metric topology. 
If 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   is a point and 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                i
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{i}\right)_{a\in A}}
   is a net, then 
  
    
      
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle x_{\bullet }\to x}
   in 
  
    
      
        (
        X
        ,
        d
        )
      
    
    {\displaystyle (X,d)}
   if and only if 
  
    
      
        d
        
          (
          
            x
            ,
            
              x
              
                ∙
              
            
          
          )
        
        →
        0
      
    
    {\displaystyle d\left(x,x_{\bullet }\right)\to 0}
   in 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   where 
  
    
      
        d
        
          (
          
            x
            ,
            
              x
              
                ∙
              
            
          
          )
        
        :=
        
          
            (
            
              d
              
                (
                
                  x
                  ,
                  
                    x
                    
                      a
                    
                  
                
                )
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle d\left(x,x_{\bullet }\right):=\left(d\left(x,x_{a}\right)\right)_{a\in A}}
   is a net of real numbers. 
In plain English, this characterization says that a net converges to a point in a metric space if and only if the distance between the net and the point converges to zero. 
If 
  
    
      
        (
        X
        ,
        ‖
        ⋅
        ‖
        )
      
    
    {\displaystyle (X,\|\cdot \|)}
   is a normed space (or a seminormed space) then 
  
    
      
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle x_{\bullet }\to x}
   in 
  
    
      
        (
        X
        ,
        ‖
        ⋅
        ‖
        )
      
    
    {\displaystyle (X,\|\cdot \|)}
   if and only if 
  
    
      
        
          ‖
          
            x
            −
            
              x
              
                ∙
              
            
          
          ‖
        
        →
        0
      
    
    {\displaystyle \left\|x-x_{\bullet }\right\|\to 0}
   in 
  
    
      
        
          R
        
        ,
      
    
    {\displaystyle \mathbb {R} ,}
   where 
  
    
      
        
          ‖
          
            x
            −
            
              x
              
                ∙
              
            
          
          ‖
        
        :=
        
          
            (
            
              ‖
              
                x
                −
                
                  x
                  
                    a
                  
                
              
              ‖
            
            )
          
          
            a
            ∈
            A
          
        
        .
      
    
    {\displaystyle \left\|x-x_{\bullet }\right\|:=\left(\left\|x-x_{a}\right\|\right)_{a\in A}.}
  
Convergence in topological subspaces
If the set 
  
    
      
        S
        =
        {
        x
        }
        ∪
        
          {
          
            
              x
              
                a
              
            
            :
            a
            ∈
            A
          
          }
        
      
    
    {\displaystyle S=\{x\}\cup \left\{x_{a}:a\in A\right\}}
   is endowed with the subspace topology induced on it by 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }\to x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }\to x}
   in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   In this way, the question of whether or not the net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   converges to the given point 
  
    
      
        x
      
    
    {\displaystyle x}
   depends solely on this topological subspace 
  
    
      
        S
      
    
    {\displaystyle S}
   consisting of 
  
    
      
        x
      
    
    {\displaystyle x}
   and the image of (that is, the points of) the net 
  
    
      
        
          x
          
            ∙
          
        
        .
      
    
    {\displaystyle x_{\bullet }.}
  


=== Limits in a Cartesian product ===
A net in the product space has a limit if and only if each projection has a limit.
Explicitly, let 
  
    
      
        
          
            (
            
              X
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle \left(X_{i}\right)_{i\in I}}
   be topological spaces, endow their Cartesian product 

with the product topology, and that for every index 
  
    
      
        l
        ∈
        I
        ,
      
    
    {\displaystyle l\in I,}
   denote the canonical projection to 
  
    
      
        
          X
          
            l
          
        
      
    
    {\displaystyle X_{l}}
   by

Let 
  
    
      
        
          f
          
            ∙
          
        
        =
        
          
            (
            
              f
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle f_{\bullet }=\left(f_{a}\right)_{a\in A}}
   be a net in 
  
    
      
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
      
    
    {\displaystyle {\textstyle \prod }X_{\bullet }}
   directed by 
  
    
      
        A
      
    
    {\displaystyle A}
   and for every index 
  
    
      
        i
        ∈
        I
        ,
      
    
    {\displaystyle i\in I,}
   let 

denote the result of ""plugging 
  
    
      
        
          f
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }}
   into 
  
    
      
        
          π
          
            i
          
        
      
    
    {\displaystyle \pi _{i}}
  "", which results in the net 
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
        :
        A
        →
        
          X
          
            i
          
        
        .
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right):A\to X_{i}.}
   
It is sometimes useful to think of this definition in terms of function composition: the net 
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right)}
   is equal to the composition of the net 
  
    
      
        
          f
          
            ∙
          
        
        :
        A
        →
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }:A\to {\textstyle \prod }X_{\bullet }}
   with the projection 
  
    
      
        
          π
          
            i
          
        
        :
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
        →
        
          X
          
            i
          
        
        ;
      
    
    {\displaystyle \pi _{i}:{\textstyle \prod }X_{\bullet }\to X_{i};}
   that is, 
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
         
        
          
            
              
                =
              
              
                
                  
                    def
                  
                
              
            
          
        
         
        
          π
          
            i
          
        
        
        ∘
        
        
          f
          
            ∙
          
        
        .
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right)~{\stackrel {\scriptscriptstyle {\text{def}}}{=}}~\pi _{i}\,\circ \,f_{\bullet }.}
  
For any given point 
  
    
      
        L
        =
        
          
            (
            
              L
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
        ∈
        
          
            
              ∏
              
                i
                ∈
                I
              
            
          
        
        
          X
          
            i
          
        
        ,
      
    
    {\displaystyle L=\left(L_{i}\right)_{i\in I}\in {\textstyle \prod \limits _{i\in I}}X_{i},}
   the net 
  
    
      
        
          f
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }}
   converges to 
  
    
      
        L
      
    
    {\displaystyle L}
   in the product space 
  
    
      
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
      
    
    {\displaystyle {\textstyle \prod }X_{\bullet }}
   if and only if for every index 
  
    
      
        i
        ∈
        I
        ,
      
    
    {\displaystyle i\in I,}
   
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
        
        
          
            
              
                =
              
              
                
                  
                    def
                  
                
              
            
          
        
        
        
          
            (
            
              
                π
                
                  i
                
              
              
                (
                
                  f
                  
                    a
                  
                
                )
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right)\;{\stackrel {\scriptscriptstyle {\text{def}}}{=}}\;\left(\pi _{i}\left(f_{a}\right)\right)_{a\in A}}
   converges to 
  
    
      
        
          L
          
            i
          
        
      
    
    {\displaystyle L_{i}}
   in 
  
    
      
        
          X
          
            i
          
        
        .
      
    
    {\displaystyle X_{i}.}
   
And whenever the net 
  
    
      
        
          f
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }}
   clusters at 
  
    
      
        L
      
    
    {\displaystyle L}
   in 
  
    
      
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
      
    
    {\displaystyle {\textstyle \prod }X_{\bullet }}
   then 
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right)}
   clusters at 
  
    
      
        
          L
          
            i
          
        
      
    
    {\displaystyle L_{i}}
   for every index 
  
    
      
        i
        ∈
        I
        .
      
    
    {\displaystyle i\in I.}
   However, the converse does not hold in general. For example, suppose 
  
    
      
        
          X
          
            1
          
        
        =
        
          X
          
            2
          
        
        =
        
          R
        
      
    
    {\displaystyle X_{1}=X_{2}=\mathbb {R} }
   and let 
  
    
      
        
          f
          
            ∙
          
        
        =
        
          
            (
            
              f
              
                a
              
            
            )
          
          
            a
            ∈
            
              N
            
          
        
      
    
    {\displaystyle f_{\bullet }=\left(f_{a}\right)_{a\in \mathbb {N} }}
   denote the sequence 
  
    
      
        (
        1
        ,
        1
        )
        ,
        (
        0
        ,
        0
        )
        ,
        (
        1
        ,
        1
        )
        ,
        (
        0
        ,
        0
        )
        ,
        …
      
    
    {\displaystyle (1,1),(0,0),(1,1),(0,0),\ldots }
   that alternates between 
  
    
      
        (
        1
        ,
        1
        )
      
    
    {\displaystyle (1,1)}
   and 
  
    
      
        (
        0
        ,
        0
        )
        .
      
    
    {\displaystyle (0,0).}
   Then 
  
    
      
        
          L
          
            1
          
        
        :=
        0
      
    
    {\displaystyle L_{1}:=0}
   and 
  
    
      
        
          L
          
            2
          
        
        :=
        1
      
    
    {\displaystyle L_{2}:=1}
   are cluster points of both 
  
    
      
        
          π
          
            1
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
      
    
    {\displaystyle \pi _{1}\left(f_{\bullet }\right)}
   and 
  
    
      
        
          π
          
            2
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
      
    
    {\displaystyle \pi _{2}\left(f_{\bullet }\right)}
   in 
  
    
      
        
          X
          
            1
          
        
        ×
        
          X
          
            2
          
        
        =
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle X_{1}\times X_{2}=\mathbb {R} ^{2}}
   but 
  
    
      
        
          (
          
            
              L
              
                1
              
            
            ,
            
              L
              
                2
              
            
          
          )
        
        =
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \left(L_{1},L_{2}\right)=(0,1)}
   is not a cluster point of 
  
    
      
        
          f
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }}
   since the open ball of radius 
  
    
      
        1
      
    
    {\displaystyle 1}
   centered at 
  
    
      
        (
        0
        ,
        1
        )
      
    
    {\displaystyle (0,1)}
   does not contain even a single point 
  
    
      
        
          f
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }}
   
Tychonoff's theorem and relation to the axiom of choice
If no 
  
    
      
        L
        ∈
        X
      
    
    {\displaystyle L\in X}
   is given but for every 
  
    
      
        i
        ∈
        I
        ,
      
    
    {\displaystyle i\in I,}
   there exists some 
  
    
      
        
          L
          
            i
          
        
        ∈
        
          X
          
            i
          
        
      
    
    {\displaystyle L_{i}\in X_{i}}
   such that 
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
        →
        
          L
          
            i
          
        
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right)\to L_{i}}
   in 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   then the tuple defined by 
  
    
      
        L
        =
        
          
            (
            
              L
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle L=\left(L_{i}\right)_{i\in I}}
   will be a limit of 
  
    
      
        
          f
          
            ∙
          
        
      
    
    {\displaystyle f_{\bullet }}
   in 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   
However, the axiom of choice might be need to be assumed in order to conclude that this tuple 
  
    
      
        L
      
    
    {\displaystyle L}
   exists; the axiom of choice is not needed in some situations, such as when 
  
    
      
        I
      
    
    {\displaystyle I}
   is finite or when every 
  
    
      
        
          L
          
            i
          
        
        ∈
        
          X
          
            i
          
        
      
    
    {\displaystyle L_{i}\in X_{i}}
   is the unique limit of the net 
  
    
      
        
          π
          
            i
          
        
        
          (
          
            f
            
              ∙
            
          
          )
        
      
    
    {\displaystyle \pi _{i}\left(f_{\bullet }\right)}
   (because then there is nothing to choose between), which happens for example, when every 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   is a Hausdorff space. If 
  
    
      
        I
      
    
    {\displaystyle I}
   is infinite and 
  
    
      
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
        =
        
          
            
              ∏
              
                j
                ∈
                I
              
            
          
        
        
          X
          
            j
          
        
      
    
    {\displaystyle {\textstyle \prod }X_{\bullet }={\textstyle \prod \limits _{j\in I}}X_{j}}
   is not empty, then the axiom of choice would (in general) still be needed to conclude that the projections 
  
    
      
        
          π
          
            i
          
        
        :
        
          
            ∏
          
        
        
          X
          
            ∙
          
        
        →
        
          X
          
            i
          
        
      
    
    {\displaystyle \pi _{i}:{\textstyle \prod }X_{\bullet }\to X_{i}}
   are surjective maps.
The axiom of choice is equivalent to Tychonoff's theorem, which states that the product of any collection of compact topological spaces is compact. 
But if every compact space is also Hausdorff, then the so called ""Tychonoff's theorem for compact Hausdorff spaces"" can be used instead, which is equivalent to the ultrafilter lemma and so strictly weaker than the axiom of choice. 
Nets can be used to give short proofs of both version of Tychonoff's theorem by using the characterization of net convergence given above together with the fact that a space is compact if and only if every net has a convergent subnet.


=== Cluster points of a net ===
A point 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   is a cluster point of a given net if and only if it has a subset that converges to 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
  
If 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   is a net in 
  
    
      
        X
      
    
    {\displaystyle X}
   then the set of all cluster points of 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   is equal to
where 
  
    
      
        
          x
          
            ≥
            a
          
        
        :=
        
          {
          
            
              x
              
                b
              
            
            :
            b
            ≥
            a
            ,
            b
            ∈
            A
          
          }
        
      
    
    {\displaystyle x_{\geq a}:=\left\{x_{b}:b\geq a,b\in A\right\}}
   for each 
  
    
      
        a
        ∈
        A
        .
      
    
    {\displaystyle a\in A.}
   
If 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   is a cluster point of some subnet of 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   then 
  
    
      
        x
      
    
    {\displaystyle x}
   is also a cluster point of 
  
    
      
        
          x
          
            ∙
          
        
        .
      
    
    {\displaystyle x_{\bullet }.}
  


=== Ultranets ===
A net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   in set 
  
    
      
        X
      
    
    {\displaystyle X}
   is called a universal net or an ultranet if for every subset 
  
    
      
        S
        ⊆
        X
        ,
      
    
    {\displaystyle S\subseteq X,}
   
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   is eventually in 
  
    
      
        S
      
    
    {\displaystyle S}
   or 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   is eventually in the complement 
  
    
      
        X
        ∖
        S
        .
      
    
    {\displaystyle X\setminus S.}
  Ultranets are closely related to ultrafilters. 
Every constant net is an ultranet. Every subnet of an ultranet is an ultranet. Every net has some subnet that is an ultranet. 
If 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   is an ultranet in 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   is a function then 
  
    
      
        f
        ∘
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              f
              
                (
                
                  x
                  
                    a
                  
                
                )
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle f\circ x_{\bullet }=\left(f\left(x_{a}\right)\right)_{a\in A}}
   is an ultranet in 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
  Given 
  
    
      
        x
        ∈
        X
        ,
      
    
    {\displaystyle x\in X,}
   an ultranet clusters at 
  
    
      
        x
      
    
    {\displaystyle x}
   if and only it converges to 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
  


=== Examples of limits of nets ===
Every limit of a sequence and limit of a function can be interpreted as a limit of a net (as described below). 
The definition of the value of a Riemann integral can be interpreted as a limit of a net of Riemann sums where the net's directed set is the set of all partitions of the interval of integration, partially ordered by inclusion.
Interpret the set 
  
    
      
        
          
            R
          
          
            
              R
            
          
        
      
    
    {\displaystyle \mathbb {R} ^{\mathbb {R} }}
   of all functions with prototype 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} }
   as the Cartesian product 
  
    
      
        
          
            
              ∏
              
                x
                ∈
                
                  R
                
              
            
          
        
        
          R
        
      
    
    {\displaystyle {\textstyle \prod \limits _{x\in \mathbb {R} }}\mathbb {R} }
   (by identifying a function 
  
    
      
        f
      
    
    {\displaystyle f}
   with the tuple 
  
    
      
        (
        f
        (
        x
        )
        
          )
          
            x
            ∈
            
              R
            
          
        
        ,
      
    
    {\displaystyle (f(x))_{x\in \mathbb {R} },}
   and conversely) and endow it with the product topology. This (product) topology on 
  
    
      
        
          
            R
          
          
            
              R
            
          
        
      
    
    {\displaystyle \mathbb {R} ^{\mathbb {R} }}
   is identical to the topology of pointwise convergence. Let 
  
    
      
        E
      
    
    {\displaystyle E}
   denote the set of all functions 
  
    
      
        f
        :
        
          R
        
        →
        {
        0
        ,
        1
        }
      
    
    {\displaystyle f:\mathbb {R} \to \{0,1\}}
   that are equal to 
  
    
      
        1
      
    
    {\displaystyle 1}
   everywhere except for at most finitely many points (that is, such that the set 
  
    
      
        {
        x
        :
        f
        (
        x
        )
        =
        0
        }
      
    
    {\displaystyle \{x:f(x)=0\}}
   is finite). Then the constant 
  
    
      
        0
      
    
    {\displaystyle 0}
   function 
  
    
      
        
          0
        
        :
        
          R
        
        →
        {
        0
        }
      
    
    {\displaystyle \mathbf {0} :\mathbb {R} \to \{0\}}
   belongs to the closure of 
  
    
      
        E
      
    
    {\displaystyle E}
   in 
  
    
      
        
          
            R
          
          
            
              R
            
          
        
        ;
      
    
    {\displaystyle \mathbb {R} ^{\mathbb {R} };}
   that is, 
  
    
      
        
          0
        
        ∈
        
          cl
          
            
              
                R
              
              
                
                  R
                
              
            
          
        
        ⁡
        E
        .
      
    
    {\displaystyle \mathbf {0} \in \operatorname {cl} _{\mathbb {R} ^{\mathbb {R} }}E.}
   This will be proven by constructing a net in 
  
    
      
        E
      
    
    {\displaystyle E}
   that converges to 
  
    
      
        
          0
        
        .
      
    
    {\displaystyle \mathbf {0} .}
   However, there does not exist any sequence in 
  
    
      
        E
      
    
    {\displaystyle E}
   that converges to 
  
    
      
        
          0
        
        ,
      
    
    {\displaystyle \mathbf {0} ,}
   which makes this one instance where (non-sequence) nets must be used because sequences alone can not reach the desired conclusion. Compare elements of 
  
    
      
        
          
            R
          
          
            
              R
            
          
        
      
    
    {\displaystyle \mathbb {R} ^{\mathbb {R} }}
   pointwise in the usual way by declaring that 
  
    
      
        f
        ≥
        g
      
    
    {\displaystyle f\geq g}
   if and only if 
  
    
      
        f
        (
        x
        )
        ≥
        g
        (
        x
        )
      
    
    {\displaystyle f(x)\geq g(x)}
   for all 
  
    
      
        x
        .
      
    
    {\displaystyle x.}
   This pointwise comparison is a partial order that makes 
  
    
      
        (
        E
        ,
        ≥
        )
      
    
    {\displaystyle (E,\geq )}
   a directed set since given any 
  
    
      
        f
        ,
        g
        ∈
        E
        ,
      
    
    {\displaystyle f,g\in E,}
   their pointwise minimum 
  
    
      
        m
        :=
        min
        {
        f
        ,
        g
        }
      
    
    {\displaystyle m:=\min\{f,g\}}
   belongs to 
  
    
      
        E
      
    
    {\displaystyle E}
   and satisfies 
  
    
      
        f
        ≥
        m
      
    
    {\displaystyle f\geq m}
   and 
  
    
      
        g
        ≥
        m
        .
      
    
    {\displaystyle g\geq m.}
   This partial order turns the identity map 
  
    
      
        Id
        :
        (
        E
        ,
        ≥
        )
        →
        E
      
    
    {\displaystyle \operatorname {Id} :(E,\geq )\to E}
   (defined by 
  
    
      
        f
        ↦
        f
      
    
    {\displaystyle f\mapsto f}
  ) into an 
  
    
      
        E
      
    
    {\displaystyle E}
  -valued net. This net converges pointwise to 
  
    
      
        
          0
        
      
    
    {\displaystyle \mathbf {0} }
   in 
  
    
      
        
          
            R
          
          
            
              R
            
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{\mathbb {R} },}
   which implies that 
  
    
      
        
          0
        
      
    
    {\displaystyle \mathbf {0} }
   belongs to the closure of 
  
    
      
        E
      
    
    {\displaystyle E}
   in 
  
    
      
        
          
            R
          
          
            
              R
            
          
        
        .
      
    
    {\displaystyle \mathbb {R} ^{\mathbb {R} }.}
  


== Examples ==


=== Sequence in a topological space ===
A sequence 
  
    
      
        
          a
          
            1
          
        
        ,
        
          a
          
            2
          
        
        ,
        …
      
    
    {\displaystyle a_{1},a_{2},\ldots }
   in a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   can be considered a net in 
  
    
      
        X
      
    
    {\displaystyle X}
   defined on 
  
    
      
        
          N
        
        .
      
    
    {\displaystyle \mathbb {N} .}
  
The net is eventually in a subset 
  
    
      
        S
      
    
    {\displaystyle S}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   if there exists an 
  
    
      
        N
        ∈
        
          N
        
      
    
    {\displaystyle N\in \mathbb {N} }
   such that for every integer 
  
    
      
        n
        ≥
        N
        ,
      
    
    {\displaystyle n\geq N,}
   the point 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   is in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
  
So 
  
    
      
        lim
        
          

          
          
            n
          
        
        
          a
          
            n
          
        
        →
        L
      
    
    {\displaystyle \lim {}_{n}a_{n}\to L}
   if and only if for every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        L
        ,
      
    
    {\displaystyle L,}
   the net is eventually in 
  
    
      
        V
        .
      
    
    {\displaystyle V.}
  
The net is frequently in a subset 
  
    
      
        S
      
    
    {\displaystyle S}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if for every 
  
    
      
        N
        ∈
        
          N
        
      
    
    {\displaystyle N\in \mathbb {N} }
   there exists some integer 
  
    
      
        n
        ≥
        N
      
    
    {\displaystyle n\geq N}
   such that 
  
    
      
        
          a
          
            n
          
        
        ∈
        S
        ,
      
    
    {\displaystyle a_{n}\in S,}
   that is, if and only if infinitely many elements of the sequence are in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   Thus a point 
  
    
      
        y
        ∈
        X
      
    
    {\displaystyle y\in X}
   is a cluster point of the net if and only if every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        y
      
    
    {\displaystyle y}
   contains infinitely many elements of the sequence.


=== Function from a metric space to a topological space ===
Fix a point 
  
    
      
        c
        ∈
        M
      
    
    {\displaystyle c\in M}
   in a metric space 
  
    
      
        (
        M
        ,
        d
        )
      
    
    {\displaystyle (M,d)}
   that has at least two point (such as 
  
    
      
        M
        :=
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle M:=\mathbb {R} ^{n}}
   with the Euclidean metric with 
  
    
      
        c
        :=
        0
      
    
    {\displaystyle c:=0}
   being the origin, for example) and direct the set 
  
    
      
        I
        :=
        M
        ∖
        {
        c
        }
      
    
    {\displaystyle I:=M\setminus \{c\}}
   reversely according to distance from 
  
    
      
        c
      
    
    {\displaystyle c}
   by declaring that 
  
    
      
        i
        ≤
        j
      
    
    {\displaystyle i\leq j}
   if and only if 
  
    
      
        d
        (
        j
        ,
        c
        )
        ≤
        d
        (
        i
        ,
        c
        )
        .
      
    
    {\displaystyle d(j,c)\leq d(i,c).}
   In other words, the relation is ""has at least the same distance to 
  
    
      
        c
      
    
    {\displaystyle c}
   as"", so that ""large enough"" with respect to this relation means ""close enough to 
  
    
      
        c
      
    
    {\displaystyle c}
  "". 
Given any function with domain 
  
    
      
        M
        ,
      
    
    {\displaystyle M,}
   its restriction to 
  
    
      
        I
        :=
        M
        ∖
        {
        c
        }
      
    
    {\displaystyle I:=M\setminus \{c\}}
   can be canonically interpreted as a net directed by 
  
    
      
        (
        I
        ,
        ≤
        )
        .
      
    
    {\displaystyle (I,\leq ).}
  A net 
  
    
      
        f
        :
        M
        ∖
        {
        c
        }
        →
        X
      
    
    {\displaystyle f:M\setminus \{c\}\to X}
   is eventually in a subset 
  
    
      
        S
      
    
    {\displaystyle S}
   of a topological space 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if there exists some 
  
    
      
        n
        ∈
        M
        ∖
        {
        c
        }
      
    
    {\displaystyle n\in M\setminus \{c\}}
   such that for every 
  
    
      
        m
        ∈
        M
        ∖
        {
        c
        }
      
    
    {\displaystyle m\in M\setminus \{c\}}
   satisfying 
  
    
      
        d
        (
        m
        ,
        c
        )
        ≤
        d
        (
        n
        ,
        c
        )
        ,
      
    
    {\displaystyle d(m,c)\leq d(n,c),}
   the point 
  
    
      
        f
        (
        m
        )
      
    
    {\displaystyle f(m)}
   is in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   
Such a net 
  
    
      
        f
      
    
    {\displaystyle f}
   converges in 
  
    
      
        X
      
    
    {\displaystyle X}
   to a given point 
  
    
      
        L
        ∈
        X
      
    
    {\displaystyle L\in X}
   if and only if 
  
    
      
        
          lim
          
            m
            →
            c
          
        
        f
        (
        m
        )
        →
        L
      
    
    {\displaystyle \lim _{m\to c}f(m)\to L}
   in the usual sense (meaning that for every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        L
        ,
      
    
    {\displaystyle L,}
   
  
    
      
        f
      
    
    {\displaystyle f}
   is eventually in 
  
    
      
        V
      
    
    {\displaystyle V}
  ).The net 
  
    
      
        f
        :
        M
        ∖
        {
        c
        }
        →
        X
      
    
    {\displaystyle f:M\setminus \{c\}\to X}
   is frequently in a subset 
  
    
      
        S
      
    
    {\displaystyle S}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if for every 
  
    
      
        n
        ∈
        M
        ∖
        {
        c
        }
      
    
    {\displaystyle n\in M\setminus \{c\}}
   there exists some 
  
    
      
        m
        ∈
        M
        ∖
        {
        c
        }
      
    
    {\displaystyle m\in M\setminus \{c\}}
   with 
  
    
      
        d
        (
        m
        ,
        c
        )
        ≤
        d
        (
        n
        ,
        c
        )
      
    
    {\displaystyle d(m,c)\leq d(n,c)}
   such that 
  
    
      
        f
        (
        m
        )
      
    
    {\displaystyle f(m)}
   is in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
  
Consequently, a point 
  
    
      
        L
        ∈
        X
      
    
    {\displaystyle L\in X}
   is a cluster point of the net 
  
    
      
        f
      
    
    {\displaystyle f}
   if and only if for every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        L
        ,
      
    
    {\displaystyle L,}
   the net is frequently in 
  
    
      
        V
        .
      
    
    {\displaystyle V.}
  


=== Function from a well-ordered set to a topological space ===
Consider a well-ordered set 
  
    
      
        [
        0
        ,
        c
        ]
      
    
    {\displaystyle [0,c]}
   with limit point 
  
    
      
        t
      
    
    {\displaystyle t}
   and a function 
  
    
      
        f
      
    
    {\displaystyle f}
   from 
  
    
      
        [
        0
        ,
        t
        )
      
    
    {\displaystyle [0,t)}
   to a topological space 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   This function is a net on 
  
    
      
        [
        0
        ,
        t
        )
        .
      
    
    {\displaystyle [0,t).}
  
It is eventually in a subset 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   if there exists an 
  
    
      
        r
        ∈
        [
        0
        ,
        t
        )
      
    
    {\displaystyle r\in [0,t)}
   such that for every 
  
    
      
        s
        ∈
        [
        r
        ,
        t
        )
      
    
    {\displaystyle s\in [r,t)}
   the point 
  
    
      
        f
        (
        s
        )
      
    
    {\displaystyle f(s)}
   is in 
  
    
      
        V
        .
      
    
    {\displaystyle V.}
  
So 
  
    
      
        
          lim
          
            x
            →
            t
          
        
        f
        (
        x
        )
        →
        L
      
    
    {\displaystyle \lim _{x\to t}f(x)\to L}
   if and only if for every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        L
        ,
      
    
    {\displaystyle L,}
   
  
    
      
        f
      
    
    {\displaystyle f}
   is eventually in 
  
    
      
        V
        .
      
    
    {\displaystyle V.}
  
The net 
  
    
      
        f
      
    
    {\displaystyle f}
   is frequently in a subset 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if for every 
  
    
      
        r
        ∈
        [
        0
        ,
        t
        )
      
    
    {\displaystyle r\in [0,t)}
   there exists some 
  
    
      
        s
        ∈
        [
        r
        ,
        t
        )
      
    
    {\displaystyle s\in [r,t)}
   such that 
  
    
      
        f
        (
        s
        )
        ∈
        V
        .
      
    
    {\displaystyle f(s)\in V.}
  
A point 
  
    
      
        y
        ∈
        X
      
    
    {\displaystyle y\in X}
   is a cluster point of the net 
  
    
      
        f
      
    
    {\displaystyle f}
   if and only if for every neighborhood 
  
    
      
        V
      
    
    {\displaystyle V}
   of 
  
    
      
        y
        ,
      
    
    {\displaystyle y,}
   the net is frequently in 
  
    
      
        V
        .
      
    
    {\displaystyle V.}
  
The first example is a special case of this with 
  
    
      
        c
        =
        ω
        .
      
    
    {\displaystyle c=\omega .}
  
See also ordinal-indexed sequence.


=== Subnets ===

The analogue of ""subsequence"" for nets is the notion of a ""subnet"". There are several different non-equivalent definitions of ""subnet"" and this article will use the definition introduced in 1970 by Stephen Willard, which is as follows: 
If 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   and 
  
    
      
        
          s
          
            ∙
          
        
        =
        
          
            (
            
              s
              
                i
              
            
            )
          
          
            i
            ∈
            I
          
        
      
    
    {\displaystyle s_{\bullet }=\left(s_{i}\right)_{i\in I}}
   are nets then 
  
    
      
        
          s
          
            ∙
          
        
      
    
    {\displaystyle s_{\bullet }}
   is called a subnet or Willard-subnet of 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   if there exists an order-preserving map 
  
    
      
        h
        :
        I
        →
        A
      
    
    {\displaystyle h:I\to A}
   such that 
  
    
      
        h
        (
        I
        )
      
    
    {\displaystyle h(I)}
   is a cofinal subset of 
  
    
      
        A
      
    
    {\displaystyle A}
   and 
 
The map 
  
    
      
        h
        :
        I
        →
        A
      
    
    {\displaystyle h:I\to A}
   is called order-preserving and an order homomorphism if whenever 
  
    
      
        i
        ≤
        j
      
    
    {\displaystyle i\leq j}
   then 
  
    
      
        h
        (
        i
        )
        ≤
        h
        (
        j
        )
        .
      
    
    {\displaystyle h(i)\leq h(j).}
   
The set 
  
    
      
        h
        (
        I
        )
      
    
    {\displaystyle h(I)}
   being cofinal in 
  
    
      
        A
      
    
    {\displaystyle A}
   means that for every 
  
    
      
        a
        ∈
        A
        ,
      
    
    {\displaystyle a\in A,}
   there exists some 
  
    
      
        b
        ∈
        h
        (
        I
        )
      
    
    {\displaystyle b\in h(I)}
   such that 
  
    
      
        b
        ≥
        a
        .
      
    
    {\displaystyle b\geq a.}
  


== Properties ==
Virtually all concepts of topology can be rephrased in the language of nets and limits. This may be useful to guide the intuition since the notion of limit of a net is very similar to that of limit of a sequence. The following set of theorems and lemmas help cement that similarity:


=== Characterizations of topological properties ===
Closed sets and closure
A subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   is closed in 
  
    
      
        X
      
    
    {\displaystyle X}
   if and only if every limit point of every convergent net in 
  
    
      
        S
      
    
    {\displaystyle S}
   necessarily belongs to 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   
Explicitly, a subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   is closed if and only if whenever 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   and 
  
    
      
        
          s
          
            ∙
          
        
        =
        
          
            (
            
              s
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle s_{\bullet }=\left(s_{a}\right)_{a\in A}}
   is a net valued in 
  
    
      
        S
      
    
    {\displaystyle S}
   (meaning that 
  
    
      
        
          s
          
            a
          
        
        ∈
        S
      
    
    {\displaystyle s_{a}\in S}
   for all 
  
    
      
        a
        ∈
        A
      
    
    {\displaystyle a\in A}
  ) such that 
  
    
      
        lim
        
          

          
          

          
        
        
          s
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle \lim {}_{}s_{\bullet }\to x}
   in 
  
    
      
        X
        ,
      
    
    {\displaystyle X,}
   then necessarily 
  
    
      
        x
        ∈
        S
        .
      
    
    {\displaystyle x\in S.}
  
More generally, if 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   is any subset then a point 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   is in the closure of 
  
    
      
        S
      
    
    {\displaystyle S}
   if and only if there exists a net 
  
    
      
        
          
            (
            
              s
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle \left(s_{a}\right)_{a\in A}}
   in 
  
    
      
        S
      
    
    {\displaystyle S}
   with limit 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
   and such that 
  
    
      
        
          s
          
            a
          
        
        ∈
        S
      
    
    {\displaystyle s_{a}\in S}
   for every index 
  
    
      
        a
        ∈
        A
        .
      
    
    {\displaystyle a\in A.}
  Open sets and characterizations of topologies

A subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   is open if and only if no net in 
  
    
      
        X
        ∖
        S
      
    
    {\displaystyle X\setminus S}
   converges to a point of 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   Also, subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   is open if and only if every net converging to an element of 
  
    
      
        S
      
    
    {\displaystyle S}
   is eventually contained in 
  
    
      
        S
        .
      
    
    {\displaystyle S.}
   
It is these characterizations of ""open subset"" that allow nets to characterize topologies. 
Topologies can also be characterized by closed subsets since a set is open if and only if its complement is closed. So the characterizations of ""closed set"" in terms of nets can also be used to characterize topologies. 
Continuity
A function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   between topological spaces is continuous at a given point 
  
    
      
        x
      
    
    {\displaystyle x}
   if and only if for every net 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   in its domain, if 
  
    
      
        
          lim
          

          
        
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle \lim _{}x_{\bullet }\to x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   then 
  
    
      
        lim
        

        
        f
        
          (
          
            x
            
              ∙
            
          
          )
        
        →
        f
        (
        x
        )
      
    
    {\displaystyle \lim {}f\left(x_{\bullet }\right)\to f(x)}
   in 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
   
Said more succinctly, a function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   is continuous if and only if whenever 
  
    
      
        
          x
          
            ∙
          
        
        →
        x
      
    
    {\displaystyle x_{\bullet }\to x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   then 
  
    
      
        f
        
          (
          
            x
            
              ∙
            
          
          )
        
        →
        f
        (
        x
        )
      
    
    {\displaystyle f\left(x_{\bullet }\right)\to f(x)}
   in 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
   
In general, this the statement would not be true if the word ""net"" was replaced by ""sequence""; that is, it is necessary to allow for directed sets other than just the natural numbers if 
  
    
      
        X
      
    
    {\displaystyle X}
   is not a first-countable space (or not a sequential space).

Compactness
A space 
  
    
      
        X
      
    
    {\displaystyle X}
   is compact if and only if every net 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   has a subnet with a limit in 
  
    
      
        X
        .
      
    
    {\displaystyle X.}
   This can be seen as a generalization of the Bolzano–Weierstrass theorem and Heine–Borel theorem.


=== Cluster and limit points ===
The set of cluster points of a net is equal to the set of limits of its convergent subnets.

A net has a limit if and only if all of its subnets have limits. In that case, every limit of the net is also a limit of every subnet.


=== Other properties ===
In general, a net in a space 
  
    
      
        X
      
    
    {\displaystyle X}
   can have more than one limit, but if 
  
    
      
        X
      
    
    {\displaystyle X}
   is a Hausdorff space, the limit of a net, if it exists, is unique. Conversely, if 
  
    
      
        X
      
    
    {\displaystyle X}
   is not Hausdorff, then there exists a net on 
  
    
      
        X
      
    
    {\displaystyle X}
   with two distinct limits.  Thus the uniqueness of the limit is equivalent to the Hausdorff condition on the space, and indeed this may be taken as the definition.  This result depends on the directedness condition; a set indexed by a general preorder or partial order may have distinct limit points even in a Hausdorff space.


== Cauchy nets ==
A Cauchy net generalizes the notion of Cauchy sequence to nets defined on uniform spaces.A net 
  
    
      
        
          x
          
            ∙
          
        
        =
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle x_{\bullet }=\left(x_{a}\right)_{a\in A}}
   is a Cauchy net if for every entourage 
  
    
      
        V
      
    
    {\displaystyle V}
   there exists 
  
    
      
        c
        ∈
        A
      
    
    {\displaystyle c\in A}
   such that for all 
  
    
      
        a
        ,
        b
        ≥
        c
        ,
      
    
    {\displaystyle a,b\geq c,}
   
  
    
      
        
          (
          
            
              x
              
                a
              
            
            ,
            
              x
              
                b
              
            
          
          )
        
      
    
    {\displaystyle \left(x_{a},x_{b}\right)}
   is a member of 
  
    
      
        V
        .
      
    
    {\displaystyle V.}
   More generally, in a Cauchy space, a net 
  
    
      
        
          x
          
            ∙
          
        
      
    
    {\displaystyle x_{\bullet }}
   is Cauchy if the filter generated by the net is a Cauchy filter.
A topological vector space (TVS) is called complete if every Cauchy net converges to some point. A normed space, which is a special type of topological vector space, is a complete TVS (equivalently, a Banach space) if and only if every Cauchy sequence converges to some point (a property that is called sequential completeness). Although Cauchy nets are not needed to describe completeness of normed spaces, they are needed to describe completeness of more general (possibly non-normable) topological vector spaces.


== Relation to filters ==

A filter is another idea in topology that allows for a general definition for convergence in general topological spaces. The two ideas are equivalent in the sense that they give the same concept of convergence. More specifically, for every filter base an associated net can be constructed, and convergence of the filter base implies convergence of the associated net—and the other way around (for every net there is a filter base, and convergence of the net implies convergence of the filter base). For instance, any net 
  
    
      
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
      
    
    {\displaystyle \left(x_{a}\right)_{a\in A}}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   induces a filter base of tails 
  
    
      
        
          {
          
            
              {
              
                
                  x
                  
                    a
                  
                
                :
                a
                ∈
                A
                ,
                
                  a
                  
                    0
                  
                
                ≤
                a
              
              }
            
            :
            
              a
              
                0
              
            
            ∈
            A
          
          }
        
      
    
    {\displaystyle \left\{\left\{x_{a}:a\in A,a_{0}\leq a\right\}:a_{0}\in A\right\}}
   where the filter in 
  
    
      
        X
      
    
    {\displaystyle X}
   generated by this filter base is called the net's eventuality filter. This correspondence allows for any theorem that can be proven with one concept to be proven with the other. For instance, continuity of a function from one topological space to the other can be characterized either by the convergence of a net in the domain implying the convergence of the corresponding net in the codomain, or by the same statement with filter bases.
Robert G. Bartle argues that despite their equivalence, it is useful to have both concepts. He argues that nets are enough like sequences to make natural proofs and definitions in analogy to sequences, especially ones using sequential elements, such as is common in analysis, while filters are most useful in algebraic topology. In any case, he shows how the two can be used in combination to prove various theorems in general topology.


== Limit superior ==
Limit superior and limit inferior of a net of real numbers can be defined in a similar manner as for sequences. Some authors work even with more general structures than the real line, like complete lattices.For a net 
  
    
      
        
          
            (
            
              x
              
                a
              
            
            )
          
          
            a
            ∈
            A
          
        
        ,
      
    
    {\displaystyle \left(x_{a}\right)_{a\in A},}
   put

Limit superior of a net of real numbers has many properties analogous to the case of sequences. For example,

where equality holds whenever one of the nets is convergent.


== See also ==
Characterizations of the category of topological spaces
Filter (set theory) – Family of sets representing ""large"" sets
Filters in topology – Use of filters to describe and characterize all basic topological notions and results.
Preorder – Reflexive and transitive binary relation
Sequential space – Topological space characterized by sequences
Ultrafilter (set theory) – Maximal proper filter


== Citations ==


== References ==
Sundström, Manya Raman (2010). ""A pedagogical history of compactness"". arXiv:1006.4131v1 [math.HO].
Aliprantis, Charalambos D.; Border, Kim C. (2006). Infinite dimensional analysis: A hitchhiker's guide (3rd ed.). Berlin: Springer. pp. xxii, 703. ISBN 978-3-540-32696-0. MR 2378491.
Beer, Gerald (1993). Topologies on closed and closed convex sets. Mathematics and its Applications 268. Dordrecht: Kluwer Academic Publishers Group. pp. xii, 340. ISBN 0-7923-2531-1. MR 1269778.
Howes, Norman R. (23 June 1995). Modern Analysis and Topology. Graduate Texts in Mathematics. New York: Springer-Verlag Science & Business Media. ISBN 978-0-387-97986-1. OCLC 31969970. OL 1272666M.
Kelley, John L. (1975). General Topology. Graduate Texts in Mathematics. Vol. 27. New York: Springer Science & Business Media. ISBN 978-0-387-90125-1. OCLC 338047.
Kelley, John L. (1991). General Topology. Springer. ISBN 3-540-90125-6.
Megginson, Robert E. (1998). An Introduction to Banach Space Theory. Graduate Texts in Mathematics. Vol. 193. New York: Springer. ISBN 0-387-98431-3.
Schechter, Eric (1997). Handbook of Analysis and Its Foundations. San Diego: Academic Press. ISBN 9780080532998. Retrieved 22 June 2013.
Schechter, Eric (1996). Handbook of Analysis and Its Foundations. San Diego, CA: Academic Press. ISBN 978-0-12-622760-4. OCLC 175294365.
Willard, Stephen (2004) [1970]. General Topology. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-43479-7. OCLC 115240."
e6da859347,The Unreasonable Effectiveness of Mathematics in the Natural Sciences,"""The Unreasonable Effectiveness of Mathematics in the Natural Sciences"" is a 1960 article by the physicist Eugene Wigner. In the paper, Wigner observes that a physical theory's mathematical structure often points the way to further advances in that theory and even to empirical predictions.


== Original paper and Wigner's observations ==
Wigner begins his paper with the belief, common among those familiar with mathematics, that mathematical concepts have applicability far beyond the context in which they were originally developed. Based on his experience, he writes, ""it is important to point out that the mathematical formulation of the physicist's often crude experience leads in an uncanny number of cases to an amazingly accurate description of a large class of phenomena"". He then invokes the fundamental law of gravitation as an example. Originally used to model freely falling bodies on the surface of the earth, this law was extended on the basis of what Wigner terms ""very scanty observations"" to describe the motion of the planets, where it ""has proved accurate beyond all reasonable expectations"".Another oft-cited example is Maxwell's equations, derived to model the elementary electrical and magnetic phenomena known as of the mid-19th century. The equations also describe radio waves, discovered by David Edward Hughes in 1879, around the time of James Clerk Maxwell's death. Wigner sums up his argument by saying that ""the enormous usefulness of mathematics in the natural sciences is something bordering on the mysterious and that there is no rational explanation for it"". He concludes his paper with the same question with which he began:

The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve. We should be grateful for it and hope that it will remain valid in future research and that it will extend, for better or for worse, to our pleasure, even though perhaps also to our bafflement, to wide branches of learning.
Wigner's work provided a fresh insight into both physics and the philosophy of mathematics, and has been fairly often cited in the academic literature on the philosophy of physics and of mathematics. Wigner speculated on the relationship between the philosophy of science and the foundations of mathematics as follows:

It is difficult to avoid the impression that a miracle confronts us here, quite comparable in its striking nature to the miracle that the human mind can string a thousand arguments together without getting itself into contradictions, or to the two miracles of laws of nature and of the human mind's capacity to divine them.
Later, Hilary Putnam (1975) explained the aforementioned two ""miracles"" as necessary consequences of a realist (but not Platonist) view of the philosophy of mathematics. But in a passage discussing cognitive bias Wigner cautiously called ""not reliable"", he went further:

The writer is convinced that it is useful, in epistemological discussions, to abandon the idealization that the level of human intelligence has a singular position on an absolute scale. In some cases it may even be useful to consider the attainment which is possible at the level of the intelligence of some other species.
Whether humans checking the results of humans can be considered an objective basis for observation of the known (to humans) universe is an interesting question, one followed up in both cosmology and the philosophy of mathematics.
Wigner also laid out the challenge of a cognitive approach to integrating the sciences:

A much more difficult and confusing situation would arise if we could, some day, establish a theory of the phenomena of consciousness, or of biology, which would be as coherent and convincing as our present theories of the inanimate world.
He further proposed that arguments could be found that might

put a heavy strain on our faith in our theories and on our belief in the reality of the concepts which we form. It would give us a deep sense of frustration in our search for what I called 'the ultimate truth'. The reason that such a situation is conceivable is that, fundamentally, we do not know why our theories work so well. Hence, their accuracy may not prove their truth and consistency. Indeed, it is this writer's belief that something rather akin to the situation which was described above exists if the present laws of heredity and of physics are confronted.


== Responses ==
Wigner's original paper has provoked and inspired many responses across a wide range of disciplines. These include Richard Hamming in computer science, Arthur Lesk in molecular biology,  Peter Norvig in data mining,  Max Tegmark in physics, Ivor Grattan-Guinness in mathematics and Vela Velupillai in economics.


=== Richard Hamming ===
Mathematician and Turing Award laureate Richard Hamming reflected on and extended Wigner's Unreasonable Effectiveness in 1980, mulling over four ""partial explanations"" for it. Hamming concluded that the four explanations he gave were unsatisfactory. They were:
1. Humans see what they look for. The belief that science is experimentally grounded is only partially true. Rather, our intellectual apparatus is such that much of what we see comes from the glasses we put on. Eddington went so far as to claim that a sufficiently wise mind could deduce all of physics, illustrating his point with the following joke: ""Some men went fishing in the sea with a net, and upon examining what they caught they concluded that there was a minimum size to the fish in the sea.""
Hamming gives four examples of nontrivial physical phenomena he believes arose from the mathematical tools employed and not from the intrinsic properties of physical reality.

Hamming proposes that Galileo discovered the law of falling bodies not by experimenting, but by simple, though careful, thinking. Hamming imagines Galileo as having engaged in the following thought experiment (the experiment, which Hamming calls ""scholastic reasoning"", is described in Galileo's book On Motion.):Suppose that a falling body broke into two pieces. Of course the two pieces would immediately slow down to their appropriate speeds. But suppose further that one piece happened to touch the other one. Would they now be one piece and both speed up? Suppose I tie the two pieces together. How tightly must I do it to make them one piece? A light string? A rope? Glue? When are two pieces one?
There is simply no way a falling body can ""answer"" such hypothetical ""questions."" Hence Galileo would have concluded that ""falling bodies need not know anything if they all fall with the same velocity, unless interfered with by another force."" After coming up with this argument, Hamming found a related discussion in Pólya (1963: 83-85). Hamming's account does not reveal an awareness of the 20th century scholarly debate over just what Galileo did.

The inverse square law of universal gravitation necessarily follows from the conservation of energy and of space having three dimensions. Measuring the exponent in the law of universal gravitation is more a test of whether space is Euclidean than a test of the properties of the gravitational field.
The inequality at the heart of the uncertainty principle of quantum mechanics follows from the properties of Fourier integrals and from assuming time invariance.
Hamming argues that Albert Einstein's pioneering work on special relativity was largely ""scholastic"" in its approach. He knew from the outset what the theory should look like (although he only knew this because of the Michelson–Morley experiment), and explored candidate theories with mathematical tools, not actual experiments. Hamming alleges that Einstein was so confident that his relativity theories were correct that the outcomes of observations designed to test them did not much interest him. If the observations were inconsistent with his theories, it would be the observations that were at fault.2. Humans create and select the mathematics that fit a situation. The mathematics at hand does not always work. For example, when mere scalars proved awkward for understanding forces, first vectors, then tensors, were invented.
3. Mathematics addresses only a part of human experience. Much of human experience does not fall under science or mathematics but under the philosophy of value, including ethics, aesthetics, and political philosophy. To assert that the world can be explained via mathematics amounts to an act of faith.
4. Evolution has primed humans to think mathematically. The earliest lifeforms must have contained the seeds of the human ability to create and follow long chains of close reasoning.


=== Max Tegmark ===
A different response, advocated by physicist Max Tegmark, is that physics is so successfully described by mathematics because the physical world is completely mathematical, isomorphic to a mathematical structure, and that we are simply uncovering this bit by bit. The same interpretation had been advanced some years previously by Peter Atkins. In this interpretation, the various approximations that constitute our current physics theories are successful because simple mathematical structures can provide good approximations of certain aspects of more complex mathematical structures. In other words, our successful theories are not mathematics approximating physics, but simple mathematics approximating more complex mathematics.


=== Ivor Grattan-Guinness ===
Ivor Grattan-Guinness found the effectiveness in question eminently reasonable and explicable in terms of concepts such as analogy, generalisation and metaphor.


=== Michael Atiyah ===
The tables were turned by Michael Atiyah with his essay ""The unreasonable effectiveness of physics in mathematics"". He argued that the toolbox of physicist enables a practitioner like Edward Witten to go beyond standard mathematics, in particular the geometry of 4-manifolds. The tools of a physicist are cited as quantum field theory, special relativity, non-abelian gauge theory, spin, chirality, supersymmetry , and the electromagnetic duality.


== See also ==


== References ==


== Further reading =="
60cd8423b2,International Mathematical Olympiad,"The International Mathematical Olympiad (IMO) is a mathematical olympiad for pre-university students, and is the oldest of the International Science Olympiads. It is one of the most prestigious mathematical competitions in the world. The first IMO was held in Romania in 1959. It has since been held annually, except in 1980. More than 100 countries, representing over 90% of the world's population. Each country send team of up to six students, plus one team leader, one deputy leader, and observers.The content ranges from extremely difficult algebra and pre-calculus problems to problems on branches of mathematics not conventionally covered in secondary or high school and often not at university level either, such as projective and complex geometry, functional equations, combinatorics, and well-grounded number theory, of which extensive knowledge of theorems is required. Calculus, though allowed in solutions, is never required, as there is a principle that anyone with a basic understanding of mathematics should understand the problems, even if the solutions require a great deal more knowledge. Supporters of this principle claim that this allows more universality and creates an incentive to find elegant, deceptively simple-looking problems which nevertheless require a certain level of ingenuity, often times a great deal of ingenuity to net all points for a given IMO problem.
The selection process differs by country, but it often consists of a series of tests which admit fewer students at each progressing test. Awards are given to approximately the top-scoring 50% of the individual contestants. Teams are not officially recognized—all scores are given only to individual contestants, but team scoring is unofficially compared more than individual scores. Contestants must be under the age of 20 and must not be registered at any tertiary institution. Subject to these conditions, an individual may participate any number of times in the IMO.


== History ==

The first IMO was held in Romania in 1959. Since then it has been held every year (except in 1980. That year, it was cancelled due to internal strife in Mongolia) It was initially founded for eastern European member countries of the Warsaw Pact, under the USSR bloc of influence, but later other countries participated as well. Because of this eastern origin, the IMOs were first hosted only in eastern European countries, and gradually spread to other nations.Sources differ about the cities hosting some of the early IMOs. This may be partly because leaders and students are generally housed at different locations, and partly because after the competition the students were sometimes based in multiple cities for the rest of the IMO. The exact dates cited may also differ, because of leaders arriving before the students, and at more recent IMOs the IMO Advisory Board arriving before the leaders.Several students, such as Lisa Sauermann, Reid W. Barton, Nicușor Dan and Ciprian Manolescu have performed exceptionally well in the IMO, winning multiple gold medals. Others, such as Terence Tao, Grigori Perelman, Ngô Bảo Châu and Maryam Mirzakhani have gone on to become notable mathematicians. Several former participants have won awards such as the Fields Medal.


== Scoring and format ==
The competition consists of 6 problems. The competition is held over two consecutive days with 3 problems each; each day the contestants have four-and-a-half hours to solve three problems. Each problem is worth 7 points for a maximum total score of 42 points. Calculators are not allowed.  The problems chosen are from various areas of secondary school mathematics, broadly classifiable as geometry, number theory, algebra, and combinatorics. They require no knowledge of higher mathematics such as calculus and analysis, and solutions are often elementary. However, they are usually disguised so as to make the solutions difficult. The problems given in the IMO are largely designed to require creativity and the ability to solve problems quickly. Thus, the prominently featured problems are algebraic inequalities, complex numbers, and construction-oriented geometrical problems, though in recent years, the latter has not been as popular as before because of the algorithmic use of theorems like Muirhead's Inequality, and Complex/Analytic Bash to solve problems.Each participating country, other than the host country, may submit suggested problems to a Problem Selection Committee provided by the host country, which reduces the submitted problems to a shortlist. The team leaders arrive at the IMO a few days in advance of the contestants and form the IMO Jury which is responsible for all the formal decisions relating to the contest, starting with selecting the six problems from the shortlist. The Jury aims to order the problems so that the order in increasing difficulty is Q1, Q4, Q2, Q5, Q3 and Q6, where the First day problems Q1, Q2, and Q3 are in increasing difficulty, and the Second day problems Q4, Q5, Q6 are in increasing difficulty. The Team Leaders of all countries are given the problems in advance of the contestants, and thus, are kept strictly separated and observed.Each country's marks are agreed between that country's leader and deputy leader and coordinators provided by the host country (the leader of the team whose country submitted the problem in the case of the marks of the host country), subject to the decisions of the chief coordinator and ultimately a jury if any disputes cannot be resolved.


== Selection process ==

The selection process for the IMO varies greatly by country. In some countries, especially those in East Asia, the selection process involves several tests of a difficulty comparable to the IMO itself. The Chinese contestants go through a camp. In others, such as the United States, possible participants go through a series of easier standalone competitions that gradually increase in difficulty. In the United States, the tests include the American Mathematics Competitions, the American Invitational Mathematics Examination, and the United States of America Mathematical Olympiad, each of which is a competition in its own right. For high scorers in the final competition for the team selection, there also is a summer camp, like that of China.In countries of the former Soviet Union and other eastern European countries, a team has in the past been chosen several years beforehand, and they are given special training specifically for the event. However, such methods have been discontinued in some countries.


== Awards ==
The participants are ranked based on their individual scores. Medals are awarded to the highest ranked participants; slightly fewer than half of them receive a medal. The cutoffs (minimum scores required to receive a gold, silver, or bronze medal respectively) are then chosen so that the numbers of gold, silver and bronze medals awarded are approximately in the ratios 1:2:3. Participants who do not win a medal but who score 7 points on at least one problem receive an honorable mention.Special prizes may be awarded for solutions of outstanding elegance or involving good generalisations of a problem. This last happened in 1995 (Nikolay Nikolov, Bulgaria) and 2005 (Iurie Boreico), but was more frequent up to the early 1980s. The special prize in 2005 was awarded to Iurie Boreico, a student from Moldova, for his solution to Problem 3, a three variable inequality.
The rule that at most half the contestants win a medal is sometimes broken if it would cause the total number of medals to deviate too much from half the number of contestants. This last happened in 2010 (when the choice was to give either 226 (43.71%) or 266 (51.45%) of the 517 contestants (excluding the 6 from North Korea — see below) a medal), 2012 (when the choice was to give either 226 (41.24%) or 277 (50.55%) of the 548 contestants a medal), and 2013, when the choice was to give either 249 (47.16%) or 278 (52.65%) of the 528 contestants a medal. In these cases, slightly more than half the contestants were awarded a medal.


== Penalties ==
North Korea was disqualified twice for cheating, once at the 32nd IMO in 1991 and again at the 51st IMO in 2010. It is the only country to have been accused of cheating.


== Summary ==


== Notable achievements ==

The following nations have achieved the highest team score in the respective competition:

China, 23 times: in 1989, 1990, 1992, 1993, 1995, 1997, 1999 (joint), 2000, 2001, 2002, 2004, 2005, 2006, 2008, 2009, 2010, 2011, 2013, 2014, 2019 (joint), 2020, 2021, 2022;
Russia (including Soviet Union), 16 times: in 1963, 1964, 1965, 1966, 1967, 1972, 1973, 1974, 1976, 1979, 1984, 1986 (joint), 1988, 1991, 1999 (joint), 2007;
United States, 8 times: in 1977, 1981, 1986 (joint), 1994, 2015, 2016, 2018, 2019 (joint);
Hungary, 6 times: in 1961, 1962, 1969, 1970, 1971, 1975;
Romania, 5 times: in 1959, 1978, 1985, 1987, 1996;
West Germany, twice: in 1982 and 1983;
South Korea, twice: in 2012 and 2017;
Bulgaria, once: in 2003;
Iran, once: in 1998;
East Germany, once: in 1968.The following nations have achieved an all-members-gold IMO with a full team:

China, 14 times: in 1992, 1993, 1997, 2000, 2001, 2002, 2004, 2006, 2009, 2010, 2011, 2019, 2021 and 2022.
United States, 4 times: in 1994, 2011, 2016, and 2019.
South Korea, 3 times: in 2012, 2017, and 2019.
Russia, twice: in 2002 and 2008.
Bulgaria, once: in 2003.The only countries to have their entire team score perfectly in the IMO were the United States in 1994 (they were coached by Paul Zeitz), China in 2022, and Luxembourg, whose 1-member team had a perfect score in 1981. The US's success earned a mention in TIME Magazine. Hungary won IMO 1975 in an unorthodox way when none of the eight team members received a gold medal (five silver, three bronze). Second place team East Germany also did not have a single gold medal winner (four silver, four bronze).Several individuals have consistently scored highly and/or earned medals on the IMO: Zhuo Qun Song (Canada) is the most highly decorated participant with five gold medals (including one perfect score in 2015) and one bronze medal. Reid Barton (United States) was the first participant to win a gold medal four times (1998–2001). Barton is also one of only eight four-time Putnam Fellows (2001–04). Christian Reiher (Germany), Lisa Sauermann (Germany), Teodor von Burg (Serbia), Nipun Pitimanaaree (Thailand) and Luke Robitaille (United States) are the only other participants to have won four gold medals (2000–03, 2008–11, 2009–12, 2010–13, 2011–14, and 2019–22 respectively); Reiher also received a bronze medal (1999), Sauermann a silver medal (2007), von Burg a silver medal (2008) and a bronze medal (2007), and Pitimanaaree a silver medal (2009). Wolfgang Burmeister (East Germany), Martin Härterich (West Germany), Iurie Boreico (Moldova), and Lim Jeck (Singapore) are the only other participants besides Reiher, Sauermann, von Burg, and Pitimanaaree to win five medals with at least three of them gold. Ciprian Manolescu (Romania) managed to write a perfect paper (42 points) for gold medal more times than anybody else in the history of the competition, doing it all three times he participated in the IMO (1995, 1996, 1997). Manolescu is also a three-time Putnam Fellow (1997, 1998, 2000). Eugenia Malinnikova (Soviet Union) is the highest-scoring female contestant in IMO history. She has 3 gold medals in IMO 1989 (41 points), IMO 1990 (42) and IMO 1991 (42), missing only 1 point in 1989 to precede Manolescu's achievement.Terence Tao (Australia) participated in IMO 1986, 1987 and 1988, winning bronze, silver and gold medals respectively. He won a gold medal when he just turned thirteen in IMO 1988, becoming the youngest person to receive a gold medal (Zhuo Qun Song of Canada also won a gold medal at age 13, in 2011, though he was older than Tao). Tao also holds the distinction of being the youngest medalist with his 1986 bronze medal, followed by 2009 bronze medalist Raúl Chávez Sarmiento (Peru), at the age of 10 and 11 respectively. Representing the United States, Noam Elkies won a gold medal with a perfect paper at the age of 14 in 1981. Both Elkies and Tao could have participated in the IMO multiple times following their success, but entered university and therefore became ineligible.


== Medals (1959–2022) ==
The current ten countries with the best all-time results are as follows:


== Gender gap and the launch of EGMO ==
Over the years, since its inception to present, the IMO has attracted far more male contestants than female contestants. During the period 2000–2021, there were only 1,102 female contestants (9.2%) out of a total of 11,950 contestants. The gap is even more significant in terms of IMO gold medallists; from 1959 to 2021, there were 43 female and 1295 male gold medal winners.This gender gap in participation and in performance at the IMO level led to the establishment of the European Girls' Mathematical Olympiad (EGMO).


== Media coverage ==
A documentary, ""Hard Problems: The Road To The World's Toughest Math Contest"" was made about the United States 2006 IMO team.
A BBC documentary titled Beautiful Young Minds aired July 2007 about the IMO.
A BBC fictional film titled X+Y released in September 2014 tells the story of an autistic boy who took part in the Olympiad.
A book named Countdown by Steve Olson tells the story of the United States team's success in the 2001 Olympiad.


== See also ==
List of International Mathematical Olympiads
International Mathematics Competition for University Students (IMC)
International Science Olympiad
List of mathematics competitions
Pan-African Mathematics Olympiads
Junior Science Talent Search Examination
Art of Problem Solving


== Notes ==


== Citations ==


== References ==
Xu, Jiagu (2012). Lecture Notes on Mathematical Olympiad Courses, For Senior Section. World Scientific Publishing. ISBN 978-981-4368-94-0.Xiong, Bin; Lee, Peng Yee (2013). Mathematical Olympiad in China (2009-2010). World Scientific Publishing. ISBN 978-981-4390-21-7.Xu, Jiagu (2009). Lecture Notes on Mathematical Olympiad Courses, For Junior Section. World Scientific Publishing. ISBN 978-981-4293-53-2.Olson, Steve (2004). Count Down. Houghton Mifflin. ISBN 0-618-25141-3.
Verhoeff, Tom (August 2002). ""The 43rd International Mathematical Olympiad: A Reflective Report on IMO 2002"" (PDF). Computing Science Report, Faculty of Mathematics and Computing Science, Eindhoven University of Technology, Vol. 2, No. 11. {{cite journal}}: Cite journal requires |journal= (help)
Djukić, Dušan (2006). The IMO Compendium: A Collection of Problems Suggested for the International Olympiads, 1959–2004. Springer. ISBN 978-0-387-24299-6.
Lord, Mary (23 July 2001). ""Michael Jordans of math - U.S. Student whizzes stun the cipher world"". U.S. News & World Report. 131 (3): 26.
Saul, Mark (2003). ""Mathematics in a Small Place: Notes on the Mathematics of Romania and Bulgaria"" (PDF). Notices of the American Mathematical Society. 50: 561–565.
Vakil, Ravi (1997). A Mathematical Mosaic: Patterns & Problem Solving. Brendan Kelly Publishing. p. 288. ISBN 978-1-895997-28-6.
Liu, Andy (1998). Chinese Mathematics Competitions and Olympiads. AMT Publishing. ISBN 1-876420-00-6.


== External links ==
Official IMO web site
Archive to the IMO 1959–2003 problems and solutions
Old central IMO web site"
ac058caa52,Constructivism (philosophy of mathematics),"In the philosophy of mathematics, constructivism asserts that it is necessary to find (or ""construct"") a specific example of a mathematical object in order to prove that an example exists. Contrastingly, in classical mathematics, one can prove the existence of a mathematical object without ""finding"" that object explicitly, by assuming its non-existence and then deriving a contradiction from that assumption. Such a proof by contradiction might be called non-constructive, and a constructivist might reject it. The constructive viewpoint involves a verificational interpretation of the existential quantifier, which is at odds with its classical interpretation.
There are many forms of constructivism. These include the program of intuitionism founded by Brouwer, the finitism of Hilbert and Bernays, the constructive recursive mathematics of Shanin and Markov, and Bishop's program of constructive analysis. Constructivism also includes the study of constructive set theories such as CZF and the study of topos theory.
Constructivism is often identified with intuitionism, although intuitionism is only one constructivist program. Intuitionism maintains that the foundations of mathematics lie in the individual mathematician's intuition, thereby making mathematics into an intrinsically subjective activity. Other forms of constructivism are not based on this viewpoint of intuition, and are compatible with an objective viewpoint on mathematics.


== Constructive mathematics ==
Much constructive mathematics uses intuitionistic logic, which is essentially classical logic without the law of the excluded middle. This law states that, for any proposition, either that proposition is true or its negation is. This is not to say that the law of the excluded middle is denied entirely; special cases of the law will be provable. It is just that the general law is not assumed as an axiom. The law of non-contradiction (which states that contradictory statements cannot both at the same time be true) is still valid.
For instance, in Heyting arithmetic, one can prove that for any proposition p that does not contain quantifiers, 
  
    
      
        ∀
        x
        ,
        y
        ,
        z
        ,
        …
        ∈
        
          N
        
        :
        p
        ∨
        ¬
        p
      
    
    {\displaystyle \forall x,y,z,\ldots \in \mathbb {N} :p\vee \neg p}
   is a theorem (where x, y, z ... are the free variables in the proposition p). In this sense, propositions restricted to the finite are still regarded as being either true or false, as they are in classical mathematics, but this bivalence does not extend to propositions that refer to infinite collections.
In fact, L.E.J. Brouwer, founder of the intuitionist school, viewed the law of the excluded middle as abstracted from finite experience, and then applied to the infinite without justification. For instance, Goldbach's conjecture is the assertion that every even number (greater than 2) is the sum of two prime numbers. It is possible to test for any particular even number whether or not it is the sum of two primes (for instance by exhaustive search), so any one of them is either the sum of two primes or it is not. And so far, every one thus tested has in fact been the sum of two primes.
But there is no known proof that all of them are so, nor any known proof that not all of them are so; nor is it even known whether either a proof or a disproof of Goldbach's conjecture must exist (the conjecture may be undecidable in traditional ZF set theory). Thus to Brouwer, we are not justified in asserting ""either Goldbach's conjecture is true, or it is not."" And while the conjecture may one day be solved, the argument applies to similar unsolved problems; to Brouwer, the law of the excluded middle was tantamount to assuming that every mathematical problem has a solution.
With the omission of the law of the excluded middle as an axiom, the remaining logical system has an existence property that classical logic does not have: whenever 
  
    
      
        
          ∃
          
            x
            ∈
            X
          
        
        P
        (
        x
        )
      
    
    {\displaystyle \exists _{x\in X}P(x)}
   is proven constructively, then in fact 
  
    
      
        P
        (
        a
        )
      
    
    {\displaystyle P(a)}
   is proven constructively for (at least) one particular 
  
    
      
        a
        ∈
        X
      
    
    {\displaystyle a\in X}
  , often called a witness. Thus the proof of the existence of a mathematical object is tied to the possibility of its construction.


=== Example from real analysis ===
In classical real analysis, one way to define a real number is as an equivalence class of Cauchy sequences of rational numbers.
In constructive mathematics, one way to construct a real number is as a function ƒ that takes a positive integer 
  
    
      
        n
      
    
    {\displaystyle n}
   and outputs a rational ƒ(n), together with a function g that takes a positive integer n and outputs a positive integer g(n) such that

  
    
      
        ∀
        n
         
        ∀
        i
        ,
        j
        ≥
        g
        (
        n
        )
        
        
          |
        
        f
        (
        i
        )
        −
        f
        (
        j
        )
        
          |
        
        ≤
        
          
            1
            n
          
        
      
    
    {\displaystyle \forall n\ \forall i,j\geq g(n)\quad |f(i)-f(j)|\leq {1 \over n}}
  so that as n increases, the values of ƒ(n) get closer and closer together. We can use ƒ and g together to compute as close a rational approximation as we like to the real number they represent.
Under this definition, a simple representation of the real number e is:

  
    
      
        f
        (
        n
        )
        =
        
          ∑
          
            i
            =
            0
          
          
            n
          
        
        
          
            1
            
              i
              !
            
          
        
        ,
        
        g
        (
        n
        )
        =
        n
        .
      
    
    {\displaystyle f(n)=\sum _{i=0}^{n}{1 \over i!},\quad g(n)=n.}
  This definition corresponds to the classical definition using Cauchy sequences, except with a constructive twist: for a classical Cauchy sequence, it is required that, for any given distance, there exists (in a classical sense) a member in the sequence after which all members are closer together than that distance. In the constructive version, it is required that, for any given distance, it is possible to actually specify a point in the sequence where this happens (this required specification is often called the modulus of convergence). In fact, the standard constructive interpretation of the mathematical statement

  
    
      
        ∀
        n
        :
        ∃
        m
        :
        ∀
        i
        ,
        j
        ≥
        m
        :
        
          |
        
        f
        (
        i
        )
        −
        f
        (
        j
        )
        
          |
        
        ≤
        
          
            1
            n
          
        
      
    
    {\displaystyle \forall n:\exists m:\forall i,j\geq m:|f(i)-f(j)|\leq {1 \over n}}
  is precisely the existence of the function computing the modulus of convergence. Thus the difference between the two definitions of real numbers can be thought of as the difference in the interpretation of the statement ""for all... there exists...""
This then opens the question as to what sort of function from a countable set to a countable set, such as f and g above, can actually be constructed. Different versions of constructivism diverge on this point. Constructions can be defined as broadly as free choice sequences, which is the intuitionistic view, or as narrowly as algorithms (or more technically, the computable functions), or even left unspecified. If, for instance, the algorithmic view is taken, then the reals as constructed here are essentially what classically would be called the computable numbers.


=== Cardinality ===
To take the algorithmic interpretation above would seem at odds with classical notions of cardinality. By enumerating algorithms, we can show classically that the computable numbers are countable. And yet Cantor's diagonal argument shows that real numbers have higher cardinality. Furthermore, the diagonal argument seems perfectly constructive. To identify the real numbers with the computable numbers would then be a contradiction.
And in fact, Cantor's diagonal argument is constructive, in the sense that given a bijection between the real numbers and natural numbers, one constructs a real number that doesn't fit, and thereby proves a contradiction. We can indeed enumerate algorithms to construct a function T, about which we initially assume that it is a function from the natural numbers onto the reals. But, to each algorithm, there may or may not correspond a real number, as the algorithm may fail to satisfy the constraints, or even be non-terminating (T is a partial function), so this fails to produce the required bijection. In short, one who takes the view that real numbers are (individually) effectively computable interprets Cantor's result as showing that the real numbers (collectively) are not recursively enumerable.
Still, one might expect that since T is a partial function from the natural numbers onto the real numbers, that therefore the real numbers are no more than countable. And, since every natural number can be trivially represented as a real number, therefore the real numbers are no less than countable. They are, therefore exactly countable. However this reasoning is not constructive, as it still does not construct the required bijection. The classical theorem proving the existence of a bijection in such circumstances, namely the Cantor–Bernstein–Schroeder theorem, is non-constructive. It has recently been shown that the Cantor–Bernstein–Schroeder theorem implies the law of the excluded middle, hence there can be no constructive proof of the theorem.


=== Axiom of choice ===
The status of the axiom of choice in constructive mathematics is complicated by the different approaches of different constructivist programs. One trivial meaning of ""constructive"", used informally by mathematicians, is ""provable in ZF set theory without the axiom of choice."" However, proponents of more limited forms of constructive mathematics would assert that ZF itself is not a constructive system.
In intuitionistic theories of type theory (especially higher-type arithmetic), many forms of the axiom of choice are permitted. For example, the axiom AC11 can be paraphrased to say that for any relation R on the set of real numbers, if you have proved that for each real number x there is a real number y such that R(x,y) holds, then there is actually a function F such that R(x,F(x)) holds for all real numbers. Similar choice principles are accepted for all finite types. The motivation for accepting these seemingly nonconstructive principles is the intuitionistic understanding of the proof that ""for each real number x there is a real number y such that R(x,y) holds"". According to the BHK interpretation, this proof itself is essentially the function F that is desired. The choice principles that intuitionists accept do not imply the law of the excluded middle.
However, in certain axiom systems for constructive set theory, the axiom of choice does imply the law of the excluded middle (in the presence of other axioms), as shown by the Diaconescu-Goodman-Myhill theorem. Some constructive set theories include weaker forms of the axiom of choice, such as the axiom of dependent choice in Myhill's set theory.


=== Measure theory ===
Classical measure theory is fundamentally non-constructive, since the classical definition of Lebesgue measure does not describe any way how to compute the measure of a set or the integral of a function. In fact, if one thinks of a function just as a rule that ""inputs a real number and outputs a real number"" then there cannot be any algorithm to compute the integral of a function, since any algorithm would only be able to call finitely many values of the function at a time, and finitely many values are not enough to compute the integral to any nontrivial accuracy. The solution to this conundrum, carried out first in Bishop (1967), is to consider only functions that are written as the pointwise limit of continuous functions (with known modulus of continuity), with information about the rate of convergence. An advantage of constructivizing measure theory is that if one can prove that a set is constructively of full measure, then there is an algorithm for finding a point in that set (again see Bishop (1967)). For example, this approach can be used to construct a real number that is normal to every base.


== The place of constructivism in mathematics ==
Traditionally, some mathematicians have been suspicious, if not antagonistic, towards mathematical constructivism, largely because of limitations they believed it to pose for constructive analysis.
These views were forcefully expressed by David Hilbert in 1928, when he wrote in Grundlagen der Mathematik, ""Taking the principle of excluded middle from the mathematician would be the same, say, as proscribing the telescope to the astronomer or to the boxer the use of his fists"".Errett Bishop, in his 1967 work Foundations of Constructive Analysis, worked to dispel these fears by developing a great deal of traditional analysis in a constructive framework.
Even though most mathematicians do not accept the constructivist's thesis that only mathematics done based on constructive methods is sound, constructive methods are increasingly of interest on non-ideological grounds. For example, constructive proofs in analysis may ensure witness extraction, in such a way that working within the constraints of the constructive methods may make finding witnesses to theories easier than using classical methods. Applications for constructive mathematics have also been found in typed lambda calculi, topos theory and categorical logic, which are notable subjects in foundational mathematics and computer science. In algebra, for such entities as topoi and Hopf algebras, the structure supports an internal language that is a constructive theory; working within the constraints of that language is often more intuitive and flexible than working externally by such means as reasoning about the set of possible concrete algebras and their homomorphisms.
Physicist Lee Smolin writes in Three Roads to Quantum Gravity that topos theory is ""the right form of logic for cosmology"" (page 30) and ""In its first forms it was called 'intuitionistic logic'"" (page 31). ""In this kind of logic, the statements an observer can make about the universe are divided into at least three groups: those that we can judge to be true, those that we can judge to be false and those whose truth we cannot decide upon at the present time"" (page 28).


== Mathematicians who have made major contributions to constructivism ==
Leopold Kronecker (old constructivism, semi-intuitionism)
L. E. J. Brouwer (founder of intuitionism)
A. A. Markov (forefather of Russian school of constructivism)
Arend Heyting (formalized intuitionistic logic and theories)
Per Martin-Löf (founder of constructive type theories)
Errett Bishop (promoted a version of constructivism claimed to be consistent with classical mathematics)
Paul Lorenzen (developed constructive analysis)


== Branches ==
Constructive logic
Constructive type theory
Constructive analysis
Constructive non-standard analysis


== See also ==
Computability theory – Study of computable functions and Turing degrees
Constructive proof – Method of proof in mathematics
Finitism – Philosophy of mathematics that accepts the existence only of finite mathematical objects
Game semantics – approach to formal semanticsPages displaying wikidata descriptions as a fallback
Inhabited set – Kind of set in constructive mathematics
Intuitionism – Approach in philosophy of mathematics and logic
Intuitionistic type theory – Alternative foundation of mathematics


== Notes ==


== References ==


== External links ==
""Constructive Mathematics"". Internet Encyclopedia of Philosophy.
Stanford Encyclopedia of Philosophy entry"
